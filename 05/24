{"id":"2305.14181","submitter":"Paolo Antonelli","authors":"Paolo Antonelli and Boris Shakarov","title":"Existence and Large Time Behavior for a Dissipative Variant of the\n  Rotational NLS Equation","comments":"32 pages, comments welcome","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP math-ph math.MP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We study a dissipative variant of the Gross-Pitaevskii equation with\nrotation. The model contains a nonlocal, nonlinear term that forces the\nconservation of $L^2$-norm of solutions. We are motivated by several physical\nexperiments and numerical simulations studying the formation of vortices in\nBose-Einstein condensates. We show local and global well-posedness of this\nmodel and investigate the asymptotic behavior of its solutions. In the linear\ncase, the solution asymptotically tends to the eigenspace associated with the\nsmallest eigenvalue in the decomposition of the initial datum. In the nonlinear\ncase, we obtain weak convergence to a stationary state. Moreover, for initial\nenergies in a specific range, we prove strong asymptotic stability of ground\nstate solutions.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:00:17 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14182","submitter":"Martenq Kaas","authors":"Marten H. L. Kaas, Zoe Porter, Ernest Lim, Aisling Higham, Sarah\n  Khavandi and Ibrahim Habli","title":"Ethics in conversation: Building an ethics assurance case for autonomous\n  AI-enabled voice agents in healthcare","comments":"19 pages, 3 figures, 1 table, pre-print of paper for Trustworthy\n  Autonomous Systems conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The deployment and use of AI systems should be both safe and broadly\nethically acceptable. The principles-based ethics assurance argument pattern is\none proposal in the AI ethics landscape that seeks to support and achieve that\naim. The purpose of this argument pattern or framework is to structure\nreasoning about, and to communicate and foster confidence in, the ethical\nacceptability of uses of specific real-world AI systems in complex\nsocio-technical contexts. This paper presents the interim findings of a case\nstudy applying this ethics assurance framework to the use of Dora, an AI-based\ntelemedicine system, to assess its viability and usefulness as an approach. The\ncase study process to date has revealed some of the positive ethical impacts of\nthe Dora platform, as well as unexpected insights and areas to prioritise for\nevaluation, such as risks to the frontline clinician, particularly in respect\nof clinician autonomy. The ethics assurance argument pattern offers a practical\nframework not just for identifying issues to be addressed, but also to start to\nconstruct solutions in the form of adjustments to the distribution of benefits,\nrisks and constraints on human autonomy that could reduce ethical disparities\nacross affected stakeholders. Though many challenges remain, this research\nrepresents a step in the direction towards the development and use of safe and\nethically acceptable AI systems and, ideally, a shift towards more\ncomprehensive and inclusive evaluations of AI systems in general.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:04:59 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14183","submitter":"Micha{\\l} Kijaczko","authors":"Micha{\\l} Kijaczko","title":"Asymptotics of weighted Gagliardo seminorms","comments":"12 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper we consider fractional Sobolev spaces equipped with weights\nbeing powers of the distance to the boundary of the domain. We prove the\nversions of Bourgain--Brezis--Mironescu and Maz'ya--Shaposhnikova asymptotic\nformulae for weighted fractional Gagliardo seminorms. For $p>1$ we also provide\na nonlocal characterization of classical weighted Sobolev spaces with power\nweights.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:06:05 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14184","submitter":"Han Cai","authors":"Han Cai, Jay C. LeFebvre, Hao Li, Ethan Y. Cho, Nobuyuki Yoshikawa and\n  Shane A. Cybart","title":"High-Temperature Superconductor Quantum Flux Parametron for\n  Energy-Efficient Logic","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.app-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  As we rapidly advance through the information age, the power consumed by\ncomputers, data centers, and networks grows exponentially. This has inspired a\nrace to develop alternative low-power computational technologies. A new\nadiabatic configuration of a decades-old superconducting digital logic device\nhas darted into the lead called quantum flux parametrons (QFP). QFP operate\nwith dissipation so low that they seemingly violate the laws of thermodynamics.\nIn just a short span of time, they have gone from simple single NOT gates to\ncomplex processors containing thousands of gates. They are fabricated from\nelemental niobium superconductors cooled to just a few degrees above absolute\nzero. However, their efficiency is so great that for large high-performance\ncomputers with several gates, the energy savings are immense. For smaller\ncomputational platforms QFPs from high-temperature superconductors (high-Tc)\nare highly desirable. In this work, we take the first steps towards this goal\nwith the demonstration of a high-T C QFP shift register. Our device is\nfabricated using focused helium ion beam lithography where the material is\nmodified with an ion beam at the nanoscale to directly pattern these circuits\ninto a high-T C thin film. We validate the correct logical operation at 25 K,\nover 6 times higher than niobium devices with an estimated bit energy of 0.1\nattoJoule at 10 GHz.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:06:37 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14185","submitter":"Fabio Chalub","authors":"Fabio A.C.C. Chalub, Antonio G\\'omez-Corral, Mart\\'in\n  L\\'opez-Garc\\'ia, and F\\'atima Palacios-Rodr\\'iguez","title":"A Markov chain model to investigate the spread of antibiotic-resistant\n  bacteria in hospitals","comments":"22 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"q-bio.PE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  This paper proposes a Markov chain model to describe the spread of a single\nbacterial species in a hospital ward where patients may be free of bacteria or\nmay carry bacterial strains that are either sensitive or resistant to\nantimicrobial agents. The aim is to determine the probability law of the exact\nreproduction number Rexact,0 which is here defined as the random number of\nsecondary infections generated by those patients who are accommodated in a\npredetermined bed before a patient who is free of bacteria is accommodated in\nthis bed for the first time. Specifically, we decompose the exact reproduction\nnumber Rexact,0 into two contributions allowing us to distinguish between\ninfections due to the sensitive and the resistant bacterial strains. Our\nmethodology is mainly based on structured Markov chains and the use of related\nmatrix-analytic methods.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:06:56 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14186","submitter":"Jessica Page","authors":"Jessica Page, Tyson Littenberg","title":"Bayesian Time Delay Interferometry for Orbiting LISA: Accounting for the\n  Time Dependence of Spacecraft Separations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Previous work demonstrated effective laser frequency noise (LFN) suppression\nfor Laser Interferometer Space Antenna (LISA) data from raw phasemeter\nmeasurements using a Markov Chain Monte Carlo (MCMC) algorithm with fractional\ndelay interpolation (FDI) techniques to estimate the spacecraft separation\nparameters required for time-delay interferometry (TDI) under the assumption of\na rigidly rotating LISA configuration. Including TDI parameters in the LISA\ndata model as part of a global fit analysis pipeline enables gravitational wave\ninferences to be marginalized over uncertainty in the spacecraft separations.\nHere we extend the algorithm's capability to perform data-driven TDI on LISA in\nKeplerian orbits, which introduce a time-dependence in the arm-length\nparameters and at least $\\mathcal{O}$(M) times greater computational cost since\nthe filter must be applied for every sample in the time series of sample size\nM. We find feasibility of arm-length estimation on $\\sim$day-long time scales\nby using a novel Taylor-expanded version of the fractional delay interpolation\nfilter that allows half of the filter computation to be calculated and stored\nbefore MCMC iterations and requires shorter filter lengths than previously\nreported. We demonstrate LFN suppression for orbiting LISA using accurate\narm-length estimates parameterized by Keplerian orbital parameters under the\nassumption of unperturbed analytical Keplerian orbits, and explore the\npotential extension of these methods to arbitrary numerical orbits.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:07:30 GMT"},{"version":"v2","created":"Fri, 26 May 2023 15:00:11 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.14187","submitter":"Steven Tomsovic","authors":"Steven Tomsovic, Juan Diego Urbina, and Klaus Richter","title":"Controlling quantum chaos: time-dependent kicked rotor","comments":"12 pages, 8 figures, 1 table","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.quant-gas nlin.CD","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  One major objective of controlling classical chaotic dynamical systems is\nexploiting the system's extreme sensitivity to initial conditions in order to\narrive at a predetermined target state. In a recent letter [Phys.~Rev.~Lett.\n130, 020201 (2023)], a generalization of this targeting method to quantum\nsystems was demonstrated using successive unitary transformations that counter\nthe natural spreading of a quantum state. In this paper further details are\ngiven and an important quite general extension is established. In particular,\nan alternate approach to constructing the coherent control dynamics is given,\nwhich introduces a new time-dependent, locally stable control Hamiltonian that\ncontinues to use the chaotic heteroclinic orbits previously introduced, but\nwithout the need of countering quantum state spreading. Implementing that\nextension for the quantum kicked rotor generates a much simpler approximate\ncontrol technique than discussed in the letter, which is a little less\naccurate, but far more easily realizable in experiments. The simpler method's\nerror can still be made to vanish as $\\hbar \\rightarrow 0$.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:07:34 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14188","submitter":"Iuri Frosio","authors":"Iuri Frosio and Jan Kautz","title":"The Best Defense is a Good Offense: Adversarial Augmentation against\n  Adversarial Attacks","comments":null,"journal-ref":"CVPR 2023","doi":null,"report-no":null,"categories":"cs.LG cs.CR cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Many defenses against adversarial attacks (\\eg robust classifiers,\nrandomization, or image purification) use countermeasures put to work only\nafter the attack has been crafted. We adopt a different perspective to\nintroduce $A^5$ (Adversarial Augmentation Against Adversarial Attacks), a novel\nframework including the first certified preemptive defense against adversarial\nattacks. The main idea is to craft a defensive perturbation to guarantee that\nany attack (up to a given magnitude) towards the input in hand will fail. To\nthis aim, we leverage existing automatic perturbation analysis tools for neural\nnetworks. We study the conditions to apply $A^5$ effectively, analyze the\nimportance of the robustness of the to-be-defended classifier, and inspect the\nappearance of the robustified images. We show effective on-the-fly defensive\naugmentation with a robustifier network that ignores the ground truth label,\nand demonstrate the benefits of robustifier and classifier co-training. In our\ntests, $A^5$ consistently beats state of the art certified defenses on MNIST,\nCIFAR10, FashionMNIST and Tinyimagenet. We also show how to apply $A^5$ to\ncreate certifiably robust physical objects. Our code at\nhttps://github.com/NVlabs/A5 allows experimenting on a wide range of scenarios\nbeyond the man-in-the-middle attack tested here, including the case of physical\nattacks.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:07:58 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14189","submitter":"Di Wu","authors":"Di Wu and Christof Monz","title":"Beyond Shared Vocabulary: Increasing Representational Word Similarities\n  across Languages for Multilingual Machine Translation","comments":"11 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Using a shared vocabulary is common practice in Multilingual Neural Machine\nTranslation (MNMT). In addition to its simple design, shared tokens play an\nimportant role in positive knowledge transfer, which manifests naturally when\nthe shared tokens refer to similar meanings across languages. However, natural\nflaws exist in such a design as well: 1) when languages use different writing\nsystems, transfer is inhibited, and 2) even if languages use similar writing\nsystems, shared tokens may have completely different meanings in different\nlanguages, increasing ambiguity. In this paper, we propose a re-parameterized\nmethod for building embeddings to alleviate the first problem. More\nspecifically, we define word-level information transfer pathways via word\nequivalence classes and rely on graph networks to fuse word embeddings across\nlanguages. Our experiments demonstrate the advantages of our approach: 1) the\nsemantics of embeddings are better aligned across languages, 2) our method\nachieves significant BLEU improvements on high- and low-resource MNMT, and 3)\nonly less than 1.0\\% additional trainable parameters are required with a\nlimited increase in computational costs.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:11:00 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14190","submitter":"Alireza Ahmadianyazdi","authors":"Alireza Ahmadianyazdi, Isaac J. Miller, Albert Folch","title":"Tunable Resins with PDMS-like Elastic Modulus for Stereolithographic\n  3D-printing of Multimaterial Microfluidic Actuators","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci physics.chem-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Stereolithographic 3D-printing (SLA) permits facile fabrication of\nhigh-precision microfluidic and lab-on-a-chip devices. SLA photopolymers often\nyield parts with low mechanical compliancy in sharp contrast to elastomers such\nas poly (dimethyl siloxane) (PDMS). On the other hand, SLA-printable elastomers\nwith soft mechanical properties do not fulfill the distinct requirements for a\nhighly manufacturable resin in microfluidics (e.g., high-resolution\nprintability, transparency, low-viscosity). These limitations restrict our\nability to SLA-print efficient microfluidic actuators containing dynamic,\nmovable elements. Here we introduce low-viscous photopolymer resins based on a\ntunable blend of poly(ethylene glycol) diacrylate (PEGDA, Mw~258) and poly\n(ethylene glycol methyl ether) methacrylate (PEGMEMA, Mw~300) monomers. In\nthese blends, which we term PEGDA-co-PEGMEMA, tuning the PEGMEMA-to-PEGDA ratio\nalters the elastic modulus of the printed plastics by ~400-fold, reaching that\nof PDMS. Through the addition of PEGMEMA, moreover, PEGDA-co-PEGMEMA retains\ndesirable properties of highly manufacturable PEGDA such as low viscosity,\nsolvent compatibility, cytocompatibility and low drug absorptivity. With\nPEGDA-co-PEGMEMA, we SLA-printed drastically enhanced fluidic actuators\nincluding microvalves, micropumps, and microregulators with a hybrid structure\ncontaining a flexible PEGDA-co-PEGMEMA membrane within a rigid PEGDA housing.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:11:49 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14191","submitter":"G\\'abor Heged\\\"us Dr","authors":"G\\'abor Heged\\\"us","title":"Non-uniform skew versions of Bollob\\'as' Theorem","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.CO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Let $A_1, \\ldots ,A_m$ and $B_1, \\ldots ,B_m$ be subsets of $[n]$ and let $t$\nbe a non-negative integer with the following property: $|A_i \\cap B_i|\\leq t$\nfor each $i$ and $|A_i\\cap B_j|>t$ whenever $i< j$. Then $m\\leq 2^{n-t}$. Our\nproof uses Lov\\'asz' tensor product method.\n  We prove the following skew version of Bollob\\'as' Theorem. Let $A_1, \\ldots\n,A_m$ and $B_1, \\ldots ,B_m$ be finite sets of $[n]$ satisfying the conditions\n$A_i \\cap B_i =\\emptyset$ for each $i$ and $A_i\\cap B_j\\ne \\emptyset$ for each\n$i< j$. Then $$ \\sum_{i=1}^m \\frac{1}{{|A_i|+|B_i| \\choose |A_i|}}\\leq n+1. $$\nBoth upper bounds are sharp.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:11:57 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14192","submitter":"Vladimir Georgiev","authors":"Daniele Barbera and Vladimir Georgiev","title":"Local and global solutions on arcs for the Ericksen -- Leslie problem in\n  the whole space","comments":"40 p","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The work deals with the Ericksen-Leslie System for nematic liquid crystals on\nthe whole space. In our work we suppose the initial condition of the\norientation field stays on an arc connecting two fixed orthogonal vectors on\nthe unit sphere. Thanks to this geometric assumption, we prove through energy a\npriori estimates the local existence and the global existence for small initial\ndata of a solution in low regularity Sobolev spaces.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:12:12 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14193","submitter":"Weite Pi","authors":"Woonam Lim, Miguel Moreira, Weite Pi","title":"Cohomological $\\chi$-dependence of ring structure for the moduli of\n  one-dimensional sheaves on $\\mathbb{P}^2$","comments":"19 pages, comments are welcome!","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We prove that the cohomology rings of the moduli space $M_{d,\\chi}$ of\none-dimensional sheaves on the projective plane are not isomorphic for general\ndifferent choices of the Euler characteristics. This stands in contrast to the\n$\\chi$-independence of the Betti numbers of these moduli spaces. As a\ncorollary, we deduce that $M_{d,\\chi}$ are topologically different unless they\nare related by obvious symmetries, strengthening a previous result of Woolf\ndistinguishing them as algebraic varieties.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:12:33 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14194","submitter":"Heejun Shin","authors":"Heejun Shin, Danielle Braun, Kezia Irene, Joseph Antonelli","title":"A spatial interference approach to account for mobility in air pollution\n  studies with multivariate continuous treatments","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We develop new methodology to improve our understanding of the causal effects\nof multivariate air pollution exposures on public health. Typically, exposure\nto air pollution for an individual is measured at their home geographic region,\nthough people travel to different regions with potentially different levels of\nair pollution. To account for this, we incorporate estimates of the mobility of\nindividuals from cell phone mobility data to get an improved estimate of their\nexposure to air pollution. We treat this as an interference problem, where\nindividuals in one geographic region can be affected by exposures in other\nregions due to mobility into those areas. We propose policy-relevant estimands\nand derive expressions showing the extent of bias one would obtain by ignoring\nthis mobility. We additionally highlight the benefits of the proposed\ninterference framework relative to a measurement error framework for accounting\nfor mobility. We develop novel estimation strategies to estimate causal effects\nthat account for this spatial spillover utilizing flexible Bayesian\nmethodology. Empirically we find that this leads to improved estimation of the\ncausal effects of air pollution exposures over analyses that ignore spatial\nspillover caused by mobility.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:13:41 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14195","submitter":"Anthony Sicilia","authors":"Anthony Sicilia, Jennifer C. Gates, and Malihe Alikhani","title":"How Old is GPT?: The HumBEL Framework for Evaluating Language Models\n  using Human Demographic Data","comments":"17 pages, 10 figures, 5 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  While large pre-trained language models (LMs) find greater use across NLP,\nexisting evaluation protocols do not consider how LM language use aligns with\nparticular human demographic groups, which can be an important consideration in\nconversational AI applications. To remedy this gap, we consider how LM language\nskills can be measured and compared to human sub-populations. We suggest\nclinical techniques from Speech Language Pathology, which has well-established\nnorms for acquisition of language skills, organized by (human) age. We conduct\nevaluation with a domain expert (i.e., a clinically licensed speech language\npathologist), and also propose automated techniques to substitute clinical\nevaluation at scale. We find LM capability varies widely depending on task with\nGPT-3.5 mimicking the ability of a typical 6-9 year old at tasks requiring\ninference about word meanings and simultaneously outperforming a typical 21\nyear old at memorization. GPT-3.5 (InstructGPT) also has trouble with social\nlanguage use, exhibiting less than 50\\% of the tested pragmatic skills. It\nshows errors in understanding particular word parts-of-speech and associative\nword relations, among other lexical features. Ultimately, findings reiterate\nthe importance of considering demographic alignment and conversational goals\nwhen using these models as public-facing tools. Our framework will be publicly\navailable via code, data, and a python package.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:15:24 GMT"},{"version":"v2","created":"Wed, 24 May 2023 02:55:24 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14196","submitter":"Uri Shaham","authors":"Uri Shaham and Maor Ivgi and Avia Efrat and Jonathan Berant and Omer\n  Levy","title":"ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG stat.ML","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We introduce ZeroSCROLLS, a zero-shot benchmark for natural language\nunderstanding over long texts, which contains only test sets, without training\nor development data. We adapt six tasks from the SCROLLS benchmark, and add\nfour new datasets, including two novel information fusing tasks, such as\naggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a\ncomprehensive evaluation of both open-source and closed large language models,\nfinding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest\naverage score. However, there is still room for improvement on multiple open\nchallenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to\npass the naive baseline. As the state of the art is a moving target, we invite\nresearchers to evaluate their ideas on the live ZeroSCROLLS leaderboard\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:15:31 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14197","submitter":"Michael Perelshtein R.","authors":"M. R. Perelshtein, A. I. Pakhomchik, Ar. A. Melnikov, M. Podobrii, A.\n  Termanova, I. Kreidich, B. Nuriev, S. Iudin, C. W. Mansell, V. M. Vinokur","title":"NISQ-compatible approximate quantum algorithm for unconstrained and\n  constrained discrete optimization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph math.OC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Quantum algorithms are getting extremely popular due to their potential to\nsignificantly outperform classical algorithms. Yet, applying quantum algorithms\nto optimization problems meets challenges related to the efficiency of quantum\nalgorithms training, the shape of their cost landscape, the accuracy of their\noutput, and their ability to scale to large-size problems. Here, we present an\napproximate gradient-based quantum algorithm for hardware-efficient circuits\nwith amplitude encoding. We show how simple linear constraints can be directly\nincorporated into the circuit without additional modification of the objective\nfunction with penalty terms. We employ numerical simulations to test it on\nMaxCut problems with complete weighted graphs with thousands of nodes and run\nthe algorithm on a superconducting quantum processor. We find that for\nunconstrained MaxCut problems with more than 1000 nodes, the hybrid approach\ncombining our algorithm with a classical solver called CPLEX can find a better\nsolution than CPLEX alone. This demonstrates that hybrid optimization is one of\nthe leading use cases for modern quantum devices.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:17:57 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14198","submitter":"Cristian Andr\\'es Giuppone","authors":"Cristian Giuppone, Adri\\'an Rodr\\'iguez, Viviam Alencastro, Fernando\n  Roig, Tabar\\'e Gallardo","title":"Mapping the structure of the planetary 2:1 mean motion resonance. The\n  TOI-216, K2-24, and HD27894 systems","comments":"16 pages. 16 figures","journal-ref":null,"doi":"10.1007/s10569-022-10112-5","report-no":null,"categories":"astro-ph.EP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Mean motion resonances (MMR) are a frequent phenomenon among extrasolar\nplanetary systems. Current observations indicate that many systems have planets\nthat are close to or inside the 2:1 MMR, when the orbital period of one of the\nplanets is twice the other. Analytical models to describe this particular MMR\ncan only be reduced to integrable approximations in a few specific cases. While\nthere are successful approaches to the study of this MMR in the case of very\nelliptic and/or very inclined orbits using semi-analytical or semi-numerical\nmethods, these may not be enough to completely understand the resonant\ndynamics.\n  In this work, we propose to apply a well-established numerical method to\nassess the global portrait of the resonant dynamics, which consists in\nconstructing dynamical maps. Combining these maps with the results from a\nsemi-analytical method, helps to better understand the underlying dynamics of\nthe 2:1 MMR, and to identify the behaviors that can be expected in different\nregions of the phase space and for different values of the model parameters.\n  We verify that the family of stable resonant equilibria bifurcate from\nsymmetric to asymmetric librations, depending on the mass ratio and\neccentricities of the resonant planets pair. This introduces new structures in\nthe phase space, that turns the classical V-shape of the MMR, in the semi-major\naxis vs. eccentricity space, into a sand clock shape. We construct dynamical\nmaps for three extrasolar planetary systems, TOI-216, HD27894, and K2-24, and\ndiscuss their phase space structure and their stability in the light of the\norbital fits available in the literature.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:18:03 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14199","submitter":"Stanley Burris","authors":"Stanley Burris","title":"Boole's Chapter XV: Syllogism Details","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.LO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In Boole's famous 1854 book {\\em The Laws of Thought\\/} the mathematical\nanalysis of Aristotelian logic was relegated to Chapter XV, the last chapter\nbefore his treatment of probability theory. This chapter is Boole's tour de\nforce to show that he had a uniform method to obtain all valid syllogisms in\nhis version of Aristotelian logic, namely he applied {\\em reduction}, {\\em\nelimination} and {\\em solution} in that order to equational expressions for the\npremises. The premises of a syllogism were expressed as a pair of equations in\n8 variables, but then all algebraic steps between this and the final\nexpressions for $x$, $vx$ and $1-x$ were omitted. The somewhat tedious details\nof those missing steps are given in this note. It is assumed that the reader is\nfamiliar with Boole's reduction, elimination and solution theorems.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:18:55 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14200","submitter":"Sida I. Wang","authors":"Sida I. Wang","title":"Accessing Higher Dimensions for Unsupervised Word Translation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  The striking ability of unsupervised word translation has been demonstrated\nwith the help of word vectors / pretraining; however, they require large\namounts of data and usually fails if the data come from different domains. We\npropose coocmap, a method that can use either high-dimensional co-occurrence\ncounts or their lower-dimensional approximations. Freed from the limits of low\ndimensions, we show that relying on low-dimensional vectors and their\nincidental properties miss out on better denoising methods and useful world\nknowledge in high dimensions, thus stunting the potential of the data. Our\nresults show that unsupervised translation can be achieved more easily and\nrobustly than previously thought -- less than 80MB and minutes of CPU time is\nrequired to achieve over 50\\% accuracy for English to Finnish, Hungarian, and\nChinese translations when trained on similar data; even under domain mismatch,\nwe show coocmap still works fully unsupervised on English NewsCrawl to Chinese\nWikipedia and English Europarl to Spanish Wikipedia, among others. These\nresults challenge prevailing assumptions on the necessity and superiority of\nlow-dimensional vectors, and suggest that similarly processed co-occurrences\ncan outperform dense vectors on other tasks too.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:19:30 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14201","submitter":"Tiedong Liu","authors":"Tiedong Liu and Bryan Kian Hsiang Low","title":"Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We introduce Goat, a fine-tuned LLaMA model that significantly outperforms\nGPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated\ndataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic\nsub-task. In particular, the zero-shot Goat-7B matches or even surpasses the\naccuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve\nnear-perfect accuracy on large-number addition and subtraction through\nsupervised fine-tuning only, which is almost impossible with previous\npretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute\nGoat's exceptional performance to LLaMA's consistent tokenization of numbers.\nTo tackle more challenging tasks like large-number multiplication and division,\nwe propose an approach that classifies tasks based on their learnability, and\nsubsequently decomposes unlearnable tasks, such as multi-digit multiplication\nand division, into a series of learnable tasks by leveraging basic arithmetic\nprinciples. We thoroughly examine the performance of our model, offering a\ncomprehensive evaluation of the effectiveness of our proposed decomposition\nsteps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM\nGPU, facilitating reproducibility for other researchers. We release our model,\ndataset, and the Python script for dataset generation.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:20:30 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14202","submitter":"Silei Xu","authors":"Silei Xu, Theo Culhane, Meng-Hsi Wu, Sina J. Semnani, Monica S. Lam","title":"Complementing GPT-3 with Few-Shot Sequence-to-Sequence Semantic Parsing\n  over Wikidata","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  As the largest knowledge base, Wikidata is a massive source of knowledge,\ncomplementing large language models with well-structured data. In this paper,\nwe present WikiWebQuestions, a high-quality knowledge base question answering\nbenchmark for Wikidata. This new benchmark uses real-world human data with\nSPARQL annotation to facilitate a more accurate comparison with large language\nmodels utilizing the up-to-date answers from Wikidata. Additionally, a baseline\nfor this benchmark is established with an effective training data synthesis\nmethodology and WikiSP, a Seq2Seq semantic parser, that handles large noisy\nknowledge graphs. Experimental results illustrate the effectiveness of this\nmethodology, achieving 69% and 59% answer accuracy in the dev set and test set,\nrespectively. We showed that we can pair semantic parsers with GPT-3 to provide\na combination of verifiable results and qualified guesses that can provide\nuseful answers to 97% of the questions in the dev set of our benchmark.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:20:43 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14203","submitter":"Keitaro Tanaka","authors":"Sara Kashiwagi, Keitaro Tanaka, Qi Feng, Shigeo Morishima","title":"Improving the Gap in Visual Speech Recognition Between Normal and Silent\n  Speech Based on Metric Learning","comments":"Accepted by INTERSPEECH 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.AS cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper presents a novel metric learning approach to address the\nperformance gap between normal and silent speech in visual speech recognition\n(VSR). The difference in lip movements between the two poses a challenge for\nexisting VSR models, which exhibit degraded accuracy when applied to silent\nspeech. To solve this issue and tackle the scarcity of training data for silent\nspeech, we propose to leverage the shared literal content between normal and\nsilent speech and present a metric learning approach based on visemes.\nSpecifically, we aim to map the input of two speech types close to each other\nin a latent space if they have similar viseme representations. By minimizing\nthe Kullback-Leibler divergence of the predicted viseme probability\ndistributions between and within the two speech types, our model effectively\nlearns and predicts viseme identities. Our evaluation demonstrates that our\nmethod improves the accuracy of silent VSR, even when limited training data is\navailable.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:20:46 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14204","submitter":"Andrea Sipos","authors":"Andrea Sipos and Nima Fazeli","title":"MultiSCOPE: Disambiguating In-Hand Object Poses with Proprioception and\n  Tactile Feedback","comments":"Accepted to RSS 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper, we propose a method for estimating in-hand object poses using\nproprioception and tactile feedback from a bimanual robotic system. Our method\naddresses the problem of reducing pose uncertainty through a sequence of\nfrictional contact interactions between the grasped objects. As part of our\nmethod, we propose 1) a tool segmentation routine that facilitates contact\nlocation and object pose estimation, 2) a loss that allows reasoning over\nsolution consistency between interactions, and 3) a loss to promote converging\nto object poses and contact locations that explain the external force-torque\nexperienced by each arm. We demonstrate the efficacy of our method in a\ntask-based demonstration both in simulation and on a real-world bimanual\nplatform and show significant improvement in object pose estimation over single\ninteractions. Visit www.mmintlab.com/multiscope/ for code and videos.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:24:17 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14205","submitter":"Fantine Huot","authors":"Fantine Huot, Joshua Maynez, Chris Alberti, Reinald Kim Amplayo,\n  Priyanka Agrawal, Constanza Fierro, Shashi Narayan, Mirella Lapata","title":"$\\mu$PLAN: Summarizing using a Content Plan as Cross-Lingual Bridge","comments":"EMNLP 2023 Submission","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Cross-lingual summarization consists of generating a summary in one language\ngiven an input document in a different language, allowing for the dissemination\nof relevant content across speakers of other languages. However, this task\nremains challenging, mainly because of the need for cross-lingual datasets and\nthe compounded difficulty of summarizing and translating. This work presents\n$\\mu$PLAN, an approach to cross-lingual summarization that uses an intermediate\nplanning step as a cross-lingual bridge. We formulate the plan as a sequence of\nentities that captures the conceptualization of the summary, i.e. identifying\nthe salient content and expressing in which order to present the information,\nseparate from the surface form. Using a multilingual knowledge base, we align\nthe entities to their canonical designation across languages. $\\mu$PLAN models\nfirst learn to generate the plan and then continue generating the summary\nconditioned on the plan and the input. We evaluate our methodology on the\nXWikis dataset on cross-lingual pairs across four languages and demonstrate\nthat this planning objective achieves state-of-the-art performance in terms of\nROUGE and faithfulness scores. Moreover, this planning approach improves the\nzero-shot transfer to new cross-lingual language pairs compared to non-planning\nbaselines.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:25:21 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14206","submitter":"Nicol\\'as F. Barrera","authors":"Nicol\\'as F. Barrera, Patricio Fuentealba, Francisco Mu\\~noz, Tatiana\n  G\\'omez, Carlos C\\'ardenas","title":"Formation of $\\text{H}_{2}$ on polycyclic aromatic hydrocarbons under\n  conditions of the ISM: an ab initio molecular dynamics study","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.GA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Understanding how the $\\mathrm{H}_2$ molecule is formed under the chemical\nconditions of the interstellar media (ISM) is critical to the whole chemistry\nof it. Formation of $\\mathrm{H}_2$ in the ISM requires a third body acting as a\nreservoir of energy. Polycyclic aromatic hydrocarbons (PAH's) are excellent\ncandidates to play that role. In this work we simulated the collisions of\nhydrogen atoms with coronene to form $\\mathrm{H}_2$ via the Eley-Rideal\nmechanism. To do so, we used Born-Oppenheimer (ab initio) Molecular Dynamics\nsimulations. Our results show that that adsorption of H atoms and subsequent\nrelease of $\\mathrm{H}_2$ readily happen on coronene for H atoms with kinetic\nenergy as large as 1 eV. Special attention is paid to dissipation and partition\nof the energy released in the reactions. The capacity of coronene to dissipate\ncollision and reaction energies depends varies with the reaction site. Inner\nsites dissipate energy easier and faster than edge sites, thus evidencing an\ninterplay between the potential energy surface around the reaction center and\nits ability to cool the projectile. As for the the recombination of H atoms and\nthe subsequent formation of $\\mathrm{H}_{2}$, it is observed that $\\sim 15\\%$\nof the energy is dissipated by the coronene molecule as vibrational energy and\nthe remaining energy is carried by $\\mathrm{H}_{2}$. The $\\mathrm{H}_{2}$\nmolecules desorb from coronene with an excited vibrational state ($\\upsilon\n\\geq 3$), a large amount of translational kinetic energy ($\\geq$ 0.4 eV) and\nwith a small activation of the rotational degree of freedom.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:25:58 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14207","submitter":"Jun Cen","authors":"Jun Cen, Yizheng Wu, Kewei Wang, Xingyi Li, Jingkang Yang, Yixuan Pei,\n  Lingdong Kong, Ziwei Liu, Qifeng Chen","title":"SAD: Segment Any RGBD","comments":"Technical report of Segment Any RGBD. Project url:\n  https://github.com/Jun-CEN/SegmentAnyRGBD","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  The Segment Anything Model (SAM) has demonstrated its effectiveness in\nsegmenting any part of 2D RGB images. However, SAM exhibits a stronger emphasis\non texture information while paying less attention to geometry information when\nsegmenting RGB images. To address this limitation, we propose the Segment Any\nRGBD (SAD) model, which is specifically designed to extract geometry\ninformation directly from images. Inspired by the natural ability of humans to\nidentify objects through the visualization of depth maps, SAD utilizes SAM to\nsegment the rendered depth map, thus providing cues with enhanced geometry\ninformation and mitigating the issue of over-segmentation. We further include\nthe open-vocabulary semantic segmentation in our framework, so that the 3D\npanoptic segmentation is fulfilled. The project is available on\nhttps://github.com/Jun-CEN/SegmentAnyRGBD.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:26:56 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14208","submitter":"Anmol Kabra","authors":"Anmol Kabra, Ethan R. Elenberg","title":"Domain Private Transformers","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Large, general purpose language models have demonstrated impressive\nperformance across many different conversational domains. While multi-domain\nlanguage models achieve low overall perplexity, their outputs are not\nguaranteed to stay within the domain of a given input prompt. This paper\nproposes domain privacy as a novel way to quantify how likely a conditional\nlanguage model will leak across domains. We also develop policy functions based\non token-level domain classification, and propose an efficient fine-tuning\nmethod to improve the trained model's domain privacy. Experiments on membership\ninference attacks show that our proposed method has comparable resiliency to\nmethods adapted from recent literature on differentially private language\nmodels.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:27:12 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14209","submitter":"Kun Qian","authors":"Kun Qian, Yuanyuan Wang, Peter Jung, Yilei Shi, and Xiao Xiang Zhu","title":"Basis Pursuit Denoising via Recurrent Neural Network Applied to\n  Super-resolving SAR Tomography","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Finding sparse solutions of underdetermined linear systems commonly requires\nthe solving of L1 regularized least squares minimization problem, which is also\nknown as the basis pursuit denoising (BPDN). They are computationally expensive\nsince they cannot be solved analytically. An emerging technique known as deep\nunrolling provided a good combination of the descriptive ability of neural\nnetworks, explainable, and computational efficiency for BPDN. Many unrolled\nneural networks for BPDN, e.g. learned iterative shrinkage thresholding\nalgorithm and its variants, employ shrinkage functions to prune elements with\nsmall magnitude. Through experiments on synthetic aperture radar tomography\n(TomoSAR), we discover the shrinkage step leads to unavoidable information loss\nin the dynamics of networks and degrades the performance of the model. We\npropose a recurrent neural network (RNN) with novel sparse minimal gated units\n(SMGUs) to solve the information loss issue. The proposed RNN architecture with\nSMGUs benefits from incorporating historical information into optimization, and\nthus effectively preserves full information in the final output. Taking TomoSAR\ninversion as an example, extensive simulations demonstrated that the proposed\nRNN outperforms the state-of-the-art deep learning-based algorithm in terms of\nsuper-resolution power as well as generalization ability. It achieved a 10% to\n20% higher double scatterers detection rate and is less sensitive to phase and\namplitude ratio differences between scatterers. Test on real TerraSAR-X\nspotlight images also shows a high-quality 3-D reconstruction of the test site.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:28:02 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14210","submitter":"Shengnan An","authors":"Shengnan An, Bo Zhou, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng,\n  Weizhu Chen and Jian-Guang Lou","title":"Skill-Based Few-Shot Selection for In-Context Learning","comments":"18 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In-Context learning is the paradigm that adapts large language models to\ndownstream tasks by providing a few examples. Few-shot selection -- selecting\nappropriate examples for each test instance separately -- is important for\nin-context learning. In this paper, we propose Skill-KNN, a skill-based\nfew-shot selection method for in-context learning. The key advantages of\nSkill-KNN include: (1) it addresses the problem that existing methods based on\npre-trained embeddings can be easily biased by surface natural language\nfeatures that are not important for the target task; (2) it does not require\ntraining or fine-tuning of any models, making it suitable for frequently\nexpanding or changing example banks. The key insight is to optimize the inputs\nfed into the embedding model, rather than tuning the model itself. Technically,\nSkill-KNN generates the skill-based representations for each test case and\ncandidate example by utilizing a pre-processing few-shot prompting, thus\neliminating unimportant surface features. Experimental results across four\ncross-domain semantic parsing tasks and four backbone models show that\nSkill-KNN significantly outperforms existing methods.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:28:29 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14211","submitter":"Yixuan Weng","authors":"Minjun Zhu, Yixuan Weng, Shizhu He, Kang Liu, Jun Zhao","title":"Towards Graph-hop Retrieval and Reasoning in Complex Question Answering\n  over Textual Database","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In Textual question answering (TQA) systems, complex questions often require\nretrieving multiple textual fact chains with multiple reasoning steps. While\nexisting benchmarks are limited to single-chain or single-hop retrieval\nscenarios. In this paper, we propose to conduct Graph-Hop -- a novel\nmulti-chains and multi-hops retrieval and reasoning paradigm in complex\nquestion answering. We construct a new benchmark called ReasonGraphQA, which\nprovides explicit and fine-grained evidence graphs for complex questions to\nsupport interpretable reasoning, comprehensive and detailed reasoning. And\nReasonGraphQA also shows an advantage in reasoning diversity and scale.\nMoreover, We propose a strong graph-hop baseline called Bidirectional Graph\nRetrieval (BGR) method for generating an explanation graph of textual evidence\nin knowledge reasoning and question answering. We have thoroughly evaluated\nexisting evidence retrieval and reasoning models on the ReasonGraphQA.\nExperiments highlight Graph-Hop is a promising direction for answering complex\nquestions, but it still has certain limitations. We have further studied\nmitigation strategies to meet these challenges and discuss future directions.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:28:42 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14212","submitter":"Anthony Bahri","authors":"A.Bahri, M. Bendersky, F.R. Cohen and S. Gitler","title":"Symmetric Products and a Cartan-type formula for polyhedral products","comments":"arXiv admin note: substantial text overlap with arXiv:2009.06818","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AT math.CO","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We give a geometric method for determining the cohomology groups of a\npolyhedral product under suitable freeness conditions or with coefficients\ntaken in a field. This is done by considering first the special case for which\nthe pairs of spaces are wedge decomposable. We derive a decomposition for these\npolyhedral products which resembles a Cartan formula. The theory of symmetric\nproducts is used then to generalize the result to polyhedral products involving\narbitrary pairs. This leads to a direct computation of the Hilbert-Poincar\\'e\nseries and to other applications.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:29:53 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14213","submitter":"Griffin Kearney","authors":"Griffin M. Kearney, Kasey M. Laurent, Reece V. Kearney","title":"Maximum Likelihood Filtering for Particle Tracking in Turbulent Flows","comments":"16 Pages, 6 Figures, Pre-print","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.flu-dyn math.OC","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Lagrangian Particle Tracking (LPT) enables practitioners to study various\nconcepts in turbulence by measuring particle positions in flows of interest.\nThis data is subject to measurement errors, and filtering techniques are\napplied to mitigate these errors and improve the accuracy of analyses utilizing\nthe data. We develop a new type of position filter through use of maximum\nlikelihood estimation by considering both measurement errors and stochastic\nprocess physics. The maximum likelihood estimation scheme we develop is general\nand can accommodate many different stochastic process models, enabling it to be\napplied to many different turbulent flows. In this work, we propose a process\nmodel similar to existing, complimentary work in the development of B-splines.\nWe compare our filtering scheme to existing schemes and find that our filter\nout performs the scheme proposed by Mordant et al. (2004) considerably, and\nproduces similar performance to spline filters, proposed by Gesemann (2018). In\ncomparing to the latter, we note that the maximum likelihood treatment provides\na general framework which is capable of producing different filters based on\nthe physics of interest, whereas the spline filters are built on less specific\nfiltering theory and are therefore more difficult to adapt across diverse use\ncases in fluids. We quantify the performance of each of the filtering methods\nusing error metrics which consider both noise reduction as well as signal\ndegradation, and together these are used to define a concept of filter\nefficiency. The maximum likelihood filter developed in this work is shown to be\nthe most efficient among all the methods examined when applied to simulated\nisotropic turbulence data from the Johns Hopkins Database.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:31:04 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14214","submitter":"Benjamin Minixhofer","authors":"Benjamin Minixhofer, Jonas Pfeiffer, Ivan Vuli\\'c","title":"CompoundPiece: Evaluating and Improving Decompounding Performance of\n  Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  While many languages possess processes of joining two or more words to create\ncompound words, previous studies have been typically limited only to languages\nwith excessively productive compound formation (e.g., German, Dutch) and there\nis no public dataset containing compound and non-compound words across a large\nnumber of languages. In this work, we systematically study decompounding, the\ntask of splitting compound words into their constituents, at a wide scale. We\nfirst address the data gap by introducing a dataset of 255k compound and\nnon-compound words across 56 diverse languages obtained from Wiktionary. We\nthen use this dataset to evaluate an array of Large Language Models (LLMs) on\nthe decompounding task. We find that LLMs perform poorly, especially on words\nwhich are tokenized unfavorably by subword tokenization. We thus introduce a\nnovel methodology to train dedicated models for decompounding. The proposed\ntwo-stage procedure relies on a fully self-supervised objective in the first\nstage, while the second, supervised learning stage optionally fine-tunes the\nmodel on the annotated Wiktionary data. Our self-supervised models outperform\nthe prior best unsupervised decompounding models by 13.9% accuracy on average.\nOur fine-tuned models outperform all prior (language-specific) decompounding\ntools. Furthermore, we use our models to leverage decompounding during the\ncreation of a subword tokenizer, which we refer to as CompoundPiece.\nCompoundPiece tokenizes compound words more favorably on average, leading to\nimproved performance on decompounding over an otherwise equivalent model using\nSentencePiece tokenization.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:32:27 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14215","submitter":"Chang-Yu Tai","authors":"Chang-You Tai, Ziru Chen, Tianshu Zhang, Xiang Deng and Huan Sun","title":"Exploring Chain-of-Thought Style Prompting for Text-to-SQL","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Conventional supervised approaches for text-to-SQL parsing often require\nlarge amounts of annotated data, which is costly to obtain in practice.\nRecently, in-context learning with large language models (LLMs) has caught\nincreasing attention due to its superior few-shot performance in a wide range\nof tasks. However, most attempts to use in-context learning for text-to-SQL\nparsing still lag behind supervised methods. We hypothesize that the\nunder-performance is because text-to-SQL parsing requires complex, multi-step\nreasoning. In this paper, we systematically study how to enhance the reasoning\nability of LLMs for text-to-SQL parsing through chain-of-thought (CoT) style\npromptings including CoT prompting and Least-to-Most prompting. Our experiments\ndemonstrate that iterative prompting as in Least-to-Most prompting may be\nunnecessary for text-to-SQL parsing and directly applying existing CoT style\nprompting methods leads to error propagation issues. By improving multi-step\nreasoning while avoiding much detailed information in the reasoning steps which\nmay lead to error propagation, our new method outperforms existing ones by 2.4\npoint absolute gains on the Spider development set.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:32:36 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14216","submitter":"Chengbin Xuan","authors":"Chengbin Xuan, Feng Zhang, Faliang Yin, Hak-Keung Lam","title":"Constrained Proximal Policy Optimization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The problem of constrained reinforcement learning (CRL) holds significant\nimportance as it provides a framework for addressing critical safety\nsatisfaction concerns in the field of reinforcement learning (RL). However,\nwith the introduction of constraint satisfaction, the current CRL methods\nnecessitate the utilization of second-order optimization or primal-dual\nframeworks with additional Lagrangian multipliers, resulting in increased\ncomplexity and inefficiency during implementation. To address these issues, we\npropose a novel first-order feasible method named Constrained Proximal Policy\nOptimization (CPPO). By treating the CRL problem as a probabilistic inference\nproblem, our approach integrates the Expectation-Maximization framework to\nsolve it through two steps: 1) calculating the optimal policy distribution\nwithin the feasible region (E-step), and 2) conducting a first-order update to\nadjust the current policy towards the optimal policy obtained in the E-step\n(M-step). We establish the relationship between the probability ratios and KL\ndivergence to convert the E-step into a convex optimization problem.\nFurthermore, we develop an iterative heuristic algorithm from a geometric\nperspective to solve this problem. Additionally, we introduce a conservative\nupdate mechanism to overcome the constraint violation issue that occurs in the\nexisting feasible region method. Empirical evaluations conducted in complex and\nuncertain environments validate the effectiveness of our proposed method, as it\nperforms at least as well as other baselines.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:33:55 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14217","submitter":"Stefaan Vaes","authors":"Amine Marrakchi and Stefaan Vaes","title":"Ergodic states on type III$_1$ factors and ergodic actions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Since the early days of Tomita-Takesaki theory, it is known that a von\nNeumann algebra $M$ that admits a state $\\varphi$ with trivial centralizer\n$M_\\varphi$ must be a type III$_1$ factor, but the converse remained open. We\nsolve this problem and prove that such ergodic states form a dense $G_\\delta$\nset among all faithful normal states on any III$_1$ factor with separable\npredual. Through Connes' Radon-Nikodym cocycle theorem, this problem is related\nto the existence of ergodic cocycle perturbations for outer group actions,\nwhich we consider in the second part of the paper.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:33:55 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14218","submitter":"Kriti Aggarwal","authors":"Kriti Aggarwal, Aditi Khandelwal, Kumar Tanmay, Owais Mohammed Khan,\n  Qiang Liu, Monojit Choudhury, Hardik Hansrajbhai Chauhan, Subhojit Som,\n  Vishrav Chaudhary, Saurabh Tiwary","title":"DUBLIN -- Document Understanding By Language-Image Network","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Visual document understanding is a complex task that involves analyzing both\nthe text and the visual elements in document images. Existing models often rely\non manual feature engineering or domain-specific pipelines, which limit their\ngeneralization ability across different document types and languages. In this\npaper, we propose DUBLIN, which is pretrained on web pages using three novel\nobjectives: Masked Document Content Generation Task, Bounding Box Task, and\nRendered Question Answering Task, that leverage both the spatial and semantic\ninformation in the document images. Our model achieves competitive or\nstate-of-the-art results on several benchmarks, such as Web-Based Structural\nReading Comprehension, Document Visual Question Answering, Key Information\nExtraction, Diagram Understanding, and Table Question Answering. In particular,\nwe show that DUBLIN is the first pixel-based model to achieve an EM of 77.75\nand F1 of 84.25 on the WebSRC dataset. We also show that our model outperforms\nthe current pixel-based SoTA models on DocVQA and AI2D datasets by 2% and 21%,\nrespectively. Also, DUBLIN is the first ever pixel-based model which achieves\ncomparable performance to text-based SoTA methods on XFUND dataset for Semantic\nEntity Recognition showcasing its multilingual capability. Moreover, we create\nnew baselines for text-based datasets by rendering them as document images to\npromote research in this direction.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:34:09 GMT"},{"version":"v2","created":"Wed, 24 May 2023 07:03:56 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14219","submitter":"Edmund Chadwick Dr","authors":"Edmund Chadwick","title":"Existence, Smoothness and Uniqueness (in smooth space) of the\n  Navier-Stokes equation by using a new Boundary Integral representation","comments":"36 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP math-ph math.MP","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Consider an exterior space-time domain where the incompressible Navier-Stokes\nequation and continuity equation hold with no bodies or force fields present,\nand smooth velocity at initial time. This is equivalent to the velocity being\nimpulsively instantaneously started into motion and further assume that this\nforce impulse is bounded. A smooth solution with a Stokeslet far-field decay\nfor all subsequent time is sought and found, demonstrating existence and\nsmoothness. This is given by a space-time boundary integral velocity\nrepresentation by a single layer potential linear distribution of Navier-Stokes\nfundamental solutions called NSlets. This is obtained by extending the theory\nof hydrodynamic potentials to also include a non-linear potential that\nsubsequently drops out of the formulation. Zero initial velocity gives the null\nsolution and so there can be only one smooth solution demonstrating uniqueness\nin smooth space, but this is not to say that there are not other possible\nsolutions in the wider class of non-smooth spaces.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:34:40 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14220","submitter":"Jiayin Dong","authors":"Jiayin Dong and Daniel Foreman-Mackey","title":"A Hierarchical Bayesian Framework for Inferring the Stellar Obliquity\n  Distribution","comments":"10 pages, 6 figures; AJ submitted, revised in response to the referee\n  report; reproducible workflow built with showyourwork; open-source code can\n  be found at https://github.com/jiayindong/obliquity","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.EP astro-ph.SR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Stellar obliquity, the angle between a planet's orbital axis and its host\nstar's spin axis, traces the formation and evolution of a planetary system. In\ntransiting exoplanet observations, only the sky-projected stellar obliquity can\nbe measured, but this can be de-projected using an estimate of the stellar\nobliquity. In this paper, we introduce a flexible, hierarchical Bayesian\nframework that can be used to infer the stellar obliquity distribution solely\nfrom sky-projected stellar obliquities, including stellar inclination\nmeasurements when available. We demonstrate that while a constraint on the\nstellar inclination is crucial for measuring the obliquity of an individual\nsystem, it is not required for robust determination of the population-level\nstellar obliquity distribution. In practice, the constraints on the stellar\nobliquity distribution are mainly driven by the sky-projected stellar\nobliquities.\n  When applying the framework to all systems with measured sky-projected\nstellar obliquity, which are mostly Hot Jupiter systems, we find that the\ninferred population-level obliquity distribution is unimodal and peaked at zero\ndegrees. The misaligned systems have nearly isotropic stellar obliquities with\nno strong clustering near 90 degrees. The diverse range of stellar obliquities\nprefers dynamic mechanisms, such as planet-planet scattering after a convergent\ndisk migration, which could produce both prograde and retrograde orbits of\nclose-in planets with no strong inclination concentrations other than 0\ndegrees.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:35:08 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14221","submitter":"Xinyu Zhu","authors":"Xinyu Zhu, Cheng Yang, Bei Chen, Siheng Li, Jian-Guang Lou, Yujiu Yang","title":"Question Answering as Programming for Solving Time-Sensitive Questions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this work we try to apply Large Language Models (LLMs) to reframe the\nQuestion Answering task as Programming (QAaP). Due to the inherent dynamic\nnature of the real world, factual questions frequently involve a symbolic\nconstraint: time, solving these questions necessitates not only extensive world\nknowledge, but also advanced reasoning ability to satisfy the temporal\nconstraints. Despite the remarkable intelligence exhibited by LLMs in various\nNLP tasks, our experiments reveal that the aforementioned problems continue to\npose a significant challenge to existing LLMs. To solve these time-sensitive\nfactual questions, considering that modern LLMs possess superior ability in\nboth natural language understanding and programming,we endeavor to leverage\nLLMs to represent diversely expressed text as well-structured code, and thereby\ngrasp the desired knowledge along with the underlying symbolic constraint.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:35:16 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14223","submitter":"Max Smith","authors":"Max Olan Smith, Michael P. Wellman","title":"Co-Learning Empirical Games and World Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.MA cs.AI cs.GT cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Game-based decision-making involves reasoning over both world dynamics and\nstrategic interactions among the agents. Typically, empirical models capturing\nthese respective aspects are learned and used separately. We investigate the\npotential gain from co-learning these elements: a world model for dynamics and\nan empirical game for strategic interactions. Empirical games drive world\nmodels toward a broader consideration of possible game dynamics induced by a\ndiversity of strategy profiles. Conversely, world models guide empirical games\nto efficiently discover new strategies through planning. We demonstrate these\nbenefits first independently, then in combination as realized by a new\nalgorithm, Dyna-PSRO, that co-learns an empirical game and a world model. When\ncompared to PSRO -- a baseline empirical-game building algorithm, Dyna-PSRO is\nfound to compute lower regret solutions on partially observable general-sum\ngames. In our experiments, Dyna-PSRO also requires substantially fewer\nexperiences than PSRO, a key algorithmic advantage for settings where\ncollecting player-game interaction data is a cost-limiting factor.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:37:21 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14224","submitter":"Jonas Pfeiffer","authors":"Jonas Pfeiffer, Francesco Piccinno, Massimo Nicosia, Xinyi Wang,\n  Machel Reid, Sebastian Ruder","title":"mmT5: Modular Multilingual Pre-Training Solves Source Language\n  Hallucinations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Multilingual sequence-to-sequence models perform poorly with increased\nlanguage coverage and fail to consistently generate text in the correct target\nlanguage in few-shot settings. To address these challenges, we propose mmT5, a\nmodular multilingual sequence-to-sequence model. mmT5 utilizes\nlanguage-specific modules during pre-training, which disentangle\nlanguage-specific information from language-agnostic information. We identify\nrepresentation drift during fine-tuning as a key limitation of modular\ngenerative models and develop strategies that enable effective zero-shot\ntransfer. Our model outperforms mT5 at the same parameter sizes by a large\nmargin on representative natural language understanding and generation tasks in\n40+ languages. Compared to mT5, mmT5 raises the rate of generating text in the\ncorrect language under zero-shot settings from 7% to 99%, thereby greatly\nalleviating the source language hallucination problem.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:38:01 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14225","submitter":"Kung-Hsiang Huang","authors":"Kung-Hsiang Huang, Hou Pong Chan, Kathleen McKeown, Heng Ji","title":"ManiTweet: A New Benchmark for Identifying Manipulation of News on\n  Social Media","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Considerable advancements have been made to tackle the misrepresentation of\ninformation derived from reference articles in the domains of fact-checking and\nfaithful summarization. However, an unaddressed aspect remains - the\nidentification of social media posts that manipulate information within\nassociated news articles. This task presents a significant challenge, primarily\ndue to the prevalence of personal opinions in such posts. We present a novel\ntask, identifying manipulation of news on social media, which aims to detect\nmanipulation in social media posts and identify manipulated or inserted\ninformation. To study this task, we have proposed a data collection schema and\ncurated a dataset called ManiTweet, consisting of 3.6K pairs of tweets and\ncorresponding articles. Our analysis demonstrates that this task is highly\nchallenging, with large language models (LLMs) yielding unsatisfactory\nperformance. Additionally, we have developed a simple yet effective basic model\nthat outperforms LLMs significantly on the ManiTweet dataset. Finally, we have\nconducted an exploratory analysis of human-written tweets, unveiling intriguing\nconnections between manipulation and the domain and factuality of news\narticles, as well as revealing that manipulated sentences are more likely to\nencapsulate the main story or consequences of a news outlet.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:40:07 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14226","submitter":"Gernot Alber","authors":"Maximilian Schumacher, Gernot Alber","title":"Bipartite entanglement detection by local generalized measurements","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Entanglement detection by local measurements, which can possibly be performed\nby far distant observers, are of particular interest for applications in\nquantum key distribution and quantum communication. In this paper sufficient\nconditions for arbitrary dimensional bipartite entanglement detection based on\ncorrelation matrices and joint probability distributions of such local\nmeasurements are investigated. In particular, their dependence on the nature of\nthe local measurements is explored for typical bipartite quantum states and for\nmeasurements involving local orthonormal hermitian operators bases (LOOs) or\ngeneralized measurements based on informationally complete positive operator\nvalued measures of the recently introduced $(N,M)$-type ($(N,M)$-POVMs)\n\\cite{NMPOVM}. It is shown that symmetry properties of $(N,M)$-POVMs imply that\nsufficient conditions for bipartite entanglement detection exhibit peculiar\nscaling properties relating different equally efficient local entanglement\ndetection scenarios. For correlation-matrix based bipartite local entanglement\ndetection, for example, this has the consequence that LOOs and all\ninformationally complete $(N,M)$-POVMs are equally powerful. With the help of a\nhit-and-run Monte-Carlo algorithm the effectiveness of local entanglement\ndetection of typical bipartite quantum states is explored numerically. For this\npurpose Euclidean volume ratios between locally detectable entangled states and\nall bipartite quantum states are determined.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:40:48 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14227","submitter":"Vladimir V Kisil","authors":"Vladimir V. Kisil","title":"Transmutations from the Covariant Transform on the Heisenberg Group and\n  an Extended Umbral Principle","comments":"16 pages, RevTex","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP math.CO math.FA","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  We discuss several seemingly assorted objects: the umbral calculus,\ngeneralised translations and associated transmutations, symbolic calculus of\noperators. The common framework for them is representations of the Weyl algebra\nof the Heisenberg group by ladder operators. Transporting various properties\nbetween different implementations we review some classic results and new\nopportunities.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:44:43 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14228","submitter":"Matthias Stiefenhofer","authors":"Matthias Stiefenhofer","title":"Formal and Analytic Diagonalization of Operator functions","comments":"54 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We give conditions for local diagonalization of analytic operator families\nacting between real or complex Banach spaces. The transformations are\nconstructed from an operator Toeplitz matrix obtained from Jordan chains of\nincreasing length. The basic assumption is given by stabilization of the Jordan\nchains at length k in the sense that no root elements with finite rank above k\nare allowed to exist. Jordan chains with infinite rank may appear. These\nassumptions ensure finite pole order equal to k of the generalized inverse. The\nSmith form arises immediately.\n  Smooth continuation of kernels and ranges towards appropriate limit spaces is\nconsidered using associated families of analytic projection functions. No\nFredholm properties or other finiteness assumptions, besides the pole order,\nare assumed. Real and complex Banach spaces are treated without difference by\nelementary analysis of the system of undetermined coefficients.\n  Formal power series solutions of the system of undetermined coefficients are\nconstructed, which are turning into convergent solutions, as soon as\nanalyticity of the operator family and continuity of the projections is\nassumed. Along these lines, results concerning linear Artin approximation\nfollow immediately, which are well known in finite dimensions. The main\ntechnical tool is given by a defining equation of Nakayama Lemma type.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:44:46 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14229","submitter":"Jack Brady","authors":"Jack Brady, Roland S. Zimmermann, Yash Sharma, Bernhard Sch\\\"olkopf,\n  Julius von K\\\"ugelgen, Wieland Brendel","title":"Provably Learning Object-Centric Representations","comments":"Oral at ICML 2023. The first two authors as well as the last two\n  authors contributed equally. Code is available at\n  https://brendel-group.github.io/objects-identifiability","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Learning structured representations of the visual world in terms of objects\npromises to significantly improve the generalization abilities of current\nmachine learning models. While recent efforts to this end have shown promising\nempirical progress, a theoretical account of when unsupervised object-centric\nrepresentation learning is possible is still lacking. Consequently,\nunderstanding the reasons for the success of existing object-centric methods as\nwell as designing new theoretically grounded methods remains challenging. In\nthe present work, we analyze when object-centric representations can provably\nbe learned without supervision. To this end, we first introduce two assumptions\non the generative process for scenes comprised of several objects, which we\ncall compositionality and irreducibility. Under this generative process, we\nprove that the ground-truth object representations can be identified by an\ninvertible and compositional inference model, even in the presence of\ndependencies between objects. We empirically validate our results through\nexperiments on synthetic data. Finally, we provide evidence that our theory\nholds predictive power for existing object-centric models by showing a close\ncorrespondence between models' compositionality and invertibility and their\nempirical identifiability.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:44:49 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14230","submitter":"Neha Verma","authors":"Neha Verma, Kenton Murray, Kevin Duh","title":"Exploring Representational Disparities Between Multilingual and\n  Bilingual Translation Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Multilingual machine translation has proven immensely useful for low-resource\nand zero-shot language pairs. However, language pairs in multilingual models\nsometimes see worse performance than in bilingual models, especially when\ntranslating in a one-to-many setting. To understand why, we examine the\ngeometric differences in the representations from bilingual models versus those\nfrom one-to-many multilingual models. Specifically, we evaluate the isotropy of\nthe representations, to measure how well they utilize the dimensions in their\nunderlying vector space. Using the same evaluation data in both models, we find\nthat multilingual model decoder representations tend to be less isotropic than\nbilingual model decoder representations. Additionally, we show that much of the\nanisotropy in multilingual decoder representations can be attributed to\nmodeling language-specific information, therefore limiting remaining\nrepresentational capacity.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:46:18 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14231","submitter":"Shuo Yang","authors":"Yuchen Guo, Jian-Hao Zhang, Zhen Bi, Shuo Yang","title":"Triggering Boundary Phase Transitions through Bulk Measurements in 2D\n  Cluster States","comments":"7 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.str-el","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We investigate the phase diagram at the boundary of an infinite\ntwo-dimensional cluster state subject to bulk measurements using tensor network\nmethods. The state is subjected to uniform measurements $M =\n\\cos{\\theta}Z+\\sin{\\theta}X$ on the lower boundary qubits and all bulk qubits.\nOur results show that the boundary of the system exhibits volume-law\nentanglement at the measurement angle $\\theta = \\pi/2$ and area-law\nentanglement for any $\\theta < \\pi/2$. Within the area-law phase, a phase\ntransition occurs at $\\theta_c=1.371$. The phase with $\\theta\n\\in(\\theta_c,\\pi/2)$ is characterized by a non-injective matrix product state,\nwhich cannot be realized as the unique ground state of a 1D local, gapped\nHamiltonian. Instead, it resembles a cat state with spontaneous symmetry\nbreaking. These findings demonstrate that the phase diagram of the boundary of\na two-dimensional system can be more intricate than that of a standard\none-dimensional system.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:46:32 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14232","submitter":"Yu Zhang","authors":"Yu Zhang, Hao Cheng, Zhihong Shen, Xiaodong Liu, Ye-Yi Wang, Jianfeng\n  Gao","title":"Pre-training Multi-task Contrastive Learning Models for Scientific\n  Literature Understanding","comments":"15 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Scientific literature understanding tasks have gained significant attention\ndue to their potential to accelerate scientific discovery. Pre-trained language\nmodels (LMs) have shown effectiveness in these tasks, especially when tuned via\ncontrastive learning. However, jointly utilizing pre-training data across\nmultiple heterogeneous tasks (e.g., extreme classification, citation\nprediction, and literature search) remains largely unexplored. To bridge this\ngap, we propose a multi-task contrastive learning framework, SciMult, with a\nfocus on facilitating common knowledge sharing across different scientific\nliterature understanding tasks while preventing task-specific skills from\ninterfering with each other. To be specific, we explore two techniques --\ntask-aware specialization and instruction tuning. The former adopts a\nMixture-of-Experts Transformer architecture with task-aware sub-layers; the\nlatter prepends task-specific instructions to the input text so as to produce\ntask-aware outputs. Extensive experiments on a comprehensive collection of\nbenchmark datasets verify the effectiveness of our task-aware specialization\nstrategy in various tasks, where we outperform state-of-the-art scientific LMs.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:47:22 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14233","submitter":"Ning Ding","authors":"Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu,\n  Zhiyuan Liu, Maosong Sun, Bowen Zhou","title":"Enhancing Chat Language Models by Scaling High-quality Instructional\n  Conversations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Fine-tuning on instruction data has been widely validated as an effective\npractice for implementing chat language models like ChatGPT. Scaling the\ndiversity and quality of such data, although straightforward, stands a great\nchance of leading to improved performance. This paper aims to improve the upper\nbound of open-source models further. We first provide a systematically\ndesigned, diverse, informative, large-scale dataset of instructional\nconversations, UltraChat, which does not involve human queries. Our objective\nis to capture the breadth of interactions that a human might have with an AI\nassistant and employs a comprehensive framework to generate multi-turn\nconversation iteratively. UltraChat contains 1.5 million high-quality\nmulti-turn dialogues and covers a wide range of topics and instructions. Our\nstatistical analysis of UltraChat reveals its superiority in various key\nmetrics, including scale, average length, diversity, coherence, etc.,\nsolidifying its position as a leading open-source dataset. Building upon\nUltraChat, we fine-tune a LLaMA model to create a powerful conversational\nmodel, UltraLLaMA. Our evaluations indicate that UltraLLaMA consistently\noutperforms other open-source models, including Vicuna, the previously\nrecognized state-of-the-art open-source model. The dataset and the model will\nbe publicly released\\footnote{\\url{https://github.com/thunlp/UltraChat}}.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:49:14 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14234","submitter":"Mingyi Hou","authors":"Benny Avelin, Mingyi Hou and Kaj Nystr\\\"om","title":"A Galerkin type method for kinetic Fokker Planck equations based on\n  Hermite expansions","comments":"24 pages, corrected the reference list","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, we develop a Galerkin-type approximation, with quantitative\nerror estimates, for weak solutions to the Cauchy problem for kinetic\nFokker-Planck equations in the domain $(0, T) \\times D \\times \\mathbb{R}^d$,\nwhere $D$ is either $\\mathbb{T}^d$ or $\\mathbb{R}^d$. Our approach is based on\na Hermite expansion in the velocity variable only, with a hyperbolic system\nthat appears as the truncation of the Brinkman hierarchy, as well as ideas from\n$\\href{arXiv:1902.04037v2}{Alb+21}$ and additional energy-type estimates that\nwe have developed. We also establish the regularity of the solution based on\nthe regularity of the initial data and the source term.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:49:58 GMT"},{"version":"v2","created":"Thu, 25 May 2023 06:44:29 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.14235","submitter":"Ruochen Zhang","authors":"Ruochen Zhang, Samuel Cahyawijaya, Jan Christian Blaise Cruz and Alham\n  Fikri Aji","title":"Multilingual Large Language Models Are Not (Yet) Code-Switchers","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  Multilingual Large Language Models (LLMs) have recently shown great\ncapability in various tasks, exhibiting state-of-the-art performance using\nfew-shot or zero-shot prompting methods. While these models have been\nextensively studied in tasks where inputs are assumed to be in a single\nlanguage, less attention has been paid to exploring their performance when\ninputs involve code-switching (CSW). In this paper, we provide an extensive\nempirical study of various multilingual LLMs and benchmark their performance in\nthree tasks: sentiment analysis, machine translation, and word-level language\nidentification. Our findings indicate that despite multilingual LLMs showing\npromising outcomes in certain tasks when using zero-/few-shot prompting, their\nperformance still falls short on average when compared to smaller finetuned\nmodels. We argue that LLMs that are \"multilingual\" are not necessarily\ncode-switching compatible and extensive future research is required to fully\nbridge this gap.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:50:48 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14236","submitter":"Lingteng Qiu","authors":"Lingteng Qiu, Guanying Chen, Jiapeng Zhou, Mutian Xu, Junle Wang and\n  Xiaoguang Han","title":"REC-MV: REconstructing 3D Dynamic Cloth from Monocular Videos","comments":"CVPR2023; Project Page:https://lingtengqiu.github.io/2023/REC-MV/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Reconstructing dynamic 3D garment surfaces with open boundaries from\nmonocular videos is an important problem as it provides a practical and\nlow-cost solution for clothes digitization. Recent neural rendering methods\nachieve high-quality dynamic clothed human reconstruction results from\nmonocular video, but these methods cannot separate the garment surface from the\nbody. Moreover, despite existing garment reconstruction methods based on\nfeature curve representation demonstrating impressive results for garment\nreconstruction from a single image, they struggle to generate temporally\nconsistent surfaces for the video input. To address the above limitations, in\nthis paper, we formulate this task as an optimization problem of 3D garment\nfeature curves and surface reconstruction from monocular video. We introduce a\nnovel approach, called REC-MV, to jointly optimize the explicit feature curves\nand the implicit signed distance field (SDF) of the garments. Then the open\ngarment meshes can be extracted via garment template registration in the\ncanonical space. Experiments on multiple casually captured datasets show that\nour approach outperforms existing methods and can produce high-quality dynamic\ngarment surfaces. The source code is available at\nhttps://github.com/GAP-LAB-CUHK-SZ/REC-MV.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:53:10 GMT"},{"version":"v2","created":"Sat, 27 May 2023 17:01:54 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.14237","submitter":"Wenting Zhao","authors":"Wenting Zhao and Justin T. Chiu and Claire Cardie and Alexander M.\n  Rush","title":"HOP, UNION, GENERATE: Explainable Multi-hop Reasoning without Rationale\n  Supervision","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Explainable multi-hop question answering (QA) not only predicts answers but\nalso identifies rationales, i. e. subsets of input sentences used to derive the\nanswers. This problem has been extensively studied under the supervised\nsetting, where both answer and rationale annotations are given. Because\nrationale annotations are expensive to collect and not always available, recent\nefforts have been devoted to developing methods that do not rely on supervision\nfor rationales. However, such methods have limited capacities in modeling\ninteractions between sentences, let alone reasoning across multiple documents.\nThis work proposes a principled, probabilistic approach for training\nexplainable multi-hop QA systems without rationale supervision. Our approach\nperforms multi-hop reasoning by explicitly modeling rationales as sets,\nenabling the model to capture interactions between documents and sentences\nwithin a document. Experimental results show that our approach is more accurate\nat selecting rationales than the previous methods, while maintaining similar\naccuracy in predicting answers.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:53:49 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14238","submitter":"Rocco D'Agostino","authors":"Rocco D'Agostino, Matteo Califano, Nicola Menadeo, Daniele Vernieri","title":"The role of spatial curvature in the primordial gravitational wave power\n  spectrum","comments":"14 pages, 4 figures","journal-ref":null,"doi":null,"report-no":"ET-0162A-23","categories":"astro-ph.CO gr-qc","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper investigates the effects of non-vanishing spatial curvature on the\npropagation of primordial gravitational waves produced during inflation. In\nparticular, we consider tensor perturbations over a homogeneous and isotropic\nbackground, and describe the propagation of gravitational waves in the de\nSitter phase with spatially curved geometries. We thus derive the expression of\nthe primordial power spectrum at the horizon crossing, in the case of open and\nclosed universes. Then, we analyze how tensor modes propagate in the\npost-inflationary era, showing the evolution of transfer functions in the\nradiation and matter epochs, as well as the matching conditions in the\nintermediate regime. To account for the intrinsic nature of different\nrelativistic species, we also explore the corrections to the standard behavior\nof the radiation energy density. For this purpose, we introduce the effective\nnumber of degrees of freedom of relativistic particles contributing to the\nprimordial energy and entropy densities. Under the subhorizon approximation, we\nobtain the spectral energy density of relic gravitational waves in terms of the\ncurvature density parameter. Finally, we discuss the capability of present and\nfuture experiments to detect the primordial gravitational wave signal at\ndifferent frequency regimes.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:54:09 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14239","submitter":"Yixin Liu","authors":"Yixin Liu, Alexander R. Fabbri, Pengfei Liu, Dragomir Radev, Arman\n  Cohan","title":"On Learning to Summarize with Large Language Models as References","comments":"GitHub Repo: https://github.com/yixinL7/SumLLM","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Recent studies have found that summaries generated by large language models\n(LLMs) are favored by human annotators over the original reference summaries in\ncommonly used summarization datasets. Therefore, we investigate a new learning\nparadigm of text summarization models that considers the LLMs as the reference\nor the gold-standard oracle on commonly used summarization datasets such as the\nCNN/DailyMail dataset. To examine the standard practices that are aligned with\nthe new learning setting, we propose a novel training method that is based on\ncontrastive learning with LLMs as a summarization quality evaluator. For this\nreward-based training method, we investigate two different methods of utilizing\nLLMs for summary quality evaluation, namely GPTScore and GPTRank. Our\nexperiments on the CNN/DailyMail dataset demonstrate that smaller summarization\nmodels trained by our proposed method can achieve performance equal to or\nsurpass that of the reference LLMs, as evaluated by the LLMs themselves. This\nunderscores the efficacy of our proposed paradigm in enhancing model\nperformance over the standard maximum likelihood estimation (MLE) training\nmethod, and its efficiency since it only requires a small budget to access the\nLLMs. We release the training scripts, model outputs, and LLM-based evaluation\nresults to facilitate future studies.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:56:04 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14240","submitter":"Mikel Artetxe","authors":"Mikel Artetxe, Vedanuj Goswami, Shruti Bhosale, Angela Fan, Luke\n  Zettlemoyer","title":"Revisiting Machine Translation for Cross-lingual Classification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Machine Translation (MT) has been widely used for cross-lingual\nclassification, either by translating the test set into English and running\ninference with a monolingual model (translate-test), or translating the\ntraining set into the target languages and finetuning a multilingual model\n(translate-train). However, most research in the area focuses on the\nmultilingual models rather than the MT component. We show that, by using a\nstronger MT system and mitigating the mismatch between training on original\ntext and running inference on machine translated text, translate-test can do\nsubstantially better than previously assumed. The optimal approach, however, is\nhighly task dependent, as we identify various sources of cross-lingual transfer\ngap that affect different tasks and approaches differently. Our work calls into\nquestion the dominance of multilingual models for cross-lingual classification,\nand prompts to pay more attention to MT-based baselines.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:56:10 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14241","submitter":"Lucien Hardy","authors":"Saba Etezad-Razavi and Lucien Hardy","title":"Paradox with Phase-Coupled Interferometers","comments":"Comments welcome","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph gr-qc","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  A pair of interferometers can be coupled by allowing one path from each to\noverlap such that if the particles meet in this overlap region, they\nannihilate. It was shown by one of us over thirty years ago that such\nannihilation-coupled interferometers can exhibit apparently paradoxical\nbehaviour. More recently, Bose et al. and Marletto and Vedral have considered a\npair of interferometers that are phase-coupled (where the coupling is through\ngravitational interaction). In this case one path from each interferometer\nundergoes a phase-coupling interaction. We show that these phase-coupled\ninterferometers exhibit the same apparent paradox as the annihilation-coupled\ninterferometers, though in a curiously dual manner.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:56:48 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14242","submitter":"Keisuke Inomata","authors":"Keisuke Harigaya, Keisuke Inomata, Takahiro Terada","title":"Axion Poltergeist","comments":"6 pages, 4 figures (Supplemental Material: 12 pages, 3 figures)","journal-ref":null,"doi":null,"report-no":"CTPU-PTC-23-19","categories":"hep-ph astro-ph.CO gr-qc hep-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Rotations of axion fields in the early universe can produce dark matter and\nthe matter-antimatter asymmetry of the universe. We point out that the rotation\ncan generate an observable amount of a stochastic gravitational-wave (GW)\nbackground. It can be doubly enhanced in a class of models in which the\nequation of state of the rotations rapidly changes from a non-relativistic\nmatter-like one to a kination-like one by 1) the so-called Poltergeist\nmechanism and 2) slower redshift of GWs compared to the axion kination fluid.\nIn supersymmetric UV completion, future GW observations can probe the\nsupersymmetry-breaking scale up to $10^7\\,$GeV even if the axion does not\ndirectly couple to the Standard Model fields.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:58:48 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14243","submitter":"Manuel Tran","authors":"Manuel Tran, Amal Lahiani, Yashin Dicente Cid, Fabian J. Theis,\n  Tingying Peng, Eldad Klaiman","title":"Training Transitive and Commutative Multimodal Transformers with LoReTTa","comments":"Paper will undergo a revision","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Collecting a multimodal dataset with two paired modalities A and B or B and C\nis difficult in practice. Obtaining a dataset with three aligned modalities A,\nB, and C is even more challenging. For example, some public medical datasets\nhave only genetic sequences and microscopic images for one patient, and only\ngenetic sequences and radiological images for another - but no dataset includes\nboth microscopic and radiological images for the same patient. This makes it\ndifficult to integrate and combine all modalities into a large pre-trained\nneural network. We introduce LoReTTa (Linking mOdalities with a tRansitive and\ncommutativE pre-Training sTrAtegy) to address this understudied problem. Our\nself-supervised framework combines causal masked modeling with the rules of\ncommutativity and transitivity to transition within and between different\nmodalities. Thus, it can model the relation A -> C with A -> B -> C. Given a\ndataset containing only the disjoint combinations (A, B) and (B, C), we show\nthat a transformer pre-trained with LoReTTa can handle any modality combination\nat inference time, including the never-seen pair (A, C) and the triplet (A, B,\nC). We evaluate our approach on a multimodal dataset derived from MNIST\ncontaining speech, vision, and language, as well as a real-world medical\ndataset containing mRNA, miRNA, and RPPA samples from TCGA. Compared to\ntraditional pre-training methods, we observe up to a 100-point reduction in\nperplexity for autoregressive generation tasks and up to a 15% improvement in\nclassification accuracy for previously unseen modality pairs during the\npre-training phase.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:58:55 GMT"},{"version":"v2","created":"Mon, 29 May 2023 08:37:21 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.14244","submitter":"Shengchao Chen","authors":"Shengchao Chen, Guodong Long, Tao Shen, Tianyi Zhou, Jing Jiang","title":"Spatial-temporal Prompt Learning for Federated Weather Forecasting","comments":"Under Review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Federated weather forecasting is a promising collaborative learning framework\nfor analyzing meteorological data across participants from different countries\nand regions, thus embodying a global-scale real-time weather data predictive\nanalytics platform to tackle climate change. This paper is to model the\nmeteorological data in a federated setting where many distributed low-resourced\nsensors are deployed in different locations. Specifically, we model the\nspatial-temporal weather data into a federated prompt learning framework that\nleverages lightweight prompts to share meaningful representation and structural\nknowledge among participants. Prompts-based communication allows the server to\nestablish the structural topology relationships among participants and further\nexplore the complex spatial-temporal correlations without transmitting private\ndata while mitigating communication overhead. Moreover, in addition to a\nglobally shared large model at the server, our proposed method enables each\nparticipant to acquire a personalized model that is highly customized to tackle\nclimate changes in a specific geographic area. We have demonstrated the\neffectiveness of our method on classical weather forecasting tasks by utilizing\nthree spatial-temporal multivariate time-series weather data.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 16:59:20 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14245","submitter":"Gregorio Garc\\'ia-Valladares","authors":"Gregorio Garc\\'ia-Valladares, Carlos A. Plata, Antonio Prados and\n  Alessandro Manacorda","title":"Optimal resetting strategies for search processes in heterogeneous\n  environments","comments":"23 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.stat-mech cond-mat.dis-nn math-ph math.MP","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In many physical situations, there appears the problem of reaching a single\ntarget that is spatially distributed. Here we analyse how stochastic resetting,\nalso spatially distributed, can be used to improve the search process when the\ntarget location is quenched, i.e. it does not evolve in time. More\nspecifically, we consider a model with minimal but sufficient ingredients that\nallows us to derive analytical results for the relevant physical quantities,\nsuch as the first passage time distribution. We focus on the minimisation of\nthe mean first passage time and its fluctuations (standard deviation), which\nproves to be non-trivial. Our analysis shows that the no-disorder case is\nsingular: for small disorder, the resetting rate distribution that minimises\nthe mean first passage time leads to diverging fluctuations -- which impinge on\nthe practicality of this minimisation. Interestingly, this issue is healed by\nminimising the fluctuations: the associated resetting rate distribution gives\nfirst passage times that are very close to the optimal ones.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:00:08 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14246","submitter":"Jocelyn Shen","authors":"Jocelyn Shen, Maarten Sap, Pedro Colon-Hernandez, Hae Won Park,\n  Cynthia Breazeal","title":"Modeling Empathic Similarity in Personal Narratives","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The most meaningful connections between people are often fostered through\nexpression of shared vulnerability and emotional experiences in personal\nnarratives. We introduce a new task of identifying similarity in personal\nstories based on empathic resonance, i.e., the extent to which two people\nempathize with each others' experiences, as opposed to raw semantic or lexical\nsimilarity, as has predominantly been studied in NLP. Using insights from\nsocial psychology, we craft a framework that operationalizes empathic\nsimilarity in terms of three key features of stories: main events, emotional\ntrajectories, and overall morals or takeaways. We create EmpathicStories, a\ndataset of 1,500 personal stories annotated with our empathic similarity\nfeatures, and 2,000 pairs of stories annotated with empathic similarity scores.\nUsing our dataset, we fine-tune a model to compute empathic similarity of story\npairs, and show that this outperforms semantic similarity models on automated\ncorrelation and retrieval metrics. Through a user study with 150 participants,\nwe also assess the effect our model has on retrieving stories that users\nempathize with, compared to naive semantic similarity-based retrieval, and find\nthat participants empathized significantly more with stories retrieved by our\nmodel. Our work has strong implications for the use of empathy-aware models to\nfoster human connection and empathy between people.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:00:45 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14247","submitter":"D\\'avid P\\'eter Kov\\'acs","authors":"David Peter Kovacs, Ilyes Batatia, Eszter Sara Arany, Gabor Csanyi","title":"Evaluation of the MACE Force Field Architecture: from Medicinal\n  Chemistry to Materials Science","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.chem-ph stat.ML","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The MACE architecture represents the state of the art in the field of machine\nlearning force fields for a variety of in-domain, extrapolation and low-data\nregime tasks. In this paper, we further evaluate MACE by fitting models for\npublished benchmark datasets. We show that MACE generally outperforms\nalternatives for a wide range of systems from amorphous carbon and general\nsmall molecule organic chemistry to large molecules and liquid water. We\ndemonstrate the capabilities of the model on tasks ranging from constrained\ngeometry optimisation to molecular dynamics simulations and find excellent\nperformance across all tested domains. We show that MACE is very data\nefficient, and can reproduce experimental molecular vibrational spectra when\ntrained on as few as 50 randomly selected reference configurations. We further\ndemonstrate that the strictly local atom-centered model is sufficient for such\ntasks even in the case of large molecules and weakly interacting molecular\nassemblies.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:01:25 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14248","submitter":"Thomas Bonis","authors":"Thomas Bonis","title":"Improved rates of convergence for the multivariate Central Limit Theorem\n  in Wasserstein distance","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We provide new bounds for rates of convergence of the multivariate Central\nLimit Theorem in Wasserstein distances of order $p \\geq 2$. In particular, we\nobtain an asymptotic bound for measures with a continuous component which we\nconjecture to be optimal.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:02:42 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14249","submitter":"Blake Jackson","authors":"Blake Jackson","title":"An Intersection-Dimension Formula for Preprojective Modules of Type\n  $\\widetilde{D}_n$","comments":"27 pages, 14 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.RT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper proves the existence of an intersection-dimension formula for\npreprojective modules over path algebras of type $\\widetilde{D}_n$. Identical\nintersection-dimension formulas have previously been provided for modules over\npath algebras of type $A_n, D_n,$ and $\\widetilde{A}_n$ due to Schiffler as\nwell as He, Zhou, and Zhu. These modules can be represented geometrically by\nsome set of curves on special surfaces. The intersection-dimension formula is\nan equality of the intersection number between two curves and the dimensions of\nthe first extension spaces between the two modules they represent. This paper\ntakes a direct approach to proving the formula utilizing the known structure of\nthe Auslander-Reiten quiver of type $\\widetilde{D}_n$. Future work will extend\nthe formula to the entire module category (not just the preprojective modules)\nover path algebras of type $\\widetilde{D}_n$.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:03:46 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14250","submitter":"Peter Clark","authors":"Nora Kassner, Oyvind Tafjord, Ashish Sabharwal, Kyle Richardson,\n  Hinrich Schutze, Peter Clark","title":"Language Models with Rationality","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  While large language models (LLMs) are proficient at question-answering (QA),\nthe dependencies between their answers and other \"beliefs\" they may have about\nthe world are typically unstated, and may even be in conflict. Our goal is to\nuncover such dependencies and reduce inconsistencies among them, so that\nanswers are supported by faithful, system-believed chains of reasoning drawn\nfrom a consistent network of beliefs. Our approach, which we call REFLEX, is to\nadd a \"rational\", self-reflecting layer on top of the LLM. First, given a\nquestion, we construct a belief graph using a backward-chaining process to\nmaterialize relevant model \"beliefs\" (including beliefs about answer\ncandidates) and the inferential relationships between them. Second, we identify\nand minimize contradictions in that graph using a formal constraint reasoner.\nWe find that REFLEX significantly improves consistency (by 8%-11% absolute)\nwithout harming overall answer accuracy, resulting in answers supported by\nfaithful chains of reasoning drawn from a more consistent belief system. This\nsuggests a new style of system architecture, in which an LLM extended with a\nrational layer of self-reflection can repair latent inconsistencies within the\nLLM alone.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:04:25 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14251","submitter":"Sewon Min","authors":"Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang\n  Wei Koh, Mohit Iyyer, Luke Zettlemoyer, Hannaneh Hajishirzi","title":"FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long\n  Form Text Generation","comments":"23 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Evaluating the factuality of long-form text generated by large language\nmodels (LMs) is non-trivial because (1) generations often contain a mixture of\nsupported and unsupported pieces of information, making binary judgments of\nquality inadequate, and (2) human evaluation is time-consuming and costly. In\nthis paper, we introduce FActScore (Factual precision in Atomicity Score), a\nnew evaluation that breaks a generation into a series of atomic facts and\ncomputes the percentage of atomic facts supported by a reliable knowledge\nsource. We conduct an extensive human evaluation to obtain FActScores of people\nbiographies generated by several state-of-the-art commercial LMs --\nInstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report\nnew analysis demonstrating the need for such a fine-grained score (e.g.,\nChatGPT only achieves 58%). Since human evaluation is costly, we also introduce\nan automated model that estimates FActScore, using retrieval and a strong\nlanguage model, with less than a 2% error rate. Finally, we use this automated\nmetric to evaluate 6,500 generations from a new set of 13 recent LMs that would\nhave cost $26K if evaluated by humans, with various findings: GPT-4 and ChatGPT\nare more factual than public models, and Vicuna and Alpaca are some of the best\npublic models.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:06:00 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14252","submitter":"Andre Souto","authors":"Mariano Lemus, Ricardo Faleiro, Paulo Mateus, Nikola Paunkovi\\'c,\n  Andr\\'e Souto","title":"Quantum Kolmogorov complexity and quantum correlations in\n  deterministic-control quantum Turing machines","comments":"31 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cs.CC math-ph math.MP","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  This work presents a study of Kolmogorov complexity for general quantum\nstates from the perspective of deterministic-control quantum Turing Machines\n(dcq-TM). We extend the dcq-TM model to incorporate mixed state inputs and\noutputs, and define dcq-computable states as those that can be approximated by\na dcq-TM. Moreover, we introduce (conditional) Kolmogorov complexity of quantum\nstates and use it to study three particular aspects of the algorithmic\ninformation contained in a quantum state: a comparison of the information in a\nquantum state with that of its classical representation as an array of real\nnumbers, an exploration of the limits of quantum state copying in the context\nof algorithmic complexity, and study of the complexity of correlations in\nquantum systems, resulting in a correlation-aware definition for algorithmic\nmutual information that satisfies symmetry of information property.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:07:58 GMT"},{"version":"v2","created":"Fri, 2 Jun 2023 10:36:33 GMT"}],"update_date":"2023-06-05"}
{"id":"2305.14253","submitter":"Chris Hughes","authors":"Christopher Hughes, Greg Martin, Andrew Pearce-Crump","title":"A heuristic for discrete mean values of the derivative of the Riemann\n  zeta function","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NT","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Shanks conjectured that $\\zeta ' (\\rho)$, where $\\rho$ ranges over\nnon-trivial zeros of the Riemann zeta function, is real and positive in the\nmean. We present a history of this problem, including a generalisation to all\nhigher-order derivatives $\\zeta^{(n)}(s)$, for which the sign of the mean\nalternatives between positive for odd $n$ and negative for even $n$.\nFurthermore, we give a simple heuristic that provides the leading term\n(including its sign) of the asymptotic formula for the average value of\n$\\zeta^{(n)}(\\rho)$.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:08:08 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14254","submitter":"Yiyun Fan","authors":"Yiyun Fan, John Billingham, and Kristoffer van der Zee","title":"A Shape-Newton Method for Free-boundary Problems Subject to The\n  Bernoulli Boundary Condition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  We develop a shape-Newton method for solving generic free-boundary problems\nwhere one of the free-boundary conditions is governed by the Bernoulli\nequation. The Newton-like scheme is developed by employing shape derivatives in\nthe weak forms, which allows us to update the position of the free surface and\nthe potential on the free boundary by solving a boundary-value problem at each\niteration. To validate the effectiveness of the approach, we apply the scheme\nto solve a problem involving the flow over a submerged triangular obstacle.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:09:14 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14255","submitter":"Tanchumin Xu","authors":"Tanchumin Xu, Yunshu Zhang and Shu Yang","title":"Augmented match weighted estimators for average treatment effects","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Propensity score matching (PSM) and augmented inverse propensity weighting\n(AIPW) are widely used in observational studies to estimate causal effects. The\ntwo approaches present complementary features. The AIPW estimator is doubly\nrobust and locally efficient but can be unstable when the propensity scores are\nclose to zero or one due to weighting by the inverse of the propensity score.\nOn the other hand, PSM circumvents the instability of propensity score\nweighting but it hinges on the correctness of the propensity score model and\ncannot attain the semiparametric efficiency bound. Besides, the fixed number of\nmatches, K, renders PSM nonsmooth and thus invalidates standard nonparametric\nbootstrap inference.\n  This article presents novel augmented match weighted (AMW) estimators that\ncombine the advantages of matching and weighting estimators. AMW adheres to the\nform of AIPW for its double robustness and local efficiency but it mitigates\nthe instability due to weighting. We replace inverse propensity weights with\nmatching weights resulting from PSM with unfixed K. Meanwhile, we propose a new\ncross-validation procedure to select K that minimizes the mean squared error\nanchored around an unbiased estimator of the causal estimand. Besides, we\nderive the limiting distribution for the AMW estimators showing that they enjoy\nthe double robustness property and can achieve the semiparametric efficiency\nbound if both nuisance models are correct. As a byproduct of unfixed K which\nsmooths the AMW estimators, nonparametric bootstrap can be adopted for variance\nestimation and inference. Furthermore, simulation studies and real data\napplications support that the AMW estimators are stable with extreme propensity\nscores and their variances can be obtained by naive bootstrap.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:10:08 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14256","submitter":"Oleg Vasilyev","authors":"Oleg Vasilyev, Fumika Isono, John Bohannon","title":"Linear Cross-Lingual Mapping of Sentence Embeddings","comments":"6 pages, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Semantics of a sentence is defined with much less ambiguity than semantics of\na single word, and it should be better preserved by translation to another\nlanguage. If multilingual sentence embeddings intend to represent sentence\nsemantics, then the similarity between embeddings of any two sentences must be\ninvariant with respect to translation. Based on this suggestion, we consider a\nsimple linear cross-lingual mapping as a possible improvement of the\nmultilingual embeddings. We also consider deviation from orthogonality\nconditions as a measure of deficiency of the embeddings.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:10:37 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14257","submitter":"Robert Lo","authors":"Abishek Sridhar, Robert Lo, Frank F. Xu, Hao Zhu, Shuyan Zhou","title":"Hierarchical Prompting Assists Large Language Model on Web Navigation","comments":"16 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large language models (LLMs) struggle on processing complicated observations\nin interactive decision making. To alleviate this issue, we propose a simple\nhierarchical prompting approach. Diverging from previous prompting approaches\nthat always put the \\emph{full} observation~(\\eg a web page) to the prompt, we\npropose to first construct an action-aware observation which is more\n\\emph{condensed} and \\emph{relevant} with a dedicated \\summ prompt. The \\actor\nprompt then predicts the next action based on the summarized history. While our\nmethod has broad applicability, we particularly demonstrate its efficacy in the\ncomplex domain of web navigation where a full observation often contains\nredundant and irrelevant information. Our approach outperforms the previous\nstate-of-the-art prompting mechanism with the same LLM by 6.2\\% on task success\nrate, demonstrating its potential on interactive decision making tasks with\nlong observation traces.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:10:39 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14258","submitter":"Zheng Xie","authors":"Zheng Xie, Yu Liu, Hao-Yuan He, Ming Li, Zhi-Hua Zhou","title":"Weakly Supervised AUC Optimization: A Unified Partial AUC Approach","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Since acquiring perfect supervision is usually difficult, real-world machine\nlearning tasks often confront inaccurate, incomplete, or inexact supervision,\ncollectively referred to as weak supervision. In this work, we present WSAUC, a\nunified framework for weakly supervised AUC optimization problems, which covers\nnoisy label learning, positive-unlabeled learning, multi-instance learning, and\nsemi-supervised learning scenarios. Within the WSAUC framework, we first frame\nthe AUC optimization problems in various weakly supervised scenarios as a\ncommon formulation of minimizing the AUC risk on contaminated sets, and\ndemonstrate that the empirical risk minimization problems are consistent with\nthe true AUC. Then, we introduce a new type of partial AUC, specifically, the\nreversed partial AUC (rpAUC), which serves as a robust training objective for\nAUC maximization in the presence of contaminated labels. WSAUC offers a\nuniversal solution for AUC optimization in various weakly supervised scenarios\nby maximizing the empirical rpAUC. Theoretical and experimental results under\nmultiple settings support the effectiveness of WSAUC on a range of weakly\nsupervised AUC optimization tasks.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:11:33 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.14259","submitter":"Qingyun Wang","authors":"Qingyun Wang, Doug Downey, Heng Ji, Tom Hope","title":"Learning to Generate Novel Scientific Directions with Contextualized\n  Literature-based Discovery","comments":"21 pages. Code and resource is available at\n  https://github.com/EagleW/CLBD","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Literature-Based Discovery (LBD) aims to discover new scientific knowledge by\nmining papers and generating hypotheses. Standard LBD is limited to predicting\npairwise relations between discrete concepts (e.g., drug-disease links). LBD\nalso ignores critical contexts like experimental settings (e.g., a specific\npatient population where a drug is evaluated) and background knowledge and\nmotivations that human scientists consider (e.g., to find a drug candidate\nwithout specific side effects). We address these limitations with a novel\nformulation of contextualized-LBD (C-LBD): generating scientific hypotheses in\nnatural language, while grounding them in a context that controls the\nhypothesis search space. We present a new modeling framework using retrieval of\n``inspirations'' from a heterogeneous network of citations and knowledge graph\nrelations, and create a new dataset derived from papers. In automated and human\nevaluations, our models improve over baselines, including powerful large\nlanguage models (LLMs), but also reveal challenges on the road to building\nmachines that generate new scientific knowledge.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:12:08 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14260","submitter":"Yue Fan","authors":"Yue Fan, Kaizhi Zheng, Jing Gu, Xin Eric Wang","title":"R2H: Building Multimodal Navigation Helpers that Respond to Help","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The ability to assist humans during a navigation task in a supportive role is\ncrucial for intelligent agents. Such agents, equipped with environment\nknowledge and conversational abilities, can guide individuals through\nunfamiliar terrains by generating natural language responses to their\ninquiries, grounded in the visual information of their surroundings. However,\nthese multimodal conversational navigation helpers are still underdeveloped.\nThis paper proposes a new benchmark, Respond to Help (R2H), to build multimodal\nnavigation helpers that can respond to help, based on existing dialog-based\nembodied datasets. R2H mainly includes two tasks: (1) Respond to Dialog History\n(RDH), which assesses the helper agent's ability to generate informative\nresponses based on a given dialog history, and (2) Respond during Interaction\n(RdI), which evaluates the helper agent's ability to maintain effective and\nconsistent cooperation with a task performer agent during navigation in\nreal-time. Furthermore, we propose a novel task-oriented multimodal response\ngeneration model that can see and respond, named SeeRee, as the navigation\nhelper to guide the task performer in embodied tasks. Through both automatic\nand human evaluations, we show that SeeRee produces more effective and\ninformative responses than baseline methods in assisting the task performer\nwith different navigation tasks. Project website:\nhttps://sites.google.com/view/respond2help/home.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:12:09 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14261","submitter":"V\\'ictor Manuel Jim\\'enez","authors":"V. M. Jim\\'enez and M. De Le\\'on","title":"New notions of uniformity and homogeneity of Cosserat media","comments":"arXiv admin note: text overlap with arXiv:1708.00337","journal-ref":null,"doi":null,"report-no":null,"categories":"math.DG math-ph math.MP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, we study internal properties of a Cosserat media. In fact, by\nusing groupoids and smooth distributions, we obtain a three canonical\nequations. The \\textit{non-holonomic material equation for Cosserat media}\ncharacterizes the uniformity of the material. The \\textit{holonomic material\nequation for Cosserat media} permits us to study when a Cosserat material is a\nsecond-grade material. It is remarkable that these two equations also provide\nus a unique and maximal division of the Cosserat medium into uniform and\nsecond-grade parts, respectively. Finally, we present a proper definition of\nhomogeneity of the Cosserat medium, without assuming uniformity. Thus, the\n\\textit{homogeneity equation for Cosserat media} characterizes this notion of\nhomogeneity.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:13:31 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14262","submitter":"Varsha Ramachandran","authors":"V. Ramachandran, J. Klencki, A. A. C. Sander, D. Pauli, T. Shenar, L.\n  M. Oskinova, and W.-R. Hamann","title":"A partially stripped massive star in a Be binary at low metallicity: A\n  missing link towards Be X-ray binaries and double neutron star mergers","comments":"To be published in A&A","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.SR astro-ph.HE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Standard binary evolutionary models predict a significant population of core\nhelium-burning stars that lost their hydrogen-rich envelope after mass transfer\nvia Roche-lobe overflow. However, there is a scarcity of observations of such\nstripped stars in the intermediate mass regime (~1.5 - 8$ M_{\\odot}$), which\nare thought to be prominent progenitors of SN Ib/c. Especially at low\nmetallicity, a significant fraction of these stars is expected to be only\npartially stripped, retaining a significant amount of hydrogen on their\nsurfaces. For the first time, we discovered a partially stripped massive star\nin a binary with a Be-type companion located in the Small Magellanic Cloud\n(SMC) using a detailed spectroscopic analysis. The stripped-star nature of the\nprimary is revealed by the extreme CNO abundance pattern and very high\nluminosity-to-mass ratio, which suggest that the primary is likely\nshell-hydrogen burning. Our target SMCSGS-FS 69 is the most luminous and most\nmassive system among the known stripped star + Be binaries, with Mstripped ~3$\nM_{\\odot}$ and MBe ~17$ M_{\\odot}$. Binary evolutionary tracks suggest an\ninitial mass of Mini $\\gtrsim 12 M_{\\odot}$ for the stripped star and predict\nit to be in a transition phase towards a hot compact He star, which will\neventually produce a stripped-envelope supernova. Our target marks the first\nrepresentative of a so-far missing evolutionary stage in the formation pathway\nof Be X-ray binaries and double neutron star mergers.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:15:35 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14263","submitter":"Milind Agarwal","authors":"Milind Agarwal, Md Mahfuz Ibn Alam, Antonios Anastasopoulos","title":"LIMIT: Language Identification, Misidentification, and Translation using\n  Hierarchical Models in 350+ Languages","comments":"25 pages, 2 figures, 13 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  Knowing the language of an input text/audio is a necessary first step for\nusing almost every natural language processing (NLP) tool such as taggers,\nparsers, or translation systems. Language identification is a well-studied\nproblem, sometimes even considered solved; in reality, most of the world's 7000\nlanguages are not supported by current systems. This lack of representation\naffects large-scale data mining efforts and further exacerbates data shortage\nfor low-resource languages. We take a step towards tackling the data bottleneck\nby compiling a corpus of over 50K parallel children's stories in 350+ languages\nand dialects, and the computation bottleneck by building lightweight\nhierarchical models for language identification. Our data can serve as\nbenchmark data for language identification of short texts and for understudied\ntranslation directions such as those between Indian or African languages. Our\nproposed method, Hierarchical LIMIT, uses limited computation to expand\ncoverage into excluded languages while maintaining prediction quality.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:15:43 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14264","submitter":"Katerina Margatina","authors":"Katerina Margatina and Timo Schick and Nikolaos Aletras and Jane\n  Dwivedi-Yu","title":"Active Learning Principles for In-Context Learning with Large Language\n  Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The remarkable advancements in large language models (LLMs) have\nsignificantly enhanced the performance in few-shot learning settings. By using\nonly a small number of labeled examples, referred to as demonstrations, LLMs\ncan effectively grasp the task at hand through in-context learning. However,\nthe process of selecting appropriate demonstrations has received limited\nattention in prior work. This paper addresses the issue of identifying the most\ninformative demonstrations for few-shot learning by approaching it as a\npool-based Active Learning (AL) problem over a single iteration. Our objective\nis to investigate how AL algorithms can serve as effective demonstration\nselection methods for in-context learning. We compare various standard AL\nalgorithms based on uncertainty, diversity, and similarity, and consistently\nobserve that the latter outperforms all other methods, including random\nsampling. Notably, uncertainty sampling, despite its success in conventional\nsupervised learning scenarios, performs poorly in this context. Our extensive\nexperimentation involving a diverse range of GPT and OPT models across $24$\nclassification and multi-choice tasks, coupled with thorough analysis,\nunambiguously demonstrates that in-context example selection through AL\nprioritizes high-quality examples that exhibit low uncertainty and bear\nsimilarity to the test examples.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:16:04 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14265","submitter":"Liyang Sun","authors":"Timothy B. Armstrong, Patrick Kline, Liyang Sun","title":"Adapting to Misspecification","comments":"58 pages, 12 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"econ.EM stat.ME","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Empirical research typically involves a robustness-efficiency tradeoff. A\nresearcher seeking to estimate a scalar parameter can invoke strong assumptions\nto motivate a restricted estimator that is precise but may be heavily biased,\nor they can relax some of these assumptions to motivate a more robust, but\nvariable, unrestricted estimator. When a bound on the bias of the restricted\nestimator is available, it is optimal to shrink the unrestricted estimator\ntowards the restricted estimator. For settings where a bound on the bias of the\nrestricted estimator is unknown, we propose adaptive shrinkage estimators that\nminimize the percentage increase in worst case risk relative to an oracle that\nknows the bound. We show that adaptive estimators solve a weighted convex\nminimax problem and provide lookup tables facilitating their rapid computation.\nRevisiting five empirical studies where questions of model specification arise,\nwe examine the advantages of adapting to -- rather than testing for --\nmisspecification.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:16:09 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14266","submitter":"Dmitry Svintsov","authors":"Dmitry Svintsov and Georgy Alymov","title":"Refraction laws for two-dimensional plasmons","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.optics cond-mat.mes-hall","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Despite numerous applications of two-dimensional plasmons for electromagnetic\nenergy manipulation at the nanoscale, their quantitative refraction and\nreflection laws (analogs of Fresnel formulas in optics) have not yet been\nestablished. This fact can be traced down to the strong non-locality of\nequations governing the 2d plasmon propagation. Here, we tackle this difficulty\nby direct solution of plasmon scattering problem with Wiener-Hopf technique. We\nobtain the reflection and transmission coefficients for 2d plasmons at the\ndiscontinuity of 2d conductivity at arbitrary incidence angle, for both gated\nand non-gated 2d systems. At a certain incidence angle, the absolute\nreflectivity has a pronounced dip reaching zero for gated plasmons. The dip is\nassociated with wave passage causing no dynamic charge accumulation at the\nboundary. For all incidence angles, the reflection has a non-trivial phase\ndifferent from zero and $\\pi$.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:16:53 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14267","submitter":"Martin Gonzalez","authors":"Martin Gonzalez, Nelson Fernandez, Thuy Tran, Elies Gherbi, Hatem\n  Hajri, Nader Masmoudi","title":"SEEDS: Exponential SDE Solvers for Fast High-Quality Sampling from\n  Diffusion Models","comments":"52 pages. Comments are welcome!","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV cs.NA math.NA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  A potent class of generative models known as Diffusion Probabilistic Models\n(DPMs) has become prominent. A forward diffusion process adds gradually noise\nto data, while a model learns to gradually denoise. Sampling from pre-trained\nDPMs is obtained by solving differential equations (DE) defined by the learnt\nmodel, a process which has shown to be prohibitively slow. Numerous efforts on\nspeeding-up this process have consisted on crafting powerful ODE solvers.\nDespite being quick, such solvers do not usually reach the optimal quality\nachieved by available slow SDE solvers. Our goal is to propose SDE solvers that\nreach optimal quality without requiring several hundreds or thousands of NFEs\nto achieve that goal. In this work, we propose Stochastic Exponential\nDerivative-free Solvers (SEEDS), improving and generalizing Exponential\nIntegrator approaches to the stochastic case on several frameworks. After\ncarefully analyzing the formulation of exact solutions of diffusion SDEs, we\ncraft SEEDS to analytically compute the linear part of such solutions. Inspired\nby the Exponential Time-Differencing method, SEEDS uses a novel treatment of\nthe stochastic components of solutions, enabling the analytical computation of\ntheir variance, and contains high-order terms allowing to reach optimal quality\nsampling $\\sim3$-$5\\times$ faster than previous SDE methods. We validate our\napproach on several image generation benchmarks, showing that SEEDS outperforms\nor is competitive with previous SDE solvers. Contrary to the latter, SEEDS are\nderivative and training free, and we fully prove strong convergence guarantees\nfor them.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:19:54 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14268","submitter":"Zi-Yi Dou","authors":"Zi-Yi Dou, Feng Gao, Nanyun Peng","title":"Masked Path Modeling for Vision-and-Language Navigation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Vision-and-language navigation (VLN) agents are trained to navigate in\nreal-world environments by following natural language instructions. A major\nchallenge in VLN is the limited availability of training data, which hinders\nthe models' ability to generalize effectively. Previous approaches have\nattempted to address this issue by introducing additional supervision during\ntraining, often requiring costly human-annotated data that restricts\nscalability. In this paper, we introduce a masked path modeling (MPM)\nobjective, which pretrains an agent using self-collected data for downstream\nnavigation tasks. Our proposed method involves allowing the agent to actively\nexplore navigation environments without a specific goal and collect the paths\nit traverses. Subsequently, we train the agent on this collected data to\nreconstruct the original path given a randomly masked subpath. This way, the\nagent can actively accumulate a diverse and substantial amount of data while\nlearning conditional action generation. To evaluate the effectiveness of our\ntechnique, we conduct experiments on various VLN datasets and demonstrate the\nversatility of MPM across different levels of instruction complexity. Our\nresults exhibit significant improvements in success rates, with enhancements of\n1.32\\%, 1.05\\%, and 1.19\\% on the val-unseen split of the Room-to-Room,\nRoom-for-Room, and Room-across-Room datasets, respectively. Furthermore, we\nconduct an analysis that highlights the potential for additional improvements\nwhen the agent is allowed to explore unseen environments prior to testing.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:20:20 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14269","submitter":"Giulia Rizzoli","authors":"Giulia Rizzoli, Donald Shenaj, Pietro Zanuttigh","title":"Source-Free Domain Adaptation for RGB-D Semantic Segmentation with\n  Vision Transformers","comments":"8 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.MM","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  With the increasing availability of depth sensors, multimodal frameworks that\ncombine color information with depth data are attracting increasing interest.\nIn the challenging task of semantic segmentation, depth maps allow to\ndistinguish between similarly colored objects at different depths and provide\nuseful geometric cues. On the other side, ground truth data for semantic\nsegmentation is burdensome to be provided and thus domain adaptation is another\nsignificant research area. Specifically, we address the challenging source-free\ndomain adaptation setting where the adaptation is performed without reusing\nsource data. We propose MISFIT: MultImodal Source-Free Information fusion\nTransformer, a depth-aware framework which injects depth information into a\nsegmentation module based on vision transformers at multiple stages, namely at\nthe input, feature and output levels. Color and depth style transfer helps\nearly-stage domain alignment while re-wiring self-attention between modalities\ncreates mixed features allowing the extraction of better semantic content.\nFurthermore, a depth-based entropy minimization strategy is also proposed to\nadaptively weight regions at different distances. Our framework, which is also\nthe first approach using vision transformers for source-free semantic\nsegmentation, shows noticeable performance improvements with respect to\nstandard strategies.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:20:47 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14270","submitter":"Haonan Lu","authors":"Haonan Lu and Shuai Mu and Siddhartha Sen and Wyatt Lloyd","title":"NCC: Natural Concurrency Control for Strictly Serializable Datastores by\n  Avoiding the Timestamp-Inversion Pitfall","comments":"This paper will appear at OSDI 23. We hope to use Arxiv to submit our\n  technical reports of this paper. We would greatly appreciate it if our\n  submission could be approved as soon as possible, so we could include a link\n  to this technical report in the the camera-ready version. Thank you","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DC","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  Strictly serializable datastores greatly simplify the development of correct\napplications by providing strong consistency guarantees. However, existing\ntechniques pay unnecessary costs for naturally consistent transactions, which\narrive at servers in an order that is already strictly serializable. We find\nthese transactions are prevalent in datacenter workloads. We exploit this\nnatural arrival order by executing transaction requests with minimal costs\nwhile optimistically assuming they are naturally consistent, and then leverage\na timestamp-based technique to efficiently verify if the execution is indeed\nconsistent. In the process of designing such a timestamp-based technique, we\nidentify a fundamental pitfall in relying on timestamps to provide strict\nserializability, and name it the timestamp-inversion pitfall. We find\ntimestamp-inversion has affected several existing works.\n  We present Natural Concurrency Control (NCC), a new concurrency control\ntechnique that guarantees strict serializability and ensures minimal costs --\ni.e., one-round latency, lock-free, and non-blocking execution -- in the best\n(and common) case by leveraging natural consistency. NCC is enabled by three\nkey components: non-blocking execution, decoupled response control, and\ntimestamp-based consistency check. NCC avoids timestamp-inversion with a new\ntechnique: response timing control, and proposes two optimization techniques,\nasynchrony-aware timestamps and smart retry, to reduce false aborts. Moreover,\nNCC designs a specialized protocol for read-only transactions, which is the\nfirst to achieve the optimal best-case performance while ensuring strict\nserializability, without relying on synchronized clocks. Our evaluation shows\nthat NCC outperforms state-of-the-art solutions by an order of magnitude on\nmany workloads.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:21:30 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14271","submitter":"Zhuoran Fang","authors":"Zhuoran Fang, Bassem Tossoun, Antoine Descos, Di Liang, Xue Huang,\n  Geza Kurczveil, Arka Majumdar, Raymond G. Beausoleil","title":"Fast and energy-efficient non-volatile III-V-on-silicon photonic phase\n  shifter based on memristors","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.optics physics.app-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Silicon photonics has evolved from lab research to commercial products in the\npast decade as it plays an increasingly crucial role in data communication for\nnext-generation data centers and high performance computing1. Recently,\nprogrammable silicon photonics has also found new applications in quantum2 and\nclassical 3 information processing. A key component of programmable silicon\nphotonic integrated circuits (PICs) is the phase shifter, traditionally\nrealized via the thermo-optic or plasma dispersion effect which are weak,\nvolatile, and power hungry. A non-volatile phase shifter can circumvent these\nlimitations by requiring zero power to maintain the switched phases. Previously\nnon-volatile phase modulation was achieved via phase-change4 or ferroelectric\nmaterials5, but the switching energy remains high (pico to nano joules) and the\nspeed is slow (micro to milli seconds). Here, we report a non-volatile\nIII-V-on-silicon photonic phase shifter based on HfO2 memristor with sub-pJ\nswitching energy (~400fJ), representing over an order of magnitude improvement\nin energy efficiency compared to the state of the art. The non-volatile phase\nshifter can be switched reversibly using a single 100ns pulse and exhibits an\nexcellent endurance over 800 cycles. This technology can enable future\nenergy-efficient programmable PICs for data centers, optical neural networks,\nand quantum information processing.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:22:41 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14272","submitter":"Kyle DeBry","authors":"Kyle DeBry, Jasmine Sinanan-Singh, Colin D. Bruzewicz, David Reens,\n  May E. Kim, Matthew P. Roychowdhury, Robert McConnell, Isaac L. Chuang, and\n  John Chiaverini","title":"Experimental quantum channel discrimination using metastable states of a\n  trapped ion","comments":"Main text: 6 pages, 4 figures. Supplementary Material: 7 pages, 5\n  figures, 2 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph physics.atom-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We present experimental demonstrations of accurate and unambiguous\nsingle-shot discrimination between three quantum channels using a single\ntrapped $^{40}\\text{Ca}^{+}$ ion. The three channels cannot be distinguished\nunambiguously using repeated single channel queries, the natural classical\nanalogue. We develop techniques for using the 6-dimensional $\\text{D}_{5/2}$\nstate space for quantum information processing, and we implement protocols to\ndiscriminate quantum channel analogues of phase shift keying and amplitude\nshift keying data encodings used in classical radio communication. The\ndemonstrations achieve discrimination accuracy exceeding $99\\%$ in each case,\nlimited entirely by known experimental imperfections.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:22:44 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14273","submitter":"Hyun Jeong","authors":"Hyun Jeong, Kohei Kamada, Alexei A. Starobinsky, Jun'ichi Yokoyama","title":"Reheating process in the $R^2$ inflationary model with the baryogenesis\n  scenario","comments":"13 pages, 9 figures","journal-ref":null,"doi":null,"report-no":"RESCEU-13/23","categories":"hep-ph astro-ph.CO","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Post-inflationary evolution and (re)heating of the viable inflationary model,\nthe $R^2$ one, is made more realistic by including the leptogenesis scenario\ninto it. For this purpose, right-handed Majorana neutrinos with a large mass\nare added to the matter sector of the Standard Model to explain the neutrino\noscillation experiments and the baryon asymmetry of the Universe. We have found\nparameters that characterize this model: non-minimal coupling of the Higgs\nfield $\\xi$ and the mass of the right-handed Majorana neutrino $M_{N_\\alpha}$.\nWe have analyzed the effect of these parameters on the reheating process and\nthe resultant physical quantities: spectral indices and baryon asymmetry.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:23:42 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14274","submitter":"Purbayan Chakraborty","authors":"B. V. Rajarama Bhat, Purbayan Chakraborty, Uwe Franz","title":"Error Basis and Quantum Channel","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph math-ph math.MP math.OA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The Weyl operators give a convenient basis of $M_n(\\mathbb{C})$ which is also\northonormal with respect to the Hilbert-Schmidt inner product. The properties\nof such a basis can be generalised to the notion of a nice error basis(NEB), as\nintroduced by E. Knill. We can use an NEB of $M_n(\\mathbb{C})$ to construct an\nNEB for $Lin(M_n(\\mathbb{C}))$, the space of linear maps on $M_n(\\mathbb{C})$.\nAny linear map on $M_n(\\mathbb{C})$ will then correspond to a $n^2\\times n^2$\ncoefficient matrix in the basis decomposition with respect to such an NEB of\n$Lin(M_n(\\mathbb{C}))$. Positivity, complete (co)positivity or other properties\nof a linear map can be characterised in terms of such a coefficient matrix.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:23:56 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14275","submitter":"Yash Patel","authors":"Yash Patel, Declan McNamara, Jackson Loper, Jeffrey Regier, Ambuj\n  Tewari","title":"Variational Inference with Coverage Guarantees","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Amortized variational inference produces a posterior approximator that can\ncompute a posterior approximation given any new observation. Unfortunately,\nthere are few guarantees about the quality of these approximate posteriors. We\npropose Conformalized Amortized Neural Variational Inference (CANVI), a\nprocedure that is scalable, easily implemented, and provides guaranteed\nmarginal coverage. Given a collection of candidate amortized posterior\napproximators, CANVI constructs conformalized predictors based on each\ncandidate, compares the predictors using a metric known as predictive\nefficiency, and returns the most efficient predictor. CANVI ensures that the\nresulting predictor constructs regions that contain the truth with high\nprobability (exactly how high is prespecified by the user). CANVI is agnostic\nto design decisions in formulating the candidate approximators and only\nrequires access to samples from the forward model, permitting its use in\nlikelihood-free settings. We prove lower bounds on the predictive efficiency of\nthe regions produced by CANVI and explore how the quality of a posterior\napproximation relates to the predictive efficiency of prediction regions based\non that approximation. Finally, we demonstrate the accurate calibration and\nhigh predictive efficiency of CANVI on a suite of simulation-based inference\nbenchmark tasks and an important scientific task: analyzing galaxy emission\nspectra.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:24:04 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14276","submitter":"Ada Chan","authors":"Ada Chan and Peter Sin","title":"Pretty good state transfer among large sets of vertices","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.CO quant-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In a continuous-time quantum walk on a network of qubits, pretty good state\ntransfer is the phenomenon of state transfer between two vertices with fidelity\narbitrarily close to 1. We construct families of graphs to demonstrate that\nthere is no bound on the size of a set of vertices that admit pretty good state\ntransfer between any two vertices of the set.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:24:14 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14277","submitter":"Yaroslav Zhumagulov","authors":"Yaroslav Zhumagulov and Denis Kochan and Jaroslav Fabian","title":"Emergent correlated phases in rhombohedral trilayer graphene induced by\n  proximity spin-orbit and exchange coupling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.str-el cond-mat.mes-hall cond-mat.mtrl-sci","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The impact of proximity-induced spin-orbit and exchange coupling on the\ncorrelated phase diagram of rhombohedral trilayer graphene (RTG) is\ninvestigated theoretically. By employing \\emph{ab initio}-fitted effective\nmodels of RTG encapsulated by transition metal dichalcogenides (spin-orbit\nproximity effect) and ferromagnetic Cr$_2$Ge$_2$Te$_6$ (exchange proximity\neffect), we incorporate the Coulomb interactions within the random-phase\napproximation to explore potential correlated phases at different displacement\nfield and doping. We find a rich spectrum of spin-valley resolved Stoner and\nintervalley coherence instabilities induced by the spin-orbit proximity\neffects, such as the emergence of a \\textit{spin-valley-coherent} phase due to\nthe presence of valley-Zeeman coupling. Similarly, proximity exchange removes\nthe phase degeneracies by biasing the spin direction, enabling a\nmagneto-correlation effect -- strong sensitivity of the correlated phases to\nthe relative magnetization orientations (parallel or antiparallel) of the\nencapsulating ferromagnetic layers.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:24:31 GMT"},{"version":"v2","created":"Fri, 2 Jun 2023 10:53:29 GMT"}],"update_date":"2023-06-05"}
{"id":"2305.14278","submitter":"Rico Huhnstock","authors":"Rico Huhnstock, Lukas Paetzold, Maximilian Merkel, Piotr Ku\\'swik,\n  Arno Ehresmann","title":"Combined funnel, concentrator, and particle valve functional element for\n  magnetophoretic bead transport based on engineered magnetic domain patterns","comments":"30 pages and 4 figures (main article), 2 pages Supporting Information","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.app-ph cond-mat.soft","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Controlled actuation of superparamagnetic beads (SPBs) within a microfluidic\nenvironment using tailored dynamic magnetic field landscapes (MFLs) is a potent\napproach for the realization of point-of-care diagnostics within Lab-on-a-chip\n(LOC) systems. Making use of an engineered magnetic domain pattern as the MFL\nsource, a functional LOC-element with combined magnetophoretic funnel,\nconcentrator, and valve functions for micron-sized SPBs is presented. A\nparallel-stripe domain pattern design with periodically increasing/decreasing\nstripe lengths has been fabricated in a topographically flat continuous\nexchange biased (EB) thin film system by ion bombardment induced magnetic\npatterning (IBMP). It is demonstrated that, upon application of external\nmagnetic field pulses, a fully reversible concentration of SPBs at the domain\npattern focal point occurs. In addition, it is shown that this functionality\nmay be used as an SPB funnel, allowing only a maximum number of particles to\npass through the focal point. Adjusting the pulse time length, the focal point\ncan be clogged up for incoming SPBs, resembling an on-and-off switchable\nparticle valve. The observations are supported by quantitative theoretical\nforce considerations.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:25:55 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14279","submitter":"Angelica Chen","authors":"Angelica Chen, Jason Phang, Alicia Parrish, Vishakh Padmakumar, Chen\n  Zhao, Samuel R. Bowman, Kyunghyun Cho","title":"Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large language models (LLMs) have achieved widespread success on a variety of\nin-context few-shot tasks, but this success is typically evaluated via\ncorrectness rather than consistency. We argue that self-consistency is an\nimportant criteria for valid multi-step reasoning and propose two types of\nself-consistency that are particularly important for multi-step logic --\nhypothetical consistency (the ability for a model to predict what its output\nwould be in a hypothetical other context) and compositional consistency\n(consistency of a model's outputs for a compositional task even when an\nintermediate step is replaced with the model's output for that step). We\ndemonstrate that four sizes of the GPT-3 model exhibit poor consistency rates\nacross both types of consistency on four different tasks (Wikipedia,\nDailyDialog, arithmetic, and GeoQuery).\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:25:59 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14280","submitter":"Elizabeth Salesky","authors":"Elizabeth Salesky, Neha Verma, Philipp Koehn, Matt Post","title":"Pixel Representations for Multilingual Translation and Data-efficient\n  Cross-lingual Transfer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We introduce and demonstrate how to effectively train multilingual machine\ntranslation models with pixel representations. We experiment with two different\ndata settings with a variety of language and script coverage, and show\nperformance competitive with subword embeddings. We analyze various properties\nof pixel representations to better understand where they provide potential\nbenefits and the impact of different scripts and data representations. We\nobserve that these properties not only enable seamless cross-lingual transfer\nto unseen scripts, but make pixel representations more data-efficient than\nalternatives such as vocabulary expansion. We hope this work contributes to\nmore extensible multilingual models for all languages and scripts.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:26:50 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14281","submitter":"Emanuele Bugliarello","authors":"Emanuele Bugliarello, Aida Nematzadeh, Lisa Anne Hendricks","title":"Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Recent work in vision-and-language pretraining has investigated supervised\nsignals from object detection data to learn better, fine-grained multimodal\nrepresentations. In this work, we take a step further and explore how we add\nsupervision from small-scale visual relation data. In particular, we propose\ntwo pretraining approaches to contextualise visual entities in a multimodal\nsetup. With verbalised scene graphs, we transform visual relation triplets into\nstructured captions, and treat them as additional views of images. With masked\nrelation prediction, we further encourage relating entities from visually\nmasked contexts. When applied to strong baselines pretrained on large amounts\nof Web data, zero-shot evaluations on both coarse-grained and fine-grained\ntasks show the efficacy of our methods in learning multimodal representations\nfrom weakly-supervised relations data.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:27:12 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14282","submitter":"Wenda Xu","authors":"Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag,\n  William Yang Wang, Lei Li","title":"INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with\n  Automatic Feedback","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The field of automatic evaluation of text generation made tremendous progress\nin the last few years. In particular, since the advent of neural metrics, like\nCOMET, BLEURT, and SEScore2, the newest generation of metrics show a high\ncorrelation with human judgment. Unfortunately, quality scores generated with\nneural metrics are not interpretable, and it is unclear which part of the\ngeneration output is criticized by the metrics. To address this limitation, we\npresent INSTRUCTSCORE, an open-source, explainable evaluation metric for text\ngeneration. By harnessing both explicit human instruction and the implicit\nknowledge of GPT4, we fine-tune a LLAMA model to create an evaluative metric\nthat can produce a diagnostic report aligned with human judgment. We evaluate\nINSTRUCTSCORE on the WMT22 Zh-En translation task, where our 7B model surpasses\nother LLM-based baselines, including those based on 175B GPT3. Impressively,\nour INSTRUCTSCORE, even without direct supervision from human-rated data,\nachieves performance levels on par with state-of-the-art metrics like COMET22,\nwhich was fine-tuned on human ratings.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:27:22 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14283","submitter":"Xinbei Ma","authors":"Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan","title":"Query Rewriting for Retrieval-Augmented Large Language Models","comments":"working in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Large Language Models (LLMs) play a powerful \\textit{Reader} of the\n\\textit{Retrieve-then-Read} pipeline, making great progress in knowledge-based\nopen-domain tasks. This work introduces a new framework,\n\\textit{Rewrite-Retrieve-Read} that improves the retrieval-augmented method\nfrom the perspective of the query rewriting. Prior studies mostly contribute to\nadapt the retriever or stimulate the reader. Different from them, our approach\npay attention of the query adaptation. Because the original query can not be\nalways optimal to retrieve for the LLM, especially in the real world.(1) We\nfirst prompt an LLM to rewrite the queries, then conduct retrieval-augmented\nreading. (2) We further apply a small language model as a trainable rewriter,\nwhich rewrite the search query to cater to the frozen retriever and the LLM\nreader. To fine-tune the rewriter, we first use a pseudo data to conduct\nsupervised warm-up training. Then the \\textit{Retrieve-then-Read} pipeline is\nmodeled as a reinforcement learning context. The rewriter is further trained as\na policy model by maximize the reward of the pipeline performance. Evaluation\nis performed on two downstream tasks, open-domain QA and multiple choice. Our\nframework is proved effective and scalable.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:27:50 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14284","submitter":"Ahmed Jellal","authors":"Nadia Benlakhouy, Ahmed Jellal, Michael Schreiber","title":"Transport properties of hybrid single-bilayer graphene interfaces in\n  magnetic field","comments":"8 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The electronic properties of a hybrid system made of single-bilayer graphene\nstructures subjected to a perpendicular magnetic field are studied for the\nzigzag boundaries of the junction, zigzag-1 (ZZ1) and zigzag-2 (ZZ2). These\nlater examples exhibit different behaviors that have been investigated using\nthe continuum Dirac model. Our results reveal that the conductance depends on\nthe width of bilayer graphene for ZZ1 and shows maxima for ZZ2 as a function of\nthe magnetic field, in contrast to ZZ1. It is found that interfaces have\nsignificant impacts on the transmission probability, with the confinement of\nthe ZZ1 boundary being more substantial than that of ZZ2\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:28:01 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14285","submitter":"Rosario Lo Franco","authors":"Matteo Piccolini, Vittorio Giovannetti, Rosario Lo Franco","title":"Robust engineering of maximally entangled states by identical particle\n  interferometry","comments":"9 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We propose a procedure for the robust preparation of maximally entangled\nstates of identical fermionic qubits, studying the role played by particle\nstatistics in the process. The protocol exploits externally activated noisy\nchannels to reset the system to a known state. The subsequent interference\neffects generated at a beam splitter result in a mixture of maximally entangled\nBell states and NOON states. We also discuss how every maximally entangled\nstate of two fermionic qubits distributed over two spatial modes can be\nobtained from one another by fermionic passive optical transformations. Using a\npseudospin-insensitive, non-absorbing, parity check detector, the proposed\ntechnique is thus shown to deterministically prepare any arbitrary maximally\nentangled state of two identical fermions. These results extend recent findings\nrelated to bosonic qubits. Finally, we analyze the performance of the protocol\nfor both bosons and fermions when the externally activated noisy channels are\nnot used and the two qubits undergo standard types of noise. The results supply\nfurther insights towards viable strategies for noise-protected entanglement\nexploitable in quantum-enhanced technologies.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:29:20 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14286","submitter":"Koen Minartz","authors":"Koen Minartz, Yoeri Poels, Simon Koop, Vlado Menkovski","title":"Equivariant Neural Simulators for Stochastic Spatiotemporal Dynamics","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Neural networks are emerging as a tool for scalable data-driven simulation of\nhigh-dimensional dynamical systems, especially in settings where numerical\nmethods are infeasible or computationally expensive. Notably, it has been shown\nthat incorporating domain symmetries in deterministic neural simulators can\nsubstantially improve their accuracy, sample efficiency, and parameter\nefficiency. However, to incorporate symmetries in probabilistic neural\nsimulators that can simulate stochastic phenomena, we need a model that\nproduces equivariant distributions over trajectories, rather than equivariant\nfunction approximations. In this paper, we propose Equivariant Probabilistic\nNeural Simulation (EPNS), a framework for autoregressive probabilistic modeling\nof equivariant distributions over system evolutions. We use EPNS to design\nmodels for a stochastic n-body system and stochastic cellular dynamics. Our\nresults show that EPNS considerably outperforms existing neural network-based\nmethods for probabilistic simulation. More specifically, we demonstrate that\nincorporating equivariance in EPNS improves simulation quality, data\nefficiency, rollout stability, and uncertainty quantification. We conclude that\nEPNS is a promising method for efficient and effective data-driven\nprobabilistic simulation in a diverse range of domains.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:30:10 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14287","submitter":"Max Weinreich","authors":"Max Weinreich","title":"The dynamical degree of billiards in an algebraic curve","comments":"46 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.DS","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We introduce an algebraic formulation of billiards on plane curves $C_0$ over\nalgebraically closed fields, extending Glutsyuk's complex billiards. For any\nnondegenerate algebraic curve $C_0$ of degree $d \\geq 2$, algebraic billiards\nis a rational $(d-1)$-to-$(d-1)$ surface correspondence on the space of unit\ncotangent vectors based on $C_0$. We prove that the dynamical degree of the\nbilliards correspondence is at most an explicit cubic algebraic integer $\\rho_d\n< d^2 - d - 3$, depending only on the degree $d$ of $C_0$. As a corollary, for\ngeneral real algebraic curves, the topological entropy of the classical\nbilliards map is at most $\\log \\rho_d$. We further show that the billiards\ncorrespondence satisfies the singularity confinement property and preserves a\nnatural $2$-form. To prove our bounds, we construct a birational model that\npartially resolves the indeterminacy of algebraic billiards.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:31:04 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14288","submitter":"Chenxi Whitehouse","authors":"Chenxi Whitehouse, Monojit Choudhury, Alham Fikri Aji","title":"LLM-powered Data Augmentation for Enhanced Crosslingual Performance","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  This paper aims to explore the potential of leveraging Large Language Models\n(LLMs) for data augmentation in crosslingual commonsense reasoning datasets,\nwhere the available training data is extremely limited. To achieve this, we\nemploy several LLMs including Dolly-v2, StableVicuna, ChatGPT, and GPT-4 to\naugment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we\nassess the effectiveness of fine-tuning smaller crosslingual models, mBERT and\nXLMR, using the synthesised data. We compare the performance of training with\ndata generated in English and target languages, as well as translating the\nEnglish-generated data into the target languages. Our experiments reveal the\noverall advantages of incorporating data generated by LLMs. Training on\nsynthetic data generated by GPT-4, whether English or multilingual, improves\nperformance consistently compared to the baseline. Other models also exhibit an\noverall increase in performance, however, their effectiveness decreases in some\nsettings. We also ask native speakers to evaluate the naturalness and logical\nsoundness of the generated examples for different languages. Human evaluation\nreveals that LLMs like ChatGPT and GPT-4 excel at generating natural text in\nmost languages, except a few such as Tamil. Moreover, ChatGPT trails behind in\ngenerating plausible alternatives in comparison to the original dataset, while\nGPT-4 demonstrates competitive logic consistency in the synthesised data.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:33:27 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14289","submitter":"Xili Yi","authors":"Xili Yi, Nima Fazeli","title":"Precise Object Sliding with Top Contact via Asymmetric Dual Limit\n  Surfaces","comments":"10 pages, 11 figures, accepted in Robotics: Science and Systems (RSS\n  2023), Daegu, Republic of Korea","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, we discuss the mechanics and planning algorithms to slide an\nobject on a horizontal planar surface via frictional patch contact made with\nits top surface. Here, we propose an asymmetric dual limit surface model to\ndetermine slip boundary conditions for both the top and bottom contact. With\nthis model, we obtain a range of twists that can keep the object in sticking\ncontact with the robot end-effector while slipping on the supporting plane.\nBased on these constraints, we derive a planning algorithm to slide objects\nwith only top contact to arbitrary goal poses without slippage between end\neffector and the object. We validate the proposed model empirically and\ndemonstrate its predictive accuracy on a variety of object geometries and\nmotions. We also evaluate the planning algorithm over a variety of objects and\ngoals demonstrate an orientation error improvement of 90\\% when compared to\nmethods naive to linear path planners.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:33:37 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14290","submitter":"Karol Gietka","authors":"Karol Gietka, Christoph Hotter, and Helmut Ritsch","title":"Unique Steady-State Squeezing in a Driven Quantum Rabi Model","comments":"9 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Squeezing is essential to many quantum technologies and our understanding of\nquantum physics. Here we develop a theory of steady-state squeezing that can be\ngenerated in the closed and open quantum Rabi as well as Dicke model. To this\nend, we eliminate the spin dynamics which effectively leads to an abstract\nharmonic oscillator whose eigenstates are squeezed with respect to the physical\nharmonic oscillator. The generated form of squeezing has the unique property of\ntime-independent uncertainties and squeezed dynamics, a novel type of quantum\nbehavior. Such squeezing might find applications in continuous back-action\nevading measurements and should already be observable in optomechanical systems\nand Coulomb crystals.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:34:12 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14291","submitter":"Nicholas Deas","authors":"Nicholas Deas and Jessi Grieser and Shana Kleiner and Desmond Patton\n  and Elsbeth Turcan and Kathleen McKeown","title":"Evaluation of African American Language Bias in Natural Language\n  Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We evaluate how well LLMs understand African American Language (AAL) in\ncomparison to their performance on White Mainstream English (WME), the\nencouraged \"standard\" form of English taught in American classrooms. We measure\nLLM performance using automatic metrics and human judgments for two tasks: a\ncounterpart generation task, where a model generates AAL (or WME) given WME (or\nAAL), and a masked span prediction (MSP) task, where models predict a phrase\nthat was removed from their input. Our contributions include: (1) evaluation of\nsix pre-trained, large language models on the two language generation tasks;\n(2) a novel dataset of AAL text from multiple contexts (social media, hip-hop\nlyrics, focus groups, and linguistic interviews) with human-annotated\ncounterparts in WME; and (3) documentation of model performance gaps that\nsuggest bias and identification of trends in lack of understanding of AAL\nfeatures.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:34:37 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14292","submitter":"Sina Semnani","authors":"Sina J. Semnani, Violet Z. Yao, Heidi C. Zhang, Monica S. Lam","title":"WikiChat: A Few-Shot LLM-Based Chatbot Grounded with Wikipedia","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Despite recent advances in Large Language Models (LLMs), users still cannot\ntrust the information provided in their responses. LLMs cannot speak accurately\nabout events that occurred after their training, which are often topics of\ngreat interest to users, and, as we show in this paper, they are highly prone\nto hallucination when talking about less popular (tail) topics. This paper\npresents WikiChat, a few-shot LLM-based chatbot that is grounded with live\ninformation from Wikipedia. Through many iterations of experimentation, we have\ncrafte a pipeline based on information retrieval that (1) uses LLMs to suggest\ninteresting and relevant facts that are individually verified against\nWikipedia, (2) retrieves additional up-to-date information, and (3) composes\ncoherent and engaging time-aware responses. We propose a novel hybrid\nhuman-and-LLM evaluation methodology to analyze the factuality and\nconversationality of LLM-based chatbots. We focus on evaluating important but\npreviously neglected issues such as conversing about recent and tail topics. We\nevaluate WikiChat against strong fine-tuned and LLM-based baselines across a\ndiverse set of conversation topics. We find that WikiChat outperforms all\nbaselines in terms of the factual accuracy of its claims, by up to 12.1%, 28.3%\nand 32.7% on head, recent and tail topics, while matching GPT-3.5 in terms of\nproviding natural, relevant, non-repetitive and informational responses.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:37:36 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14293","submitter":"Chenxi Whitehouse","authors":"Chenxi Whitehouse, Clara Vania, Alham Fikri Aji, Christos\n  Christodoulopoulos, Andrea Pierleoni","title":"WebIE: Faithful and Robust Information Extraction on the Web","comments":"ACL 2023 Main Conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Extracting structured and grounded fact triples from raw text is a\nfundamental task in Information Extraction (IE). Existing IE datasets are\ntypically collected from Wikipedia articles, using hyperlinks to link entities\nto the Wikidata knowledge base. However, models trained only on Wikipedia have\nlimitations when applied to web domains, which often contain noisy text or text\nthat does not have any factual information. We present WebIE, the first\nlarge-scale, entity-linked closed IE dataset consisting of 1.6M sentences\nautomatically collected from the English Common Crawl corpus. WebIE also\nincludes negative examples, i.e. sentences without fact triples, to better\nreflect the data on the web. We annotate ~25K triples from WebIE through\ncrowdsourcing and introduce mWebIE, a translation of the annotated set in four\nother languages: French, Spanish, Portuguese, and Hindi. We evaluate the\nin-domain, out-of-domain, and zero-shot cross-lingual performance of generative\nIE models and find models trained on WebIE show better generalisability. We\nalso propose three training strategies that use entity linking as an auxiliary\ntask. Our experiments show that adding Entity-Linking objectives improves the\nfaithfulness of our generative IE models.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:37:53 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14294","submitter":"Alessandro Sinibaldi","authors":"Alessandro Sinibaldi, Clemens Giuliani, Giuseppe Carleo, Filippo\n  Vicentini","title":"Unbiasing time-dependent Variational Monte Carlo by projected quantum\n  evolution","comments":"8+6 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.other physics.comp-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We analyze the accuracy and sample complexity of variational Monte Carlo\napproaches to simulate the dynamics of many-body quantum systems classically.\nBy systematically studying the relevant stochastic estimators, we are able to:\n(i) prove that the most used scheme, the time-dependent Variational Monte Carlo\n(tVMC), is affected by a systematic statistical bias or exponential sample\ncomplexity when the wave function contains some (possibly approximate) zeros,\nan important case for fermionic systems and quantum information protocols; (ii)\nshow that a different scheme based on the solution of an optimization problem\nat each time step is free from such problems; (iii) improve the sample\ncomplexity of this latter approach by several orders of magnitude with respect\nto previous proofs of concept. Finally, we apply our advancements to study the\nhigh-entanglement phase in a protocol of non-Clifford unitary dynamics with\nlocal random measurements in 2D, first benchmarking on small spin lattices and\nthen extending to large systems.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:38:10 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14295","submitter":"Francesco Giovanni Celiberto","authors":"Francesco Giovanni Celiberto","title":"Vector quarkonia at the LHC with JETHAD: A high-energy viewpoint","comments":"38 pages, 8 figures, 473 references. Invited review article","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph hep-ex nucl-ex nucl-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this review we discuss and extend the study of the inclusive production of\nvector quarkonia, $J/\\psi$ and $\\Upsilon$, emitted with large transverse\nmomenta and rapidities at the LHC. We adopt the novel ZCW19$^+$ determination\nto depict the quarkonium production mechanism at the next-to-leading level of\nperturbative QCD. This approach is based on the nonrelativistic QCD formalism\nwell adapted to describe the production of a quarkonium state from the\ncollinear fragmentation of a gluon or a constituent heavy quark at the lowest\nenergy scale. We rely upon the NLL/NLO$^+$ hybrid high-energy and collinear\nfactorization for differential cross sections, where the standard collinear\nformalism is enhanced by the BFKL resummation of next-to-leading energy\nlogarithms arising in the $t$-channel. We employ the JETHAD method to analyze\nthe behavior of rapidity distributions for double inclusive vector-quarkonium\nand inclusive vector-quarkonium plus jet emissions. We discovered that the\nnatural stability of the high-energy series, previously observed in observables\nsensitive to the emission of hadrons with heavy flavor detected in the rapidity\nacceptance of LHC barrel calorimeters, becomes even more manifest when these\nparticles are tagged in forward regions covered by endcaps. Our findings brace\nthe important message that vector quarkonia at the LHC via the hybrid\nfactorization offer a unique chance to perform precision studies of high-energy\nQCD, as well as an intriguing opportunity to shed light on the quarkonium\nproduction puzzle.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:39:36 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14296","submitter":"Kundan Krishna","authors":"Kundan Krishna, Prakhar Gupta, Sanjana Ramprasad, Byron C. Wallace,\n  Jeffrey P. Bigham, Zachary C. Lipton","title":"USB: A Unified Summarization Benchmark Across Tasks and Domains","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  An abundance of datasets exist for training and evaluating models on the task\nof summary generation.However, these datasets are often derived heuristically,\nand lack sufficient annotations to support research into all aspects of\nsummarization, such as evidence extraction and controllable summarization. We\nintroduce a benchmark comprising 8 tasks that require multi-dimensional\nunderstanding of summarization, e.g., surfacing evidence for a summary,\nassessing its correctness, and gauging its relevance to different topics. We\ncompare various methods on this benchmark and discover that on multiple tasks,\nmoderately-sized fine-tuned models consistently outperform much larger few-shot\nprompted language models. For factuality related tasks, we also evaluate\nexisting heuristics to create training data and find that training on them\nperforms worse than training on $20\\times$ less human-labeled data. Our\nbenchmark consists of data from 6 different domains, allowing us to study\ncross-domain performance of trained models. We find that for some tasks, the\namount of training data matters more than the domain where it comes from, while\nfor other tasks training specifically on data from the target domain, even if\nlimited, is more beneficial. Our work fulfills the need for a well-annotated\nsummarization benchmark with diverse tasks, and provides useful insights about\nthe impact of the quality, size and domain of training data.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:39:54 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14297","submitter":"Thomas Izgin","authors":"Thomas Izgin and David I. Ketcheson and Andreas Meister","title":"Order conditions for Runge--Kutta-like methods with solution-dependent\n  coefficients","comments":"22 pages, 0 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In recent years, many positivity-preserving schemes for initial value\nproblems have been constructed by modifying a Runge--Kutta (RK) method by\nweighting the right-hand side of the system of differential equations with\nsolution-dependent factors. These include the classes of modified\nPatankar--Runge--Kutta (MPRK) and Geometric Conservative (GeCo) methods.\nCompared to traditional RK methods, the analysis of accuracy and stability of\nthese methods is more complicated. In this work, we provide a comprehensive and\nunifying theory of order conditions for such RK-like methods, which differ from\noriginal RK schemes in that their coefficients are solution-dependent. The\nresulting order conditions are themselves solution-dependent and obtained using\nthe theory of NB-series, and thus, can easily be read off from labeled N-trees.\nWe present for the first time order conditions for MPRK and GeCo schemes of\narbitrary order; For MPRK schemes, the order conditions are given implicitly in\nterms of the stages. From these results, we recover as particular cases all\nknown order conditions from the literature for first- and second-order GeCo as\nwell as first-, second- and third-order MPRK methods. Additionally, we derive\nsufficient and necessary conditions in an explicit form for 3rd and 4th order\nGeCo schemes as well as 4th order MPRK methods.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:40:08 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14298","submitter":"En Yu","authors":"En Yu, Tiancai Wang, Zhuoling Li, Yuang Zhang, Xiangyu Zhang, Wenbing\n  Tao","title":"MOTRv3: Release-Fetch Supervision for End-to-End Multi-Object Tracking","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Although end-to-end multi-object trackers like MOTR enjoy the merits of\nsimplicity, they suffer from the conflict between detection and association\nseriously, resulting in unsatisfactory convergence dynamics. While MOTRv2\npartly addresses this problem, it demands an additional detection network for\nassistance. In this work, we serve as the first to reveal that this conflict\narises from the unfair label assignment between detect queries and track\nqueries during training, where these detect queries recognize targets and track\nqueries associate them. Based on this observation, we propose MOTRv3, which\nbalances the label assignment process using the developed release-fetch\nsupervision strategy. In this strategy, labels are first released for detection\nand gradually fetched back for association. Besides, another two strategies\nnamed pseudo label distillation and track group denoising are designed to\nfurther improve the supervision for detection and association. Without the\nassistance of an extra detection network during inference, MOTRv3 achieves\nimpressive performance across diverse benchmarks, e.g., MOT17, DanceTrack.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:40:13 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14299","submitter":"Minsik Oh","authors":"Minsik Oh, Jiwei Li, Guoyin Wang","title":"TaDSE: Template-aware Dialogue Sentence Embeddings","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Learning high quality sentence embeddings from dialogues has drawn increasing\nattentions as it is essential to solve a variety of dialogue-oriented tasks\nwith low annotation cost. However, directly annotating and gathering utterance\nrelationships in conversations are difficult, while token-level annotations,\n\\eg, entities, slots and templates, are much easier to obtain. General sentence\nembedding methods are usually sentence-level self-supervised frameworks and\ncannot utilize token-level extra knowledge. In this paper, we introduce\nTemplate-aware Dialogue Sentence Embedding (TaDSE), a novel augmentation method\nthat utilizes template information to effectively learn utterance\nrepresentation via self-supervised contrastive learning framework. TaDSE\naugments each sentence with its corresponding template and then conducts\npairwise contrastive learning over both sentence and template. We further\nenhance the effect with a synthetically augmented dataset that enhances\nutterance-template relation, in which entity detection (slot-filling) is a\npreliminary step. We evaluate TaDSE performance on five downstream benchmark\ndatasets. The experiment results show that TaDSE achieves significant\nimprovements over previous SOTA methods, along with a consistent Intent\nClassification task performance improvement margin. We further introduce a\nnovel analytic instrument of Semantic Compression method, for which we discover\na correlation with uniformity and alignment. Our code will be released soon.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:40:41 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14300","submitter":"Orr Fischer","authors":"Orr Fischer, Merav Parter","title":"Distributed CONGEST Algorithms against Mobile Adversaries","comments":"Accepted to PODC23","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DS cs.DC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In their seminal PODC 1991 paper, Ostrovsky and Yung introduced the study of\ndistributed computation in the presence of mobile adversaries which can\ndynamically appear throughout the network. Over the years, this setting has\nbeen studied mostly under the assumption that the communication graph is\nfully-connected. Resilient CONGEST algorithms for general graphs, on the other\nhand, are currently known only for the classical static setting, i.e., where\nthe set of corrupted edges (or nodes) is fixed throughout the entire\ncomputation.\n  We fill this gap by providing round-efficient simulations that translate\ngiven CONGEST algorithms into equivalent algorithms that are resilient against\n$f$-mobile edge adversaries. Our main results are:\n  -Perfect-Security with Mobile Eavesdroppers: A translation of any $r$-round\n$f$-static-secure algorithm into an equivalent $\\Theta(f)$-mobile-secure\nalgorithm with $\\Theta(r)$ rounds. We also show that the $f$-static-secure\nalgorithms of [Hitron, Parter and Yogev, DISC 2022 & ITCS 2023] can be modified\ninto $f$-mobile-secure algorithms with the same number of rounds.\n  -Resilience with Mobile Byzantine Adversaries: An $f$-mobile-byzantine\nsimulation which is based on a decomposition of the graph into low-diameter\nedge-disjoint spanning trees. This provides us with near-optimal CONGEST\ncompilers for expander graphs. It also leads to near-optimal compilers in the\ncongested-clique model against $\\Theta(n)$-mobile adversaries. For general\n$(2f+1)$ edge-connected graphs with $f$-mobile adversary, we almost match the\nbounds known for the $f$-static setting, when provided a trusted pre-processing\nphase.\n  Our results are based on a collection of tools from interactive coding\n[Gelles, Found. Trends Theor. Comput. Sci. 2017], linear sketches and\nlow-congestion graph decomposition. The introduced toolkit might have further\napplications for resilient computation.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:42:29 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14301","submitter":"Fangda Li","authors":"Fangda Li, Zhiqiang Hu, Wen Chen, Avinash Kak","title":"A Laplacian Pyramid Based Generative H&E Stain Augmentation Network","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Hematoxylin and Eosin (H&E) staining is a widely used sample preparation\nprocedure for enhancing the saturation of tissue sections and the contrast\nbetween nuclei and cytoplasm in histology images for medical diagnostics.\nHowever, various factors, such as the differences in the reagents used, result\nin high variability in the colors of the stains actually recorded. This\nvariability poses a challenge in achieving generalization for machine-learning\nbased computer-aided diagnostic tools. To desensitize the learned models to\nstain variations, we propose the Generative Stain Augmentation Network (G-SAN)\n-- a GAN-based framework that augments a collection of cell images with\nsimulated yet realistic stain variations. At its core, G-SAN uses a novel and\nhighly computationally efficient Laplacian Pyramid (LP) based generator\narchitecture, that is capable of disentangling stain from cell morphology.\nThrough the task of patch classification and nucleus segmentation, we show that\nusing G-SAN-augmented training data provides on average 15.7% improvement in F1\nscore and 7.3% improvement in panoptic quality, respectively. Our code is\navailable at https://github.com/lifangda01/GSAN-Demo.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:43:18 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14302","submitter":"Yongfeng Zhang","authors":"Shijie Geng and Juntao Tan and Shuchang Liu and Zuohui Fu and Yongfeng\n  Zhang","title":"VIP5: Towards Multimodal Foundation Models for Recommendation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR cs.AI cs.HC cs.LG cs.MM","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Computer Vision (CV), Natural Language Processing (NLP), and Recommender\nSystems (RecSys) are three prominent AI applications that have traditionally\ndeveloped independently, resulting in disparate modeling and engineering\nmethodologies. This has impeded the ability for these fields to directly\nbenefit from each other's advancements. With the increasing availability of\nmultimodal data on the web, there is a growing need to consider various\nmodalities when making recommendations for users. With the recent emergence of\nfoundation models, large language models have emerged as a potential\ngeneral-purpose interface for unifying different modalities and problem\nformulations. In light of this, we propose the development of a multimodal\nfoundation model by considering both visual and textual modalities under the P5\nrecommendation paradigm (VIP5) to unify various modalities and recommendation\ntasks. This will enable the processing of vision, language, and personalization\ninformation in a shared architecture for improved recommendations. To achieve\nthis, we introduce multimodal personalized prompts to accommodate multiple\nmodalities under a shared format. Additionally, we propose a\nparameter-efficient training method for foundation models, which involves\nfreezing the backbone and fine-tuning lightweight adapters, resulting in\nimproved recommendation performance and increased efficiency in terms of\ntraining time and memory usage.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:43:46 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14303","submitter":"Yilun Zhao","authors":"Yilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, Yixin Liu, Weijin Zou,\n  Simeng Han, Xiangru Tang, Yumo Xu, Arman Cohan, Dragomir Radev","title":"QTSumm: A New Benchmark for Query-Focused Table Summarization","comments":"work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  People primarily consult tables to conduct data analysis or answer specific\nquestions. Text generation systems that can provide accurate table summaries\ntailored to users' information needs can facilitate more efficient access to\nrelevant data insights. However, existing table-to-text generation studies\nprimarily focus on converting tabular data into coherent statements, rather\nthan addressing information-seeking purposes. In this paper, we define a new\nquery-focused table summarization task, where text generation models have to\nperform human-like reasoning and analysis over the given table to generate a\ntailored summary, and we introduce a new benchmark named QTSumm for this task.\nQTSumm consists of 5,625 human-annotated query-summary pairs over 2,437 tables\non diverse topics. Moreover, we investigate state-of-the-art models (i.e., text\ngeneration, table-to-text generation, and large language models) on the QTSumm\ndataset. Experimental results and manual analysis reveal that our benchmark\npresents significant challenges in table-to-text generation for future\nresearch.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:43:51 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14304","submitter":"Jianxin Chen","authors":"Fang Zhang, Xing Zhu, Rui Chao, Cupjin Huang, Linghang Kong, Guoyang\n  Chen, Dawei Ding, Haishan Feng, Yihuai Gao, Xiaotong Ni, Liwei Qiu, Zhe Wei,\n  Yueming Yang, Yang Zhao, Yaoyun Shi, Weifeng Zhang, Peng Zhou, Jianxin Chen","title":"A Classical Architecture For Digital Quantum Computers","comments":"12 pages, 12 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cs.AR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Scaling bottlenecks the making of digital quantum computers, posing\nchallenges from both the quantum and the classical components. We present a\nclassical architecture to cope with a comprehensive list of the latter\nchallenges {\\em all at once}, and implement it fully in an end-to-end system by\nintegrating a multi-core RISC-V CPU with our in-house control electronics.\n  Our architecture enables scalable, high-precision control of large quantum\nprocessors and accommodates evolving requirements of quantum hardware. A\ncentral feature is a microarchitecture executing quantum operations in parallel\non arbitrary predefined qubit groups. Another key feature is a reconfigurable\nquantum instruction set that supports easy qubit re-grouping and instructions\nextensions.\n  As a demonstration, we implement the widely-studied surface code quantum\ncomputing workflow, which is instructive for being demanding on both the\ncontrollers and the integrated classical computation. Our design, for the first\ntime, reduces instruction issuing and transmission costs to constants, which do\nnot scale with the number of qubits, without adding any overheads in decoding\nor dispatching.\n  Rather than relying on specialized hardware for syndrome decoding, our system\nuses a dedicated multi-core CPU for both qubit control and classical\ncomputation, including syndrome decoding. This simplifies the system design and\nfacilitates load-balancing between the quantum and classical components. We\nimplement recent proposals as decoding firmware on a RISC-V system-on-chip\n(SoC) that parallelizes general inner decoders. By using our in-house\nUnion-Find and PyMatching 2 implementations, we can achieve unprecedented\ndecoding capabilities of up to distances 47 and 67 with the currently available\nSoCs, under realistic and optimistic assumptions of physical error rate\n$p=0.001 and p=0.0001, respectively, all in just 1 \\textmu s.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:44:06 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14305","submitter":"Genly Le\\'on","authors":"Esteban Gonz\\'alez, Kimet Jusufi, Genly Leon and Emmanuel N. Saridakis","title":"Observational Constraints on Yukawa Cosmology and Connection with Black\n  Hole Shadows","comments":"11 pages, 4 figures. Minor changes","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.CO gr-qc hep-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We confront Yukawa modified cosmology, proposed in arXiv:2304.11492 by Jusufi\net al., with data from Supernovae Type Ia (SNe Ia) and Hubble parameter (OHD)\nobservations. Yukawa cosmology is obtained from a Yukawa-like gravitational\npotential, with coupling parameter $\\alpha$ and wavelength parameter $\\lambda$,\nwhich gives rise to modified Friedmann equations. We show that the agreement\nwith observations is very efficient, and within $1\\sigma$ confidence level we\nfind the best-fit parameters $\\lambda=2693_{-1262}^{+1191}\\, \\rm Mpc$, and\n$\\alpha=0.416_{-0.326}^{+1.137}$, and a graviton mass of approximately $m_g\n\\simeq 5.6\\times 10^{-43}$ GeV. Additionally, we establish a connection between\nthe effective dark matter and dark energy density parameters and the angular\nradius of the black hole shadow of the SgrA and M87 black holes in the\nlow-redshift limit, which is consistent with the findings of the Event Horizon\nTelescope.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:45:06 GMT"},{"version":"v2","created":"Tue, 30 May 2023 16:41:37 GMT"}],"update_date":"2023-05-31"}
{"id":"2305.14306","submitter":"Junyuan Ouyang","authors":"Junyuan Ouyang and Xiao Liu and Haoyao Chen","title":"Hierarchical Adaptive Voxel-guided Sampling for Real-time Applications\n  in Large-scale Point Clouds","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  While point-based neural architectures have demonstrated their efficacy, the\ntime-consuming sampler currently prevents them from performing real-time\nreasoning on scene-level point clouds. Existing methods attempt to overcome\nthis issue by using random sampling strategy instead of the commonly-adopted\nfarthest point sampling~(FPS), but at the expense of lower performance. So the\neffectiveness/efficiency trade-off remains under-explored. In this paper, we\nreveal the key to high-quality sampling is ensuring an even spacing between\npoints in the subset, which can be naturally obtained through a grid. Based on\nthis insight, we propose a hierarchical adaptive voxel-guided point sampler\nwith linear complexity and high parallelization for real-time applications.\nExtensive experiments on large-scale point cloud detection and segmentation\ntasks demonstrate that our method achieves competitive performance with the\nmost powerful FPS, at an amazing speed that is more than 100 times faster. This\nbreakthrough in efficiency addresses the bottleneck of the sampling step when\nhandling scene-level point clouds. Furthermore, our sampler can be easily\nintegrated into existing models and achieves a 20$\\sim$80\\% reduction in\nruntime with minimal effort. The code will be available at\nhttps://github.com/OuyangJunyuan/pointcloud-3d-detector-tensorrt\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:45:49 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14307","submitter":"Robert Morabito","authors":"Robert Morabito, Jad Kabbara, Ali Emami","title":"Debiasing should be Good and Bad: Measuring the Consistency of Debiasing\n  Techniques in Language Models","comments":"9 pages (excluding references), accepted at ACL Findings 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Debiasing methods that seek to mitigate the tendency of Language Models (LMs)\nto occasionally output toxic or inappropriate text have recently gained\ntraction. In this paper, we propose a standardized protocol which distinguishes\nmethods that yield not only desirable results, but are also consistent with\ntheir mechanisms and specifications. For example, we ask, given a debiasing\nmethod that is developed to reduce toxicity in LMs, if the definition of\ntoxicity used by the debiasing method is reversed, would the debiasing results\nalso be reversed? We used such considerations to devise three criteria for our\nnew protocol: Specification Polarity, Specification Importance, and Domain\nTransferability. As a case study, we apply our protocol to a popular debiasing\nmethod, Self-Debiasing, and compare it to one we propose, called Instructive\nDebiasing, and demonstrate that consistency is as important an aspect to\ndebiasing viability as is simply a desirable result. We show that our protocol\nprovides essential insights into the generalizability and interpretability of\ndebiasing methods that may otherwise go overlooked.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:45:54 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14308","submitter":"Haldun \\\"Ozg\\\"ur Bay{\\i}nd{\\i}r","authors":"Haldun \\\"Ozg\\\"ur Bay{\\i}nd{\\i}r","title":"Algebraic $K$-theory of the two-periodic first Morava $K$-theory","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.AT math.KT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Using the root adjunction formalism developed in an earlier work and\nlogarithmic THH, we obtain a simplified computation of $T(2)_*\\text{K}(ku)$ for\n$p>3$. Our computational methods also provide $T(2)_*\\text{K}(ku/p)$, where\n$ku/p$ is the $2$-periodic Morava $K$-theory spectrum of height $1$.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:46:14 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14309","submitter":"Yongtao Liu","authors":"Yongtao Liu, Anna N. Morozovska, Ayana Ghosh, Kyle P. Kelley, Eugene\n  A. Eliseev, Jinyuan Yao, Ying Liu, and Sergei V. Kalinin","title":"Disentangling stress and curvature effects in layered 2D ferroelectric\n  CuInP2S6","comments":"20 pages; 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Nanoscale ferroelectric 2D materials offer unique opportunity to investigate\ncurvature and strain effects on materials functionalities. Among these,\nCuInP2S6 (CIPS) has attracted tremendous research interest in recent years due\nto combination of room temperature ferroelectricity, scalability to a few\nlayers thickness, and unique ferrielectric properties due to coexistence of 2\npolar sublattices. Here, we explore the local curvature and strain effect on\nthe polarization in CIPS via piezoresponse force microscopy and spectroscopy.\nTo explain the observed behaviors and decouple the curvature and strain effects\nin 2D CIPS, we introduce finite element Landau-Ginzburg-Devonshire model. The\nresults show that bending induces ferrielectric domains in CIPS, and the\npolarization-voltage hysteresis loops differ in bending and non-bending\nregions. Our simulation indicates that the flexoelectric effect can affect\nlocal polarization hysteresis. These studies open a novel pathway for the\nfabrication of curvature-engineered nanoelectronic devices.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:48:08 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14310","submitter":"Yida Mu","authors":"Yida Mu, Ben P. Wu, William Thorne, Ambrose Robinson, Nikolaos\n  Aletras, Carolina Scarton, Kalina Bontcheva, Xingyi Song","title":"Navigating Prompt Complexity for Zero-Shot Classification: A Study of\n  Large Language Models in Computational Social Science","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Instruction-tuned Large Language Models (LLMs) have exhibited impressive\nlanguage understanding and the capacity to generate responses that follow\nspecific instructions. However, due to the computational demands associated\nwith training these models, their applications often rely on zero-shot\nsettings. In this paper, we evaluate the zero-shot performance of two publicly\naccessible LLMs, ChatGPT and OpenAssistant, in the context of Computational\nSocial Science classification tasks, while also investigating the effects of\nvarious prompting strategies. Our experiment considers the impact of prompt\ncomplexity, including the effect of incorporating label definitions into the\nprompt, using synonyms for label names, and the influence of integrating past\nmemories during the foundation model training. The findings indicate that in a\nzero-shot setting, the current LLMs are unable to match the performance of\nsmaller, fine-tuned baseline transformer models (such as BERT). Additionally,\nwe find that different prompting strategies can significantly affect\nclassification accuracy, with variations in accuracy and F1 scores exceeding\n10%.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:48:21 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14311","submitter":"Grigoris Velegkas","authors":"Alkis Kalavasis, Amin Karbasi, Shay Moran, Grigoris Velegkas","title":"Statistical Indistinguishability of Learning Algorithms","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.DS stat.ML","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  When two different parties use the same learning rule on their own data, how\ncan we test whether the distributions of the two outcomes are similar? In this\npaper, we study the similarity of outcomes of learning rules through the lens\nof the Total Variation (TV) distance of distributions. We say that a learning\nrule is TV indistinguishable if the expected TV distance between the posterior\ndistributions of its outputs, executed on two training data sets drawn\nindependently from the same distribution, is small. We first investigate the\nlearnability of hypothesis classes using TV indistinguishable learners. Our\nmain results are information-theoretic equivalences between TV\nindistinguishability and existing algorithmic stability notions such as\nreplicability and approximate differential privacy. Then, we provide\nstatistical amplification and boosting algorithms for TV indistinguishable\nlearners.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:49:56 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14312","submitter":"Tsu-Jui Fu","authors":"Tsu-Jui Fu and Wenhan Xiong and Yixin Nie and Jingyu Liu and Barlas\n  O\\u{g}uz and William Yang Wang","title":"Text-guided 3D Human Generation from 2D Collections","comments":"Project website: https://text-3dh.github.io/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  3D human modeling has been widely used for engaging interaction in gaming,\nfilm, and animation. The customization of these characters is crucial for\ncreativity and scalability, which highlights the importance of controllability.\nIn this work, we introduce Text-guided 3D Human Generation (\\texttt{T3H}),\nwhere a model is to generate a 3D human, guided by the fashion description.\nThere are two goals: 1) the 3D human should render articulately, and 2) its\noutfit is controlled by the given text. To address this \\texttt{T3H} task, we\npropose Compositional Cross-modal Human (CCH). CCH adopts cross-modal attention\nto fuse compositional human rendering with the extracted fashion semantics.\nEach human body part perceives relevant textual guidance as its visual\npatterns. We incorporate the human prior and semantic discrimination to enhance\n3D geometry transformation and fine-grained consistency, enabling it to learn\nfrom 2D collections for data efficiency. We conduct evaluations on DeepFashion\nand SHHQ with diverse fashion attributes covering the shape, fabric, and color\nof upper and lower clothing. Extensive experiments demonstrate that CCH\nachieves superior results for \\texttt{T3H} with high efficiency.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:50:15 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14313","submitter":"Katharina Laubscher","authors":"Katharina Laubscher, Jay D. Sau, and Sankar Das Sarma","title":"Majorana zero modes in gate-defined germanium hole nanowires","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We theoretically study gate-defined one-dimensional channels in planar Ge\nhole gases as a potential platform for non-Abelian Majorana zero modes. We\nmodel the valence band holes in the Ge channel by adding appropriate\nconfinement potentials to the 3D Luttinger-Kohn Hamiltonian, additionally\ntaking into account a magnetic field applied parallel to the channel, an\nout-of-plane electric field, as well as the effect of compressive strain in the\nparent quantum well. Assuming that the Ge channel is proximitized by an\n$s$-wave superconductor (such as, e.g., Al) we calculate the topological phase\ndiagrams for different channel geometries, showing that sufficiently narrow Ge\nhole channels can indeed enter a topological superconducting phase with\nMajorana zero modes at the channel ends. We estimate the size of the\ntopological gap and its dependence on various system parameters such as channel\nwidth, strain, and the applied out-of-plane electric field, allowing us to\ncritically discuss under which conditions Ge hole channels may manifest\nMajorana zero modes. Since ultra-clean Ge quantum wells with hole mobilities\nexceeding one million and mean-free paths on the order of many microns already\nexist, gate-defined Ge hole channels may be able to overcome some of the\nproblems caused by the presence of substantial disorder in more conventional\nMajorana platforms.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:50:30 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14314","submitter":"Tim Dettmers","authors":"Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer","title":"QLoRA: Efficient Finetuning of Quantized LLMs","comments":"Extended NeurIPS submission","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We present QLoRA, an efficient finetuning approach that reduces memory usage\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\na frozen, 4-bit quantized pretrained language model into Low Rank\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\nsingle GPU. QLoRA introduces a number of innovations to save memory without\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\ninformation theoretically optimal for normally distributed weights (b) double\nquantization to reduce the average memory footprint by quantizing the\nquantization constants, and (c) paged optimziers to manage memory spikes. We\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\ninstruction following and chatbot performance across 8 instruction datasets,\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\nshow that QLoRA finetuning on a small high-quality dataset leads to\nstate-of-the-art results, even when using smaller models than the previous\nSoTA. We provide a detailed analysis of chatbot performance based on both human\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\nalternative to human evaluation. Furthermore, we find that current chatbot\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\nChatGPT. We release all of our models and code, including CUDA kernels for\n4-bit training.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:50:33 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14315","submitter":"Maximilian F. Steffen","authors":"Maximilian F. Steffen","title":"Estimating a multivariate L\\'evy density based on discrete observations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.ST stat.TH","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Existing results for the estimation of the L\\'evy measure are mostly limited\nto the onedimensional setting. We apply the spectral method to multidimensional\nL\\'evy processes in order to construct a nonparametric estimator for the\nmultivariate jump distribution. We prove convergence rates for the uniform\nestimation error under both a low- and a high-frequency observation regime. The\nmethod is robust to various dependence structures. Along the way, we present a\nuniform risk bound for the multivariate empirical characteristic function and\nits partial derivatives. The method is illustrated with simulation examples.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:51:00 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14316","submitter":"Timo Peltola","authors":"N. Akchurin, G. Altopp, B. Burkle, W. D. Frey, U. Heintz, N. Hinton,\n  M. Hoeferkamp, Y. Kazhykarim, V. Kuryatkov, T. Mengke, T. Peltola, S. Seidel,\n  E. Spencer, M. Tripathi, J. Voelker","title":"Modeling of Surface Damage at the Si/SiO$_2$-interface of Irradiated\n  MOS-capacitors","comments":"Corresponding author: T. Peltola. 24 pages, 17 figures, 6 tables","journal-ref":null,"doi":null,"report-no":"APDL-2023-001","categories":"physics.ins-det","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  Surface damage caused by ionizing radiation in SiO$_2$ passivated silicon\nparticle detectors consists mainly of the accumulation of a positively charged\nlayer along with trapped-oxide-charge and interface traps inside the oxide and\nclose to the Si/SiO$_2$-interface. High density positive interface net charge\ncan be detrimental to the operation of a multi-channel $n$-on-$p$ sensor since\nthe inversion layer generated under the Si/SiO$_2$-interface can cause loss of\nposition resolution by creating a conduction channel between the electrodes. In\nthe investigation of the radiation-induced accumulation of oxide charge and\ninterface traps, a capacitance-voltage characterization study of n/$\\gamma$-\nand $\\gamma$-irradiated Metal-Oxide-Semiconductor (MOS) capacitors showed that\nclose agreement between measurement and simulation were possible when oxide\ncharge density was complemented by both acceptor- and donor-type deep interface\ntraps with densities comparable to the oxide charges. Corresponding inter-strip\nresistance simulations of a $n$-on-$p$ sensor with the tuned oxide charge\ndensity and interface traps show close agreement with experimental results. The\nbeneficial impact of radiation-induced accumulation of deep interface traps on\ninter-electrode isolation may be considered in the optimization of the\nprocessing parameters of isolation implants on $n$-on-$p$ sensors for the\nextreme radiation environments.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:51:06 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14317","submitter":"Kate Napier","authors":"Kate Napier and Mike Gladders and Keren Sharon and H{\\aa}kon Dahle and\n  Aidan P. Cloonan and Guillaume Mahler and Isaiah Escapa and Josh Garza and\n  Andrew Kisare and Natalie Malagon and Simon Mork and Kunwanhui Niu and Riley\n  Rosener and Jamar Sullivan Jr. and Marie Tagliavia and Marcos Tamargo and\n  Raul Teixeira and Kabelo Tsiane and Grace Wagner and Yunchong Zhang and Megan\n  Zhao","title":"COOL-LAMPS. V. Discovery of COOL J0335$-$1927, a Gravitationally Lensed\n  Quasar at $z$=3.27 with an Image Separation of 23.3\"","comments":"8 pages, 4 figures, 1 table","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.GA astro-ph.CO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We report the discovery of COOL J0335$-$1927, a quasar at $z$ = 3.27 lensed\ninto three images with a maximum separation of 23.3\" by a galaxy cluster at $z$\n= 0.4178. We construct a parametric strong gravitational lens model using\nground-based imaging, constrained by the redshift and positions of the quasar\nimages as well as the positions of three other multiply-imaged background\ngalaxies. Using our best-fit lens model, we calculate the predicted time delays\nbetween the three quasar images to be $\\Delta$t$_{AB}=$ $241^{+41}_{-12}$ and\n$\\Delta$t$_{AC}=$ $-64^{+3}_{-33}$ days. We also present g-band photometry from\narchival DECaLS imaging, and new multi-epoch observations obtained between\nSeptember 18, 2022 UT and February 22, 2023 UT, which demonstrate significant\nvariability in the quasar and which will eventually enable a measurement of the\ntime delay between the three quasar images.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:51:10 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14318","submitter":"Cheng Qian","authors":"Cheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, Heng Ji","title":"CREATOR: Disentangling Abstract and Concrete Reasonings of Large\n  Language Models through Tool Creation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Large Language Models (LLMs) have demonstrated significant progress in\nutilizing external APIs as tools for various tasks. However, their tool-using\nability is limited by the availability of suitable APIs and the instability of\nimplicit reasoning, particularly when simultaneously engaging in reasoning\nabout plans and actual calculations. To address these limitations, we propose\nCREATOR, a novel framework that empowers LLMs to create their own tools through\ndocumentation and code realization. CREATOR disentangles the LLM's ability into\ntwo distinct phases: abstract tool creation and concrete decision execution,\nwhich results in improved LLM performance. We evaluate CREATOR on two\nestablished benchmarks: MATH, which consists of challenging math competition\nproblems, and TabMWP, which includes diverse tabular contents for\nproblem-solving. Remarkably, CREATOR significantly outperforms existing\nchain-of-thought (CoT), program-of-thought (PoT), and tool-using baselines on\nthese two benchmarks. Additionally, we present a new dataset, Creation\nChallenge, comprising 2K diverse questions, to highlight the necessity and\nbenefits of LLMs' tool creation ability in effectively addressing these\nproblems. Furthermore, our research reveals that leveraging LLMs as tool\ncreators facilitates knowledge transfer, and LLMs exhibit varying levels of\ntool creation abilities, enabling them to flexibly tackle diverse situations.\nOur study represents a promising avenue for maximizing the potential of LLMs\nand advancing toward truly intelligent and adaptable AI systems.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:51:52 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14319","submitter":"Thomas Trogdon","authors":"Thomas Trogdon","title":"On the convergence of spectral methods involving non-compact operators","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA math.FA math.SP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Motivated by Fredholm theory, we develop a framework to establish the\nconvergence of spectral methods for operator equations $\\mathcal L u = f$. The\nframework posits the existence of a left-Fredholm regulator for $\\mathcal L$\nand the existence of a sufficiently good approximation of this regulator.\nImportantly, the numerical method itself need not make use of this extra\napproximant. We apply the framework to finite-section and collocation-based\nnumerical methods for solving differential equations with periodic boundary\nconditions and to solving Riemann--Hilbert problems on the unit circle. We also\nobtain improved results concerning the approximation of eigenvalues of\ndifferential operators with periodic coefficients.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:53:12 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14320","submitter":"Vivek Nair","authors":"Vivek Nair, Viktor Radulov, James F. O'Brien","title":"Results of the 2023 Census of Beat Saber Users: Virtual Reality Gaming\n  Population Insights and Factors Affecting Virtual Reality E-Sports\n  Performance","comments":"for interactive version, see https://www.beatleader.xyz/census2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The emergence of affordable standalone virtual reality (VR) devices has\nallowed VR technology to reach mass-market adoption in recent years, driven\nprimarily by the popularity of VR gaming applications such as Beat Saber.\nHowever, despite being the top-grossing VR application to date and the most\npopular VR e-sport, the population of over 6 million Beat Saber users has not\nyet been widely studied. In this report, we present a large-scale comprehensive\nsurvey of Beat Saber players (N=1,006) that sheds light on several important\naspects of this population, including their background, biometrics,\ndemographics, health information, behavioral patterns, and technical device\nspecifications. We further provide insights into the emerging field of VR\ne-sports by analyzing correlations between responses and an authoritative\nmeasure of in-game performance.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:53:29 GMT"},{"version":"v2","created":"Tue, 30 May 2023 04:10:11 GMT"}],"update_date":"2023-05-31"}
{"id":"2305.14321","submitter":"William Brannon","authors":"William Brannon, Suyash Fulay, Hang Jiang, Wonjune Kang, Brandon Roy,\n  Jad Kabbara, Deb Roy","title":"ConGraT: Self-Supervised Contrastive Pretraining for Joint Graph and\n  Text Embeddings","comments":"3 figures, 9 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We propose ConGraT(Contrastive Graph-Text pretraining), a general,\nself-supervised method for jointly learning separate representations of texts\nand nodes in a parent (or ``supervening'') graph, where each text is associated\nwith one of the nodes. Datasets fitting this paradigm are common, from social\nmedia (users and posts), to citation networks over articles, to link graphs\nover web pages. We expand on prior work by providing a general,\nself-supervised, joint pretraining method, one which does not depend on\nparticular dataset structure or a specific task. Our method uses two separate\nencoders for graph nodes and texts, which are trained to align their\nrepresentations within a common latent space. Training uses a batch-wise\ncontrastive learning objective inspired by prior work on joint text and image\nencoding. As graphs are more structured objects than images, we also extend the\ntraining objective to incorporate information about node similarity and\nplausible next guesses in matching nodes and texts. Experiments on various\ndatasets reveal that ConGraT outperforms strong baselines on various downstream\ntasks, including node and text category classification and link prediction.\nCode and certain datasets are available at\nhttps://github.com/wwbrannon/congrat.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:53:30 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14322","submitter":"Ali Modarressi","authors":"Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, Hinrich Sch\\\"utze","title":"RET-LLM: Towards a General Read-Write Memory for Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP) through their extensive parameters and comprehensive\ndata utilization. However, existing LLMs lack a dedicated memory unit, limiting\ntheir ability to explicitly store and retrieve knowledge for various tasks. In\nthis paper, we propose RET-LLM a novel framework that equips LLMs with a\ngeneral write-read memory unit, allowing them to extract, store, and recall\nknowledge from the text as needed for task performance. Inspired by Davidsonian\nsemantics theory, we extract and save knowledge in the form of triplets. The\nmemory unit is designed to be scalable, aggregatable, updatable, and\ninterpretable. Through qualitative evaluations, we demonstrate the superiority\nof our proposed framework over baseline approaches in question answering tasks.\nMoreover, our framework exhibits robust performance in handling temporal-based\nquestion answering tasks, showcasing its ability to effectively manage\ntime-dependent information.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:53:38 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14323","submitter":"Zhipeng Chen","authors":"Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao and\n  Ji-Rong Wen","title":"ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large\n  Language Models","comments":"11 pages, working in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Although large language models (LLMs) have achieved excellent performance in\na variety of evaluation benchmarks, they still struggle in complex reasoning\ntasks which require specific knowledge and multi-hop reasoning. To improve the\nreasoning abilities, we propose \\textbf{ChatCoT}, a tool-augmented\nchain-of-thought reasoning framework for chat-based LLMs. In ChatCoT, we model\nthe chain-of-thought~(CoT) reasoning as multi-turn conversations, to utilize\ntools in a more natural way through chatting. At each turn, LLMs can either\ninteract with tools or perform the reasoning. Our approach can effectively\nleverage the multi-turn conversation ability of chat-based LLMs, and integrate\nthe thought chain following and tools manipulation in a unified way. Specially,\nwe initialize the early turns of the conversation by the tools, tasks and\nreasoning format, and propose an iterative \\emph{tool-augmented reasoning} step\nto perform step-by-step tool-augmented reasoning. The experiment results on two\ncomplex reasoning datasets (MATH and HotpotQA) have shown the effectiveness of\nChatCoT on complex reasoning tasks, achieving a 6.8\\% relative improvement over\nthe state-of-the-art baseline. Our code and data are available at:\n\\url{https://github.com/RUCAIBOX/ChatCoT}.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:54:33 GMT"},{"version":"v2","created":"Wed, 24 May 2023 11:40:59 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14324","submitter":"Daniel Deutsch","authors":"Daniel Deutsch and George Foster and Markus Freitag","title":"Ties Matter: Modifying Kendall's Tau for Modern Metric Meta-Evaluation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Kendall's tau is frequently used to meta-evaluate how well machine\ntranslation (MT) evaluation metrics score individual translations. Its focus on\npairwise score comparisons is intuitive but raises the question of how ties\nshould be handled, a gray area that has motivated different variants in the\nliterature. We demonstrate that, in settings like modern MT meta-evaluation,\nexisting variants have weaknesses arising from their handling of ties, and in\nsome situations can even be gamed. We propose a novel variant that gives\nmetrics credit for correctly predicting ties, as well as an optimization\nprocedure that automatically introduces ties into metric scores, enabling fair\ncomparison between metrics that do and do not predict ties. We argue and\nprovide experimental evidence that these modifications lead to fairer\nKendall-based assessments of metric performance.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:54:57 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14325","submitter":"Yilun Du","authors":"Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, Igor\n  Mordatch","title":"Improving Factuality and Reasoning in Language Models through Multiagent\n  Debate","comments":"Project Webpage and Code:\n  https://composable-models.github.io/llm_debate/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CV cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large language models (LLMs) have demonstrated remarkable capabilities in\nlanguage generation, understanding, and few-shot learning in recent years. An\nextensive body of work has explored how their performance may be further\nimproved through the tools of prompting, ranging from verification,\nself-consistency, or intermediate scratchpads. In this paper, we present a\ncomplementary approach to improve language responses where multiple language\nmodel instances propose and debate their individual responses and reasoning\nprocesses over multiple rounds to arrive at a common final answer. Our findings\nindicate that this approach significantly enhances mathematical and strategic\nreasoning across a number of tasks. We also demonstrate that our approach\nimproves the factual validity of generated content, reducing fallacious answers\nand hallucinations that contemporary models are prone to. Our approach may be\ndirectly applied to existing black-box models and uses identical procedure and\nprompts for all tasks we investigate. Overall, our findings suggest that such\n\"society of minds\" approach has the potential to significantly advance the\ncapabilities of LLMs and pave the way for further breakthroughs in language\ngeneration and understanding.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:55:11 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14326","submitter":"Chan Young Park","authors":"Lucille Njoo, Chan Young Park, Octavia Stappart, Marvin Thielk, Yi Chu\n  and Yulia Tsvetkov","title":"TalkUp: A Novel Dataset Paving the Way for Understanding Empowering\n  Language","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Empowering language is important in many real-world contexts, from education\nto workplace dynamics to healthcare. Though language technologies are growing\nmore prevalent in these contexts, empowerment has not been studied in NLP, and\nmoreover, it is inherently challenging to operationalize because of its subtle,\nimplicit nature. This work presents the first computational exploration of\nempowering language. We first define empowerment detection as a new task,\ngrounding it in linguistic and social psychology literature. We then\ncrowdsource a novel dataset of Reddit posts labeled for empowerment, reasons\nwhy these posts are empowering to readers, and the social relationships between\nposters and readers. Our preliminary analyses show that this dataset, which we\ncall TalkUp, can be used to train language models that capture empowering and\ndisempowering language. More broadly, as it is rich with the ambiguities and\ndiverse interpretations of real-world language, TalkUp provides an avenue to\nexplore implication, presuppositions, and how social context influences the\nmeaning of language.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:55:34 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14327","submitter":"Da Yin","authors":"Da Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal, Jiawei Han,\n  Kai-Wei Chang","title":"Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Instruction tuning has emerged to enhance the capabilities of large language\nmodels (LLMs) in providing appropriate outputs based on input instructions.\nHowever, existing methods for collecting instruction-tuning data suffer from\nlimitations in scalability and affordability. In this paper, we propose\nDynosaur, a dynamic growth paradigm for instruction-tuning data curation. Built\nupon the metadata of existing NLP datasets, we generate multiple task\ninstructions applicable to various NLP datasets and determine the relevant data\nfields for constructing instruction-tuning data with LLMs. Dynosaur offers\nseveral advantages: 1) lower generation costs (less than $12 for generating\n800K instruction-tuning data), 2) good quality of instruction-tuning data\n(better performance than Alpaca and Instruction GPT-4 on Super-NI with\ncomparable data sizes), and 3) the ability to grow dynamically by incorporating\nnew datasets from Huggingface Datasets Platform. We further investigate\ncontinual learning as an approach to learning with the ever-growing\ninstruction-tuning dataset. We demonstrate that replay methods not only help\nmitigate forgetting issues but help generalize to unseen tasks better. As a\nnovel continual learning scenario for instruction tuning, selecting tasks based\non instruction representations can be an effective replaying strategy. Code and\ndata are released at \\url{https://github.com/WadeYin9712/Dynosaur}.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:56:26 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14328","submitter":"Binwei Yao","authors":"Binwei Yao, Ming Jiang, Diyi Yang, Junjie Hu","title":"Empowering LLM-based Machine Translation with Cultural Awareness","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Traditional neural machine translation (NMT) systems often fail to translate\nsentences that contain culturally specific information. Most previous NMT\nmethods have incorporated external cultural knowledge during training, which\nrequires fine-tuning on low-frequency items specific to the culture. Recent\nin-context learning utilizes lightweight prompts to guide large language models\n(LLMs) to perform machine translation, however, whether such an approach works\nin terms of injecting culture awareness into machine translation remains\nunclear. To this end, we introduce a new data curation pipeline to construct a\nculturally relevant parallel corpus, enriched with annotations of\ncultural-specific entities. Additionally, we design simple but effective\nprompting strategies to assist this LLM-based translation. Extensive\nexperiments show that our approaches can largely help incorporate cultural\nknowledge into LLM-based machine translation, outperforming traditional NMT\nsystems in translating cultural-specific sentences.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:56:33 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14329","submitter":"Fivos Kalogiannis","authors":"Fivos Kalogiannis, Ioannis Panageas","title":"Zero-sum Polymatrix Markov Games: Equilibrium Collapse and Efficient\n  Computation of Nash Equilibria","comments":"Added missing proofs for the infinite-horizon","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GT cs.MA cs.SI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The works of (Daskalakis et al., 2009, 2022; Jin et al., 2022; Deng et al.,\n2023) indicate that computing Nash equilibria in multi-player Markov games is a\ncomputationally hard task. This fact raises the question of whether or not\ncomputational intractability can be circumvented if one focuses on specific\nclasses of Markov games. One such example is two-player zero-sum Markov games,\nin which efficient ways to compute a Nash equilibrium are known. Inspired by\nzero-sum polymatrix normal-form games (Cai et al., 2016), we define a class of\nzero-sum multi-agent Markov games in which there are only pairwise interactions\ndescribed by a graph that changes per state. For this class of Markov games, we\nshow that an $\\epsilon$-approximate Nash equilibrium can be found efficiently.\nTo do so, we generalize the techniques of (Cai et al., 2016), by showing that\nthe set of coarse-correlated equilibria collapses to the set of Nash\nequilibria. Afterwards, it is possible to use any algorithm in the literature\nthat computes approximate coarse-correlated equilibria Markovian policies to\nget an approximate Nash equilibrium.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:56:45 GMT"},{"version":"v2","created":"Mon, 29 May 2023 17:57:58 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.14330","submitter":"Susung Hong","authors":"Susung Hong, Junyoung Seo, Sunghwan Hong, Heeseong Shin, Seungryong\n  Kim","title":"Large Language Models are Frame-level Directors for Zero-shot\n  Text-to-Video Generation","comments":"The code and demo will be available at\n  https://github.com/KU-CVLAB/DirecT2V","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In the paradigm of AI-generated content (AIGC), there has been increasing\nattention in extending pre-trained text-to-image (T2I) models to text-to-video\n(T2V) generation. Despite their effectiveness, these frameworks face challenges\nin maintaining consistent narratives and handling rapid shifts in scene\ncomposition or object placement from a single user prompt. This paper\nintroduces a new framework, dubbed DirecT2V, which leverages instruction-tuned\nlarge language models (LLMs) to generate frame-by-frame descriptions from a\nsingle abstract user prompt. DirecT2V utilizes LLM directors to divide user\ninputs into separate prompts for each frame, enabling the inclusion of\ntime-varying content and facilitating consistent video generation. To maintain\ntemporal consistency and prevent object collapse, we propose a novel value\nmapping method and dual-softmax filtering. Extensive experimental results\nvalidate the effectiveness of the DirecT2V framework in producing visually\ncoherent and consistent videos from abstract user prompts, addressing the\nchallenges of zero-shot video generation.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:57:09 GMT"},{"version":"v2","created":"Thu, 1 Jun 2023 04:14:59 GMT"}],"update_date":"2023-06-02"}
{"id":"2305.14331","submitter":"Navita Goyal","authors":"Navita Goyal, Eleftheria Briakou, Amanda Liu, Connor Baumler, Claire\n  Bonial, Jeffrey Micher, Clare R. Voss, Marine Carpuat, Hal Daum\\'e III","title":"What Else Do I Need to Know? The Effect of Background Information on\n  Users' Reliance on AI Systems","comments":"12 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  AI systems have shown impressive performance at answering questions by\nretrieving relevant context. However, with the increasingly large models, it is\nimpossible and often undesirable to constrain models' knowledge or reasoning to\nonly the retrieved context. This leads to a mismatch between the information\nthat these models access to derive the answer and the information available to\nthe user consuming the AI predictions to assess the AI predicted answer. In\nthis work, we study how users interact with AI systems in absence of sufficient\ninformation to assess AI predictions. Further, we ask the question of whether\nadding the requisite background alleviates the concerns around over-reliance in\nAI predictions. Our study reveals that users rely on AI predictions even in the\nabsence of sufficient information needed to assess its correctness. Providing\nthe relevant background, however, helps users catch AI errors better, reducing\nover-reliance on incorrect AI predictions. On the flip side, background\ninformation also increases users' confidence in their correct as well as\nincorrect judgments. Contrary to common expectation, aiding a user's perusal of\nthe context and the background through highlights is not helpful in alleviating\nthe issue of over-confidence stemming from availability of more information.\nOur work aims to highlight the gap between how NLP developers perceive\ninformational need in human-AI interaction and the actual human interaction\nwith the information available to them.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:57:12 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14332","submitter":"Benjamin Muller","authors":"Benjamin Muller, John Wieting, Jonathan H. Clark, Tom Kwiatkowski,\n  Sebastian Ruder, Livio Baldini Soares, Roee Aharoni, Jonathan Herzig, Xinyi\n  Wang","title":"Evaluating and Modeling Attribution for Cross-Lingual Question Answering","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Trustworthy answer content is abundant in many high-resource languages and is\ninstantly accessible through question answering systems, yet this content can\nbe hard to access for those that do not speak these languages. The leap forward\nin cross-lingual modeling quality offered by generative language models offers\nmuch promise, yet their raw generations often fall short in factuality. To\nimprove trustworthiness in these systems, a promising direction is to attribute\nthe answer to a retrieved source, possibly in a content-rich language different\nfrom the query. Our work is the first to study attribution for cross-lingual\nquestion answering. First, we collect data in 5 languages to assess the\nattribution level of a state-of-the-art cross-lingual QA system. To our\nsurprise, we find that a substantial portion of the answers is not attributable\nto any retrieved passages (up to 50% of answers exactly matching a gold\nreference) despite the system being able to attend directly to the retrieved\ntext. Second, to address this poor attribution level, we experiment with a wide\nrange of attribution detection techniques. We find that Natural Language\nInference models and PaLM 2 fine-tuned on a very small amount of attribution\ndata can accurately detect attribution. Based on these models, we improve the\nattribution level of a cross-lingual question-answering system. Overall, we\nshow that current academic generative cross-lingual QA systems have substantial\nshortcomings in attribution and we build tooling to mitigate these issues.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:57:46 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14333","submitter":"Xu Zhao","authors":"Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, Qizhe Xie","title":"Automatic Model Selection with Large Language Models for Reasoning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Chain-of-Thought and Program-Aided Language Models represent two distinct\nreasoning methods, each with its own strengths and weaknesses. We demonstrate\nthat it is possible to combine the best of both worlds by using different\nmodels for different problems, employing a large language model (LLM) to\nperform model selection. Through a theoretical analysis, we discover that the\nperformance improvement is determined by the differences between the combined\nmethods and the success rate of choosing the correct model. On eight reasoning\ndatasets, our proposed approach shows significant improvements. Furthermore, we\nachieve new state-of-the-art results on GSM8K and SVAMP with accuracies of\n96.5% and 93.7%, respectively. Our code is publicly available at\nhttps://github.com/XuZhao0/Model-Selection-Reasoning.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:57:59 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14334","submitter":"Grace Luo","authors":"Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, Trevor\n  Darrell","title":"Diffusion Hyperfeatures: Searching Through Time and Space for Semantic\n  Correspondence","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Diffusion models have been shown to be capable of generating high-quality\nimages, suggesting that they could contain meaningful internal representations.\nUnfortunately, the feature maps that encode a diffusion model's internal\ninformation are spread not only over layers of the network, but also over\ndiffusion timesteps, making it challenging to extract useful descriptors. We\npropose Diffusion Hyperfeatures, a framework for consolidating multi-scale and\nmulti-timestep feature maps into per-pixel feature descriptors that can be used\nfor downstream tasks. These descriptors can be extracted for both synthetic and\nreal images using the generation and inversion processes. We evaluate the\nutility of our Diffusion Hyperfeatures on the task of semantic keypoint\ncorrespondence: our method achieves superior performance on the SPair-71k real\nimage benchmark. We also demonstrate that our method is flexible and\ntransferable: our feature aggregation network trained on the inversion features\nof real image pairs can be used on the generation features of synthetic image\npairs with unseen objects and compositions. Our code is available at\n\\url{https://diffusion-hyperfeatures.github.io}.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:58:05 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14335","submitter":"Henghui Ding","authors":"Shuting He, Xudong Jiang, Wei Jiang, Henghui Ding","title":"Prototype Adaption and Projection for Few- and Zero-shot 3D Point Cloud\n  Semantic Segmentation","comments":"IEEE TIP","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this work, we address the challenging task of few-shot and zero-shot 3D\npoint cloud semantic segmentation. The success of few-shot semantic\nsegmentation in 2D computer vision is mainly driven by the pre-training on\nlarge-scale datasets like imagenet. The feature extractor pre-trained on\nlarge-scale 2D datasets greatly helps the 2D few-shot learning. However, the\ndevelopment of 3D deep learning is hindered by the limited volume and instance\nmodality of datasets due to the significant cost of 3D data collection and\nannotation. This results in less representative features and large intra-class\nfeature variation for few-shot 3D point cloud segmentation. As a consequence,\ndirectly extending existing popular prototypical methods of 2D few-shot\nclassification/segmentation into 3D point cloud segmentation won't work as well\nas in 2D domain. To address this issue, we propose a Query-Guided Prototype\nAdaption (QGPA) module to adapt the prototype from support point clouds feature\nspace to query point clouds feature space. With such prototype adaption, we\ngreatly alleviate the issue of large feature intra-class variation in point\ncloud and significantly improve the performance of few-shot 3D segmentation.\nBesides, to enhance the representation of prototypes, we introduce a\nSelf-Reconstruction (SR) module that enables prototype to reconstruct the\nsupport mask as well as possible. Moreover, we further consider zero-shot 3D\npoint cloud semantic segmentation where there is no support sample. To this\nend, we introduce category words as semantic information and propose a\nsemantic-visual projection model to bridge the semantic and visual spaces. Our\nproposed method surpasses state-of-the-art algorithms by a considerable 7.90%\nand 14.82% under the 2-way 1-shot setting on S3DIS and ScanNet benchmarks,\nrespectively. Code is available at https://github.com/heshuting555/PAP-FZS3D.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:58:05 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14336","submitter":"Fan Bai","authors":"Fan Bai, Junmo Kang, Gabriel Stanovsky, Dayne Freitag, Alan Ritter","title":"Schema-Driven Information Extraction from Heterogeneous Tables","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper, we explore the question of whether language models (LLMs) can\nsupport cost-efficient information extraction from complex tables. We introduce\nschema-driven information extraction, a new task that uses LLMs to transform\ntabular data into structured records following a human-authored schema. To\nassess various LLM's capabilities on this task, we develop a benchmark composed\nof tables from three diverse domains: machine learning papers, chemistry\ntables, and webpages. Accompanying the benchmark, we present InstrucTE, a table\nextraction method based on instruction-tuned LLMs. This method necessitates\nonly a human-constructed extraction schema, and incorporates an error-recovery\nstrategy. Notably, InstrucTE demonstrates competitive performance without\ntask-specific labels, achieving an F1 score ranging from 72.3 to 95.7.\nMoreover, we validate the feasibility of distilling more compact table\nextraction models to minimize extraction costs and reduce API reliance. This\nstudy paves the way for the future development of instruction-following models\nfor cost-efficient table extraction.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:58:10 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14337","submitter":"Nelson F. Liu","authors":"Nelson F. Liu and Kenton Lee and Kristina Toutanova","title":"Anchor Prediction: Automatic Refinement of Internet Links","comments":"10 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.IR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Internet links enable users to deepen their understanding of a topic by\nproviding convenient access to related information. However, the majority of\nlinks are unanchored -- they link to a target webpage as a whole, and readers\nmay expend considerable effort localizing the specific parts of the target\nwebpage that enrich their understanding of the link's source context. To help\nreaders effectively find information in linked webpages, we introduce the task\nof anchor prediction, where the goal is to identify the specific part of the\nlinked target webpage that is most related to the source linking context. We\nrelease the AuthorAnchors dataset, a collection of 34K naturally-occurring\nanchored links, which reflect relevance judgments by the authors of the source\narticle. To model reader relevance judgments, we annotate and release\nReaderAnchors, an evaluation set of anchors that readers find useful. Our\nanalysis shows that effective anchor prediction often requires jointly\nreasoning over lengthy source and target webpages to determine their implicit\nrelations and identify parts of the target webpage that are related but not\nredundant. We benchmark a performant T5-based ranking approach to establish\nbaseline performance on the task, finding ample room for improvement.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:58:21 GMT"},{"version":"v2","created":"Wed, 24 May 2023 07:12:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14338","submitter":"Matthew Smart","authors":"Matthew Smart, Stanislav Shvartsman, Hayden Nunley","title":"A model of replicating coupled oscillators generates naturally occurring\n  cell networks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"nlin.AO math.DS q-bio.CB","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  When a founder cell and its progeny divide with incomplete cytokinesis, a\nnetwork forms in which each intercellular bridge corresponds to a past mitotic\nevent. Networks built in this manner are required for gamete production in many\nanimals, and different species have evolved very different final network\ntopologies. While mechanisms regulating network assembly have been identified\nin particular organisms, we lack a quantitative framework to understand network\nassembly and inter-species variability. Motivated by cell networks responsible\nfor oocyte production in invertebrates, where the final topology is typically\ninvariant within each species, we devise a mathematical model for generating\ncell networks: each node is an oscillator, and after a full cycle, the node\nproduces a daughter to which it remains connected. These cell cycle\noscillations on the nodes are transient and coupled via diffusion over the\nnetwork's edges. By variation of three biologically motivated parameters, our\nmodel generates nearly all such networks currently reported across\ninvertebrates. Furthermore, small parameter variations can rationalize cases of\nwithin-species variation. Because cell networks outside of the ovary often form\nless deterministically, we propose generalizations of our model to account for\ndifferent sources of stochasticity.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:59:01 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14339","submitter":"Bin Zhuang","authors":"Bin Zhuang, No\\'e Lugaz, Nada Al-Haddad, R\\'eka M. Winslow, Camilla\n  Scolini, Charles J. Farrugia, and Antoinette B. Galvin","title":"Evolution of the Radial Size and Expansion of Coronal Mass Ejections\n  Investigated by Combining Remote and In-Situ Observations","comments":"Accepted by ApJ","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.SR physics.space-ph","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  A fundamental property of coronal mass ejections (CMEs) is their radial\nexpansion, which determines the increase in the CME radial size and the\ndecrease in the CME magnetic field strength as the CME propagates. CME radial\nexpansion can be investigated either by using remote observations or by in-situ\nmeasurements based on multiple spacecraft in radial conjunction. However, there\nhave been only few case studies combining both remote and in-situ observations.\nIt is therefore unknown if the radial expansion estimated remotely in the\ncorona is consistent with that estimated locally in the heliosphere. To address\nthis question, we first select 22 CME events between the years 2010 and 2013,\nwhich were well observed by coronagraphs and by two or three spacecraft in\nradial conjunction. We use the graduated cylindrical shell model to estimate\nthe radial size, radial expansion speed, and a measure of the dimensionless\nexpansion parameter of CMEs in the corona. The same parameters and two\nadditional measures of the radial-size increase and magnetic-field-strength\ndecrease with heliocentric distance of CMEs based on in-situ measurements are\nalso calculated. For most of the events, the CME radial size estimated by\nremote observations is inconsistent with the in-situ estimates. We further\nstatistically analyze the correlations of these expansion parameters estimated\nusing remote and in-situ observations, and discuss the potential reasons for\nthe inconsistencies and their implications for the CME space weather\nforecasting.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:59:13 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14340","submitter":"Ramalingam Kailasham","authors":"R. Kailasham and Aditya S. Khair","title":"Effect of speed fluctuations on the collective dynamics of active disks","comments":"10 pages, 10 figures, 8 supplementary videos","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.soft","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Numerical simulations are performed on the collective dynamics of active\ndisks, whose self-propulsion speed ($U$) varies in time, and whose orientation\nevolves according to rotational Brownian motion. Two protocols for the\nevolution of speed are considered: (i) a deterministic one involving a periodic\nchange in U at a frequency $\\omega$; and (ii) a stochastic one in which the\nspeeds are drawn from a power-law distribution at time-intervals governed by a\nPoissonian process of rate $\\beta$. In the first case, an increase in $\\omega$\ncauses the disks to go from a clustered state to a homogeneous one through a\ntransition that has an intriguing analogy to the 2D Ising model, provided that\nthe direction of self-propulsion is allowed to reverse. Similarly, in the\nsecond case, for a fixed value of $\\beta$, the extent of cluster-breakup is\nlarger when reversals in the self-propulsion direction are permitted.\nMotility-induced phase separation of the disks may therefore be avoided in\nactive matter suspensions in which the constituents are allowed to reverse\ntheir self-propulsion direction, immaterial of the precise temporal nature of\nthe reversal (deterministic or stochastic). Equally, our results demonstrate\nthat phase separation could occur even in the absence of a time-averaged\nmotility of an individual active agent, provided that the rate of direction\nreversals is smaller than the orientational diffusion rate.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:59:13 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14341","submitter":"Lucy Lu Wang","authors":"Yue Guo, Tal August, Gondy Leroy, Trevor Cohen, Lucy Lu Wang","title":"APPLS: A Meta-evaluation Testbed for Plain Language Summarization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  While there has been significant development of models for Plain Language\nSummarization (PLS), evaluation remains a challenge. This is in part because\nPLS involves multiple, interrelated language transformations (e.g., adding\nbackground explanations, removing specialized terminology). No metrics are\nexplicitly engineered for PLS, and the suitability of other text generation\nevaluation metrics remains unclear. To address these concerns, our study\npresents a granular meta-evaluation testbed, APPLS, designed to evaluate\nexisting metrics for PLS. Drawing on insights from previous research, we define\ncontrolled perturbations for our testbed along four criteria that a metric of\nplain language should capture: informativeness, simplification, coherence, and\nfaithfulness. Our analysis of metrics using this testbed reveals that current\nmetrics fail to capture simplification, signaling a crucial gap. In response,\nwe introduce POMME, a novel metric designed to assess text simplification in\nPLS. We demonstrate its correlation with simplification perturbations and\nvalidate across a variety of datasets. Our research contributes the first\nmeta-evaluation testbed for PLS and a comprehensive evaluation of existing\nmetrics, offering insights with relevance to other text generation tasks.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:59:19 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14342","submitter":"Hong Liu","authors":"Hong Liu, Zhiyuan Li, David Hall, Percy Liang, Tengyu Ma","title":"Sophia: A Scalable Stochastic Second-order Optimizer for Language Model\n  Pre-training","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL math.OC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Given the massive cost of language model pre-training, a non-trivial\nimprovement of the optimization algorithm would lead to a material reduction on\nthe time and cost of training. Adam and its variants have been state-of-the-art\nfor years, and more sophisticated second-order (Hessian-based) optimizers often\nincur too much per-step overhead. In this paper, we propose Sophia,\nSecond-order Clipped Stochastic Optimization, a simple scalable second-order\noptimizer that uses a light-weight estimate of the diagonal Hessian as the\npre-conditioner. The update is the moving average of the gradients divided by\nthe moving average of the estimated Hessian, followed by element-wise clipping.\nThe clipping controls the worst-case update size and tames the negative impact\nof non-convexity and rapid change of Hessian along the trajectory. Sophia only\nestimates the diagonal Hessian every handful of iterations, which has\nnegligible average per-step time and memory overhead. On language modeling with\nGPT-2 models of sizes ranging from 125M to 770M, Sophia achieves a 2x speed-up\ncompared with Adam in the number of steps, total compute, and wall-clock time.\nTheoretically, we show that Sophia adapts to the curvature in different\ncomponents of the parameters, which can be highly heterogeneous for language\nmodeling tasks. Our run-time bound does not depend on the condition number of\nthe loss.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:59:21 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14343","submitter":"Alejandro Escontrela","authors":"Alejandro Escontrela and Ademi Adeniji and Wilson Yan and Ajay Jain\n  and Xue Bin Peng and Ken Goldberg and Youngwoon Lee and Danijar Hafner and\n  Pieter Abbeel","title":"Video Prediction Models as Rewards for Reinforcement Learning","comments":"22 pages, 18 figures, 4 tables. under review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Specifying reward signals that allow agents to learn complex behaviors is a\nlong-standing challenge in reinforcement learning. A promising approach is to\nextract preferences for behaviors from unlabeled videos, which are widely\navailable on the internet. We present Video Prediction Rewards (VIPER), an\nalgorithm that leverages pretrained video prediction models as action-free\nreward signals for reinforcement learning. Specifically, we first train an\nautoregressive transformer on expert videos and then use the video prediction\nlikelihoods as reward signals for a reinforcement learning agent. VIPER enables\nexpert-level control without programmatic task rewards across a wide range of\nDMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction\nmodel allows us to derive rewards for an out-of-distribution environment where\nno expert data is available, enabling cross-embodiment generalization for\ntabletop manipulation. We see our work as starting point for scalable reward\nspecification from unlabeled videos that will benefit from the rapid advances\nin generative modeling. Source code and datasets are available on the project\nwebsite: https://escontrela.me/viper\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:59:33 GMT"},{"version":"v2","created":"Tue, 30 May 2023 17:38:44 GMT"}],"update_date":"2023-05-31"}
{"id":"2305.14344","submitter":"Agrim Gupta","authors":"Agrim Gupta, Jiajun Wu, Jia Deng, Li Fei-Fei","title":"Siamese Masked Autoencoders","comments":"Project page https://siam-mae-video.github.io/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Establishing correspondence between images or scenes is a significant\nchallenge in computer vision, especially given occlusions, viewpoint changes,\nand varying object appearances. In this paper, we present Siamese Masked\nAutoencoders (SiamMAE), a simple extension of Masked Autoencoders (MAE) for\nlearning visual correspondence from videos. SiamMAE operates on pairs of\nrandomly sampled video frames and asymmetrically masks them. These frames are\nprocessed independently by an encoder network, and a decoder composed of a\nsequence of cross-attention layers is tasked with predicting the missing\npatches in the future frame. By masking a large fraction ($95\\%$) of patches in\nthe future frame while leaving the past frame unchanged, SiamMAE encourages the\nnetwork to focus on object motion and learn object-centric representations.\nDespite its conceptual simplicity, features learned via SiamMAE outperform\nstate-of-the-art self-supervised methods on video object segmentation, pose\nkeypoint propagation, and semantic part propagation tasks. SiamMAE achieves\ncompetitive results without relying on data augmentation, handcrafted\ntracking-based pretext tasks, or other techniques to prevent representational\ncollapse.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:59:46 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14345","submitter":"Taeksoo Kim","authors":"Taeksoo Kim, Shunsuke Saito, Hanbyul Joo","title":"NCHO: Unsupervised Learning for Neural 3D Composition of Humans and\n  Objects","comments":"The project page is available at https://taeksuu.github.io/ncho/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Deep generative models have been recently extended to synthesizing 3D digital\nhumans. However, previous approaches treat clothed humans as a single chunk of\ngeometry without considering the compositionality of clothing and accessories.\nAs a result, individual items cannot be naturally composed into novel\nidentities, leading to limited expressiveness and controllability of generative\n3D avatars. While several methods attempt to address this by leveraging\nsynthetic data, the interaction between humans and objects is not authentic due\nto the domain gap, and manual asset creation is difficult to scale for a wide\nvariety of objects. In this work, we present a novel framework for learning a\ncompositional generative model of humans and objects (backpacks, coats,\nscarves, and more) from real-world 3D scans. Our compositional model is\ninteraction-aware, meaning the spatial relationship between humans and objects,\nand the mutual shape change by physical contact is fully incorporated. The key\nchallenge is that, since humans and objects are in contact, their 3D scans are\nmerged into a single piece. To decompose them without manual annotations, we\npropose to leverage two sets of 3D scans of a single person with and without\nobjects. Our approach learns to decompose objects and naturally compose them\nback into a generative human model in an unsupervised manner. Despite our\nsimple setup requiring only the capture of a single subject with objects, our\nexperiments demonstrate the strong generalization of our model by enabling the\nnatural composition of objects to diverse identities in various poses and the\ncomposition of multiple objects, which is unseen in training data.\nhttps://taeksuu.github.io/ncho/\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:59:52 GMT"},{"version":"v2","created":"Mon, 29 May 2023 13:51:25 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.14346","submitter":"Theresa Anderson","authors":"Theresa C. Anderson and Eyvindur A. Palsson","title":"A framework for discrete bilinear spherical averages and applications to\n  $\\ell^p$-improving estimates","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.CA math.NT","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We provide a refined pointwise upper bound for the discrete bilinear\nspherical averaging operator. This builds on the slicing techniques of Jeong\nand Lee in the discrete (number theoretic) setting, and significantly tightens\nthe bounds from previous work of the authors. A variety of applications are\npresented, most notably to $\\ell^p$-improving estimates.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:59:58 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14347","submitter":"Nikhil Sarin","authors":"Nikhil Sarin, Axel Brandenburg, Brynmor Haskell","title":"Confronting the neutron star population with inverse cascades","comments":"Submitted. 6 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.HE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The origin and evolution of magnetic fields of neutron stars from birth has\nlong been a source of debate. Here, motivated by recent simulations of the Hall\ncascade with magnetic helicity, we invoke a model where the large-scale\nmagnetic field of neutron stars grows as a product of small-scale turbulence\nthrough an inverse cascade. We apply this model to a simulated population of\nneutron stars at birth and show how this model can account for the evolution of\nsuch objects across the $P\\dot{P}$ diagram, explaining both pulsar and magnetar\nobservations. Under the assumption that small-scale turbulence is responsible\nfor large-scale magnetic fields, we place a lower limit on the spherical\nharmonic degree of the energy-carrying magnetic eddies of $\\approx 40$. Our\nresults favor the presence of a highly resistive pasta layer at the base of the\nneutron star crust. We further discuss the implications of this paradigm on\ndirect observables, such as the nominal age and braking index of pulsars.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:59:59 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14348","submitter":"Robert Throckmorton","authors":"Robert E. Throckmorton and S. Das Sarma","title":"A generalized model of the noise spectrum of a two-level fluctuator in\n  the presence of an electron subbath","comments":"6+$\\epsilon$ pages, 4 figures. To be submitted for publication","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall quant-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The work of Ahn derives the noise power spectrum of a two-level fluctuator\n(TLF) in the case that it interacts only with a subregion of a full electron\nbath and thus is subject to a fluctuating temperature. However, Eq.~(1), which\ngives the variance of the subbath temperature in terms of the heat capacity, in\nthat work carries the implicit assumption that the heat capacity of this\nsubbath may be taken to be a constant, which is a good approximation at higher\ntemperatures, but breaks down at lower temperatures. We thus extend this work\nto the case in which the fact that the electronic heat capacity of a\ntwo-dimensional electron gas (2DEG) $C_V\\propto T$, rather than constant in\ntemperature, is fully taken into account. We show that, at low temperatures,\nthe resulting power spectrum of the noise $S(\\omega)\\propto e^{-C/T^{3/8}}$, in\ncontrast to $S(\\omega)\\propto e^{-C'/T^{1/3}}$ as found previously, where $C$\nand $C'$ are constants. We also compare the numerical results that one would\nobtain from the two models and find that our results for $S(\\omega)$ can differ\nfrom those of Ahn by several orders of magnitude at low temperatures.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:59:59 GMT"}],"update_date":"2023-05-24"}
{"id":"2305.14410","submitter":"Dinesh Khandelwal","authors":"Harman Singh, Poorva Garg, Mohit Gupta, Kevin Shah, Arnab Kumar\n  Mondal, Dinesh Khandelwal, Parag Singla, Dinesh Garg","title":"Image Manipulation via Multi-Hop Instructions -- A New Dataset and\n  Weakly-Supervised Neuro-Symbolic Approach","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We are interested in image manipulation via natural language text -- a task\nthat is useful for multiple AI applications but requires complex reasoning over\nmulti-modal spaces. We extend recently proposed Neuro Symbolic Concept Learning\n(NSCL), which has been quite effective for the task of Visual Question\nAnswering (VQA), for the task of image manipulation. Our system referred to as\nNeuroSIM can perform complex multi-hop reasoning over multi-object scenes and\nonly requires weak supervision in the form of annotated data for VQA. NeuroSIM\nparses an instruction into a symbolic program, based on a Domain Specific\nLanguage (DSL) comprising of object attributes and manipulation operations,\nthat guides its execution. We create a new dataset for the task, and extensive\nexperiments demonstrate that NeuroSIM is highly competitive with or beats SOTA\nbaselines that make use of supervised data for manipulation.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:59:10 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14411","submitter":"Temple He","authors":"Temple He, Ana-Maria Raclariu, Kathryn M. Zurek","title":"From Shockwaves to the Gravitational Memory Effect","comments":"30 pages, 1 figure","journal-ref":null,"doi":null,"report-no":"CALT-TH 2023-013","categories":"hep-th gr-qc hep-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We study the relationship between shockwave geometries and the gravitational\nmemory effect in four-dimensional asymptotically flat spacetime. In particular,\nwe show the 't Hooft commutation relations of shockwave operators are\nequivalent to the commutation relation between soft and Goldstone modes\nparametrizing a sector of the gravitational phase space. We demonstrate this\nequivalence via a diffeomorphism that takes the shockwave metric to a metric\nwhose transverse traceless component is the gravitational memory. The shockwave\nmomentum in 't Hooft's analysis is related to the soft graviton mode, which is\nresponsible for the memory effect, while the shift in the shockwave position is\nrelated to the Goldstone mode. This equivalence opens new directions to utilize\nthe gravitational memory effect to explore the observational implications of\nshockwave geometries in flat space.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14412","submitter":"Pierre Heidmann","authors":"Pierre Heidmann, Nicholas Speeney, Emanuele Berti and Ibrahima Bah","title":"Cavity effect in the quasinormal mode spectrum of topological stars","comments":"17 pages + Appendix, 11 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc hep-ph hep-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We study scalar perturbations of topological solitons, smooth horizonless\nsolutions in five-dimensional Einstein-Maxwell theory that correspond to\ncoherent states of gravity via the dynamics of extra compact dimensions. First,\nwe compute scalar quasinormal modes for topological stars that have a single\nunstable photon sphere, and we show that the spectrum is very similar to that\nof a black hole with the same photon sphere. Next, we study topological stars\nthat have both a stable inner photon sphere and an unstable one. The first few\nquasinormal modes are localized around the inner photon sphere. The spectrum\nalso contains ''black-hole like modes'' localized at the unstable outer photon\nsphere. The frequencies of these modes are similar to those of a black hole,\nbut their imaginary part is smaller due to a cavity effect associated with the\ninner photon sphere. The longer damping produced by this trapping effect may\nhave implications for black hole spectroscopy.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14413","submitter":"Eros Vanzella","authors":"E. Vanzella, F. Loiacono, P. Bergamini, U. Mestric, M. Castellano, P.\n  Rosati, M. Meneghetti, C. Grillo, F. Calura, M. Mignoli, M. Bradac, A. Adamo,\n  G. Rihtarsic, M. Dickinson, M. Gronke, A. Zanella, F. Annibali, C. Willott,\n  M. Messa, E. Sani, A. Acebron, A. Bolamperti, A. Comastri, R. Gilli, K. I.\n  Caputi, M. Ricotti, C. Gruppioni, S. Ravindranath, A. Mercurio, V. Strait, N.\n  Martis, R. Pascale, G. B. Caminha, M. Annunziatella","title":"An extremely metal poor star complex in the reionization era:\n  Approaching Population III stars with JWST","comments":"15 pages, 10 figures, 1 table. Submitted to A&A. Comments welcome","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.GA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We present JWST/NIRSpec integral field spectroscopy (IFS) of a lensed\nPopulation III candidate stellar complex (dubbed Lensed And Pristine 1, LAP1),\nwith a lensing-corrected stellar mass ~<10^4 Msun, absolute luminosity M_UV >\n-11.2 (m_UV > 35.6), confirmed at redshift 6.639 +/- 0.004. The system is\nstrongly amplified (\\mu >~ 100) by straddling a critical line of the Hubble\nFrontier Field galaxy cluster MACS J0416. Despite the stellar continuum is\ncurrently not detected in the Hubble and JWST/NIRCam and NIRISS imaging,\narclet-like shapes of Lyman and Balmer lines, Lya, Hg, Hb and Ha are detected\nwith NIRSpec IFS with signal-to-noise ratios SNR=5-13 and large equivalent\nwidths (>300-2000A), along with a remarkably weak [OIII]4959-5007 at SNR ~ 4.\nLAP1 shows a large ionizing photon production efficiency,\nlog(\\xi_{ion}[erg~Hz^{-1}])>26. From the metallicity indexes R23 =\n([OIII]4959-5007 + [OII]3727) / Hb ~< 0.74 and R3 = ([OIII]5007 / Hb) = 0.55\n+/- 0.14, we derive an oxygen abundance 12+log(O/H) ~< 6.3. Intriguingly, the\nHa emission is also measured in mirrored sub-components where no [OIII] is\ndetected, providing even more stringent upper limits on the metallicity if\nin-situ star formation is ongoing in this region (12+log(O/H) < 6, or Z < 0.002\nZsun). The formal stellar mass limit of the sub-components would correspond to\n~10^{3} Msun or M_UV fainter than -10. Alternatively, such a metal-free pure\nline emitting region could be the first case of a fluorescing HI gas region,\ninduced by transverse escaping ionizing radiation from a nearby star-complex.\nThe presence of large equivalent-width hydrogen lines and the deficiency of\nmetal lines in such a small region, make LAP1 the most metal poor star-forming\nregion currently known in the reionization era and a promising site that may\nhost isolated, pristine stars.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14414","submitter":"Annalisa Citro","authors":"Annalisa Citro, Danielle A. Berg, Dawn K. Erb, Matthew W. Auger,\n  George D. Becker, Bethan L. James and Evan D. Skillman","title":"J0332-3557: A comprehensive metallicity analysis of a z~3.8\n  gravitationally lensed galaxy","comments":"26 pages, 16 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.GA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We provide one of the most comprehensive metallicity studies at z ~ 4 by\nanalyzing the UV/optical HST photometry, and rest-frame VLT-FORS2 ultraviolet\nand VLT-XSHOOTER optical spectra of J0332-3557, a gravitationally lensed galaxy\nmagnified by a factor of 20. With a 5{\\sigma} detection of the auroral O III]\n{\\lambda}1666 line, we are able to derive a direct gas metallicity estimate for\nour target. We find Zgas = 12 + log(O/H) = 8.26 +/- 0.06, which is compatible\nwith an increasing of both the gas fraction and the outflow metal loading\nfactor from z ~ 0 to z ~ 4. J0332 is the most metal-rich individual galaxy at z\n> 3.6 for which the C/O ratio has been measured. We derive a low log(C/O)=\n-1.02 +/- 0.2, which suggests that J0332 is in the early stages of ISM carbon\nenrichment driven mostly by massive stars. The low C/O also indicates that\nJ0332 is characterized by a low star formation efficiency, higher yields of\noxygen and longer burst duration. We find that the EW[C III]1906,9 is as low as\n~ 3 {\\AA}. The main drivers of the low EW[C III]1906,9 are the higher gas\nmetallicity and the low C/O. J0332 is characterized by one diffuse and two more\ncompact regions ~ 1 kpc in size. We find that the carbon emission mostly\noriginates in the compact knots.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14415","submitter":"Alexandros Ziampras","authors":"Alexandros Ziampras, Richard P. Nelson, Roman R. Rafikov","title":"Modeling planet-induced gaps and rings in ALMA disks: the role of\n  in-plane radiative diffusion","comments":"18 pages, 28 figures, 4 tables; submitted to MNRAS","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.EP","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  ALMA observations of protoplanetary disks in dust continuum emission reveal a\nvariety of annular structures. Attributing the existence of such features to\nembedded planets is a popular scenario, supported by studies using\nhydrodynamical models. Recent work has shown that radiative cooling greatly\ninfluences the capability of planet-driven spiral density waves to transport\nangular momentum, ultimately deciding the number, position, and depth of rings\nand gaps that a planet can carve in a disk. However, radiation transport has\nonly been treated via local thermal relaxation, not taking into account\nradiative diffusion along the disk plane. We compare the previous\nstate-of-the-art models of planet-disk interaction with local cooling\nprescriptions to our new models that include cooling in the vertical direction\nand radiative diffusion in the plane of the disk, and show that the response of\nthe disk to the induced spiral waves can differ significantly when comparing\nthese two treatments of the disk thermodynamics. We follow up with synthetic\nemission maps of ALMA systems, and show that our new models reproduce the\nobservations found in the literature better than models with local cooling. We\nconclude that appropriate treatment of radiation transport is key to\nconstraining the parameter space when interpreting ALMA observations using the\nplanet-disk interaction scenario.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14416","submitter":"Ivan Khaymovich","authors":"Daniil Kochergin, Ivan M. Khaymovich, Olga Valba, and Alexander Gorsky","title":"Anatomy of the fragmented Hilbert space: eigenvalue tunneling, quantum\n  scars and localization in the perturbed random regular graph","comments":"27 pages, 9 figures, 87 references + 4 pages, 4 figures in Appendices","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.dis-nn hep-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We consider the properties of the random regular graph with node degree $d$\nperturbed by chemical potentials $\\mu_k$ for a number of short $k$-cycles. We\nanalyze both numerically and analytically the phase diagram of the model in the\n$(\\mu_k,d)$ plane. The critical curve separating the homogeneous and\nclusterized phases is found and it is demonstrated that the clusterized phase\nitself generically is separated as the function of $d$ into the phase with\nideal clusters and phase with coupled ones when the continuous spectrum gets\nformed. The eigenstate spatial structure of the model is investigated and it is\nfound that there are localized scar-like states in the delocalized part of the\nspectrum, that are related to the topologically equivalent nodes in the graph.\nWe also reconsider the localization of the states in the non-perturbative band\nformed by eigenvalue instantons and find the semi-Poisson level spacing\ndistribution. The Anderson transition for the case of combined ($k$-cycle)\nstructural and diagonal (Anderson) disorders is investigated. It is found that\nthe critical diagonal disorder gets reduced sharply at the clusterization phase\ntransition, but does it unevenly in non-perturbative and mid-spectrum bands,\ndue to the scars, present in the latter. The applications of our findings to\n$2$d quantum gravity are discussed.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14417","submitter":"William Pannell","authors":"William H. Pannell, Andreas Stergiou","title":"Scalar-Fermion Fixed Points in the $\\varepsilon$ Expansion","comments":"45 pages, 20 figures, 10 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-th cond-mat.str-el","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The one-loop beta functions for systems of $N_s$ scalars and $N_f$ fermions\ninteracting via a general potential are analysed as tensorial equations in\n$4-\\varepsilon$ dimensions. Two distinct bounds on combinations of invariants\nconstructed from the couplings are derived and, subject to an assumption, are\nused to prove that at one-loop order the anomalous dimensions of the elementary\nfields are universally restricted by $\\gamma_\\phi\\leq\\frac{1}{2}\\,\\varepsilon$\nand $\\gamma_\\psi\\leq\\frac{1}{2}N_s\\,\\varepsilon$. For each root of the Yukawa\nbeta function there is a number of roots of the quartic beta function, giving\nrise to the concept of `levels' of fixed points in scalar-fermion theories. It\nis proven that if a stable fixed point exists within a certain level, then it\nis the only such fixed point at that level. Solving the beta function\nequations, both analytically and numerically, for low numbers of scalars and\nfermions, well-known and novel fixed points are found and their stability\nproperties are examined. While a number of fixed points saturate one out of the\ntwo bounds, only one fixed point is found which saturates both of them.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:01 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14418","submitter":"Guillermo Barro","authors":"Guillermo Barro, Pablo G. Perez-Gonzalez, Dale D. Kocevski, Elizabeth\n  J. McGrath, Jonathan R. Trump, Raymond C. Simons, Rachel S. Somerville, L. Y.\n  Aaron Yung, Pablo Arrabal Haro, Michaela B. Bagley, Nikko J. Cleri, Luca\n  Costantin, Kelcey Davis, Mark Dickinson, Steve L. Finkelstein, Mauro\n  Giavalisco, Carlos Gomez-Guijarro, Nimish P. Hathi, Michaela Hirschmann,\n  Hollis B. Akins, Benne W. Holwerda, Marc Huertas-Company, Ray A. Lucas, Casey\n  Papovich, Lise-Marie Seille, Sandro Tacchella, Stephen M. Wilkins, Alexander\n  de la Vega, Guang Yang, Jorge A. Zavala","title":"Extremely red galaxies at $z=5-9$ with MIRI and NIRSpec: dusty galaxies\n  or obscured AGNs?","comments":"26 pages, 10 figures, submitted to ApJ","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.GA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We study a new population of extremely red objects (EROs) recently discovered\nby JWST based on their NIRCam colors F277W$-$F444W $>1.5$ mag. We find 37 EROs\nin the CEERS field with F444W $<28$ mag and photometric redshifts between\n$5<z<7$, with median $z=6.9^{+1.0}_{-1.6}$. Surprisingly, despite their red\nlong-wavelength colors, these EROs have blue short-wavelength colors\n(F150W$-$F200W$\\sim$0 mag) indicative of bimodal SEDs with a red, steep slope\nin the rest-frame optical, and a blue, flat slope in the rest-frame UV.\nMoreover, all these EROs are unresolved, point-like sources in all NIRCam\nbands. We analyze the spectral energy distributions of 8 of them with MIRI and\nNIRSpec observations using stellar population models and AGN templates. We find\nthat a dusty galaxy or an obscured AGN provide similarly good SED fits but\ndifferent stellar properties: massive and dusty, log M/M_sun$\\sim$10 and\nA$_{\\rm V}\\gtrsim3$ mag, or low mass and obscuration, log M/M_sun$\\sim$7.5 and\nA$_{\\rm V}\\sim0$ mag, hosting an obscured QSO. SED modeling does not favor\neither scenario, but their unresolved sizes are more suggestive of an AGN. If\nany EROs are confirmed to have log M/M_sun$\\gtrsim10.5$, it would increase\npre-JWST number densities at $z>7$ by up to a factor $\\sim$60. Similarly, if\nthey are OSOs with luminosities in the L$_{\\rm bol}>10^{46-47}$ erg s$^{-1}$\nrange, their number would exceed that of bright blue QSOs by more than two\norders of magnitude. Additional photometry at mid-IR wavelengths will reveal\nthe true nature of the red continuum emission in these EROs and will place this\npuzzling population in the right context of galaxy evolution.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:01 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14419","submitter":"Constant Auclair","authors":"Constant Auclair, Erwan Allys, Fran\\c{c}ois Boulanger, Matthieu\n  B\\'ethermin, Athanasia Gkogkou, Guilaine Lagache, Antoine Marchal,\n  Marc-Antoine Miville-Desch\\^enes, Bruno R\\'egaldo-Saint Blancard and Pablo\n  Richard","title":"Separation of dust emission from the Cosmic Infrared Background in\n  Herschel observations with Wavelet Phase Harmonics","comments":"Submitted to A&A. Comments welcome","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.GA astro-ph.CO astro-ph.IM","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The low brightness dust emission at high Galactic latitude is of interest to\nstudy the interplay between physical processes in shaping the structure of the\ninterstellar medium (ISM), as well as to statistically characterize dust\nemission as a foreground to the Cosmic Microwave Background (CMB). Progress in\nthis avenue of research have been hampered by the difficulty of separating the\ndust emission from the Cosmic Infrared Background (CIB). We demonstrate that\ndust and CIB may be effectively separated based on their different structure on\nthe sky and use the separation to characterize the structure of diffuse dust\nemission on angular scales where CIB is a significant component in terms of\npower. We use scattering transform statistics, the Wavelet Phase Harmonics\n(WPH), to perform a statistical component separation using Herschel SPIRE\nobservations. This component separation is done only from observational data\nusing non-Gaussian properties as a lever arm, and is done at a single 250\nmicrons frequency. This method, that we validate on mock data, gives us access\nto non-Gaussian statistics of the interstellar dust and an output dust map\nessentially free from CIB contamination. Our statistical modelling\ncharacterizes the non-Gaussian structure of the diffuse ISM down to the\nsmallest scales observed by Herschel. We recover the power-law shape of the\ndust power spectrum up to a wavenumber of 2 arcmin$^{-1}$ where the dust signal\nrepresents 2 percent of the total power. The output dust map reveals coherent\nstructures at the smallest scales which were hidden by the CIB anisotropies. It\nopens new observational perspectives on the formation of structure in the\ndiffuse ISM which we discuss with reference to past work. We have succeeded to\nperform a statistical separation from observational data only at a single\nfrequency by using non-Gaussian statistics.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:02 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14420","submitter":"Seokhoon Yun","authors":"Francesco D'Eramo, Giuseppe Lucente, Newton Nath, Seokhoon Yun","title":"Terrestrial detection of hidden vectors produced by solar nuclear\n  reactions","comments":"40 pages, 9 figures; v2: numerical mistake in the analysis corrected,\n  updated results and discussion","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph astro-ph.SR hep-ex","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Solar nuclear reactions can occasionally produce sub-MeV elusive beyond the\nStandard Model particles that escape the solar interior without further\ninteractions. This study focuses on massive spin-one particles. We construct\nthe general theoretical framework and identify two crucial mixing sources\ninvolving the photon, which facilitate communication between the hidden and\nvisible sectors: kinetic mixing with the photon, and plasma-induced mixing due\nto thermal electron loops. For both cases, we focus on the second stage of the\nsolar proton-proton chain and evaluate the fluxes of monochromatic 5.49~MeV\nhidden vectors produced by the $p(d, ^3{\\rm He})\\gamma^\\prime$ nuclear\nreaction. We then investigate their terrestrial detection via Compton-like\nscatterings. The incoming fluxes are polarized, and we evaluate the cross\nsections for Compton-like scatterings for transverse and longitudinal vectors.\nFinally, we apply this framework to a concrete case by investigating the\nsensitivity of the forthcoming Jiangmen Underground Neutrino Observatory (JUNO)\nexperiment and identifying parameter space where current terrestrial bounds\nwill be improved.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:02 GMT"},{"version":"v2","created":"Sun, 28 May 2023 19:35:40 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.14421","submitter":"A.I. Malz","authors":"Alex I. Malz, Mi Dai, Kara A. Ponder, Emille E.O. Ishida, Santiago\n  Gonzalez-Gaitain, Rupesh Durgesh, Alberto Krone-Martins, Rafael S. de Souza,\n  Noble Kennamer, Sreevarsha Sreejith, Lluis Galbany, The LSST Dark Energy\n  Science Collaboration (DESC), The Cosmostatistics Initiative (COIN)","title":"Are classification metrics good proxies for SN Ia cosmological\n  constraining power?","comments":"9 pages, 6 figures; submitted to A&A","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.CO astro-ph.IM","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Context: When selecting a classifier to use for a supernova Ia (SN Ia)\ncosmological analysis, it is common to make decisions based on metrics of\nclassification performance, i.e. contamination within the photometrically\nclassified SN Ia sample, rather than a measure of cosmological constraining\npower. If the former is an appropriate proxy for the latter, this practice\nwould save those designing an analysis pipeline from the computational expense\nof a full cosmology forecast. Aims: This study tests the assumption that\nclassification metrics are an appropriate proxy for cosmology metrics. Methods:\nWe emulate photometric SN Ia cosmology samples with controlled contamination\nrates of individual contaminant classes and evaluate each of them under a set\nof classification metrics. We then derive cosmological parameter constraints\nfrom all samples under two common analysis approaches and quantify the impact\nof contamination by each contaminant class on the resulting cosmological\nparameter estimates. Results: We observe that cosmology metrics are sensitive\nto both the contamination rate and the class of the contaminating population,\nwhereas the classification metrics are insensitive to the latter. Conclusions:\nWe therefore discourage exclusive reliance on classification-based metrics for\ncosmological analysis design decisions, e.g. classifier choice, and instead\nrecommend optimizing using a metric of cosmological parameter constraining\npower.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:02 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14422","submitter":"Misha Yutushui","authors":"Misha Yutushui and David F. Mross","title":"Identifying non-Abelian anyons with upstream noise","comments":"4 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.str-el","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Non-Abelian phases are among the most highly-prized but elusive states of\nmatter. We show that upstream noise measurements can identify the putative\nnon-Abelian fractional quantum Hall plateaus at filling factors\n$\\nu=\\frac{12}{5}$ or in any half-filled Landau level. Interfacing these states\nwith any readily-available Abelian state yields a binary outcome of upstream\nnoise or no noise. Judicious choices of the Abelian states can produce a\nsequence of yes--no outcomes that fingerprint the possible non-Abelian phase by\nruling out its competitors.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:03 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14423","submitter":"Valentin Cr\\'epel","authors":"Valentin Cr\\'epel, Aaron Dunbrack, Daniele Guerci, John Bonini,\n  Jennifer Cano","title":"Chiral model of twisted bilayer graphene realized in a monolayer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall cond-mat.str-el","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We demonstrate that a single layer of graphene subject to a superlattice\npotential nearly commensurate to a $\\sqrt{3} \\times \\sqrt{3}$ supercell exactly\nmaps to the chiral model of twisted bilayer graphene, albeit with half as many\ndegrees of freedom. We comprehensively review the properties of this\n``half-chiral model,'' including the interacting phases stabilized at integer\nfillings and the effects of substrate-induced symmetry breaking. We list\ncandidate substrates that could produce a superlattice potential on graphene\nwith the correct periodicity to access the flat band limit. Experimental\nmeasurements on a half-chiral moire heterostructure, in which valley-skyrmions\ncannot form, could yield insights on the physics they mediate in twisted\nbilayer graphene.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:04 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14424","submitter":"Zun Yi Brent Tan","authors":"Brent Tan and Drummond B. Fielding","title":"Cloud Atlas: Navigating the Multiphase Landscape of Tempestuous Galactic\n  Winds","comments":"32 pages, 34 figures; Submitted to MNRAS","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.GA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Galaxies comprise intricate networks of interdependent processes which\ntogether govern their evolution. Central among these are the multiplicity of\nfeedback channels, which remain incompletely understood. One outstanding\nproblem is the understanding and modeling of the multiphase nature of galactic\nwinds, which play a crucial role in galaxy formation and evolution. We present\nthe results of three dimensional magnetohydrodynamical tall box interstellar\nmedium patch simulations with clustered supernova driven outflows.\nFragmentation of the interstellar medium during superbubble breakout seeds the\nresulting hot outflow with a population of cool clouds. We focus on analyzing\nand modeling the origin and properties of these clouds. Their presence induces\nlarge scale turbulence, which in turn leads to complex cloud morphologies.\nCloud sizes are well described by a power law distribution and mass growth\nrates can be modelled using turbulent radiative mixing layer theory. Turbulence\nprovides significant pressure support in the clouds, while magnetic fields only\nplay a minor role. We conclude that many of the physical insights and analytic\nscalings derived from idealized small scale simulations translate well to\nlarger scale, more realistic turbulent magnetized winds, thus paving a path\ntowards their necessary yet challenging inclusion in global-scale galaxy\nmodels.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:04 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14425","submitter":"Mohamed Anber","authors":"Mohamed M. Anber, Erich Poppitz","title":"Noninvertible anomalies in $SU(N)\\times U(1)$ gauge theories","comments":"18 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-th cond-mat.str-el hep-lat","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We study $4$-dimensional $SU(N)\\times U(1)$ gauge theories with a single\nmassless Dirac fermion in the $2$-index symmetric/antisymmetric representations\nand show that they are endowed with a noninvertible $0$-form $\\widetilde\n{\\mathbb Z}_{2(N\\pm 2)}^{\\chi}$ chiral symmetry along with a $1$-form $\\mathbb\nZ_N^{(1)}$ center symmetry. By using the Hamiltonian formalism and putting the\ntheory on a spatial three-torus $\\mathbb T^3$, we construct the non-unitary\ngauge invariant operator corresponding to $\\widetilde {\\mathbb Z}_{2(N\\pm\n2)}^{\\chi}$ and find that it acts nontrivially in sectors of the Hilbert space\ncharacterized by selected magnetic fluxes. When we subject $\\mathbb T^3$ to\n$\\mathbb Z_N^{(1)}$ twists, for $N$ even, in selected magnetic flux sectors,\nthe algebra of $\\widetilde {\\mathbb Z}_{2(N\\pm 2)}^{\\chi}$ and $\\mathbb\nZ_N^{(1)}$ fails to commute by a $\\mathbb Z_2$ phase. We interpret this\nnoncommutativity as a mixed anomaly between the noninvertible and the $1$-form\nsymmetries. The anomaly implies that all states in the torus Hilbert space with\nthe selected magnetic fluxes exhibit a two-fold degeneracy for arbitrary\n$\\mathbb T^3$ size. The degenerate states are labeled by discrete electric\nfluxes and are characterized by nonzero expectation values of condensates.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:10 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14426","submitter":"Andrea Solfanelli","authors":"Andrea Solfanelli, Stefano Ruffo, Sauro Succi, Nicol\\`o Defenu","title":"Stabilization of Discrete Time-Crystaline Response on a Superconducting\n  Quantum Computer by increasing the Interaction Range","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.stat-mech","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This work presents a novel method for reproducing the dynamics of systems\nwith couplings beyond nearest neighbors using a superconducting quantum\nprocessor. Quantum simulation of complex quantum many-body systems is a\npromising short-term goal of noisy intermediate-scale quantum (NISQ) devices.\nHowever, the limited connectivity of native qubits hinders the implementation\nof quantum algorithms that require long-range interactions. We show that\nutilizing the universality of quantum processor native gates allows the\nimplementation of couplings among physically disconnected qubits. To\ndemonstrate the effectiveness of our method, we implement a quantum simulation,\non IBM quantum superconducting processors, of a Floquet-driven quantum spin\nchain featuring interactions beyond nearest neighbors. Specifically, we\nbenchmark the prethermal stabilization of discrete Floquet time crystalline\nresponse as the interaction range increases, a phenomenon which was never\nexperimentally observed before. Our method enables the study of systems with\ntunable interaction ranges, opening up new opportunities to explore the physics\nof long-range interacting quantum systems.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:12 GMT"},{"version":"v2","created":"Thu, 25 May 2023 21:18:34 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.14427","submitter":"Stefan Sandner","authors":"S. Sandner, P. Hernandez, J. Lopez-Pavon and N. Rius","title":"Predicting the baryon asymmetry with degenerate right-handed neutrinos","comments":"28 + 4 pages, 8 figures","journal-ref":null,"doi":null,"report-no":"IFIC/23-18, FTUV-23-0519.8654","categories":"hep-ph astro-ph.CO","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We consider the generation of a baryon asymmetry in an extension of the\nStandard Model with two singlet Majorana fermions that are degenerate above the\nelectroweak phase transition. The model can explain neutrino masses as well as\nthe observed matter-antimatter asymmetry, for masses of the heavy singlets\nbelow the electroweak scale. The only physical CP violating phases in the model\nare those in the PMNS mixing matrix, i.e. the Dirac phase and a Majorana phase\nthat enter light neutrino observables. We present an accurate analytic\napproximation for the baryon asymmetry in terms of CP flavour invariants, and\nderive the correlations with neutrino observables. We demonstrate that the\nmeasurement of CP violation in neutrino oscillations as well as the mixings of\nthe heavy neutral leptons with the electron, muon and tau flavours suffice to\npin down the matter-antimatter asymmetry from laboratory measurements.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:14 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14428","submitter":"Wentao Bao","authors":"Wentao Bao, Lichang Chen, Heng Huang, Yu Kong","title":"Prompting Language-Informed Distribution for Compositional Zero-Shot\n  Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The compositional zero-shot learning (CZSL) task aims to recognize unseen\ncompositional visual concepts (i.e., sliced tomatoes), where the models are\nlearned only from the seen compositions (i.e., sliced potatoes and red\ntomatoes). Thanks to the prompt tuning on large pre-trained visual language\nmodels such as CLIP, recent literature shows impressively better CZSL\nperformance than traditional vision-based methods. However, the key aspects\nthat impact the generalization to unseen compositions, including the diversity\nand informativeness of class context, and the entanglement between visual\nprimitives (i.e., states and objects), are not properly addressed in existing\nCLIP-based CZSL literature. In this paper, we propose a model by prompting the\nlanguage-informed distribution, aka., PLID, for the CZSL task. Specifically,\nthe PLID leverages pre-trained large language models (LLM) to 1) formulate the\nlanguage-informed class distribution, and 2) enhance the compositionality of\nthe softly prompted class embedding. Moreover, a stochastic logit mixup\nstrategy is proposed to dynamically fuse the decisions from the predictions in\nthe compositional and the primitive logit space. Orthogonal to the existing\nliterature of soft, hard, or distributional prompts, our method advocates\nprompting the LLM-supported class distribution that leads to a better\ncompositional zero-shot generalization. Experimental results on MIT-States,\nUT-Zappos, and C-GQA datasets show the superior performance of the PLID to the\nprior arts. The code and models will be publicly released.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:22 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14429","submitter":"Ben King","authors":"B. King","title":"Classical Radiation Reaction in Red-Shifted Harmonics","comments":"7 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph physics.class-ph physics.plasm-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The collision of a finite electromagnetic plane wave with an electron subject\nto the Landau-Lifshitz radiation reaction force is studied. A locally\nmonochromatic approximation is derived and compared to numerical evaluation of\nthe exact plane wave result. Energy and transverse momentum spectra are\ncalculated, which clearly display the red-shifting of harmonic features due to\nradiation reaction effects. Simple formulas are presented to predict the\nshifting of harmonic edges, whose position can be used in experiments as\nevidence of radiation reaction.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:28 GMT"}],"update_date":"2023-05-31"}
{"id":"2305.14430","submitter":"Jorge Segovia","authors":"P.G. Ortega, D.R. Entem, F. Fernandez and J. Segovia","title":"Unraveling the nature of the novel $\\mathbf{T_{cs}}$ and\n  $\\mathbf{T_{c\\bar s}}$ tetraquark candidates","comments":"10 pages, 4 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph hep-ex hep-lat nucl-ex nucl-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Using proton-proton collisions at centre-of-mass energies $7$, $8$, and $13$\nTeV, with a total integrated luminosity of $9\\,\\text{fb}^{-1}$, the LHCb\ncollaboration has performed amplitude analyses of the $B^+\\to D^+D^-K^+$,\n$B^+\\to D^- D_s^+ \\pi^+$ and $B^0\\to \\bar{D}^0 D_s^+ \\pi^-$ decays, observing\nthat new $T_{cs}$ and $T_{c\\bar s}$ resonances are required in order to explain\nthe experimental data. These signals could be the first observation of\ntetraquark candidates that do not contain a heavy quark-antiquark pair; in\nfact, they consist of four different flavours of quarks, one of which is a\ndoubly charged open-charm state. We present herein an analysis of the $T_{cs}$\nand $T_{c\\bar s}$ states, which is an extension of our recently published study\nof similar $T_{cc}^+$ exotic candidates. Our theoretical framework is a\nconstituent-quark-model-based coupled-channels calculation of $qq^\\prime \\bar s\n\\bar c$ and $cq\\bar s\\bar q^{\\prime}$ tetraquark sectors for $T_{cs}$ and\n$T_{c\\bar s}$ structures, respectively. We explore the nature, and pole\nposition, of the singularities that appear in the scattering matrix with\nspin-parity quantum numbers: $J^P=0^\\pm$, $1^\\mp$, and $2^\\pm$. The constituent\nquark model has been widely used in the heavy quark sector, and thus all model\nparameters are already constrained from previous works. This makes our\npredictions robust and parameter-free. We find many singularities in the\nsolution of various scattering-matrix problems which are either virtual states\nor resonances, but not bound states. Some of them fit well with the\nexperimental observations of the spin-parity, mass and width of $T_{cs}$ and\n$T_{c\\bar s}$ candidates, and thus tentative assignments are made; however,\nwith caution, because the experimental Breit-Wigner parameters are related to\nthe pole characteristics.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:28 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14431","submitter":"Avia Noah","authors":"Avia Noah, Yishay Zur, Nofar Fridman, Sourabh Singh, Alon Gutfreund,\n  Edwin Herrera, Atzmon Vakahi, Sergei Remennik, Martin Emile Huber, Snir\n  Gazit, Hermann Suderow, Hadar Steinberg, Oded Millo, and Yonathan Anahory","title":"Nano-Patterned Magnetic Edges in CrGeTe3 for Quasi 1-D Spintronic\n  Devices","comments":"Main text: 13 pages, 4 figures. Supplementary information: 8 pages, 6\n  figures","journal-ref":null,"doi":"10.1021/acsanm.3c01008","report-no":null,"categories":"cond-mat.mes-hall","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The synthesis of two-dimensional van der Waals magnets has paved the way for\nboth technological applications and fundamental research on magnetism confined\nto ultra-small length scales. Edge magnetic moments in ferromagnets are\nexpected to be less magnetized than in the sample interior because of the\nreduced amount of neighboring ferromagnetic spins at the sample edge. We\nrecently demonstrated that CrGeTe3 (CGT) flakes thinner than 10 nm are hard\nferromagnets; i.e., they exhibit an open hysteresis loop. In contrast, thicker\nflakes exhibit zero net remnant field in the interior, with hard ferromagnetism\npresent only at the cleaved edges. This experimental observation suggests that\na nontrivial interaction exists between the sample edge and the interior. Here,\nwe demonstrate that artificial edges fabricated by focus ion beam etching also\ndisplay hard ferromagnetism. This enables us to write magnetic nanowires in CGT\ndirectly and use this method to characterize the magnetic interaction between\nthe interior and edge. The results indicate that the interior saturation and\ndepolarization fields depend on the lateral dimensions of the sample. Most\nnotably, the interior region between the edges of a sample narrower than 300 nm\nbecomes a hard ferromagnet, suggesting an enhancement of the magnetic exchange\ninduced by the proximity of the edges. Last, we find that the CGT regions\namorphized by the gallium beam are nonmagnetic, which introduces a novel method\nto tune the local magnetic properties of CGT films, potentially enabling\nintegration into spintronic devices.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14432","submitter":"Tsun Hin Navin Tsung","authors":"Tsun Hin Navin Tsung, S. Peng Oh, Chad Bustard","title":"The Impact of Cosmic Rays on Thermal and Hydrostatic Stability in\n  Galactic Halos","comments":"33 pages, 25 figures, submitted to MNRAS","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.GA astro-ph.HE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We investigate how cosmic rays (CRs) affect thermal and hydrostatic stability\nof circumgalactic (CGM) gas, in simulations with both CR streaming and\ndiffusion. Local thermal instability can be suppressed by CR-driven entropy\nmode propagation, in accordance with previous analytic work. However, there is\nonly a narrow parameter regime where this operates, before CRs overheat the\nbackground gas. As mass dropout from thermal instability causes the background\ndensity and hence plasma $\\beta \\equiv P_g/P_B$ to fall, the CGM becomes\nglobally unstable. At the cool disk to hot halo interface, a sharp drop in\ndensity boosts Alfven speeds and CR gradients, driving a transition from\ndiffusive to streaming transport. CR forces and heating strengthen, while\ncountervailing gravitational forces and radiative cooling weaken, resulting in\na loss of both hydrostatic and thermal equilibrium. In lower $\\beta$ halos, CR\nheating drives a hot, single-phase diffuse wind with velocities $v \\propto\n(t_\\mathrm{heat}/t_\\mathrm{ff})^{-1}$, which exceeds the escape velocity when\n$t_\\mathrm{heat}/t_\\mathrm{ff} \\lesssim 0.4$. In higher $\\beta$ halos, CR\nforces drive multi-phase winds with cool, dense fountain flows and significant\nturbulence. These flows are CR dominated due to \"trapping\" of CRs by weak\ntransverse B-fields, and have the highest mass loading factors. Thus, local\nthermal instability can result in winds or fountain flows where either the heat\nor momentum input of CRs dominates.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:38 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14433","submitter":"Daniel Parrochia","authors":"Daniel Parrochia","title":"Global warming in figures and the question of its treatment: some\n  historical and epistemological views","comments":"34 pages, 10 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.soc-ph physics.pop-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We first recall fundamentals of elementary climate physics: solar constant,\nradiative balance, greenhouse effect, astronomical parameters of the climate\n(theory of Milankovitch). Without disputing the analyzes of climatologists and\nthe famous Keeling curve revealing in an indisputable way the increase in\nCO$_{2}$ in the atmosphere since the industrial revolution, we nevertheless\ninsist on the main contributor to the greenhouse effect which is, as we know,\nwater vapor. Faced with the difficulties that there will be in imposing\nzero-carbon policies everywhere in the world (and especially in developing\ncountries), we show that it would perhaps be in our interest to act on soil\ndrought, which amounts, in fact, to being interested in the clouds. The\ndecrease in cloud cover, due to a lack of water fixation in the soil, in fact\nincreases the general temperature and therefore the greenhouse effect. Acting\non CO$_{2}$ will always have, in this context, much less effect than acting on\nwater vapor, even indirectly. Despite the difficulty of making this action\nsustainable, due to the balance of atmospheric water vapor and the oceans, it\nwould be in our interest not to neglect this path and also possibly increase\nforest cover for this purpose, given the problems of setting up zero-carbon\npolicy on a global scale. In desperation, one can also consider protecting the\nEarth with an artificial dust cloud.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:00:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14434","submitter":"Yew Ken Chia","authors":"Yew Ken Chia, Hui Chen, Wei Han, Guizhen Chen, Sharifah Mahani\n  Aljunied, Soujanya Poria, Lidong Bing","title":"Domain-Expanded ASTE: Rethinking Generalization in Aspect Sentiment\n  Triplet Extraction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Aspect Sentiment Triplet Extraction (ASTE) is a subtask of Aspect-Based\nSentiment Analysis (ABSA) that considers each opinion term, their expressed\nsentiment, and the corresponding aspect targets. However, existing methods are\nlimited to the in-domain setting with two domains. Hence, we propose a\ndomain-expanded benchmark to address the in-domain, out-of-domain and\ncross-domain settings. We support the new benchmark by annotating more than\n4000 data samples for two new domains based on hotel and cosmetics reviews. Our\nanalysis of five existing methods shows that while there is a significant gap\nbetween in-domain and out-of-domain performance, generative methods have a\nstrong potential for domain generalization. Our datasets, code implementation\nand models are available at https://github.com/DAMO-NLP-SG/domain-expanded-aste .\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:01:49 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14435","submitter":"Amir Sharon","authors":"Amir Sharon, Doron Kushnir, Wenlong Yuan, Lucas Macri, Adam Riess","title":"Reassessing the Constraints from SH0ES Extragalactic Cepheid Amplitudes\n  on Systematic Blending Bias","comments":"20 pages, 12 figures. Submitted to MNRAS","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.CO astro-ph.GA astro-ph.SR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The SH0ES collaboration Hubble constant determination is in a\n$\\mathord{\\sim}5\\sigma$ difference with the \\textit{Planck} value, known as the\nHubble tension. The accuracy of the Hubble constant measured with extragalactic\nCepheids depends on robust stellar-crowding background estimation. Riess et al.\n2020 (R20) compared the light curves amplitudes of extragalactic and MW\nCepheids to constrain an unaccounted systematic blending bias,\n$\\gamma=-0.029\\pm0.037\\,\\rm{mag}$, which cannot explain the required,\n$\\gamma=0.24\\pm0.05\\,\\rm{mag}$, to resolve the Hubble tension. Further checks\nby Riess et al. 2022 demonstrate that a possible blending is not likely related\nto the size of the crowding correction. We repeat the R20 analysis, with the\nfollowing main differences: 1. We limit the extragalactic and MW Cepheids\ncomparison to periods $P\\lesssim50\\,\\rm{d}$, since the number of MW Cepheids\nwith longer periods is minimal; 2. We use publicly available data to\nrecalibrate amplitude ratios of MW Cepheids in standard passbands; 3. We\nremeasure the amplitudes of Cepheids in NGC 5584 and NGC 4258 in two HST\nfilters (F555W and F350LP) to improve the empirical constraint on their\namplitude ratio $A^{555}/A^{350}$. We show that the filter transformations\nintroduce an $\\mathord{\\approx}0.04\\,\\rm{mag}$ uncertainty in determining\n$\\gamma$, not included by R20. While our final estimate,\n$\\gamma=0.013\\pm0.057\\,\\rm{mag}$, is consistent with the value derived by R20,\nthe error is somewhat larger, and the best-fit value is shifted by\n$\\mathord{\\approx}0.04\\,\\rm{mag}$. Although the obtained $\\gamma$ for this\ncrowding test is consistent with zero, folding (in quadratures) it is\n$\\mathord{\\approx}3.0\\sigma$ away from aligning with \\textit{Planck}. Future\nobservations, especially with JWST, would allow better calibration of $\\gamma$.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:02:04 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14436","submitter":"Giles Blaney Ph.D.","authors":"Giles Blaney, Fernando Ivich, Angelo Sassaroli, Mark Niedre, and\n  Sergio Fantini","title":"Dual-ratio approach for detection of point fluorophores in biological\n  tissue","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.med-ph physics.optics q-bio.QM","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Significance: Diffuse in-vivo Flow Cytometry (DiFC) is an emerging\nfluorescence sensing method to non-invasively detect labeled circulating cells\nin-vivo. However, due to Signal-to-Noise Ratio (SNR) constraints largely\nattributed to background tissue autofluorescence, DiFC's measurement depth is\nlimited.\n  Aim: The Dual-Ratio (DR) / Dual-Slope (DS) is a new optical measurement\nmethod that aims to suppress noise and enhance SNR to deep tissue regions. We\naim to investigate the combination of DR and DiFC to improve circulating cells'\nmaximum detectable depth and SNR.\n  Approach: Phantom experiments were used to estimate the key parameters in a\ndiffuse fluorescence excitation and emission model. This model and parameters\nwere implemented in Monte-Carlo to simulate DR DiFC while varying noise and\nautofluorescence parameters to identify the advantages and limitations of the\nproposed technique.\n  Results: Two key factors must be true to give DR DiFC an advantage over\ntraditional DiFC; first, the fraction of noise that DR methods cannot cancel\ncannot be above the order of 10%. Second, DR DiFC has an advantage if the\ndistribution of tissue autofluorescence contributors is surface-weighted.\n  Conclusions: DR cancelable noise may be designed for (e.g. through the use of\nsource multiplexing), and indications point to the autofluorescence\ncontributors' distribution being truly surface-weighted in-vivo. Successful and\nworthwhile implementation of DR DiFC depends on these considerations, but\nindications point to DR DiFC having possible advantages over traditional DiFC.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:04:05 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14437","submitter":"Mattia Carlo Sormani","authors":"Mattia C. Sormani, Ashley T. Barnes, Jiayi Sun, Sophia K. Stuber, Eva\n  Schinnerer, Eric Emsellem, Adam K. Leroy, Simon C.O. Glover, Jonathan D.\n  Henshaw, Sharon E. Meidt, Justus Neumann, Miguel Querejeta, Thomas G.\n  Williams, Frank Bigiel, Cosima Eibensteiner, Francesca Fragkoudi, Rebecca C.\n  Levy, Kathryn Grasha, Ralf S. Klessen, J. M. Diederik Kruijssen, Nadine\n  Neumayer, Francesca Pinna, Erik W. Rosolowsky, Rowan J. Smith, Yu-Hsuan Teng,\n  Robin G. Tress, Elizabeth J. Watkins","title":"Fuelling the nuclear ring of NGC 1097","comments":"Accepted in MNRAS","journal-ref":null,"doi":"10.1093/mnras/stad1554","report-no":null,"categories":"astro-ph.GA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Galactic bars can drive cold gas inflows towards the centres of galaxies. The\ngas transport happens primarily through the so-called bar ``dust lanes'', which\nconnect the galactic disc at kpc scales to the nuclear rings at hundreds of pc\nscales much like two gigantic galactic rivers. Once in the ring, the gas can\nfuel star formation activity, galactic outflows, and central supermassive black\nholes. Measuring the mass inflow rates is therefore important to understanding\nthe mass/energy budget and evolution of galactic nuclei. In this work, we use\nCO datacubes from the PHANGS-ALMA survey and a simple geometrical method to\nmeasure the bar-driven mass inflow rate onto the nuclear ring of the barred\ngalaxy NGC~1097. The method assumes that the gas velocity in the bar lanes is\nparallel to the lanes in the frame co-rotating with the bar, and allows one to\nderive the inflow rates from sufficiently sensitive and resolved\nposition-position-velocity diagrams if the bar pattern speed and galaxy\norientations are known. We find an inflow rate of $\\dot{M}=(3.0 \\pm 2.1)\\, \\rm\nM_\\odot\\, yr^{-1}$ averaged over a time span of 40 Myr, which varies by a\nfactor of a few over timescales of $\\sim$10 Myr. Most of the inflow appears to\nbe consumed by star formation in the ring which is currently occurring at a\nrate of ${\\rm SFR}\\simeq~1.8$-$2 \\rm M_\\odot\\, yr^{-1}$, suggesting that the\ninflow is causally controlling the star formation rate in the ring as a\nfunction of time.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:04:23 GMT"}],"update_date":"2023-05-31"}
{"id":"2305.14438","submitter":"Lennart Klebl","authors":"Ammon Fischer and Lennart Klebl and Jonas B. Hauck and Alexander\n  Rothstein and Lutz Waldecker and Bernd Beschoten and Tim O. Wehling and Dante\n  M. Kennes","title":"Spin and Charge Fluctuation Induced Pairing in ABCB Tetralayer Graphene","comments":"5 pages, 4 figures, supplementary information","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.supr-con cond-mat.mes-hall cond-mat.str-el","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Motivated by the recent experimental realization of ABCB stacked tetralayer\ngraphene [Wirth et al., ACS Nano 16, 16617 (2022)], we study correlated\nphenomena in moir\\'e-less graphene tetralayers for realistic interaction\nprofiles using an orbital resolved random phase approximation approach. We\ndemonstrate that magnetic fluctuations originating from local interactions are\ncrucial close to the van Hove singularities on the electron- and hole-doped\nside promoting layer selective ferrimagnetic states. Spin fluctuations around\nthese magnetic states enhance unconventional spin-triplet, valley-singlet\nsuperconductivity with $f$-wave symmetry due to intervalley scattering. Charge\nfluctuations arising from long range Coulomb interactions promote doubly\ndegenerate $p$-wave superconductivity close to the van Hove singularities. At\nthe conduction band edge of ABCB graphene, we find that both spin and charge\nfluctuations drive $f$-wave superconductivity. Our analysis suggests a strong\ncompetition between superconducting states emerging from long- and short-ranged\nCoulomb interactions and thus stresses the importance of microscopically\nderived interaction profiles to make reliable predictions for the origin of\nsuperconductivity in graphene based heterostructures.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:04:48 GMT"},{"version":"v2","created":"Thu, 25 May 2023 13:00:43 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.14439","submitter":"Ewelina Mulawa","authors":"Ewelina Mulawa","title":"A detailed description of the generalized Calabi type Kahler surfaces","comments":"28 pages. arXiv admin note: text overlap with arXiv:1906.11640","journal-ref":null,"doi":null,"report-no":null,"categories":"math.DG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper we study QCH K\\\"ahler surfaces, i.e. 4-dimensional Riemannian\nmanifolds (of signature (++++)) admitting a K\\\"ahler complex structure with\nquasi-constant holomorphic sectional curvature. We give a detailed description\nof QCH K\\\"ahler surfaces of generalized Calabi type.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:06:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14440","submitter":"Andrea Placidi PhD","authors":"Andrea Placidi, Gianluca Grignani, Troels Harmark, Marta Orselli, Sara\n  Gliorio, and Alessandro Nagar","title":"2.5PN accurate waveform information for generic-planar-orbit binaries in\n  effective one-body models","comments":"14 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We provide the post-Newtonian (PN) waveform for binary systems in motion\nalong generic planar orbits at 2.5PN accuracy, in terms of the dynamical\nvariables of the effective one-body (EOB) formalism. In addition to the\ncalculation of the higher order terms for all the contributions to the waveform\nthat have been already considered in previous avatars of EOB models, we also\ncompute the EOB expression of the oscillatory memory terms. These are purely\nnon-circular contributions, first appearing at 1.5PN order, that have been so\nfar neglected in the EOB literature. This should foster their inclusion in EOB\nmodels and the definitive assessment of their role in shaping gravitational\nwave signals at infinity. To further promote the application of our results, we\nalso derive associated non-circular factors according to the waveform\nfactorization prescription of the non-circular EOB model TEOBResumS-DALI; the\nresult is a set of ready-to-use non-circular factors that can be directly\nimplemented as extra non-circular corrections in the waveform of\nTEOBResumS-DALI.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:07:03 GMT"},{"version":"v2","created":"Mon, 29 May 2023 07:59:59 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.14441","submitter":"Zhihan Zhang","authors":"Zhihan Zhang and Wenhao Yu and Zheng Ning and Mingxuan Ju and Meng\n  Jiang","title":"Exploring Contrast Consistency of Open-Domain Question Answering Systems\n  on Minimally Edited Questions","comments":"Accepted at TACL. This is a pre-MIT Press publication version","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Contrast consistency, the ability of a model to make consistently correct\npredictions in the presence of perturbations, is an essential aspect in NLP.\nWhile studied in tasks such as sentiment analysis and reading comprehension, it\nremains unexplored in open-domain question answering (OpenQA) due to the\ndifficulty of collecting perturbed questions that satisfy factuality\nrequirements. In this work, we collect minimally edited questions as\nchallenging contrast sets to evaluate OpenQA models. Our collection approach\ncombines both human annotation and large language model generation. We find\nthat the widely used dense passage retriever (DPR) performs poorly on our\ncontrast sets, despite fitting the training set well and performing\ncompetitively on standard test sets. To address this issue, we introduce a\nsimple and effective query-side contrastive loss with the aid of data\naugmentation to improve DPR training. Our experiments on the contrast sets\ndemonstrate that DPR's contrast consistency is improved without sacrificing its\naccuracy on the standard test sets.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:07:04 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14442","submitter":"Michalis Titsias","authors":"Michalis K. Titsias","title":"Optimal Preconditioning and Fisher Adaptive Langevin Sampling","comments":"20 pages, 14 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG stat.CO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We define an optimal preconditioning for the Langevin diffusion by\nanalytically maximizing the expected squared jumped distance. This yields as\nthe optimal preconditioning an inverse Fisher information covariance matrix,\nwhere the covariance matrix is computed as the outer product of log target\ngradients averaged under the target. We apply this result to the Metropolis\nadjusted Langevin algorithm (MALA) and derive a computationally efficient\nadaptive MCMC scheme that learns the preconditioning from the history of\ngradients produced as the algorithm runs. We show in several experiments that\nthe proposed algorithm is very robust in high dimensions and significantly\noutperforms other methods, including a closely related adaptive MALA scheme\nthat learns the preconditioning with standard adaptive MCMC as well as the\nposition-dependent Riemannian manifold MALA sampler.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:07:44 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14443","submitter":"J. Stanton","authors":"J. Stanton (1), R. Sharankova (1), K. Seiya (1), M. Wesley (1) ((1)\n  Fermilab)","title":"Beam Loss Monitoring with Fixed and Translating Scintillation Detectors\n  Along the Fermilab Drift-tube Linac","comments":"14th International Particle Accelerator Conf. (IPAC23), Venice,\n  Italy, May 2023","journal-ref":null,"doi":null,"report-no":"FERMILAB-CONF-23-202-AD","categories":"physics.acc-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The Fermilab Linac is a roughly 145 meter linear accelerator that accelerates\nH- beam from 750 keV to 400 MeV and provides beam for the Booster and the rest\nof the accelerator chain. The first section of the Linac is a Drift-Tube Linac\n(DTL), which in its current state, suffers from a lack of instrumentation along\nits length. As a result, operational staff do not have access to the diagnostic\ninformation needed to tune the critical components of this accelerator, such as\nthe quadrupole magnets within the drift tubes. This work presents an effort to\nutilize both fixed and translating scintillation detectors to investigate beam\nloss along the first two cavities of the DTL.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:08:43 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14444","submitter":"Valerio De Luca","authors":"Valerio De Luca, Justin Khoury, Sam S. C. Wong","title":"Non-linearities in the tidal Love numbers of black holes","comments":"30 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc hep-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Tidal Love numbers describe the linear response of a compact object under the\npresence of external tidal perturbations, and they are found to vanish exactly\nfor black holes within General Relativity. In this paper we investigate the\ntidal deformability of neutral black holes when non-linearities in the theory\nare taken into account. As a case in point, we consider scalar tidal\nperturbations on the black hole background, and find that the tidal Love\nnumbers may be non vanishing depending on the scalar interactions in the bulk\ntheory. Remarkably, for non-linear sigma models, we find that the tidal Love\nnumbers vanish to all orders in perturbation theory.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:09:46 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14445","submitter":"Jenny Chen","authors":"Jenny Chen (1), Benjamin Ades-Aron (1), Hong-Hsi Lee (2), Michelle\n  Pang (3), Dmitry S. Novikov (1), Jelle Veraart (1), Els Fieremans (1)","title":"Optimization and Validation of the DESIGNER dMRI preprocessing pipeline\n  in white matter aging","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.med-ph physics.bio-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  DESIGNER, a preprocessing pipeline for clinically acquired diffusion MRI\ndata, has been modified to improve denoising and target Gibbs ringing for\npartial Fourier acquisitions. Here, we compare DESIGNER against other pipelines\non a large clinical dMRI dataset (554 controls, 25 to 75 years old) and\nassessed DESIGNER's denoise and degibbs methods using ground truth phantom.\nResults show DESIGNER provides more accurate and robust parameter maps.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:09:56 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14446","submitter":"Sarunas Verner","authors":"Marcos A. G. Garcia, Mathias Pierre, Sarunas Verner","title":"A New Window into Gravitationally Produced Scalar Dark Matter","comments":"6 pages, 3 figures (Supplementary Material: 13 pages, 5 figures)","journal-ref":null,"doi":null,"report-no":"DESY-23-065","categories":"hep-ph astro-ph.CO gr-qc","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Conventional scenarios of purely gravitationally produced dark matter with\nmasses below the Hubble parameter at the end of inflation are in tension with\nCosmic Microwave Background (CMB) constraints on the isocurvature power\nspectrum. We explore a more general scenario with a non-minimal coupling\nbetween the scalar dark matter field and gravity, which allows for\nsignificantly lighter scalar dark matter masses compared to minimal coupling\npredictions. By imposing relic abundance, isocurvature, Lyman-$\\alpha$, and Big\nBang Nucleosynthesis (BBN) constraints, we show the viable parameter space for\nthese models. Our findings demonstrate that the presence of a non-minimal\ncoupling expands the parameter space, yielding a dark matter mass lower bound\nof $2 \\times 10^{-4} \\, \\rm{eV}$.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:10:23 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14447","submitter":"Joanne Pledger Dr","authors":"Joanne L. Pledger and Michael M. Shara","title":"Possible detection of the progenitor of the Type II supernova SN2023ixf","comments":"5 pages, 3 figures, 1 table, submitted to ApJL","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.SR astro-ph.HE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Stellar evolution theory predicts multiple pathways to the explosive deaths\nof stars as supernovae. Locating and characterizing the progenitors of\nwell-studied supernovae is important to constrain the theory, and to justify\nand design future surveys to improve on progenitor detections. Here we report\nthe serendipitous pre-explosion imaging, by the Hubble Space Telescope, of\nSN2023ixf, one of the nearest extragalactic supernovae ever discovered, in the\ngalaxy M101. The extremely red color and absolute magnitude\nM(F814W)=-5.42+/-0.06mag suggest that the progenitor was a red supergiant and\ncomparison with stellar evolutionary isochrones suggests a mass of ~12M_Sun.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:11:14 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14448","submitter":"Daniel Gra\\c{c}a","authors":"Daniel S. Gra\\c{c}a and Ning Zhong","title":"Robust non-computability and stability of dynamical systems","comments":"arXiv admin note: substantial text overlap with arXiv:2109.15080","journal-ref":null,"doi":null,"report-no":null,"categories":"math.LO cs.LO math.DS","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, we examine the relationship between the stability of the\ndynamical system $x^{\\prime}=f(x)$ and the computability of its basins of\nattraction. We present a computable $C^{\\infty}$ system $x^{\\prime}=f(x)$ that\npossesses a computable and stable equilibrium point, yet whose basin of\nattraction is robustly non-computable in a neighborhood of $f$ in the sense\nthat both the equilibrium point and the non-computability of its associated\nbasin of attraction persist when $f$ is slightly perturbed. This indicates that\nlocal stability near a stable equilibrium point alone is insufficient to\nguarantee the computability of its basin of attraction. However, we also\ndemonstrate that the basins of attraction associated with a structurally stable\n- globally stable - planar system are computable. Our findings suggest that the\nglobal stability of a system plays a pivotal role in determining the\ncomputability of its basins of attraction.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:12:56 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14449","submitter":"Zheng Chen","authors":"Zheng Chen, Ziyan Jiang, Fan Yang, Eunah Cho, Xing Fan, Xiaojiang\n  Huang, Yanbin Lu, Aram Galstyan","title":"Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust\n  Conversational Understanding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.IR cs.LG","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Conversational AI systems such as Alexa need to understand defective queries\nto ensure robust conversational understanding and reduce user friction. These\ndefective queries often arise from user ambiguities, mistakes, or errors in\nautomatic speech recognition (ASR) and natural language understanding (NLU).\n  Personalized query rewriting is an approach that focuses on reducing defects\nin queries by taking into account the user's individual behavior and\npreferences. It typically relies on an index of past successful user\ninteractions with the conversational AI. However, unseen interactions within\nthe user's history present additional challenges for personalized query\nrewriting. This paper presents our \"Collaborative Query Rewriting\" approach,\nwhich specifically addresses the task of rewriting new user interactions that\nhave not been previously observed in the user's history. This approach builds a\n\"User Feedback Interaction Graph\" (FIG) of historical user-entity interactions\nand leverages multi-hop graph traversal to enrich each user's index to cover\nfuture unseen defective queries. The enriched user index is called a\nCollaborative User Index and contains hundreds of additional entries. To\ncounteract precision degradation from the enlarged index, we add additional\ntransformer layers to the L1 retrieval model and incorporate graph-based and\nguardrail features into the L2 ranking model.\n  Since the user index can be pre-computed, we further investigate the\nutilization of a Large Language Model (LLM) to enhance the FIG for user-entity\nlink prediction in the Video/Music domains. Specifically, this paper\ninvestigates the Dolly-V2 7B model. We found that the user index augmented by\nthe fine-tuned Dolly-V2 generation significantly enhanced the coverage of\nfuture unseen user interactions, thereby boosting QR performance on unseen\nqueries compared with the graph traversal only approach.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:15:29 GMT"},{"version":"v2","created":"Sat, 3 Jun 2023 12:42:45 GMT"}],"update_date":"2023-06-06"}
{"id":"2305.14450","submitter":"Ridong Han","authors":"Ridong Han, Tao Peng, Chaohao Yang, Benyou Wang, Lu Liu, Xiang Wan","title":"Is Information Extraction Solved by ChatGPT? An Analysis of Performance,\n  Evaluation Criteria, Robustness and Errors","comments":"23 pages, version 1.0","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  ChatGPT has stimulated the research boom in the field of large language\nmodels. In this paper, we assess the capabilities of ChatGPT from four\nperspectives including Performance, Evaluation Criteria, Robustness and Error\nTypes. Specifically, we first evaluate ChatGPT's performance on 17 datasets\nwith 14 IE sub-tasks under the zero-shot, few-shot and chain-of-thought\nscenarios, and find a huge performance gap between ChatGPT and SOTA results.\nNext, we rethink this gap and propose a soft-matching strategy for evaluation\nto more accurately reflect ChatGPT's performance. Then, we analyze the\nrobustness of ChatGPT on 14 IE sub-tasks, and find that: 1) ChatGPT rarely\noutputs invalid responses; 2) Irrelevant context and long-tail target types\ngreatly affect ChatGPT's performance; 3) ChatGPT cannot understand well the\nsubject-object relationships in RE task. Finally, we analyze the errors of\nChatGPT, and find that \"unannotated spans\" is the most dominant error type.\nThis raises concerns about the quality of annotated data, and indicates the\npossibility of annotating data with ChatGPT. The data and code are released at\nGithub site.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:17:43 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14451","submitter":"Mohit Yadav","authors":"Mohit Yadav, Daniel Sheldon, Cameron Musco","title":"Kernel Interpolation with Sparse Grids","comments":"Accepted at Neural Information Processing Systems (NeurIPS) 2022","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI stat.ML","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Structured kernel interpolation (SKI) accelerates Gaussian process (GP)\ninference by interpolating the kernel covariance function using a dense grid of\ninducing points, whose corresponding kernel matrix is highly structured and\nthus amenable to fast linear algebra. Unfortunately, SKI scales poorly in the\ndimension of the input points, since the dense grid size grows exponentially\nwith the dimension. To mitigate this issue, we propose the use of sparse grids\nwithin the SKI framework. These grids enable accurate interpolation, but with a\nnumber of points growing more slowly with dimension. We contribute a novel\nnearly linear time matrix-vector multiplication algorithm for the sparse grid\nkernel matrix. Next, we describe how sparse grids can be combined with an\nefficient interpolation scheme based on simplices. With these changes, we\ndemonstrate that SKI can be scaled to higher dimensions while maintaining\naccuracy.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:17:49 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.14452","submitter":"Qidong Yang","authors":"Qidong Yang, Alex Hernandez-Garcia, Paula Harder, Venkatesh Ramesh,\n  Prasanna Sattegeri, Daniela Szwarcman, Campbell D. Watson, David Rolnick","title":"Fourier Neural Operators for Arbitrary Resolution Climate Data\n  Downscaling","comments":"Presented at the ICLR 2023 workshop on \"Tackling Climate Change with\n  Machine Learning\"","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG physics.ao-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Climate simulations are essential in guiding our understanding of climate\nchange and responding to its effects. However, it is computationally expensive\nto resolve complex climate processes at high spatial resolution. As one way to\nspeed up climate simulations, neural networks have been used to downscale\nclimate variables from fast-running low-resolution simulations, but\nhigh-resolution training data are often unobtainable or scarce, greatly\nlimiting accuracy. In this work, we propose a downscaling method based on the\nFourier neural operator. It trains with data of a small upsampling factor and\nthen can zero-shot downscale its input to arbitrary unseen high resolution.\nEvaluated both on ERA5 climate model data and on the Navier-Stokes equation\nsolution data, our downscaling model significantly outperforms state-of-the-art\nconvolutional and generative adversarial downscaling models, both in standard\nsingle-resolution downscaling and in zero-shot generalization to higher\nupsampling factors. Furthermore, we show that our method also outperforms\nstate-of-the-art data-driven partial differential equation solvers on\nNavier-Stokes equations. Overall, our work bridges the gap between simulation\nof a physical process and interpolation of low-resolution output, showing that\nit is possible to combine both approaches and significantly improve upon each\nother.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:25:08 GMT"},{"version":"v2","created":"Tue, 30 May 2023 13:03:25 GMT"}],"update_date":"2023-05-31"}
{"id":"2305.14453","submitter":"Subba Reddy Oota","authors":"Pavan Kalyan Reddy Neerudu, Subba Reddy Oota, Mounika Marreddy,\n  Venkateswara Rao Kagita, Manish Gupta","title":"On Robustness of Finetuned Transformer-based NLP Models","comments":"16 pages, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Transformer-based pretrained models like BERT, GPT-2 and T5 have been\nfinetuned for a large number of natural language processing (NLP) tasks, and\nhave been shown to be very effective. However, while finetuning, what changes\nacross layers in these models with respect to pretrained checkpoints is\nunder-studied. Further, how robust are these models to perturbations in input\ntext? Does the robustness vary depending on the NLP task for which the models\nhave been finetuned? While there exists some work on studying robustness of\nBERT finetuned for a few NLP tasks, there is no rigorous study which compares\nthis robustness across encoder only, decoder only and encoder-decoder models.\n  In this paper, we study the robustness of three language models (BERT, GPT-2\nand T5) with eight different text perturbations on the General Language\nUnderstanding Evaluation (GLUE) benchmark. Also, we use two metrics (CKA and\nSTIR) to quantify changes between pretrained and finetuned language model\nrepresentations across layers. GPT-2 representations are more robust than BERT\nand T5 across multiple types of input perturbation. Although models exhibit\ngood robustness broadly, dropping nouns, verbs or changing characters are the\nmost impactful. Overall, this study provides valuable insights into\nperturbation-specific weaknesses of popular Transformer-based models which\nshould be kept in mind when passing inputs.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:25:18 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14454","submitter":"Ben Anson","authors":"Sebastian Ober, Ben Anson, Edward Milsom and Laurence Aitchison","title":"An Improved Variational Approximate Posterior for the Deep Wishart\n  Process","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Deep kernel processes are a recently introduced class of deep Bayesian models\nthat have the flexibility of neural networks, but work entirely with Gram\nmatrices. They operate by alternately sampling a Gram matrix from a\ndistribution over positive semi-definite matrices, and applying a deterministic\ntransformation. When the distribution is chosen to be Wishart, the model is\ncalled a deep Wishart process (DWP). This particular model is of interest\nbecause its prior is equivalent to a deep Gaussian process (DGP) prior, but at\nthe same time it is invariant to rotational symmetries, leading to a simpler\nposterior distribution. Practical inference in the DWP was made possible in\nrecent work (\"A variational approximate posterior for the deep Wishart process\"\nOber and Aitchison 2021a) where the authors used a generalisation of the\nBartlett decomposition of the Wishart distribution as the variational\napproximate posterior. However, predictive performance in that paper was less\nimpressive than one might expect, with the DWP only beating a DGP on a few of\nthe UCI datasets used for comparison. In this paper, we show that further\ngeneralising their distribution to allow linear combinations of rows and\ncolumns in the Bartlett decomposition results in better predictive performance,\nwhile incurring negligible additional computation cost.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:26:29 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14455","submitter":"Bruno Leonardo Canto Martins","authors":"Bruno L. Canto Martins, Yuri S. Messias, Maria I. Arruda\n  Gon\\c{c}alves, Izan C. Le\\~ao, Roseane L. Gomes, Lorenza F. Barraza, Dasaev\n  O. Fontinele, and Jos\\'e R. De Medeiros","title":"On the behaviour of spin-orbit connection of exoplanets","comments":"15 pages, 1 figure in main paper, 6 supplementary figures. Published\n  in Nature Astronomy, May 2023","journal-ref":null,"doi":"10.1038/s41550-023-01976-0","report-no":null,"categories":"astro-ph.EP astro-ph.SR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Star-planet interactions play, among other things, a crucial role in\nplanetary orbital configurations by circularizing orbits, aligning the star and\nplanet spin and synchronizing stellar rotation with orbital motions. This is\nespecially true for innermost giant planets, which can be schematized as binary\nsystems with a very large mass ratio. Despite a few examples where spin-orbit\nsynchronization has been obtained, there is no demographic study on synchronous\nregimes in those systems yet. Here we use a sample of 1,055 stars with\ninnermost planet companions to show the existence of three observational loci\nof star-planet synchronization regimes. Two of them have dominant fractions of\nsubsynchronous and supersynchronous star-planet systems, and a third less\npopulated regime of potentially synchronized systems. No synchronous\nstar-planet system with a period higher than 40 days has been detected yet.\nThis landscape is different from eclipsing binary systems, most of which are\nsynchronized. We suggest that planets in a stable asynchronous spin state\nbelonging to star-planet systems in a supersynchronized regime offer the most\nfavourable conditions for habitability.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:27:35 GMT"},{"version":"v2","created":"Sun, 28 May 2023 13:25:08 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.14456","submitter":"Tarek Naous","authors":"Tarek Naous, Michael J. Ryan, Wei Xu","title":"Having Beer after Prayer? Measuring Cultural Bias in Large Language\n  Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Are language models culturally biased? It is important that language models\nconform to the cultural aspects of the communities they serve. However, we show\nin this paper that language models suffer from a significant bias towards\nWestern culture when handling and generating text in Arabic, often preferring,\nand producing Western-fitting content as opposed to the relevant Arab content.\nWe quantify this bias through a likelihood scoring-based metric using naturally\noccurring contexts that we collect from online social media. Our experiments\nreveal that both Arabic monolingual and multilingual models exhibit bias\ntowards Western culture in eight different cultural aspects: person names,\nfood, clothing, location, literature, beverage, religion, and sports. Models\nalso tend to exhibit more bias when prompted with Arabic sentences that are\nmore linguistically aligned with English. These findings raise concerns about\nthe cultural relevance of current language models. Our analyses show that\nproviding culture-indicating tokens or culturally-relevant demonstrations to\nthe model can help in debiasing.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:27:51 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14457","submitter":"Mengxia Yu","authors":"Mengxia Yu, Zhihan Zhang, Wenhao Yu, Meng Jiang","title":"Pre-training Language Models for Comparative Reasoning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper, we propose a novel framework to pre-train language models for\nenhancing their abilities of comparative reasoning over texts. While recent\nresearch has developed models for NLP tasks that require comparative reasoning,\nthey suffer from costly manual data labeling and limited generalizability to\ndifferent tasks. Our approach involves a scalable method for collecting data\nfor text-based entity comparison, which leverages both structured and\nunstructured data, and the design of three novel pre-training tasks. Evaluation\non a range of downstream tasks including comparative question answering,\nquestion generation, and summarization shows that our pre-training framework\nsignificantly improves the comparative reasoning abilities of language models,\nespecially under low-resource conditions. This work also releases the first\nintegrated benchmark for comparative reasoning over texts.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:28:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14458","submitter":"Yao Dou","authors":"David Heineman, Yao Dou, Mounica Maddela, Wei Xu","title":"Dancing Between Success and Failure: Edit-level Simplification\n  Evaluation using SALSA","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  Large language models (e.g., GPT-3.5) are uniquely capable of producing\nhighly rated text simplification, yet current human evaluation methods fail to\nprovide a clear understanding of systems' specific strengths and weaknesses. To\naddress this limitation, we introduce SALSA, an edit-based human annotation\nframework that enables holistic and fine-grained text simplification\nevaluation. We develop twenty one linguistically grounded edit types, covering\nthe full spectrum of success and failure across dimensions of conceptual,\nsyntactic and lexical simplicity. Using SALSA, we collect 12K edit annotations\non 700 simplifications, revealing discrepancies in the distribution of\ntransformation approaches performed by fine-tuned models, few-shot LLMs and\nhumans, and finding GPT-3.5 performs more quality edits than humans, but still\nexhibits frequent errors. Using our fine-grained annotations, we develop\nLENS-SALSA, a reference-free automatic simplification metric, trained to\npredict sentence- and word-level quality simultaneously. Additionally, we\nintroduce word-level quality estimation for simplification and report promising\nbaseline results. Our training material, annotation toolkit, and data are\nreleased at http://salsa-eval.com.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:30:49 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14459","submitter":"Yunzhe Li","authors":"Yunzhe Li, Qian Chen, Weixiang Yan, Wen Wang, Qinglin Zhang, Hari\n  Sundaram","title":"Enhancing Generation through Summarization Duality and Explicit Outline\n  Control","comments":"6 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Automatically open-ended long text generation poses significant challenges\ndue to semantic incoherence and plot implausibility. Previous works usually\nalleviate this problem through outlines in the form of short phrases or\nabstractive signals by designing unsupervised tasks, which tend to be unstable\nand weakly interpretable.\n  Assuming that a summary serves as a mature outline, we introduce a two-stage,\nsummary-enhanced outline supervised generation framework. This framework\nleverages the dual characteristics of the summarization task to improve outline\nprediction, resulting in more explicit and plausible outlines. Furthermore, we\nidentify an underutilization issue in outline-based generation with both\nstandard pretrained language models (e.g., GPT-2, BART) and large language\nmodels (e.g., Vicuna, ChatGPT). To address this, we propose a novel explicit\noutline control method for more effective utilization of generated outlines.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:33:52 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14460","submitter":"Abbas Sharifi","authors":"Mohsen Ahmadi, Ahmad Gholizadeh Lonbar, Mohammadsadegh Nouri, Amir\n  Sharifzadeh Javidi, Ali Tarlani Beris, Abbas Sharifi, Ali Salimi-Tarazouj","title":"Supervised Multi-Regional Segmentation Machine Learning Architecture for\n  Digital Twin Applications in Coastal Regions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This study explores the use of a digital twin model and deep learning method\nto build a global terrain and altitude map based on USGS information. The goal\nis to artistically represent various landforms while incorporating precise\nelevation modifications in the terrain map and encoding land height in the\naltitude map. A random selection of 5000 segments from the worldwide map\nguarantees the inclusion of significant characteristics in the subsets, with\nrescaling according to latitude accounting for distortions caused by map\nprojection. The process of generating segmentation maps involves using\nunsupervised clustering and classification methods, segmenting the terrain into\nseven groups: Water, Grassland, Forest, Hills, Desert, Mountain, and Tundra.\nEach group is assigned a unique color, and median filtering is used to improve\nmap characteristics. Random parameters are added to provide diversity and avoid\nduplication in overlapping image sets. The U-Net network is deployed for the\nsegmentation task, with training conducted on the seven terrain classes.\nCross-validation is carried out every 10 epochs to gauge the model's\nperformance. The segmentation maps produced accurately categorize the terrain,\nas evidenced by the ROC curve and AUC values. The main goal of this research is\nto create a digital twin model of Florida's coastal area. This is achieved\nthrough the application of deep learning methods and satellite imagery from\nGoogle Earth, resulting in a detailed depiction of the coast of Florida. The\ndigital twin acts as both a physical and a simulation model of the area,\nemphasizing its capability to capture and replicate real-world locations. The\nmodel effectively creates a global terrain and altitude map with precise\nsegmentation and capture of important land features. The results confirm the\neffectiveness of the digital twin, especially in depicting Florida's coastline.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:35:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14461","submitter":"Diego Arroyuelo Darroyue","authors":"Diego Arroyuelo, Gabriel Carmona, H\\'ector Larra\\~naga, Francisco\n  Riveros, Erick Sep\\'ulveda","title":"Engineering Rank/Select Data Structures for Big-Alphabet Strings","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DS","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  Big-alphabet strings are common in several scenarios such as information\nretrieval and natural-language processing. The efficient storage and processing\nof such strings usually introduces several challenges that are not witnessed in\nsmaller-alphabets strings. This paper studies the efficient implementation of\none of the most effective approaches for dealing with big-alphabet strings,\nnamely the \\emph{alphabet-partitioning} approach. The main contribution is a\ncompressed data structure that supports the fundamental operations rank and\nselect efficiently. We show experimental results that indicate that our\nimplementation outperforms the current realizations of the\nalphabet-partitioning approach. In particular, the time for operation select\ncan be improved by about 80%, using only 11% more space than current\nalphabet-partitioning schemes. We also show the impact of our data structure on\nseveral applications, like the intersection of inverted lists (where\nimprovements of up to 60% are achieved, using only 2% of extra space), the\nrepresentation of run-length compressed strings, and the\ndistributed-computation processing of rank and select operations.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:35:39 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14462","submitter":"Hanlin Mo","authors":"Hanlin Mo and Guoying Zhao","title":"Sorted Convolutional Network for Achieving Continuous Rotational\n  Invariance","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The topic of achieving rotational invariance in convolutional neural networks\n(CNNs) has gained considerable attention recently, as this invariance is\ncrucial for many computer vision tasks such as image classification and\nmatching. In this letter, we propose a Sorting Convolution (SC) inspired by\nsome hand-crafted features of texture images, which achieves continuous\nrotational invariance without requiring additional learnable parameters or data\naugmentation. Further, SC can directly replace the conventional convolution\noperations in a classic CNN model to achieve its rotational invariance. Based\non MNIST-rot dataset, we first analyze the impact of convolutional kernel\nsizes, different sampling and sorting strategies on SC's rotational invariance,\nand compare our method with previous rotation-invariant CNN models. Then, we\ncombine SC with VGG, ResNet and DenseNet, and conduct classification\nexperiments on popular texture and remote sensing image datasets. Our results\ndemonstrate that SC achieves the best performance in the aforementioned tasks.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:37:07 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14463","submitter":"Tarek Naous","authors":"Tarek Naous, Michael J. Ryan, Mohit Chandra, Wei Xu","title":"Towards Massively Multi-domain Multilingual Readability Assessment","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We present ReadMe++, a massively multi-domain multilingual dataset for\nautomatic readability assessment. Prior work on readability assessment has been\nmostly restricted to the English language and one or two text domains.\nAdditionally, the readability levels of sentences used in many previous\ndatasets are assumed on the document-level other than sentence-level, which\nraises doubt about the quality of previous evaluations. We address those gaps\nin the literature by providing an annotated dataset of 6,330 sentences in\nArabic, English, and Hindi collected from 64 different domains of text. Unlike\nprevious datasets, ReadMe++ offers more domain and language diversity and is\nmanually annotated at a sentence level using the Common European Framework of\nReference for Languages (CEFR) and through a Rank-and-Rate annotation framework\nthat reduces subjectivity in annotation. Our experiments demonstrate that\nmodels fine-tuned using ReadMe++ achieve strong cross-lingual transfer\ncapabilities and generalization to unseen domains. ReadMe++ will be made\npublicly available to the research community.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:37:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14464","submitter":"Doyeol (David) Ahn","authors":"Doyeol Ahn (1,2) ((1) Department of Electrical and Computer\n  Engineering, University of Seoul, Seoul, Republic of Korea (2) First Quantum,\n  Inc, Seoul, Republic of Korea)","title":"Non-Markovian cost function for quantum error mitigation with Dirac\n  Gamma matrices representation","comments":"arXiv admin note: text overlap with arXiv:2302.05053","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this study, we explore the non-Markovian cost function for quantum error\nmitigation (QEM) and the representation of two-qubit operators using Dirac\nGamma matrices, central to the structure of relativistic quantum mechanics. The\nprimary focus of quantum computing research, particularly with noisy\nintermediate-scale quantum (NISQ) devices, is on reducing errors and\ndecoherence for practical application. While much of the existing research\nconcentrates on Markovian noise sources, the study of non-Markovian sources is\ncrucial given their inevitable presence in most solid-state quantum computing\ndevices. We introduce a non-Markovian model of quantum state evolution and a\ncorresponding QEM cost function for NISQ devices, considering an environment\ntypified by simple harmonic oscillators as a noise source. The Dirac Gamma\nmatrices, integral to areas of physics like quantum field theory and\nsupersymmetry, share a common algebraic structure with two-qubit gate\noperators. By representing the latter using Gamma matrices, we are able to more\neffectively analyze and manipulate these operators due to the distinct\nproperties of Gamma matrices. We evaluate the fluctuations of the output\nquantum state for identity and SWAP gate operations in two-qubit operations\nacross various input states. By comparing these results with experimental data\nfrom ion-trap and superconducting quantum computing systems, we estimate the\nkey parameters of the QEM cost functions. Our results reveal that as the\ncoupling strength between the quantum system and its environment increases, so\ndoes the QEM cost function. This study underscores the importance of\nnon-Markovian models for understanding quantum state evolution and the\npractical implications of the QEM cost function when assessing experimental\nresults from NISQ devices.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:38:09 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14465","submitter":"Wei Zhang","authors":"Chao Li, Michael Rapoport, Wei Zhang","title":"Arithmetic Fundamental Lemma for the spherical Hecke algebra","comments":"47 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.NT math.AG math.RT","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We define Hecke correspondences and Hecke operators on unitary RZ spaces and\nstudy their basic geometric properties, including a commutativity conjecture on\nHecke operators. Then we formulate the Arithmetic Fundamental Lemma conjecture\nfor the spherical Hecke algebra. We also formulate a conjecture on the\nabundance of spherical Hecke functions with identically vanishing first\nderivative of orbital integrals. We prove these conjectures for the case\n$U(1)\\times U(2)$.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:39:56 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14466","submitter":"Soumik Bhattacharya Dr.","authors":"Soumik Bhattacharya, Vandana Tripathi, S. L. Tabor, A. Volya, P. C.\n  Bender, C. Benetti, M. P. Carpenter, J. J. Carroll, A. Chester, C. J. Chiara,\n  K. Childers, B. R. Clark, B. P. Crider, J. T. Harke, S. N. Liddick, R. S.\n  Lubna, S. Luitel, B. Longfellow, M. J. Mogannam, T. H. Ogunbeku, J. Perello,\n  A. L. Richard, E. Rubino, S. Saha, O. A. Shehu, R. Unz, Y. Xiao, and Yiyi Zhu","title":"$\\beta^-$ decay of neutron-rich $^{45}$Cl at magic number N=28","comments":"11 pages, 10 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"nucl-ex","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Results from the study of $\\beta^-$-decay of $^{45}$Cl, produced in the\nfragmentation of a 140-MeV/u $^{48}$Ca beam, are presented. The half-life for\n$^{45}$Cl $\\beta$-decay is measured to be 513(36) ms. The $\\beta^-$ and\n$\\beta^- 1n$ decay of $^{45}$Cl populated excited states in $^{45,44}$Ar,\nrespectively. On the basis of $\\gamma$-ray singles and $\\gamma$-$\\gamma$\ncoincidence data, decay schemes for the two daughter nuclei have been\nestablished. They are compared with shell model calculations using the FSU\ninteraction. The low-lying negative parity states for $^{45}$Ar are well\ndescribed by a single particle (neutron) occupying orbitals near the Fermi\nsurface, whereas neutron excitations across the $N = 20$ shell gap are needed\nto explain the positive-parity states which are expected to be populated in\nallowed Gamow-Teller $\\beta$-decay of $^{45}$Cl. The highest $\\beta$-feeding to\nthe 5/2$^+$ state in $^{45}$Ar from the ground state of $^{45}$Cl points\ntowards a 3/2$^+$ spin-parity assignment of the ground state of the parent over\nthe other possibility of 1/2$^+$. The high Q$_{\\beta^-}$ value of $^{45}$Cl\ndecay allows for the population of $1p1h$ states above the neutron separation\nenergy in $^{45}$Ar leading to positive parity states of $^{44}$Ar being\npopulated by removal of one neutron from the $sd$ shell. The spin-parities of\nthe excited levels in $^{44}$Ar are tentatively assigned for the first time by\ncomparison with the shell model calculations. The 2978~keV level of $^{44}$Ar\nis identified as the excited 0$^+$ level which could correspond to a different\nconfiguration from the ground state.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:45:03 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14467","submitter":"Anatol Garioud","authors":"Anatol Garioud, Apolline De Wit, Marc Poup\\'ee, Marion Valette,\n  S\\'ebastien Giordano, Boris Wattrelos","title":"FLAIR #2: textural and temporal information for semantic segmentation\n  from multi-source optical imagery","comments":null,"journal-ref":null,"doi":"10.13140/RG.2.2.30938.93128/1","report-no":null,"categories":"cs.CV cs.LG eess.IV","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  The FLAIR #2 dataset hereby presented includes two very distinct types of\ndata, which are exploited for a semantic segmentation task aimed at mapping\nland cover. The data fusion workflow proposes the exploitation of the fine\nspatial and textural information of very high spatial resolution (VHR)\nmono-temporal aerial imagery and the temporal and spectral richness of high\nspatial resolution (HR) time series of Copernicus Sentinel-2 satellite images.\nThe French National Institute of Geographical and Forest Information (IGN), in\nresponse to the growing availability of high-quality Earth Observation (EO)\ndata, is actively exploring innovative strategies to integrate these data with\nheterogeneous characteristics. IGN is therefore offering this dataset to\npromote innovation and improve our knowledge of our territories.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:47:19 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14468","submitter":"Eleonora Gualdoni","authors":"Sophia Harrison, Eleonora Gualdoni, Gemma Boleda","title":"Run Like a Girl! Sports-Related Gender Bias in Language and Vision","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Gender bias in Language and Vision datasets and models has the potential to\nperpetuate harmful stereotypes and discrimination. We analyze gender bias in\ntwo Language and Vision datasets. Consistent with prior work, we find that both\ndatasets underrepresent women, which promotes their invisibilization. Moreover,\nwe hypothesize and find that a bias affects human naming choices for people\nplaying sports: speakers produce names indicating the sport (e.g. 'tennis\nplayer' or 'surfer') more often when it is a man or a boy participating in the\nsport than when it is a woman or a girl, with an average of 46% vs. 35% of\nsports-related names for each gender. A computational model trained on these\nnaming data reproduces the bias. We argue that both the data and the model\nresult in representational harm against women.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:52:11 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14469","submitter":"Mathieu Mivelle Dr","authors":"Ye Mou, Xingyu Yang, Bruno Gallas, and Mathieu Mivelle","title":"A Reversed Inverse Faraday Effect","comments":"arXiv admin note: text overlap with arXiv:2301.05971","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.optics","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The inverse Faraday effect is a magneto-optical process allowing the\nmagnetization of matter by an optical excitation carrying a non-zero spin of\nlight. In particular, a right circular polarization generates a magnetization\nin the direction of light propagation and a left circular polarization in the\nopposite direction to this propagation. We demonstrate here that by\nmanipulating the spin density of light, i.e., its polarization, in a plasmonic\nnanostructure, we generate a reversed inverse Faraday effect. A right circular\npolarization will generate a magnetization in the opposite direction of the\nlight propagation, a left circular polarization in the direction of\npropagation. Also, we demonstrate that this new physical phenomenon is chiral,\ngenerating a strong magnetic field only for one helicity of the light, the\nopposite helicity producing this effect only for the mirror structure. This new\noptical concept opens the way to the generation of magnetic fields with\nunpolarized light, finding application in the ultrafast manipulation of\nmagnetic domains and processes, such as spin precession, spin currents, and\nwaves, magnetic skyrmion or magnetic circular dichroism, with direct\napplications in data storage and processing technologies.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:52:21 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14470","submitter":"Mark Van der Merwe","authors":"Mark Van der Merwe, Youngsun Wi, Dmitry Berenson, Nima Fazeli","title":"Integrated Object Deformation and Contact Patch Estimation from\n  Visuo-Tactile Feedback","comments":"12 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Reasoning over the interplay between object deformation and force\ntransmission through contact is central to the manipulation of compliant\nobjects. In this paper, we propose Neural Deforming Contact Field (NDCF), a\nrepresentation that jointly models object deformations and contact patches from\nvisuo-tactile feedback using implicit representations. Representing the object\ngeometry and contact with the environment implicitly allows a single model to\npredict contact patches of varying complexity. Additionally, learning geometry\nand contact simultaneously allows us to enforce physical priors, such as\nensuring contacts lie on the surface of the object. We propose a neural network\narchitecture to learn a NDCF, and train it using simulated data. We then\ndemonstrate that the learned NDCF transfers directly to the real-world without\nthe need for fine-tuning. We benchmark our proposed approach against a baseline\nrepresenting geometry and contact patches with point clouds. We find that NDCF\nperforms better on simulated data and in transfer to the real-world.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:53:24 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14471","submitter":"Xuanyu Zhang","authors":"Xuanyu Zhang and Bingbing Li and Qing Yang","title":"CGCE: A Chinese Generative Chat Evaluation Benchmark for General and\n  Financial Domains","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Generative chat models, such as ChatGPT and GPT-4, have revolutionized\nnatural language generation (NLG) by incorporating instructions and human\nfeedback to achieve significant performance improvements. However, the lack of\nstandardized evaluation benchmarks for chat models, particularly for Chinese\nand domain-specific models, hinders their assessment and progress. To address\nthis gap, we introduce the Chinese Generative Chat Evaluation (CGCE) benchmark,\nfocusing on general and financial domains. The CGCE benchmark encompasses\ndiverse tasks, including 200 questions in the general domain and 150 specific\nprofessional questions in the financial domain. Manual scoring evaluates\nfactors such as accuracy, coherence, expression clarity, and completeness. The\nCGCE benchmark provides researchers with a standardized framework to assess and\ncompare Chinese generative chat models, fostering advancements in NLG research.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:54:15 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14472","submitter":"Spenser Talkington","authors":"Spenser Talkington, Eugene J. Mele","title":"Terahertz Circular Dichroism in Commensurate Twisted Bilayer Graphene","comments":"9 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We report calculations of terahertz ellipticities in large-angle,\n21.79$^\\circ$ and 38.21$^\\circ$, commensurate twisted bilayer graphene, and\npredict values as high as 1.5 millidegrees in the terahertz region for this\nnon-magnetic material. This terahertz circular dichroism exhibits a magnitude\ncomparable to that of chiral materials in the visible region. At low\nfrequencies, the dichroic response is mediated by strong interlayer\nhybridization, which allows us to probe the symmetry and strength of these\ncouplings. Crucially, lateral interlayer translation tunes this response, in\ncontrast to small twist angle bilayer graphene's near invariance under under\ninterlayer translation. We examine the magnitude and phase of the interlayer\ncoupling for all structures containing fewer than 400 atoms per unit cell.\nFinally, we find that the dichroism can be manipulated by applying an electric\nfield or with doping.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:00:03 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14473","submitter":"Jorge Antezana","authors":"Jorge Antezana and Sheldy Ombrosi","title":"Weighted maximal inequalities on hyperbolic spaces","comments":"21 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.CA math.FA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this work we develop a weight theory in the setting of hyperbolic spaces.\nOur starting point is a variant of the well-known endpoint Fefferman-Stein\ninequality for the centered Hardy-Littlewood maximal function. This inequality\ngeneralizes, in the hyperbolic setting, the weak $(1,1)$ estimates obtained by\nStr\\\"omberg in \"Weak type L1 estimates for maximal functions on noncompact\nsymmetric spaces\", Ann. of Math. 114 (1981), where Str\\\"omberg answered a\nquestion posed by Stein and Wainger in \"Problems in harmonic analysis related\nto curvature\", Bull. Amer. Math. Soc. 84 (1978). Our approach is based on a\ncombination of geometrical arguments and the techniques used in the discrete\nsetting of regular trees by Naor and Tao in \"Random martingales and\nlocalization of maximal inequalities\", J. Funct. Anal. 259 (2010). This variant\nof the Fefferman-Stein inequality paves the road to weighted estimates for the\nmaximal function for $p>1$. On the one hand, we show that the classical $A_p$\nconditions are not the right ones in this setting. On the other hand, we\nprovide sharp sufficient conditions for weighted weak and strong type $(p,p)$\nboundedness of the centered maximal function, when $p>1$. The sharpness is in\nthe sense that, given $p>1$, we can construct a weight satisfying our\nsufficient condition for that $p$, and so it satisfies the weak type $(p,p)$\ninequality, but the strong type $(p,p)$ inequality fails. In particular, the\nweak type $(q,q)$ fails as well for every $q < p$.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:02:41 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14474","submitter":"Maria Giovanna Mora","authors":"Maria Giovanna Mora","title":"Nonlocal anisotropic interactions of Coulomb type","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper we review some recent results on nonlocal interaction problems.\nThe focus is on interaction kernels that are anisotropic variants of the\nclassical Coulomb kernel. In other words, while preserving the same singularity\nat zero of the Coulomb kernel, they present preferred directions of\ninteraction. For kernels of this kind and general confinement we will prove\nexistence and uniqueness of minimisers of the corresponding energy. In the case\nof a quadratic confinement we will review a recent result by Carrillo & Shu\nabout the explicit characterisation of minimisers, and present a new proof,\nwhich has the advantage of being extendable to higher dimension. In light of\nthis result, we will re-examine some previous works motivated by applications\nto dislocation theory in materials science. Finally, we will discuss some\nrelated results and open questions.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:05:01 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14475","submitter":"Juan Flores Torres","authors":"Juan Flores Torres","title":"Moduli Space of Bi-Invariant Metrics","comments":"14 pages, 1 figure","journal-ref":null,"doi":null,"report-no":null,"categories":"math.DG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this work, we focus on describing the space of bi-invariant metrics in a\nLie group up to isometry. I.e, that is, metrics invariant under both left and\nright translations. We show that $\\mathfrak{BI}$, the moduli space of\nbi-invariant metrics, is an orbifold. Moreover we give an explicit description\nof this orbifold, and of $\\mathfrak{EBI}$, the space of bi-invariant metrics\nequivalent under isometries and scalar multiplies.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:06:21 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14476","submitter":"Alvio Renzini","authors":"Alvio Renzini","title":"A Transient Overcooling in the Early Universe? Clues from Globular\n  Clusters Formation","comments":"Four pages, no figures, submitted to MNRAS on May 22, 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.GA","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  The mere existence of multiple stellar generations in Milky Way globular\nclusters indicates that each generation was unable to stop star formation, that\ninstead persisted unimpeded for several million years. This evidence argues for\nan extended stage of star formation within a forming globular cluster, during\nwhich stellar feedback was substantially ineffective and the nascent globular\ncluster was able to accrete processed gas from its surrounding, and efficiently\nconvert it into successive stellar generations. It has been argued that such\ndelayed feedback results from core collapse in most massive stars failing to\ntrigger an energetic supernova explosion, but rather leading directly to black\nhole formation. Thus, globular clusters offer a concrete phenomenological\nexample for the lack of feedback in young starbursts, an option that has been\nwidely advocated to account for the unexpected abundance of UV-luminous\ngalaxies at z = 9-16, as revealed by JWST observations. The paper is meant to\nattract attention to this opportunity for a synergic cooperation of globular\ncluster and high redshift research.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:08:06 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14477","submitter":"Elisa Riccietti","authors":"Serge Gratton, Valentin Mercier, Elisa Riccietti, Philippe L. Toint","title":"A Block-Coordinate Approach of Multi-level Optimization with an\n  Application to Physics-Informed Neural Networks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG math.OC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Multi-level methods are widely used for the solution of large-scale problems,\nbecause of their computational advantages and exploitation of the\ncomplementarity between the involved sub-problems. After a re-interpretation of\nmulti-level methods from a block-coordinate point of view, we propose a\nmulti-level algorithm for the solution of nonlinear optimization problems and\nanalyze its evaluation complexity. We apply it to the solution of partial\ndifferential equations using physics-informed neural networks (PINNs) and show\non a few test problems that the approach results in better solutions and\nsignificant computational savings\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:12:02 GMT"},{"version":"v2","created":"Thu, 25 May 2023 06:48:38 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.14478","submitter":"Lars Vilhuber","authors":"Lars Vilhuber","title":"Reproducibility and Transparency versus Privacy and Confidentiality:\n  Reflections from a Data Editor","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"econ.GN q-fin.EC stat.OT","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Transparency and reproducibility are often seen in opposition to privacy and\nconfidentiality. Data that need to be kept confidential are seen as an\nimpediment to reproducibility, and privacy would seem to inhibit transparency.\nI bring a more nuanced view to the discussion, and show, using examples from\nover 1,000 reproducibility assessments, that confidential data can very well be\nused in reproducible and transparent research. The key insight is that access\nto most confidential data, while tedious, is open to hundreds if not thousands\nof researchers. In cases where few researchers can consider accessing such data\nin the future, reproducibility services, such as those provided by some\njournals, can provide some evidence for effective reproducibility even when the\nsame data may not be available for future research.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:17:38 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14479","submitter":"Fumiya Okazaki","authors":"Fumiya Okazaki","title":"The probabilistic characterization of weakly harmonic maps with respect\n  to non-local Dirichlet forms","comments":"26 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We characterize weakly harmonic maps with respect to non-local Dirichlet\nforms by Markov processes and martingales. In particular, we can obtain\ndiscontinuous martingales on Riemanian manifolds by inserting symmetric stable\nprocesses into fractional harmonic maps in a weak sense. We also consider the\ncontinuity of weakly harmonic maps along the paths of Markov processes, which\nis called the fine continuity. We show that the fine continuity implies the\ncontinuity with respect to the Euclidean topology in some situations containing\ncases of energy minimizing maps.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:19:37 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14480","submitter":"Zihao Fu","authors":"Zihao Fu, Meiru Zhang, Zaiqiao Meng, Yannan Shen, Anya Okhmatovskaia,\n  David Buckeridge, Nigel Collier","title":"BAND: Biomedical Alert News Dataset","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Infectious disease outbreaks continue to pose a significant threat to human\nhealth and well-being. To improve disease surveillance and understanding of\ndisease spread, several surveillance systems have been developed to monitor\ndaily news alerts and social media. However, existing systems lack thorough\nepidemiological analysis in relation to corresponding alerts or news, largely\ndue to the scarcity of well-annotated reports data. To address this gap, we\nintroduce the Biomedical Alert News Dataset (BAND), which includes 1,508\nsamples from existing reported news articles, open emails, and alerts, as well\nas 30 epidemiology-related questions. These questions necessitate the model's\nexpert reasoning abilities, thereby offering valuable insights into the\noutbreak of the disease. The BAND dataset brings new challenges to the NLP\nworld, requiring better disguise capability of the content and the ability to\ninfer important information. We provide several benchmark tasks, including\nNamed Entity Recognition (NER), Question Answering (QA), and Event Extraction\n(EE), to show how existing models are capable of handling these tasks in the\nepidemiology domain. To the best of our knowledge, the BAND corpus is the\nlargest corpus of well-annotated biomedical outbreak alert news with\nelaborately designed questions, making it a valuable resource for\nepidemiologists and NLP researchers alike.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:21:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14481","submitter":"Konstantin Dobler","authors":"Konstantin Dobler and Gerard de Melo","title":"FOCUS: Effective Embedding Initialization for Specializing Pretrained\n  Multilingual Models on a Single Language","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Using model weights pretrained on a high-resource language as a warm start\ncan reduce the need for data and compute to obtain high-quality language models\nin low-resource languages. To accommodate the new language, the pretrained\nvocabulary and embeddings need to be adapted. Previous work on embedding\ninitialization for such adapted vocabularies has mostly focused on monolingual\nsource models. In this paper, we investigate the multilingual source model\nsetting and propose FOCUS - Fast Overlapping Token Combinations Using\nSparsemax, a novel embedding initialization method that outperforms previous\nwork when adapting XLM-R. FOCUS represents newly added tokens as combinations\nof tokens in the overlap of the pretrained and new vocabularies. The\noverlapping tokens are selected based on semantic similarity in an auxiliary\ntoken embedding space. Our implementation of FOCUS is publicly available on\nGitHub.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:21:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14482","submitter":"Jind\\v{r}ich Libovick\\'y","authors":"Jind\\v{r}ich Libovick\\'y","title":"Is a Prestigious Job the same as a Prestigious Country? A Case Study on\n  Multilingual Sentence Embeddings and European Countries","comments":"10 pages, 1 figure","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We study how multilingual sentence representations capture European countries\nand how this differs across European languages. We prompt the models with\ntemplated sentences that we machine-translate into 12 European languages and\nanalyze the most prominent dimensions in the embeddings. Our analysis reveals\nthat the most prominent country feature in the embedding is its economic\nstrength in terms of GPD. When prompted specifically for job prestige, the\nembedding space clearly distinguishes high and low-prestige jobs. The\noccupational dimension is uncorrelated with the most dominant country\ndimensions for three out of four studied models. One model: Distilled\nMultilingual Universal Sentence Encoder, however, exhibited a connection\nbetween occupational prestige and country of origin, which is a potential\nsource of nationality-based discrimination. Our findings are consistent across\nlanguages and, to some extent, with the exception mentioned above, across\nstudied representation models.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:24:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14483","submitter":"Yang Yu","authors":"Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-Hui Chen, Jiacheng\n  Xu, Zongzhang Zhang, and Yang Yu","title":"Language Model Self-improvement by Reinforcement Learning Contemplation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large Language Models (LLMs) have exhibited remarkable performance across\nvarious natural language processing (NLP) tasks. However, fine-tuning these\nmodels often necessitates substantial supervision, which can be expensive and\ntime-consuming to obtain. This paper introduces a novel unsupervised method\ncalled LanguageModel Self-Improvement by Reinforcement Learning Contemplation\n(SIRLC) that improves LLMs without reliance on external labels. Our approach is\ngrounded in the observation that it is simpler for language models to assess\ntext quality than to generate text. Building on this insight, SIRLC assigns\nLLMs dual roles as both student and teacher. As a student, the LLM generates\nanswers to unlabeled questions, while as a teacher, it evaluates the generated\ntext and assigns scores accordingly. The model parameters are updated using\nreinforcement learning to maximize the evaluation score. We demonstrate that\nSIRLC can be applied to various NLP tasks, such as reasoning problems, text\ngeneration, and machine translation. Our experiments show that SIRLC\neffectively improves LLM performance without external supervision, resulting in\na 5.6% increase in answering accuracy for reasoning tasks and a rise in\nBERTScore from 0.82 to 0.86 for translation tasks. Furthermore, SIRLC can be\napplied to models of different sizes, showcasing its broad applicability.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:25:52 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14484","submitter":"Massimiliano Galeazzi","authors":"Sicong Huang, Nico Cappelluti, Massimiliano Galeazzi, Anjali Gupta,\n  Wenhao Liu, Eugenio Ursino, Tomykkutty J. Velliyedathu","title":"Point source contribution to the Diffuse X-ray Background below 1 keV\n  and its effect on our understanding of the circum-galactic medium","comments":"18 pages, 13 figures, 5 tables","journal-ref":"The Astrophysical Journal, Volume 947, Issue 2, id.49, 2023","doi":"10.3847/1538-4357/acaf7b","report-no":null,"categories":"astro-ph.HE","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We studied the spectral signature of different components of the Diffuse\nX-ray Background (DXB), including Local Hot Bubble (LHB), Solar Wind Charge\nExchange (SWCX), Galactic Halo, and typically unresolved point sources\n(galaxies and AGN), in the direction of the Chandra Deep Field South (CDFS)\nusing the 4 Ms XMM-Newton survey and Chandra 4 Ms Source Catalog. In this\npaper, we present our results showing how the different components contribute\nto the DXB below 1 keV. In particular, we have found that ~6% of the emission\nat 3/4 keV (all-sky average value ~ 3$\\times10^{-3}$ cm$^{-6}$pc), which is\ntypically associated with Galactic Halo (GH) and Circum-galactic medium (CGM)\nis, in fact, due to emission from typically unresolved galaxies.\n  We will discuss the effect that this has on our understanding of GH and CGM,\nand to our understanding of the missing CGM baryons.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:27:41 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14485","submitter":"Arijit Khan","authors":"Arijit Khan","title":"Knowledge Graphs Querying","comments":"accepted at ACM SIGMOD Record 2023","journal-ref":"ACM SIGMOD Record 2023","doi":null,"report-no":null,"categories":"cs.DB cs.IR cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Knowledge graphs (KGs) such as DBpedia, Freebase, YAGO, Wikidata, and NELL\nwere constructed to store large-scale, real-world facts as (subject, predicate,\nobject) triples -- that can also be modeled as a graph, where a node (a subject\nor an object) represents an entity with attributes, and a directed edge (a\npredicate) is a relationship between two entities. Querying KGs is critical in\nweb search, question answering (QA), semantic search, personal assistants, fact\nchecking, and recommendation. While significant progress has been made on KG\nconstruction and curation, thanks to deep learning recently we have seen a\nsurge of research on KG querying and QA. The objectives of our survey are\ntwo-fold. First, research on KG querying has been conducted by several\ncommunities, such as databases, data mining, semantic web, machine learning,\ninformation retrieval, and natural language processing (NLP), with different\nfocus and terminologies; and also in diverse topics ranging from graph\ndatabases, query languages, join algorithms, graph patterns matching, to more\nsophisticated KG embedding and natural language questions (NLQs). We aim at\nuniting different interdisciplinary topics and concepts that have been\ndeveloped for KG querying. Second, many recent advances on KG and query\nembedding, multimodal KG, and KG-QA come from deep learning, IR, NLP, and\ncomputer vision domains. We identify important challenges of KG querying that\nreceived less attention by graph databases, and by the DB community in general,\ne.g., incomplete KG, semantic matching, multimodal data, and NLQs. We conclude\nby discussing interesting opportunities for the data management community, for\ninstance, KG as a unified data model and vector-based query processing.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:32:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14486","submitter":"Jadie Adams","authors":"Jadie Adams and Shireen Elhabian","title":"Point2SSM: Learning Morphological Variations of Anatomies from Point\n  Cloud","comments":"Under review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We introduce Point2SSM, a novel unsupervised learning approach that can\naccurately construct correspondence-based statistical shape models (SSMs) of\nanatomy directly from point clouds. SSMs are crucial in clinical research for\nanalyzing the population-level morphological variation in bones and organs.\nHowever, traditional methods for creating SSMs have limitations that hinder\ntheir widespread adoption, such as the need for noise-free surface meshes or\nbinary volumes, reliance on assumptions or predefined templates, and\nsimultaneous optimization of the entire cohort leading to lengthy inference\ntimes given new data. Point2SSM overcomes these barriers by providing a\ndata-driven solution that infers SSMs directly from raw point clouds, reducing\ninference burdens and increasing applicability as point clouds are more easily\nacquired. Deep learning on 3D point clouds has seen recent success in\nunsupervised representation learning, point-to-point matching, and shape\ncorrespondence; however, their application to constructing SSMs of anatomies is\nlargely unexplored. In this work, we benchmark state-of-the-art point cloud\ndeep networks on the task of SSM and demonstrate that they are not robust to\nthe challenges of anatomical SSM, such as noisy, sparse, or incomplete input\nand significantly limited training data. Point2SSM addresses these challenges\nvia an attention-based module that provides correspondence mappings from\nlearned point features. We demonstrate that the proposed method significantly\noutperforms existing networks in terms of both accurate surface sampling and\ncorrespondence, better capturing population-level statistics.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:36:24 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14487","submitter":"Erik Fitzke","authors":"Jakob Kaltwasser, Joschka Seip, Erik Fitzke, Maximilian Tippmann, and\n  Thomas Walther","title":"Reducing the number of single-photon detectors in quantum key\n  distribution networks by time multiplexing","comments":"6 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We demonstrate a method to reduce the number of single-photon detectors\n(SPDs) required in multi-party quantum key distribution (QKD) networks by a\nfactor of two by using detector time multiplexing (DTM). We implement the DTM\nscheme for an entanglement-based time-bin protocol and compare QKD results with\nand without DTM in our QKD network with four users. When small efficiency\nlosses are acceptable, DTM enables cost-effective, scalable implementations of\nmulti-user QKD networks.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:36:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14488","submitter":"Peter Ralph","authors":"Alison M. Etheridge and Thomas G. Kurtz and Ian Letter and Peter L.\n  Ralph and Terence Tsui Ho Lung","title":"Looking forwards and backwards: dynamics and genealogies of locally\n  regulated populations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR q-bio.PE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We introduce a broad class of spatial models to describe how spatially\nheterogeneous populations live, die, and reproduce. Individuals are represented\nby points of a point measure, whose birth and death rates can depend both on\nspatial position and local population density, defined via the convolution of\nthe point measure with a nonnegative kernel. We pass to three different scaling\nlimits: an interacting superprocess, a nonlocal partial differential equation\n(PDE), and a classical PDE. The classical PDE is obtained both by first scaling\ntime and population size to pass to the nonlocal PDE, and then scaling the\nkernel that determines local population density; and also (when the limit is a\nreaction-diffusion equation) by simultaneously scaling the kernel width,\ntimescale and population size in our individual based model. A novelty of our\nmodel is that we explicitly model a juvenile phase: offspring are thrown off in\na Gaussian distribution around the location of the parent, and reach (instant)\nmaturity with a probability that can depend on the population density at the\nlocation at which they land. Although we only record mature individuals, a\ntrace of this two-step description remains in our population models, resulting\nin novel limits governed by a nonlinear diffusion. Using a lookdown\nrepresentation, we retain information about genealogies and, in the case of\ndeterministic limiting models, use this to deduce the backwards in time motion\nof the ancestral lineage of a sampled individual. We observe that knowing the\nhistory of the population density is not enough to determine the motion of\nancestral lineages in our model. We also investigate the behaviour of lineages\nfor three different deterministic models of a population expanding its range as\na travelling wave: the Fisher-KPP equation, the Allen-Cahn equation, and a\nporous medium equation with logistic growth.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:37:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14489","submitter":"Nghia T. Le","authors":"Nghia T. Le, Alan Ritter","title":"Are Large Language Models Robust Zero-shot Coreference Resolvers?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  Recent progress in domain adaptation for coreference resolution relies on\ncontinued training using annotated data from target domains. At the same time,\npre-trained large language models (LMs) have exhibited strong zero- and\nfew-shot learning abilities across a wide range of NLP tasks including pronoun\nresolution. While this demonstrates evidence of coreference ability, previous\nwork has mostly studied this ability using simple sentence-level datasets such\nas the Winograd Schema Challenge. In this work, we assess the feasibility of\nzero-shot learning for coreference resolution by evaluating instruction-tuned\nlanguage models on more difficult, linguistically-complex coreference\nbenchmarks (e.g., CoNLL-2012). We demonstrate that zero-shot prompting\noutperforms current unsupervised coreference systems. Further investigations\nreveal the robust zero-shot generalization ability of instruction-tuned LMs\nacross a wide range of domains, languages, and time periods, as well as a\nstrong reliance on high-quality mention detection systems.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:38:28 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14490","submitter":"Xiang Zhang","authors":"Xiang Zhang, Yu Gu, Huan Yan, Yantong Wang, Mianxiong Dong, Kaoru Ota,\n  Fuji Ren, Yusheng Ji","title":"Wital: A COTS WiFi Devices Based Vital Signs Monitoring System Using\n  NLOS Sensing Model","comments":"Accepted by IEEE THMS","journal-ref":"IEEE Transactions on Human-Machine Systems,2023","doi":"10.1109/THMS.2023.3264247","report-no":null,"categories":"cs.HC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Vital sign (breathing and heartbeat) monitoring is essential for patient care\nand sleep disease prevention. Most current solutions are based on wearable\nsensors or cameras; however, the former could affect sleep quality, while the\nlatter often present privacy concerns. To address these shortcomings, we\npropose Wital, a contactless vital sign monitoring system based on low-cost and\nwidespread commercial off-the-shelf (COTS) Wi-Fi devices. There are two\nchallenges that need to be overcome. First, the torso deformations caused by\nbreathing/heartbeats are weak. How can such deformations be effectively\ncaptured? Second, movements such as turning over affect the accuracy of vital\nsign monitoring. How can such detrimental effects be avoided? For the former,\nwe propose a non-line-of-sight (NLOS) sensing model for modeling the\nrelationship between the energy ratio of line-of-sight (LOS) to NLOS signals\nand the vital sign monitoring capability using Ricean K theory and use this\nmodel to guide the system construction to better capture the deformations\ncaused by breathing/heartbeats. For the latter, we propose a motion\nsegmentation method based on motion regularity detection that accurately\ndistinguishes respiration from other motions, and we remove periods that\ninclude movements such as turning over to eliminate detrimental effects. We\nhave implemented and validated Wital on low-cost COTS devices. The experimental\nresults demonstrate the effectiveness of Wital in monitoring vital signs.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:38:40 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14491","submitter":"Miti Patel","authors":"M. Patel, B. P. Gompertz, P. T. O'Brien, G. P. Lamb, R. L. C.\n  Starling, P. A Evans, L. Amati, A. J. Levan, M. Nicholl, J. Lyman, K. Ackley,\n  M. J. Dyer, K. Ulaczyk, D. Steeghs, D. K. Galloway, V. S. Dhillon, G. Ramsay,\n  K. Noysena, R. Kotak, R. P. Breton, L. K. Nuttall, E. Palle, D. Pollacco","title":"GRB 201015A and the nature of low-luminosity soft gamma-ray bursts","comments":"15 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.HE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  GRB 201015A is a peculiarly low luminosity, spectrally soft gamma-ray burst\n(GRB), with $T_{\\rm 90} = 9.8 \\pm 3.5$ s (time interval of detection of 90\\% of\nphotons from the GRB), and an associated supernova (likely to be type Ic or\nIc-BL). GRB 201015A has an isotropic energy $E_{\\gamma,\\rm iso} = 1.75 ^{+0.60}\n_{-0.53} \\times 10^{50}$ erg, and photon index $\\Gamma = 3.00 ^{+0.50}\n_{-0.42}$ (15-150 keV). It follows the Amati relation, a correlation between\n$E_{\\gamma,\\rm iso}$ and spectral peak energy $E_{\\rm p}$ followed by long\nGRBs. It appears exceptionally soft based on $\\Gamma$, the hardness ratio of HR\n= $0.47 \\pm 0.24$, and low-$E_{\\rm p}$, so we have compared it to other GRBs\nsharing these properties. These events can be explained by shock breakout,\npoorly collimated jets, and off-axis viewing. Follow-up observations of the\nafterglow taken in the X-ray, optical, and radio, reveal a surprisingly late\nflattening in the X-ray from $t = (2.61 \\pm 1.27)\\times 10^4$ s to $t = 1.67\n^{+1.14} _{-0.65} \\times 10^6$ s. We fit the data to closure relations\ndescribing the synchrotron emission, finding the electron spectral index to be\n$p = 2.42 ^{+0.44} _{-0.30}$, and evidence of late-time energy injection with\ncoefficient $q = 0.24 ^{+0.24} _{-0.18}$. The jet half opening angle lower\nlimit ($\\theta_{j} \\ge 16^{\\circ}$) is inferred from the non-detection of a jet\nbreak. The launch of SVOM and Einstein Probe in 2023, should enable detection\nof more low luminosity events like this, providing a fuller picture of the\nvariety of GRBs.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:42:29 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14492","submitter":"Arkadiy Saakyan","authors":"Sky CH-Wang, Arkadiy Saakyan, Oliver Li, Zhou Yu, Smaranda Muresan","title":"Sociocultural Norm Similarities and Differences via Situational\n  Alignment and Explainable Textual Entailment","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Designing systems that can reason across cultures requires that they are\ngrounded in the norms of the contexts in which they operate. However, current\nresearch on developing computational models of social norms has primarily\nfocused on American society. Here, we propose a novel approach to discover and\ncompare descriptive social norms across Chinese and American cultures. We\ndemonstrate our approach by leveraging discussions on a Chinese Q&A\nplatform-Zhihu-and the existing SocialChemistry dataset as proxies for\ncontrasting cultural axes, align social situations cross-culturally, and\nextract social norms from texts using in-context learning. Embedding\nChain-of-Thought prompting in a human-AI collaborative framework, we build a\nhigh-quality dataset of 3,069 social norms aligned with social situations\nacross Chinese and American cultures alongside corresponding free-text\nexplanations. To test the ability of models to reason about social norms across\ncultures, we introduce the task of explainable social norm entailment, showing\nthat existing models under 3B parameters have significant room for improvement\nin both automatic and human evaluation. Further analysis of cross-cultural norm\ndifferences based on our dataset shows empirical alignment with the social\norientations framework, revealing several situational and descriptive nuances\nin norms across these cultures.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:43:47 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14493","submitter":"Junyu Mao","authors":"Junyu Mao and Stuart E. Middleton and Mahesan Niranjan","title":"Prompt position really matters in few-shot and zero-shot NLU tasks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Prompt-based models have made remarkable advancements in the fields of\nzero-shot and few-shot learning, attracting a lot of attention from\nresearchers. Developing an effective prompt template plays a critical role.\nHowever, prior studies have mainly focused on prompt vocabulary selection or\nembedding initialization with the reserved prompt position fixed. In this\nempirical study, we conduct the most comprehensive analysis to date of prompt\nposition option for natural language understanding tasks. Our findings quantify\nthe substantial impact prompt position has on model performance. We observe\nthat the prompt position used in prior studies is often sub-optimal for both\nzero-shot and few-shot settings. These findings suggest prompt position\noptimisation as an interesting research direction alongside the existing focus\non prompt engineering.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:45:45 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14494","submitter":"Wang Ruijie","authors":"Xinyi Liu, Jinning Li, Dachun Sun, Ruijie Wang, Tarek Abdelzaher, Matt\n  Brown, Anthony Barricelli, Matthias Kirchner, Arslan Basharat","title":"Unsupervised Image Classification by Ideological Affiliation from\n  User-Content Interaction Patterns","comments":"n Proc. PhoMemes (in conjunction with ICWSM), Limassol, Cyprus, June\n  2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The proliferation of political memes in modern information campaigns calls\nfor efficient solutions for image classification by ideological affiliation.\nWhile significant advances have recently been made on text classification in\nmodern natural language processing literature, understanding the political\ninsinuation in imagery is less developed due to the hard nature of the problem.\nUnlike text, where meaning arises from juxtaposition of tokens (words) within\nsome common linguistic structures, image semantics emerge from a much less\nconstrained process of fusion of visual concepts. Thus, training a model to\ninfer visual insinuation is possibly a more challenging problem. In this paper,\nwe explore an alternative unsupervised approach that, instead, infers\nideological affiliation from image propagation patterns on social media. The\napproach is shown to improve the F1-score by over 0.15 (nearly 25%) over\nprevious unsupervised baselines, and then by another 0.05 (around 7%) in the\npresence of a small amount of supervision.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:51:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14495","submitter":"Fiorenzo Stoppa","authors":"F. Stoppa, R. Ruiz de Austri, P. Vreeswijk, S. Bhattacharyya, S.\n  Caron, S. Bloemen, G. Zaharijas, G. Principe, V. Vodeb, E. Cator, and G.\n  Nelemans","title":"AutoSourceID-FeatureExtractor. Optical images analysis using a Two-Step\n  MVE Network for feature estimation and uncertainty characterization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.IM","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Aims. In astronomy, machine learning has demonstrated success in various\ntasks such as source localization, classification, anomaly detection, and\nsegmentation. However, feature regression remains an area with room for\nimprovement. We aim to design a network that can accurately estimate sources'\nfeatures and their uncertainties from single-band image cutouts, given the\napproximated locations of the sources provided by the previously developed code\nASID-L or other external catalogues. Methods. The algorithm presented here,\nAutoSourceID-FeatureExtractor (ASID-FE), uses single-band cutouts of 32x32\npixels around the localized sources to estimate flux, sub-pixel centre\ncoordinates, and their uncertainties. ASID-FE employs what we call a TS-MVE, a\nTwo-Step Mean Variance Estimator approach to first estimate the features and\nthen their uncertainties without the need for additional information, e.g.\nPoint Spread Function (PSF). Results. We show that ASID-FE, trained on\nsynthetic images from the MeerLICHT telescope, can predict more accurate\nfeatures with respect to similar codes like SourceExtractor and that the\ntwo-step method can estimate well-calibrated uncertainties that are better\nbehaved compared to similar methods that use deep ensembles of simple MVE\nnetworks. Finally, we evaluate the model on real images from the MeerLICHT\ntelescope and the Zwicky Transients Facility (ZTF) to test its Transfer\nLearning abilities.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:53:12 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14496","submitter":"Tobias Sutter","authors":"Arnab Ganguly, Tobias Sutter","title":"Optimal Learning via Moderate Deviations Theory","comments":"35 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML math.OC math.PR math.ST stat.TH","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper proposes a statistically optimal approach for learning a function\nvalue using a confidence interval in a wide range of models, including general\nnon-parametric estimation of an expected loss described as a stochastic\nprogramming problem or various SDE models. More precisely, we develop a\nsystematic construction of highly accurate confidence intervals by using a\nmoderate deviation principle-based approach. It is shown that the proposed\nconfidence intervals are statistically optimal in the sense that they satisfy\ncriteria regarding exponential accuracy, minimality, consistency,\nmischaracterization probability, and eventual uniformly most accurate (UMA)\nproperty. The confidence intervals suggested by this approach are expressed as\nsolutions to robust optimization problems, where the uncertainty is expressed\nvia the underlying moderate deviation rate function induced by the\ndata-generating process. We demonstrate that for many models these optimization\nproblems admit tractable reformulations as finite convex programs even when\nthey are infinite-dimensional.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:57:57 GMT"},{"version":"v2","created":"Wed, 31 May 2023 19:51:18 GMT"}],"update_date":"2023-06-02"}
{"id":"2305.14497","submitter":"Zhiheng Xi","authors":"Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Tao Gui,\n  Qi Zhang, Xuanjing Huang","title":"Self-Polish: Enhance Reasoning in Large Language Models via Problem\n  Refinement","comments":"Preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Prompting methods such as Chain-of-Thought (CoT) have shed new light on\nenhancing the reasoning capabilities of large language models, and researchers\nhave extensively explored the generation process of rationales and answers.\nHowever, they have overlooked the potential challenges posed by the poor\nquality of reasoning problems, which may influence the reasoning performance\nsignificantly. In this work, we propose Self-Polish (SP), a novel method that\nfacilitates the model's problem-solving process by prompting them to\nprogressively refine the given problems to be more comprehensible and solvable.\nSpecifically, the method teaches models to eliminate irrelevant information,\nrearrange the logic structure and organize local conditions into new ones\nparallelly. SP is orthogonal to all other prompting methods, making it\nconvenient to integrate with state-of-the-art techniques for further\nimprovement. We conduct thorough experiments on five benchmarks to illustrate\nthe effectiveness of the proposed method. For example, with Text-davinci-003,\nour method boosts the performance of standard few-shot prompting by $8.0\\%$ on\nGSM8K and $17.8\\%$ on MultiArith; it also improves the performance of CoT by\n$6.0\\%$ on GSM8K and $6.0\\%$ on MathQA, respectively. Furthermore, our method\nalso showcases impressive performance on robustness evaluation.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:58:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14498","submitter":"Andrew Cameron","authors":"Andrew R. Cameron, Kate L. Fenwick, Sandra W. L. Cheng, Sacha Schwarz,\n  Benjamin MacLellan, Philip J. Bustard, Duncan England, Benjamin Sussman,\n  Kevin J. Resch","title":"Ultrafast Measurement of Energy-Time Entanglement with an Optical Kerr\n  Shutter","comments":"5 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Recent experimental progress in quantum optics has enabled measurement of\nsingle photons on ultrafast timescales, beyond the resolution limit of single\nphoton detectors. The energy-time degree of freedom has emerged as a promising\navenue for quantum technologies, as entanglement between the frequency and\ntemporal properties of two photons can be fully explored and utilized. Here, we\nimplement optical Kerr shutters in single mode fibers to map out the\nsub-picosecond correlations of energy-time entangled photon pairs. These\nmeasurements, in addition to joint spectral measurements of the photon pair\nstate, are used to verify entanglement by means of the violation of a\ntime-bandwidth inequality.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:02:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14499","submitter":"Livio Baldini Soares","authors":"Livio Baldini Soares, Daniel Gillick, Jeremy R. Cole, Tom Kwiatkowski","title":"NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive\n  Decoders","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.IR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Neural document rerankers are extremely effective in terms of accuracy.\nHowever, the best models require dedicated hardware for serving, which is\ncostly and often not feasible. To avoid this serving-time requirement, we\npresent a method of capturing up to 86% of the gains of a Transformer\ncross-attention model with a lexicalized scoring function that only requires\n10-6% of the Transformer's FLOPs per document and can be served using commodity\nCPUs. When combined with a BM25 retriever, this approach matches the quality of\na state-of-the art dual encoder retriever, that still requires an accelerator\nfor query encoding. We introduce NAIL (Non-Autoregressive Indexing with\nLanguage models) as a model architecture that is compatible with recent\nencoder-decoder and decoder-only large language models, such as T5, GPT-3 and\nPaLM. This model architecture can leverage existing pre-trained checkpoints and\ncan be fine-tuned for efficiently constructing document representations that do\nnot require neural processing of queries.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:09:52 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14500","submitter":"Sukrit Venkatagiri","authors":"Jacob Thebault-Spieker, Sukrit Venkatagiri, Naomi Mine, Kurt Luther","title":"Diverse Perspectives Can Mitigate Political Bias in Crowdsourced Content\n  Moderation","comments":"Accepted to the 2023 ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT 2023). To cite please use the reference available at this\n  URL: https://doi.org/10.1145/3593013.3594080","journal-ref":null,"doi":"10.1145/3593013.3594080","report-no":null,"categories":"cs.HC cs.CY","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  In recent years, social media companies have grappled with defining and\nenforcing content moderation policies surrounding political content on their\nplatforms, due in part to concerns about political bias, disinformation, and\npolarization. These policies have taken many forms, including disallowing\npolitical advertising, limiting the reach of political topics, fact-checking\npolitical claims, and enabling users to hide political content altogether.\nHowever, implementing these policies requires human judgement to label\npolitical content, and it is unclear how well human labelers perform at this\ntask, or whether biases affect this process. Therefore, in this study we\nexperimentally evaluate the feasibility and practicality of using crowd workers\nto identify political content, and we uncover biases that make it difficult to\nidentify this content. Our results problematize crowds composed of seemingly\ninterchangeable workers, and provide preliminary evidence that aggregating\njudgements from heterogeneous workers may help mitigate political biases. In\nlight of these findings, we identify strategies to achieving fairer labeling\noutcomes, while also better supporting crowd workers at this task and\npotentially mitigating biases.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:10:43 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14501","submitter":"Carla Figueira de Morisson Faria","authors":"L. Cruz Rodriguez, T. Rook, B. B. Augstein, A. S. Maxwell, C. Figueira\n  de Morisson Faria","title":"Forward and hybrid path-integral methods in photoelectron holography:\n  sub-barrier corrections, initial sampling and momentum mapping","comments":"23 pages revtex, 14 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.atom-ph quant-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We construct a strong-field path integral method with full Coulomb\ndistortion, in which electron orbits are forward propagated, and contrast the\nresults with those from a hybrid forward-boundary method. These methods are\napplied to ultrafast photoelectron holography. In the forward method, we derive\na non-adiabatic ionization rate from the Coulomb quantum-orbit strong-field\napproximation (CQSFA), which includes sub-barrier Coulomb corrections and is\nused to weight the initial orbit ensemble. In the hybrid forward-boundary CQSFA\n(H-CQSFA), we probe different initial sampling distributions, uniform and\notherwise, and their influence on photoelectron momentum distributions (PMDs).\nWe show that the sub-barrier Coulomb corrections broaden the resulting PMDs and\nimprove the agreement of the rate-based method with the H-CQSFA and\n\\textit{ab-initio} methods. Furthermore, in the hybrid approach, initial biased\nsampling emphasizes rescattering ridges and interferences in high-energy\nranges, while an initial uniform sampling guarantees accurate modeling of the\nholographic patterns near the ionization threshold or polarization axis. Our\nresults are explained using the initial to final momentum mapping for different\ntypes of interfering trajectories.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:11:07 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14502","submitter":"Alexander Scarlatos","authors":"Alexander Scarlatos and Andrew Lan","title":"RetICL: Sequential Retrieval of In-Context Examples with Reinforcement\n  Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Many recent developments in large language models focus on prompting them to\nperform specific tasks. One effective prompting method is in-context learning,\nwhere the model performs a (possibly new) generation/prediction task given one\n(or more) examples. Past work has shown that the choice of examples can make a\nlarge impact on task performance. However, finding good examples is not\nstraightforward since the definition of a representative group of examples can\nvary greatly depending on the task. While there are many existing methods for\nselecting in-context examples, they generally score examples independently,\nignoring the dependency between them and the order in which they are provided\nto the large language model. In this work, we propose Retrieval for In-Context\nLearning (RetICL), a learnable method for modeling and optimally selecting\nexamples sequentially for in-context learning. We frame the problem of\nsequential example selection as a Markov decision process, design an example\nretriever model using an LSTM, and train it using proximal policy optimization\n(PPO). We validate RetICL on math problem solving datasets and show that it\noutperforms both heuristic and learnable baselines, and achieves\nstate-of-the-art accuracy on the TabMWP dataset. We also use case studies to\nshow that RetICL implicitly learns representations of math problem solving\nstrategies.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:15:56 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14503","submitter":"Heon Lee","authors":"Heon Lee","title":"On the Instability of Fractional Reserve Banking","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"econ.TH","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  This paper develops a dynamic monetary model to study the (in)stability of\nthe fractional reserve banking system. The model shows that the fractional\nreserve banking system can endanger stability in that equilibrium is more prone\nto exhibit endogenous cyclic, chaotic, and stochastic dynamics under lower\nreserve requirements, although it can increase consumption in the steady-state.\nIntroducing endogenous unsecured credit to the baseline model does not change\nthe main results. This paper also provides empirical evidence that is\nconsistent with the prediction of the model. The calibrated exercise suggests\nthat this channel could be another source of economic fluctuations.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:16:50 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14504","submitter":"Peter Schiansky","authors":"Peter Schiansky, Julia Kalb, Esther Sztatecsny, Marie-Christine\n  Roehsner, Tobias Guggemos, Alessandro Trenti, Mathieu Bozzio, Philip Walther","title":"Demonstration of quantum-digital payments","comments":"Data and supplementary material will be made openly available upon\n  publication","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cs.CR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Digital contactless payments have replaced physical banknotes in many aspects\nof our daily lives. Similarly to banknotes, they are easy to use, unique,\ntamper-resistant and untraceable, but additionally have to withstand attackers\nand data breaches in the digital world. Current technology substitutes\ncustomers' sensitive data by randomized tokens, and secures the uniqueness of\neach digital purchase with a cryptographic function, called a cryptogram.\nHowever, computationally powerful attacks violate the security of these\nfunctions. Quantum technology, on the other hand, has the unique potential to\nguarantee payment protection even in the presence of infinite computational\npower. Here, we show how quantum light can secure daily digital payments in a\npractical manner by generating inherently unforgeable quantum-cryptograms. We\nimplement the full scheme over an urban optical fiber link, and show its\nrobustness to noise and loss-dependent attacks. Unlike previously proposed\nquantum-security protocols, our solution does not depend on challenging\nlong-term quantum storage or a network of trusted agents and authenticated\nchannels. The envisioned scenario is practical with near-term technology and\nhas the potential to herald a new era of real-world, quantum-enabled security.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:20:14 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14505","submitter":"Simon Markfelder","authors":"Daniel W. Boutros, Simon Markfelder and Edriss S. Titi","title":"Nonuniqueness of generalised weak solutions to the primitive and Prandtl\n  equations","comments":"73 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We develop a convex integration scheme for constructing nonunique weak\nsolutions to the hydrostatic Euler equations (also known as the inviscid\nprimitive equations of oceanic and atmospheric dynamics) in both two and three\ndimensions. We also develop such a scheme for the construction of nonunique\nweak solutions to the three-dimensional viscous primitive equations, as well as\nthe two-dimensional Prandtl equations.\n  While in [D.W. Boutros, S. Markfelder and E.S. Titi, arXiv:2208.08334 (2022)]\nthe classical notion of weak solution to the hydrostatic Euler equations was\ngeneralised, we introduce here a further generalisation. For such generalised\nweak solutions we show the existence and nonuniqueness for a large class of\ninitial data. Moreover, we construct infinitely many examples of generalised\nweak solutions which do not conserve energy. The barotropic and baroclinic\nmodes of solutions to the hydrostatic Euler equations (which are the average\nand the fluctuation of the horizontal velocity in the $z$-coordinate,\nrespectively) that are constructed have different regularities.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:23:27 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14506","submitter":"Y. Samuel Wang","authors":"Y. Samuel Wang, Mladen Kolar, Mathias Drton","title":"Confidence Sets for Causal Orderings","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Causal discovery procedures aim to deduce causal relationships among\nvariables in a multivariate dataset. While various methods have been proposed\nfor estimating a single causal model or a single equivalence class of models,\nless attention has been given to quantifying uncertainty in causal discovery in\nterms of confidence statements. The primary challenge in causal discovery is\ndetermining a causal ordering among the variables. Our research offers a\nframework for constructing confidence sets of causal orderings that the data do\nnot rule out. Our methodology applies to structural equation models and is\nbased on a residual bootstrap procedure to test the goodness-of-fit of causal\norderings. We demonstrate the asymptotic validity of the confidence set\nconstructed using this goodness-of-fit test and explain how the confidence set\nmay be used to form sub/supersets of ancestral relationships as well as\nconfidence intervals for causal effects that incorporate model uncertainty.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:23:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14507","submitter":"Shashank Sonkar","authors":"Shashank Sonkar, Richard G. Baraniuk","title":"Deduction under Perturbed Evidence: Probing Student Simulation\n  Capabilities of Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We explore whether Large Language Models (LLMs) are capable of logical\nreasoning with distorted facts, which we call Deduction under Perturbed\nEvidence (DUPE). DUPE presents a unique challenge to LLMs since they typically\nrely on their parameters, which encode mostly accurate information, to reason\nand make inferences. However, in DUPE, LLMs must reason over manipulated or\nfalsified evidence present in their prompts, which can result in false\nconclusions that are valid only under the manipulated evidence. Our goal with\nDUPE is to determine whether LLMs can arrive at these false conclusions and\nidentify whether the dominant factor influencing the deduction process is the\nencoded data in the parameters or the manipulated evidence in the prompts. To\nevaluate the DUPE capabilities of LLMs, we create a DUPEd version of the\nStrategyQA dataset, where facts are manipulated to reverse the answer to the\nquestion. Our findings show that even the most advanced GPT models struggle to\nreason on manipulated facts - showcasing poor DUPE skills - with accuracy\ndropping by 45% compared to the original dataset. We also investigate prompt\nsettings inspired from student simulation models, which mitigate the accuracy\ndrop to some extent. Our findings have practical implications for understanding\nthe performance of LLMs in real-world applications such as student simulation\nmodels that involve reasoning over inaccurate information.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:26:03 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14508","submitter":"Jesse Madnick","authors":"Gavin Ball, Jesse Madnick","title":"A Spinorial Hopf Differential for Associative Submanifolds","comments":"10 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.DG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Given a CMC surface in $R^3$, its traceless second fundamental form can be\nviewed as a holomorphic section called the Hopf differential. By analogy, we\nshow that for an associative submanifold of a 7-manifold $M^7$ with\n$G_2$-structure, its traceless second fundamental form can be viewed as a\ntwisted spinor. Moreover, if $M$ is $R^7$, $T^7$, or $S^7$ with the standard\n$G_2$-structure, then this twisted spinor is harmonic. Consequently, every\nnon-totally-geodesic associative 3-fold in $R^7$, $T^7$, and $S^7$ admits\nnon-vanishing harmonic twisted spinors. Analogous results hold for special\nLagrangians in $R^6$ and $T^6$, coassociative 4-folds in $R^7$ and $T^7$, and\nCayley 4-folds in $R^8$ and $T^8$.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:26:32 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14509","submitter":"Francesco Coti Zelati","authors":"M. C. Baglio, F. Coti Zelati, S. Campana, G. Busquet, P. D'Avanzo, S.\n  Giarratana, M. Giroletti, F. Ambrosino, S. Crespi, A. Miraval Zanon, X. Hou,\n  D. Li, J. Li, P. Wang, D. M. Russell, D. F. Torres, K. Alabarta, P. Casella,\n  S. Covino, D. M. Bramich, D. de Martino, M. M\\'endez, S. E. Motta, A.\n  Papitto, P. Saikia, F. Vincentelli","title":"Matter ejections behind the highs and lows of the transitional\n  millisecond pulsar PSR J1023+0038","comments":"25 pages, 12 figures, 9 tables. Submitted to Astronomy and\n  Astrophysics","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.HE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Transitional millisecond pulsars are an emerging class of sources linking\nlow-mass X-ray binaries to millisecond radio pulsars in binary systems. These\npulsars alternate between a radio pulsar state and an active low-luminosity\nX-ray disc state. During the active state, these sources exhibit two distinct\nemission modes (high and low) that alternate unpredictably, abruptly, and\nincessantly. X-ray to optical pulsations are observed only during the high\nmode. Knowledge of the root reason for this puzzling behaviour remains elusive.\nThis paper presents the results of the most extensive multi-wavelength campaign\never conducted on the transitional pulsar prototype, PSR J1023+0038, covering\nfrom radio to X-rays. The campaign was carried out over two nights in June\n2021, and involved 12 different telescopes and instruments including\nXMM-Newton, HST, VLT/FORS2 (in polarimetric mode), ALMA, VLA and FAST. By\nmodelling the broadband spectral energy distributions in both emission modes,\nwe show that the mode switches are caused by changes in the innermost region of\nthe accretion disc. These changes trigger the emission of discrete mass\nejections, which occur on top of a compact jet, as testified by the detection\nof at least one short-duration millimetre flare with ALMA at the high-to-low\nmode switch. A subsequent re-enshrouding of the pulsar completes the scenario\nbehind the mode switches.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:29:26 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14510","submitter":"Vladislav Kochev D.","authors":"Vladislav D. Kochev, Seidali S. Seidov, Pavel D. Grigoriev","title":"On the size of superconducting islands on the density-wave background in\n  organic metals","comments":"13 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.supr-con","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Most high-$T_c$ superconductors are spatially inhomogeneous. Usually, this\nheterogeneity originates from the interplay of various types of electronic\nordering. It affects various superconducting properties, such as the transition\ntemperature, magnetic upper critical field, critical current, etc. In this\npaper we analyze the parameters of spatial phase segregation during the\nfirst-order transition between superconductivity (SC) and a charge- or\nspin-density wave state in quasi-one-dimensional metals with imperfect nesting,\ntypical to organic superconductors. An external pressure or another driving\nparameter increases the transfer integrals in electron dispersion, which only\nslightly affects SC but violates the Fermi-surface nesting and suppresses the\ndensity wave (DW). At a critical pressure $P_{c}$ the transition from DW to SC\noccurs. We estimate the characteristic size of SC islands during this phase\ntransition in organic metals in two ways. Using the Ginzburg-Landau expansion\nwe analytically obtain a lower bound for the size of SC domains. To estimate\nmore specific interval of possible size of the SC islands in (TMTSF)$_2$PF$_6$\nsamples we perform numerical calculations of the percolation probability via SC\ndomains and compare it with experimental resistivity data. This helps to\ndevelop a consistent microscopic description of SC spatial heterogeneity in\nvarious organic superconductors.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:30:09 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14511","submitter":"Andr\\'es Dar\\'io Berm\\'udez Manjarres","authors":"A. D. Berm\\'udez Manjarres","title":"Adiabatic driving and geometric phases in classical systems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We study the concepts of adiabatic driving and geometric phases of classical\nintegrable systems under the Koopman-von Neumann formalism. In close relation\nto what happens to a quantum state, a classical Koopman-von Neumann eigenstate\nwill acquire a geometric phase factor $exp\\left\\{ i\\Phi\\right\\} $ after a\nclosed variation of the parameters $\\lambda$ in its associated Hamiltonian. The\nexplicit form of $\\Phi$ is then derived for integrable systems, and its\nrelation with the Hannay angles is shown. Additionally, we use quantum formulas\nto write a classical adiabatic gauge potential that generates adiabatic unitary\nflow between classical eigenstates, and we explicitly show the relationship\nbetween the potential and the classical geometric phase.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:32:57 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14512","submitter":"Jakob Gabriel","authors":"Jakob Gabriel, Joachim Deutscher","title":"Robust Cooperative Output Regulation for Networks of Hyperbolic PIDE-ODE\n  Systems","comments":"16 pages, 5 figures, accepted for publication in IEEE Transactions on\n  Automatic Control","journal-ref":null,"doi":"10.1109/TAC.2023.3272871","report-no":null,"categories":"math.OC cs.SY eess.SY","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper the robust cooperative output regulation problem for\nmulti-agent systems (MAS) with general heterodirectional hyperbolic PIDE-ODE\nagents is considered. This setup also covers networks of ODEs with arbitrarily\nlong input and output delays. The output of the agents can be defined at all\nboundaries, in-domain and may depend on the ODE state, while disturbances act\non the agents in-domain, at the boundaries, the output and the ODE. The\ncommunication network is described by a constant digraph and if its Laplacian\nis reducible, then heterogeneous agents are permitted also in the nominal case.\nThe solution is based on the cooperative internal model principle, which\nrequires to include a diffusively driven internal model in the controller. The\ncorresponding state feedback regulator design starts with a local backstepping\nstabilization of the coupled hyperbolic PIDE-ODE systems. It is shown that the\nremaining simultaneous stabilization of the MAS can be traced back to the\nsimultaneous stabilization of the finite-dimensional cooperative internal\nmodel. Solvability conditions in terms of the network topology and the agents\ntransfer behavior are presented. The new design method is applied to the\nformation control of a platoon of uncertain heavy ropes carrying loads to\nverify its applicability. Simulations confirm the synchronization performance\nachieved by the resulting networked controller.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:37:37 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14513","submitter":"Dominik Werner Wolf","authors":"Dominik Werner Wolf and Markus Ulrich and Alexander Braun","title":"Windscreen Optical Quality for AI Algorithms: Refractive Power and MTF\n  not Sufficient","comments":"Submitted to IEEE ITSC-2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Windscreen optical quality is an important aspect of any advanced driver\nassistance system, and also for future autonomous driving, as today at least\nsome cameras of the sensor suite are situated behind the windscreen. Automotive\nmass production processes require measurement systems that characterize the\noptical quality of the windscreens in a meaningful way, which for modern\nperception stacks implies meaningful for artificial intelligence (AI)\nalgorithms. The measured optical quality needs to be linked to the performance\nof these algorithms, such that performance limits - and thus production\ntolerance limits - can be defined. In this article we demonstrate that the main\nmetric established in the industry - refractive power - is fundamentally not\ncapable of capturing relevant optical properties of windscreens. Further, as\nthe industry is moving towards the modulation transfer function (MTF) as an\nalternative, we mathematically show that this metric cannot be used on\nwindscreens alone, but that the windscreen forms a novel optical system\ntogether with the optics of the camera system. Hence, the required goal of a\nqualification system that is installed at the windscreen supplier and\nindependently measures the optical quality cannot be achieved using MTF. We\npropose a novel concept to determine the optical quality of windscreens and to\nuse simulation to link this optical quality to the performance of AI\nalgorithms, which can hopefully lead to novel inspection systems.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:41:04 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14514","submitter":"Mario Neves Junior","authors":"O. Abla and M. J. Neves","title":"Effects of wave propagation in canonical Poisson gauge theory under an\n  external magnetic field","comments":"18 pages, no figures","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-th","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The non-commutative electrodynamics based on the canonical Poisson gauge\ntheory is studied in this paper. We investigate the effects of the plane wave\nsolutions in the non-commutative field equations when the theory is submitted\nto an external and uniform magnetic field. The energy-momentum tensor,\nsymmetric and gauge invariant, is obtained from the non-commutative field\nequations. The plane wave solutions for the gauge potential yield the wave\nequations in the momentum space, using the linear approximation on the\nnon-commutative parameter. Thereby, we obtain the dispersion relations of the\ngauge theory in the presence of an external and uniform electromagnetic field.\nWe discuss the birefringence phenomenon that depends on the non-commutative\nparameter, and using the bound of the PVLAS experiment for the vacuum magnetic\nbirefringence, we estimate a theoretical value for non-commutative parameter.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:41:09 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14515","submitter":"Clare C. Yu","authors":"Dan Mickelsen, Herve M. Carruzzo, Susan N. Coppersmith and Clare C. Yu","title":"Effects of Temperature Fluctuations on Charge Noise in Quantum Dot\n  Qubits","comments":"8 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall quant-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Silicon quantum dot qubits show great promise but suffer from charge noise\nwith a 1/f^\\alpha spectrum, where f is frequency and \\alpha \\lesssim 1. It has\nrecently been proposed that 1/f^\\alpha noise spectra can emerge from a few\nthermally activated two-level fluctuators in the presence of sub-bath\ntemperature fluctuations associated with a two-dimensional electron gas\n(2DEG)~\\cite{Ahn2021}. We investigate this proposal by doing Monte Carlo\nsimulations of a single Ising spin in a bath with a fluctuating temperature. We\nfind that to obtain noise with a $1/f^\\alpha$ spectrum with $alpha \\lesssim 1\ndown to low frequencies, the duration of temperature fluctuations must be\ncomparable to the inverse of the lowest frequency at which the noise is\nmeasured. This result is consistent with an analytic calculation in which the\nfluctuator is a two-state system with dynamics governed by time-dependent\nswitching rates. In this case we find that the noise spectrum follows a\nLorentzian at frequencies lower than the inverse of the average duration of the\nlowest switching rate. We then estimate relaxation times of thermal\nfluctuations by considering thermal diffusion in an electron gas in a confined\ngeometry. We conclude that temperature fluctuations in a 2DEG sub-bath would\nrequire an unphysically long duration to be consistent with experimental\nmeasurements of 1/f-like charge noise in quantum dots at frequencies extending\nwell below 1 Hz.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:45:07 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14516","submitter":"Taekyung Heo","authors":"Srinivas Sridharan, Taekyung Heo, Louis Feng, Zhaodong Wang, Matt\n  Bergeron, Wenyin Fu, Shengbao Zheng, Brian Coutinho, Saeed Rashidi, Changhai\n  Man, Tushar Krishna","title":"Chakra: Advancing Performance Benchmarking and Co-design using\n  Standardized Execution Traces","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.DC","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Benchmarking and co-design are essential for driving optimizations and\ninnovation around ML models, ML software, and next-generation hardware. Full\nworkload benchmarks, e.g. MLPerf, play an essential role in enabling fair\ncomparison across different software and hardware stacks especially once\nsystems are fully designed and deployed. However, the pace of AI innovation\ndemands a more agile methodology to benchmark creation and usage by simulators\nand emulators for future system co-design. We propose Chakra, an open graph\nschema for standardizing workload specification capturing key operations and\ndependencies, also known as Execution Trace (ET). In addition, we propose a\ncomplementary set of tools/capabilities to enable collection, generation, and\nadoption of Chakra ETs by a wide range of simulators, emulators, and\nbenchmarks. For instance, we use generative AI models to learn latent\nstatistical properties across thousands of Chakra ETs and use these models to\nsynthesize Chakra ETs. These synthetic ETs can obfuscate key proprietary\ninformation and also target future what-if scenarios. As an example, we\ndemonstrate an end-to-end proof-of-concept that converts PyTorch ETs to Chakra\nETs and uses this to drive an open-source training system simulator\n(ASTRA-sim). Our end-goal is to build a vibrant industry-wide ecosystem of\nagile benchmarks and tools to drive future AI system co-design.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:45:45 GMT"},{"version":"v2","created":"Fri, 26 May 2023 16:22:27 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.14517","submitter":"Oleksii Tsepa","authors":"Oleksii Tsepa, Bohdan Naida, Bo Wang","title":"CongFu: Conditional Graph Fusion for Drug Synergy Prediction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI q-bio.QM","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Drug synergy, characterized by the amplified combined effect of multiple\ndrugs, presents a critical phenomenon for optimizing therapeutic outcomes.\nHowever, limited data on drug synergy, arising from the vast number of possible\ndrug combinations and computational costs, motivate the need for predictive\nmethods. In this work, we introduce CongFu, a novel Conditional Graph Fusion\nLayer, designed to predict drug synergy. CongFu employs an attention mechanism\nand a bottleneck to extract local graph contexts and conditionally fuse graph\ndata within a global context. Its modular architecture enables flexible\nreplacement of layer modules, including readouts and graph encoders,\nfacilitating customization for diverse applications. To evaluate the\nperformance of CongFu, we conduct comprehensive experiments on four datasets,\nencompassing three distinct setups for drug synergy prediction. Remarkably,\nCongFu achieves state-of-the-art results on 11 out of 12 benchmark datasets,\ndemonstrating its ability to capture intricate patterns of drug synergy.\nThrough extensive ablation studies, we validate the significance of individual\nlayer components, affirming their contributions to overall predictive\nperformance. By addressing the challenge of predicting drug synergy in untested\ndrug pairs, CongFu opens new avenues for optimizing drug combinations and\nadvancing personalized medicine.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:46:17 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14518","submitter":"Michael Wolman","authors":"Alexander S. Kechris and Michael S. Wolman","title":"An effective version of Nadkarni's Theorem","comments":"26 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.LO math.DS","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Nadkarni's Theorem asserts that for a countable Borel equivalence relation\n(CBER) exactly one of the following holds: (1) It has an invariant Borel\nprobability measure or (2) it admits a Borel compression, i.e., a Borel\ninjection that maps each equivalence class to a proper subset of it. We prove\nin this paper an effective version of Nadkarni's Theorem, which shows that if a\nCBER is effectively Borel, then either alternative (1) above holds or else it\nadmits an effectively Borel compression. As a consequence if a CBER is\neffectively Borel and admits a Borel compression, then it actually admits an\neffectively Borel compression. We also prove an effective version of the\nergodic decomposition theorem. Finally a counterexample is given to show that\nalternative (1) above does not admit an effective version.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:47:24 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14519","submitter":"Xiaoshan Xu","authors":"Ahsan Ullah, Balamurugan Balasubramanian, Bibek Tiwari, Bharat Giri,\n  David J. Sellmyer, Ralph Skomski, Xiaoshan Xu","title":"Berry Curvature and Topological Hall Effect in Magnetic Nanoparticles","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Analytical calculations and micromagnetic simulations are used to determine\nthe Berry curvature and topological Hall effect (THE) due to conduction\nelectrons in small ferromagnetic particles. Our focus is on small particles of\nnonellipsoidal shapes, where noncoplanar spin structures yield a nonzero\ntopological Hall signal quantified by the skyrmion number Q. We consider two\nmechanisms leading to noncoplanarity in aligned nanoparticles, namely\nflower-state spin configurations due to stray fields near corners and edges,\nand curling-type magnetostatic selfinteractions. In very small particles, the\nreverse magnetic fields enhance Q due to the flower state until the reversal\noccurs, whereas for particles with a radius greater than coherence radius Rcoh\nthe Q jumps to a larger value at the nucleation field representing the\ntransition from the flower state to the curling state. We calculate the\nSkyrmion density (average Berry curvature) from these spin structures as a\nfunction of particle size and applied magnetic field. Our simulation results\nagree with analytical calculations for both flower state and flux closure\nstates. We showed the presence of Berry curvature in small particles as long as\nthe size of the particle is less than the single domain limit. Using magnetic\nforce microscopy (MFM), we also showed that in a nanodot of Co with a suitable\nsize, a magnetic vortex state with perpendicular (turned-up) magnetization at\nthe core is realized which can be manifested for Berry curvature and emergent\nmagnetic field in confined geometries for single domain state at room\ntemperature.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:48:15 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14520","submitter":"Francesco Pecora","authors":"Francesco Pecora, Sergio Servidio, Yan Yang, William H. Matthaeus,\n  Alexandros Chasapis, Antonella Greco, Daniel J. Gershman, Barbara L. Giles,\n  and James L. Burch","title":"Three-dimensional energy transfer in space plasma turbulence from\n  multipoint measurement","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.space-ph astro-ph.IM astro-ph.SR physics.plasm-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  A novel multispacecraft technique applied to Magnetospheric Multiscale (MMS)\nmission data collected in the Earth's magnetosheath enables evaluation of the\nenergy cascade rate solving the full Yaglom's equation in a turbulent space\nplasma. The method differs from existing approaches in that (i) it is\ninherently three-dimensional; (ii) it provides a statistically significant\nnumber of estimates from a single data stream; and (iii) it allows for a direct\nvisualization of energy flux in turbulent plasmas. This new technique will\nultimately provide a realistic, comprehensive picture of the turbulence process\nin plasmas.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:48:51 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14521","submitter":"Yihao Xue","authors":"Yihao Xue, Ali Payani, Yu Yang, Baharan Mirzasoleiman","title":"Eliminating Spurious Correlations from Pre-trained Models via Data\n  Mixing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Machine learning models pre-trained on large datasets have achieved\nremarkable convergence and robustness properties. However, these models often\nexploit spurious correlations between certain attributes and labels, which are\nprevalent in the majority of examples within specific categories but are not\npredictive of these categories in general. The learned spurious correlations\nmay persist even after fine-tuning on new data, which degrades models'\nperformance on examples that do not exhibit the spurious correlation. In this\nwork, we propose a simple and highly effective method to eliminate spurious\ncorrelations from pre-trained models. The key idea of our method is to leverage\na small set of examples with spurious attributes, and balance the spurious\nattributes across all classes via data mixing. We theoretically confirm the\neffectiveness of our method, and empirically demonstrate its state-of-the-art\nperformance on various vision and NLP tasks, including eliminating spurious\ncorrelations from pre-trained ResNet50 on Waterbirds and CelebA, adversarially\npre-trained ResNet50 on ImageNet, and BERT pre-trained on CivilComments.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:49:45 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14523","submitter":"Artemis Tsantiri","authors":"A. Tsantiri (1, 2, 3), A. Palmisano-Kyle (1, 2, 3), A. Spyrou (1, 2,\n  3), P. Mohr (4), H. C. Berg (1, 2, 3), P. A. DeYoung (5), A. C. Dombos (1, 2,\n  3), P. Gastis (6, 3), E. C. Good (2, 3), C. M. Harris (1, 2, 3), S. N.\n  Liddick (7, 2, 3), S. M. Lyons (1, 2, 3), O. Olivas-Gomez (8), G. Owens-Fryar\n  (1, 2, 3), J. Pereira (1, 2, 3), A. L. Richard (1, 2, 3), A. Simon (8), M. K.\n  Smith (1, 2) and R. G. T. Zegers (1, 2, 3) ((1) Department of Physics and\n  Astronomy, Michigan State University, East Lansing, MI 48824, USA, (2)\n  Facility for Rare Isotope Beams, Michigan State University, East Lansing, MI\n  48824, USA, (3) Joint Institute for Nuclear Astrophysics - Center for the\n  Evolution of the Elements, East Lansing, MI 48824, USA, (4) Institute for\n  Nuclear Research (Atomki), H-4001 Debrecen, Hungary, (5) Physics Department,\n  Hope College, Holland, MI 49423, USA, (6) Department of Physics, Central\n  Michigan University, Mt Pleasant, MI 48859, USA, (7) Department of Chemistry,\n  Michigan State University, East Lansing, MI 48824, USA, (8) Department of\n  Physics, University of Notre Dame, Notre Dame, IN 46556, USA)","title":"Cross Section Measurement of the $^{82}$Kr(p,$\\gamma$)$^{83}$Rb Reaction\n  in Inverse Kinematics","comments":null,"journal-ref":"Phys. Rev. C 107, 035808 (2023)","doi":"10.1103/PhysRevC.107.035808","report-no":null,"categories":"nucl-ex","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The total cross section of the $^{82}$Kr(p,$\\gamma$)$^{83}$Rb reaction was\nmeasured for the first time at effective center-of-mass energies between 2.4\nand 3.0 MeV, within the relevant Gamow window for the astrophysical $\\gamma$\nprocess. The experiment took place at the National Superconducting Cyclotron\nLaboratory at Michigan State University using the ReA facility. A $^{82}$Kr\nbeam was directed onto a hydrogen gas cell located at the center of the Summing\nNaI(Tl) (SuN) detector. The obtained spectra were analyzed using the\n$\\gamma$-summing technique and the extracted cross section was compared to\nstandard statistical model calculations using the \\textsc{non-smoker} and\n\\textsc{talys} codes. The comparison indicates that standard statistical model\ncalculations tend to overproduce the cross section of the\n$^{82}$Kr(p,$\\gamma$)$^{83}$Rb reaction relative to the experimentally measured\nvalues. Furthermore, the experimental data was used to provide additional\nconstraints on the nuclear level density and $\\gamma$-ray strength function\nused in the statistical model calculations.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:56:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14524","submitter":"Alexey Khartov","authors":"A. A. Khartov","title":"Some criteria of rational infinite divisibility for distribution\n  functions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We study distribution functions $F$ that have the property of rational\ninfinite divisibility: there exist some infinitely divisible distribution\nfunctions $F_1$ and $F_2$ such that $F_1=F*F_2$. We study a class\n$\\boldsymbol{Q}$ of such distribution functions, which is a wide natural\nextension of the well-known class of infinitely divisible distribution\nfunctions. We are interested in general criteria of membership of\n$\\boldsymbol{Q}$. The obtained conditions are formulated in terms of\ncharacteristic functions and they seem to be convenient for the application.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:56:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14525","submitter":"Chen Chen","authors":"Chen Chen, Zhicheng Liu","title":"The State of the Art in Creating Visualization Corpora for Automated\n  Chart Analysis","comments":"To appear at EuroVis 2023","journal-ref":null,"doi":"10.1111/cgf.14855","report-no":null,"categories":"cs.HC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We present a state-of-the-art report on visualization corpora in automated\nchart analysis research. We survey 56 papers that created or used a\nvisualization corpus as the input of their research techniques or systems.\nBased on a multi-level task taxonomy that identifies the goal, method, and\noutputs of automated chart analysis, we examine the property space of existing\nchart corpora along five dimensions: format, scope, collection method,\nannotations, and diversity. Through the survey, we summarize common patterns\nand practices of creating chart corpora, identify research gaps and\nopportunities, and discuss the desired properties of future benchmark corpora\nand the required tools to create them.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:59:15 GMT"},{"version":"v2","created":"Sun, 4 Jun 2023 21:19:40 GMT"}],"update_date":"2023-06-06"}
{"id":"2305.14526","submitter":"Barry Sanders","authors":"Barry C Sanders","title":"Perspective on electromagnetically induced transparency vs Autler-Townes\n  splitting","comments":"4 pp","journal-ref":"AVS Quantum Sci. 5, 024403 (2023)","doi":"10.1116/5.0149908","report-no":null,"categories":"quant-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Electromagnetically induced transparency and Autler-Townes splitting are two\ndistinct yet related effects. These phenomena are relevant to quantum\ntechnologies, including quantum memory, quantum switching, and quantum\ntransduction. Here, the similarities and differences between these phenomena\nalong historical and conceptual lines are discussed and their realizations on\nvarious physical platforms including atomic gases, superconducting circuits,\nand optomechanics are elaborated. In particular, the author clarifies two\napproaches to assessing which phenomenon is observed based on a black-box\napproach of modeling the output, given a particular input vs analyzing the\nunderpinning physics. Furthermore, the author highlights the ability to effect\na continuous transition between the two seemingly disparate phenomena.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:59:24 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14527","submitter":"Alexander Kapitanov","authors":"Alexander Kapitanov, Karina Kvanchiani, Alexander Nagaev, Elizaveta\n  Petrova","title":"Slovo: Russian Sign Language Dataset","comments":"russian sign language recognition dataset, open-source","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  One of the main challenges of the sign language recognition task is the\ndifficulty of collecting a suitable dataset due to the gap between deaf and\nhearing society. In addition, the sign language in each country differs\nsignificantly, which obliges the creation of new data for each of them. This\npaper presents the Russian Sign Language (RSL) video dataset Slovo, produced\nusing crowdsourcing platforms. The dataset contains 20,000 FullHD recordings,\ndivided into 1,000 classes of RSL gestures received by 194 signers. We also\nprovide the entire dataset creation pipeline, from data collection to video\nannotation, with the following demo application. Several neural networks are\ntrained and evaluated on the Slovo to demonstrate its teaching ability.\nProposed data and pre-trained models are publicly available.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:00:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14528","submitter":"Alex Shtoff","authors":"Alex Shtoff and Elie Abboud and Rotem Stram and Oren Somekh","title":"Basis Function Encoding of Numerical Features in Factorization Machines\n  for Improved Accuracy","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Factorization machine (FM) variants are widely used for large scale real-time\ncontent recommendation systems, since they offer an excellent balance between\nmodel accuracy and low computational costs for training and inference. These\nsystems are trained on tabular data with both numerical and categorical\ncolumns. Incorporating numerical columns poses a challenge, and they are\ntypically incorporated using a scalar transformation or binning, which can be\neither learned or chosen a-priori. In this work, we provide a systematic and\ntheoretically-justified way to incorporate numerical features into FM variants\nby encoding them into a vector of function values for a set of functions of\none's choice.\n  We view factorization machines as approximators of segmentized functions,\nnamely, functions from a field's value to the real numbers, assuming the\nremaining fields are assigned some given constants, which we refer to as the\nsegment. From this perspective, we show that our technique yields a model that\nlearns segmentized functions of the numerical feature spanned by the set of\nfunctions of one's choice, namely, the spanning coefficients vary between\nsegments. Hence, to improve model accuracy we advocate the use of functions\nknown to have strong approximation power, and offer the B-Spline basis due to\nits well-known approximation power, availability in software libraries, and\nefficiency. Our technique preserves fast training and inference, and requires\nonly a small modification of the computational graph of an FM model. Therefore,\nit is easy to incorporate into an existing system to improve its performance.\nFinally, we back our claims with a set of experiments, including synthetic,\nperformance evaluation on several data-sets, and an A/B test on a real online\nadvertising system which shows improved performance.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:10:17 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14529","submitter":"Chong Wang","authors":"Chong Wang, Xiu Gu, Shu Chen and Yu-xi Liu","title":"Topological edge state transfer via topological adiabatic passage","comments":"arXiv admin note: substantial text overlap with arXiv:1711.06829","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.mes-hall","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The study of quantum state transfer has led to a variety of research efforts\nutilizing quantum simulators. By exploiting the tunability of the qubit\nfrequency and qubit-qubit coupling, a superconducting qubit chain can simulate\nvarious topological band models. In our study, we demonstrate that a spin-up\nstate can be transported along a topological qubit chain by modulating the\ncoupling strengths and the qubit frequencies. We here propose another more\nstraightforward approach to theoretically interpret this state transfer\nprocess. We show that the Hilebert space of the qubit chain can be restricted\ninto the subspace of the only two edge states when investigating this process,\nand the Hamiltonian can degenerate to a two-state Landau-Zener (LZ) model.\nTherefore the state transfer process in this topological qubit chain is\nequivalent to the same process through the adiabatic passage of the LZ model.\nFurther more, we show how to use this approach to generalize the state transfer\nprocess from one-qubit Fock state to two-qubit Bell state.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:10:30 GMT"},{"version":"v2","created":"Tue, 30 May 2023 12:02:39 GMT"}],"update_date":"2023-05-31"}
{"id":"2305.14530","submitter":"Sushant Saryal","authors":"Sushant Saryal and Deepak Dhar","title":"Cusp singularities in the distribution of orientations of\n  asymmetrically-pivoted hard discs on a lattice","comments":"8 pages, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.stat-mech","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We study a system of equal-sized circular discs each with an asymmetrically\nplaced pivot at a fixed distance from the center. The pivots are fixed at the\nvertices of a regular triangular lattice. The discs can rotate freely about the\npivots, with the constraint that no discs can overlap with each other. Our\nMonte Carlo simulations show that the one-point probability distribution of\norientations shows multiple cusp-like singularities. We determine the exact\npositions and qualitative behavior of these singularities. In addition to these\ngeometrical singularities, we also find that the system shows order-disorder\ntransitions, with a disordered phase at large lattice spacings, a phase with\nspontaneously broken orientational lattice symmetry at small lattice spacings,\nand an intervening Berezinskii-Kosterlitz-Thouless phase in between.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:15:44 GMT"},{"version":"v2","created":"Sun, 28 May 2023 19:54:55 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.14531","submitter":"David Mohaisen","authors":"Mohammed Alqadhi, Ali Alkinoon, Saeed Salem, David Mohaisen","title":"Understanding the Country-Level Security of Free Content Websites and\n  their Hosting Infrastructure","comments":"10 pages, 2 figures, 4 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  This paper examines free content websites (FCWs) and premium content websites\n(PCWs) in different countries, comparing them to general websites. The focus is\non the distribution of malicious websites and their correlation with the\nnational cyber security index (NCSI), which measures a country's cyber security\nmaturity and its ability to deter the hosting of such malicious websites. By\nanalyzing a dataset comprising 1,562 FCWs and PCWs, along with Alexa's top\nmillion websites dataset sample, we discovered that a majority of the\ninvestigated websites are hosted in the United States. Interestingly, the\nUnited States has a relatively low NCSI, mainly due to a lower score in privacy\npolicy development. Similar patterns were observed for other countries With\nvarying NCSI criteria. Furthermore, we present the distribution of various\ncategories of FCWs and PCWs across countries. We identify the top hosting\ncountries for each category and provide the percentage of discovered malicious\nwebsites in those countries. Ultimately, the goal of this study is to identify\nregional vulnerabilities in hosting FCWs and guide policy improvements at the\ncountry level to mitigate potential cyber threats.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:31:02 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14532","submitter":"Christophe De Beule","authors":"Christophe De Beule and E. J. Mele","title":"Berry Curvature Spectroscopy from Bloch Oscillations","comments":"5 + 4 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We demonstrate that the Berry curvature of an isolated Bloch miniband in\ntwo-dimensional superlattices can be probed by the dressed linear optical\nresponse when a uniform static field is applied to the system. In particular,\nwhen the static field is sufficiently strong such that full Bloch oscillations\noccur before the crystal momentum relaxes to equilibrium, the optical response\nof the dressed system becomes resonant at the Bloch frequencies. The latter are\nin the THz regime when the superlattice periodicity is of the order of 10 nm.\nUsing a band-projected semiclassical theory, we define a dressed optical\nconductivity and find that the height of the resonances in the dressed Hall\nconductivity are proportional to the Fourier components of the Berry curvature.\nWe illustrate our results with a low-energy model on an effective honeycomb\nlattice.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:33:06 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14533","submitter":"Jo\\~ao Sedoc","authors":"Huda Khayrallah and Zuhaib Akhtar and Edward Cohen and Jo\\~ao Sedoc","title":"How to Choose How to Choose Your Chatbot: A Massively Multi-System\n  MultiReference Data Set for Dialog Metric Evaluation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We release MMSMR, a Massively Multi-System MultiReference dataset to enable\nfuture work on metrics and evaluation for dialog. Automatic metrics for\ndialogue evaluation should be robust proxies for human judgments; however, the\nverification of robustness is currently far from satisfactory. To quantify the\nrobustness correlation and understand what is necessary in a test set, we\ncreate and release an 8-reference dialog dataset by extending single-reference\nevaluation sets and introduce this new language learning conversation dataset.\nWe then train 1750 systems and evaluate them on our novel test set and the\nDailyDialog dataset. We release the novel test set, and model hyper parameters,\ninference outputs, and metric scores for each system on a variety of datasets.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:33:43 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14534","submitter":"Muhammad Umar Salman","authors":"Muhammad Umar Salman, Asif Hanif, Shady Shehata, Preslav Nakov","title":"Detecting Propaganda Techniques in Code-Switched Social Media Text","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Propaganda is a form of communication intended to influence the opinions and\nthe mindset of the public to promote a particular agenda. With the rise of\nsocial media, propaganda has spread rapidly, leading to the need for automatic\npropaganda detection systems. Most work on propaganda detection has focused on\nhigh-resource languages, such as English, and little effort has been made to\ndetect propaganda for low-resource languages. Yet, it is common to find a mix\nof multiple languages in social media communication, a phenomenon known as\ncode-switching. Code-switching combines different languages within the same\ntext, which poses a challenge for automatic systems. With this in mind, here we\npropose the novel task of detecting propaganda techniques in code-switched\ntext. To support this task, we create a corpus of 1,030 texts code-switching\nbetween English and Roman Urdu, annotated with 20 propaganda techniques, which\nwe make publicly available. We perform a number of experiments contrasting\ndifferent experimental setups, and we find that it is important to model the\nmultilinguality directly (rather than using translation) as well as to use the\nright fine-tuning strategy. The code and the dataset are publicly available at\nhttps://github.com/mbzuai-nlp/propaganda-codeswitched-text\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:37:26 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14535","submitter":"Kexin Huang","authors":"Kexin Huang, Ying Jin, Emmanuel Candes, Jure Leskovec","title":"Uncertainty Quantification over Graph with Conformalized Graph Neural\n  Networks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Graph Neural Networks (GNNs) are powerful machine learning prediction models\non graph-structured data. However, GNNs lack rigorous uncertainty estimates,\nlimiting their reliable deployment in settings where the cost of errors is\nsignificant. We propose conformalized GNN (CF-GNN), extending conformal\nprediction (CP) to graph-based models for guaranteed uncertainty estimates.\nGiven an entity in the graph, CF-GNN produces a prediction set/interval that\nprovably contains the true label with pre-defined coverage probability (e.g.\n90%). We establish a permutation invariance condition that enables the validity\nof CP on graph data and provide an exact characterization of the test-time\ncoverage. Moreover, besides valid coverage, it is crucial to reduce the\nprediction set size/interval length for practical use. We observe a key\nconnection between non-conformity scores and network structures, which\nmotivates us to develop a topology-aware output correction model that learns to\nupdate the prediction and produces more efficient prediction sets/intervals.\nExtensive experiments show that CF-GNN achieves any pre-defined target marginal\ncoverage while significantly reducing the prediction set/interval size by up to\n74% over the baselines. It also empirically achieves satisfactory conditional\ncoverage over various raw and network features.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:38:23 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14536","submitter":"Jakub Macina","authors":"Jakub Macina, Nico Daheim, Sankalan Pal Chowdhury, Tanmay Sinha, Manu\n  Kapur, Iryna Gurevych, Mrinmaya Sachan","title":"MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties\n  Grounded in Math Reasoning Problems","comments":"Jakub Macina, Nico Daheim, and Sankalan Pal Chowdhury contributed\n  equally to this work. Code and dataset available:\n  https://github.com/eth-nlped/mathdial","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Although automatic dialogue tutors hold great potential in making education\npersonalized and more accessible, research on such systems has been hampered by\na lack of sufficiently large and high-quality datasets. However, collecting\nsuch datasets remains challenging, as recording tutoring sessions raises\nprivacy concerns and crowdsourcing leads to insufficient data quality. To\naddress this problem, we propose a framework to semi-synthetically generate\nsuch dialogues by pairing real teachers with a large language model (LLM)\nscaffolded to represent common student errors. In this paper, we describe our\nongoing efforts to use this framework to collect MathDial, a dataset of\ncurrently ca. 1.5k tutoring dialogues grounded in multi-step math word\nproblems. We show that our dataset exhibits rich pedagogical properties,\nfocusing on guiding students using sense-making questions to let them explore\nproblems. Moreover, we outline that MathDial and its grounding annotations can\nbe used to finetune language models to be more effective tutors (and not just\nsolvers) and highlight remaining challenges that need to be addressed by the\nresearch community. We will release our dataset publicly to foster research in\nthis socially important area of NLP.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:44:56 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14537","submitter":"Christian Ikeokwu","authors":"Christian Borgs, Jennifer Chayes, Christian Ikeokwu, Ellen Vitercik","title":"Disincentivizing Polarization in Social Networks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  On social networks, algorithmic personalization drives users into filter\nbubbles where they rarely see content that deviates from their interests. We\npresent a model for content curation and personalization that avoids filter\nbubbles, along with algorithmic guarantees and nearly matching lower bounds. In\nour model, the platform interacts with $n$ users over $T$ timesteps, choosing\ncontent for each user from $k$ categories. The platform receives stochastic\nrewards as in a multi-arm bandit. To avoid filter bubbles, we draw on the\nintuition that if some users are shown some category of content, then all users\nshould see at least a small amount of that content. We first analyze a naive\nformalization of this intuition and show it has unintended consequences: it\nleads to ``tyranny of the majority'' with the burden of diversification borne\ndisproportionately by those with minority interests. This leads us to our model\nwhich distributes this burden more equitably. We require that the probability\nany user is shown a particular type of content is at least $\\gamma$ times the\naverage probability all users are shown that type of content. Full\npersonalization corresponds to $\\gamma = 0$ and complete homogenization\ncorresponds to $\\gamma = 1$; hence, $\\gamma$ encodes a hard cap on the level of\npersonalization. We also analyze additional formulations where the platform can\nexceed its cap but pays a penalty proportional to its constraint violation. We\nprovide algorithmic guarantees for optimizing recommendations subject to these\nconstraints. These include nearly matching upper and lower bounds for the\nentire range of $\\gamma \\in [0,1]$ showing that the reward of a multi-agent\nvariant of UCB is nearly optimal. Using real-world preference data, we\nempirically verify that under our model, users share the burden of\ndiversification with only minor utility loss under our constraints.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:47:31 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14538","submitter":"Fr\\'ed\\'eric Odermatt","authors":"Fr\\'ed\\'eric Odermatt and B\\'eni Egressy and Roger Wattenhofer","title":"Cascaded Beam Search: Plug-and-Play Terminology-Forcing For Neural\n  Machine Translation","comments":"14 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  This paper presents a plug-and-play approach for translation with terminology\nconstraints. Terminology constraints are an important aspect of many modern\ntranslation pipelines. In both specialized domains and newly emerging domains\n(such as the COVID-19 pandemic), accurate translation of technical terms is\ncrucial. Recent approaches often train models to copy terminologies from the\ninput into the output sentence by feeding the target terminology along with the\ninput. But this requires expensive training whenever the underlying language\nmodel is changed or the system should specialize to a new domain. We propose\nCascade Beam Search, a plug-and-play terminology-forcing approach that requires\nno training. Cascade Beam Search has two parts: 1) logit manipulation to\nincrease the probability of target terminologies and 2) a cascading beam setup\nbased on grid beam search, where beams are grouped by the number of\nterminologies they contain. We evaluate the performance of our approach by\ncompeting against the top submissions of the WMT21 terminology translation\ntask. Our plug-and-play approach performs on par with the winning submissions\nwithout using a domain-specific language model and with no additional training.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:48:02 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14539","submitter":"Rebecca Martin","authors":"Rebecca G. Martin","title":"Superorbital periods of Be/X-ray binaries driven by stellar spin\n  precession","comments":"Accepted for publication in MNRAS Letters","journal-ref":null,"doi":"10.1093/mnrasl/slad061","report-no":null,"categories":"astro-ph.HE","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Superorbital periods are observed in the optical light curves of many\nBe/X-ray binaries yet their origin has remained somewhat elusive. We suggest\nthat precession of the spin axis of the Be star can drive superorbital periods,\nparticularly for short orbital period binaries. We consider the short orbital\nperiod ($P_{\\rm orb}=16.6\\,\\rm day$) and highly eccentric ($e_{\\rm b}=0.72$)\nBe/X-ray binary A0538-66 that has a superorbital period of $421\\,\\rm day$.\nFirst we show that the spin axis precession timescale is about twice the\nobserved superorbital period. Then, with hydrodynamic simulations we show that\nthe Be star decretion disc can remain locked to the equator of the precessing\nBe star. At each periastron passage of the neutron star, material is accreted\ninto a disc around the neutron star. The neutron star disc nodally precesses on\nthe same timescale as the Be star disc and therefore both discs can contribute\nto the observed superorbital period. For wider and less eccentric binary\nsystems, the Be star disc can have a larger radial extent and more complex\nbehaviour is expected as a result of disc warping and breaking.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:49:17 GMT"}],"update_date":"2023-06-07"}
{"id":"2305.14540","submitter":"Philippe Laban","authors":"Philippe Laban, Wojciech Kry\\'sci\\'nski, Divyansh Agarwal, Alexander\n  R. Fabbri, Caiming Xiong, Shafiq Joty, Chien-Sheng Wu","title":"LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  With the recent appearance of LLMs in practical settings, having methods that\ncan effectively detect factual inconsistencies is crucial to reduce the\npropagation of misinformation and improve trust in model outputs. When testing\non existing factual consistency benchmarks, we find that a few large language\nmodels (LLMs) perform competitively on classification benchmarks for factual\ninconsistency detection compared to traditional non-LLM methods. However, a\ncloser analysis reveals that most LLMs fail on more complex formulations of the\ntask and exposes issues with existing evaluation benchmarks, affecting\nevaluation precision. To address this, we propose a new protocol for\ninconsistency detection benchmark creation and implement it in a 10-domain\nbenchmark called SummEdits. This new benchmark is 20 times more cost-effective\nper sample than previous benchmarks and highly reproducible, as we estimate\ninter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with\nperformance close to random chance. The best-performing model, GPT-4, is still\n8\\% below estimated human performance, highlighting the gaps in LLMs' ability\nto reason about facts and detect inconsistencies when they occur.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:50:06 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14541","submitter":"Eric Ruzomberka","authors":"Eric Ruzomberka and Yongkyu Jang and David J. Love and H. Vincent Poor","title":"Adversarial Channels with O(1)-Bit Partial Feedback","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT math.IT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We consider point-to-point communication over $q$-ary adversarial channels\nwith partial noiseless feedback. In this setting, a sender Alice transmits $n$\nsymbols from a $q$-ary alphabet over a noisy forward channel to a receiver Bob,\nwhile Bob sends feedback to Alice over a noiseless reverse channel. In the\nforward channel, an adversary can inject both symbol errors and erasures up to\nan error fraction $p \\in [0,1]$ and erasure fraction $r \\in [0,1]$,\nrespectively. In the reverse channel, Bob's feedback is partial such that he\ncan send at most $B(n) \\geq 0$ bits during the communication session.\n  As a case study on minimal partial feedback, we initiate the study of the\n$O(1)$-bit feedback setting in which $B$ is $O(1)$ in $n$. As our main result,\nwe provide a tight characterization of zero-error capacity under $O(1)$-bit\nfeedback for all $q \\geq 2$, $p \\in [0,1]$ and $r \\in [0,1]$, which we prove\nthis result via novel achievability and converse schemes inspired by recent\nstudies of causal adversarial channels without feedback. Perhaps surprisingly,\nwe show that $O(1)$-bits of feedback are sufficient to achieve the zero-error\ncapacity of the $q$-ary adversarial error channel with full feedback when the\nerror fraction $p$ is sufficiently small.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:51:38 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14542","submitter":"Agustina Czenky","authors":"Agustina Czenky, William Gvozdjak, Julia Plavnik","title":"Classification of low-rank odd-dimensional modular categories","comments":"Comments welcome!","journal-ref":null,"doi":null,"report-no":"Hamburger Beitr. zur Mathematik Nr 942; ZMP-HH/23-8","categories":"math.QA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We prove that any odd-dimensional modular tensor category of rank at most 23\nis pointed. We also show that an odd-dimensional modular tensor category of\nrank 25 is either pointed, perfect, or equivalent to\n$\\text{Rep}(D^\\omega(\\mathbb Z_7\\rtimes \\mathbb Z_3))$. In addition, we prove\nthat odd-dimensional modular tensor categories of rank between $27$ and $49$\nnot congruent to $1$ modulo $8$ are either pointed or perfect. Finally, we give\npartial classification results for ranks 33, 41 and 49.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:57:10 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14543","submitter":"Yirui Liu","authors":"Yirui Liu, Xinghao Qiao, Yulong Pei, Liying Wang","title":"DF2M: An Explainable Deep Bayesian Nonparametric Model for\n  High-Dimensional Functional Time Series","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  In this paper, we present Deep Functional Factor Model (DF2M), a Bayesian\nnonparametric model for analyzing high-dimensional functional time series. The\nDF2M makes use of the Indian Buffet Process and the multi-task Gaussian Process\nwith a deep kernel function to capture non-Markovian and nonlinear temporal\ndynamics. Unlike many black-box deep learning models, the DF2M provides an\nexplainable way to use neural networks by constructing a factor model and\nincorporating deep neural networks within the kernel function. Additionally, we\ndevelop a computationally efficient variational inference algorithm for\ninferring the DF2M. Empirical results from four real-world datasets demonstrate\nthat the DF2M offers better explainability and superior predictive accuracy\ncompared to conventional deep learning models for high-dimensional functional\ntime series.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:59:13 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14544","submitter":"Shengwen Gan","authors":"Shengwen Gan","title":"Hausdorff dimension of unions of $k$-planes","comments":"14 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.CA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We prove a conjecture of H\\'era on the dimension of unions of $k$-planes. Let\n$0<k \\le d<n$ be integers, and $\\beta\\in[0,k+1)$. If $\\mathcal{V}\\subset\nA(k,n)$, with $\\text{dim}(\\mathcal{V})=(k+1)(d-k)+\\beta$, then\n$\\text{dim}(\\bigcup_{V\\in\\mathcal{V}}V)\\ge d+\\min\\{1,\\beta\\}$. The proof\ncombines a recent idea of Zahl and the Brascamp-Lieb inequality.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:00:16 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14545","submitter":"Nicol\\'as Matte Bon","authors":"Nicol\\'as Matte Bon, Volodymyr Nekrashevych and Tianyi Zheng","title":"Liouville property for groups and conformal dimension","comments":"36 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.GR math.DS math.PR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We study the (Alfhors-regular) conformal dimension of the limit space of a\ncontracting self-similar group and its connection to random walk entropy on the\ngroup. We show that if $G$ is a finitely generated contracting group, and if\nthe conformal dimension of its limit space is strictly less than 2, then every\nsymmetric random walk with finite second moment on $G$ has the Liouville\nproperty. As a corollary, every such group is amenable. This criterion applies\nto all previously known examples of amenable contracting groups, and to many\nnew ones. In particular, it implies that for every post-critically finite\ncomplex rational function $f$ whose Julia set is not the whole sphere, the\niterated monodromy group of $f$ is amenable.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:01:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14546","submitter":"Vamsikrishna Chemudupati","authors":"Vamsikrishna Chemudupati, Marzieh Tahaei, Heitor Guimaraes, Arthur\n  Pimentel, Anderson Avila, Mehdi Rezagholizadeh, Boxing Chen, Tiago Falk","title":"On the Transferability of Whisper-based Representations for\n  \"In-the-Wild\" Cross-Task Downstream Speech Applications","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.AS cs.CL cs.LG cs.SD","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Large self-supervised pre-trained speech models have achieved remarkable\nsuccess across various speech-processing tasks. The self-supervised training of\nthese models leads to universal speech representations that can be used for\ndifferent downstream tasks, ranging from automatic speech recognition (ASR) to\nspeaker identification. Recently, Whisper, a transformer-based model was\nproposed and trained on large amount of weakly supervised data for ASR; it\noutperformed several state-of-the-art self-supervised models. Given the\nsuperiority of Whisper for ASR, in this paper we explore the transferability of\nthe representation for four other speech tasks in SUPERB benchmark. Moreover,\nwe explore the robustness of Whisper representation for ``in the wild'' tasks\nwhere speech is corrupted by environment noise and room reverberation.\nExperimental results show Whisper achieves promising results across tasks and\nenvironmental conditions, thus showing potential for cross-task real-world\ndeployment.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:02:55 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14547","submitter":"Yuting Wu","authors":"Yuting Wu, Qiwen Wang, Ziyu Wang, Xinxin Wang, Buvna Ayyagari,\n  Siddarth Krishnan, Michael Chudzik and Wei D. Lu","title":"Bulk-Switching Memristor-based Compute-In-Memory Module for Deep Neural\n  Network Training","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AR cs.ET cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The need for deep neural network (DNN) models with higher performance and\nbetter functionality leads to the proliferation of very large models. Model\ntraining, however, requires intensive computation time and energy.\nMemristor-based compute-in-memory (CIM) modules can perform vector-matrix\nmultiplication (VMM) in situ and in parallel, and have shown great promises in\nDNN inference applications. However, CIM-based model training faces challenges\ndue to non-linear weight updates, device variations, and low-precision in\nanalog computing circuits. In this work, we experimentally implement a\nmixed-precision training scheme to mitigate these effects using a\nbulk-switching memristor CIM module. Lowprecision CIM modules are used to\naccelerate the expensive VMM operations, with high precision weight updates\naccumulated in digital units. Memristor devices are only changed when the\naccumulated weight update value exceeds a pre-defined threshold. The proposed\nscheme is implemented with a system-on-chip (SoC) of fully integrated analog\nCIM modules and digital sub-systems, showing fast convergence of LeNet training\nto 97.73%. The efficacy of training larger models is evaluated using realistic\nhardware parameters and shows that that analog CIM modules can enable efficient\nmix-precision DNN training with accuracy comparable to full-precision software\ntrained models. Additionally, models trained on chip are inherently robust to\nhardware variations, allowing direct mapping to CIM inference chips without\nadditional re-training.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:03:08 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14548","submitter":"Qi Zeng","authors":"Hou Pong Chan, Qi Zeng, Heng Ji","title":"Interpretable Automatic Fine-grained Inconsistency Detection in Text\n  Summarization","comments":"Accepted by ACL Findings 2023. Code and data are available at\n  https://github.com/kenchan0226/fineGrainedFact","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Existing factual consistency evaluation approaches for text summarization\nprovide binary predictions and limited insights into the weakness of\nsummarization systems. Therefore, we propose the task of fine-grained\ninconsistency detection, the goal of which is to predict the fine-grained types\nof factual errors in a summary. Motivated by how humans inspect factual\ninconsistency in summaries, we propose an interpretable fine-grained\ninconsistency detection model, FineGrainFact, which explicitly represents the\nfacts in the documents and summaries with semantic frames extracted by semantic\nrole labeling, and highlights the related semantic frames to predict\ninconsistency. The highlighted semantic frames help verify predicted error\ntypes and correct inconsistent summaries. Experiment results demonstrate that\nour model outperforms strong baselines and provides evidence to support or\nrefute the summary.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:11:47 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14549","submitter":"Yinghao Li","authors":"Yinghao Li, Colin Lockard, Prashant Shiralkar, Chao Zhang","title":"Extracting Shopping Interest-Related Product Types from the Web","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Recommending a diversity of product types (PTs) is important for a good\nshopping experience when customers are looking for products around their\nhigh-level shopping interests (SIs) such as hiking. However, the SI-PT\nconnection is typically absent in e-commerce product catalogs and expensive to\nconstruct manually due to the volume of potential SIs, which prevents us from\nestablishing a recommender with easily accessible knowledge systems. To\nestablish such connections, we propose to extract PTs from the Web pages\ncontaining hand-crafted PT recommendations for SIs. The extraction task is\nformulated as binary HTML node classification given the general observation\nthat an HTML node in our target Web pages can present one and only one PT\nphrase. Accordingly, we introduce TrENC, which stands for Tree-Transformer\nEncoders for Node Classification. It improves the inter-node dependency\nmodeling with modified attention mechanisms that preserve the long-term sibling\nand ancestor-descendant relations. TrENC also injects SI into node features for\nbetter semantic representation. Trained on pages regarding limited SIs, TrEnc\nis ready to be applied to other unobserved interests. Experiments on our\nmanually constructed dataset, WebPT, show that TrENC outperforms the best\nbaseline model by 2.37 F1 points in the zero-shot setup. The performance\nindicates the feasibility of constructing SI-PT relations and using them to\npower downstream applications such as search and recommendation.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:18:01 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14550","submitter":"Prajjwal Bhargava","authors":"Prajjwal Bhargava, Rohan Chitnis, Alborz Geramifard, Shagun Sodhani,\n  Amy Zhang","title":"Sequence Modeling is a Robust Contender for Offline Reinforcement\n  Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Offline reinforcement learning (RL) allows agents to learn effective,\nreturn-maximizing policies from a static dataset. Three major paradigms for\noffline RL are Q-Learning, Imitation Learning, and Sequence Modeling. A key\nopen question is: which paradigm is preferred under what conditions? We study\nthis question empirically by exploring the performance of representative\nalgorithms -- Conservative Q-Learning (CQL), Behavior Cloning (BC), and\nDecision Transformer (DT) -- across the commonly used D4RL and Robomimic\nbenchmarks. We design targeted experiments to understand their behavior\nconcerning data suboptimality and task complexity. Our key findings are: (1)\nSequence Modeling requires more data than Q-Learning to learn competitive\npolicies but is more robust; (2) Sequence Modeling is a substantially better\nchoice than both Q-Learning and Imitation Learning in sparse-reward and\nlow-quality data settings; and (3) Sequence Modeling and Imitation Learning are\npreferable as task horizon increases, or when data is obtained from human\ndemonstrators. Based on the overall strength of Sequence Modeling, we also\ninvestigate architectural choices and scaling trends for DT on Atari and D4RL\nand make design recommendations. We find that scaling the amount of data for DT\nby 5x gives a 2.5x average score improvement on Atari.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:19:14 GMT"},{"version":"v2","created":"Fri, 26 May 2023 17:48:31 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.14551","submitter":"Andrey Palaev","authors":"Andrey Palaev and Rustam A. Lukmanov and Adil Khan","title":"Exploring Semantic Variations in GAN Latent Spaces via Matrix\n  Factorization","comments":"Accepted at ICLR 2023 Tiny Papers","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  Controlled data generation with GANs is desirable but challenging due to the\nnonlinearity and high dimensionality of their latent spaces. In this work, we\nexplore image manipulations learned by GANSpace, a state-of-the-art method\nbased on PCA. Through quantitative and qualitative assessments we show: (a)\nGANSpace produces a wide range of high-quality image manipulations, but they\ncan be highly entangled, limiting potential use cases; (b) Replacing PCA with\nICA improves the quality and disentanglement of manipulations; (c) The quality\nof the generated images can be sensitive to the size of GANs, but regardless of\ntheir complexity, fundamental controlling directions can be observed in their\nlatent spaces.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:23:37 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14552","submitter":"Tianyi Li","authors":"Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark\n  Johnson, Mark Steedman","title":"Sources of Hallucination by Large Language Models on Inference Tasks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large Language Models (LLMs) are claimed to be capable of Natural Language\nInference (NLI), necessary for applied tasks like question answering and\nsummarization, yet this capability is under-explored. We present a series of\nbehavioral studies on several LLM families (LLaMA, GPT-3.5, and PaLM) which\nprobe their behavior using controlled experiments. We establish two factors\nwhich predict much of their performance, and propose that these are major\nsources of hallucination in generative LLM. First, the most influential factor\nis memorization of the training data. We show that models falsely label NLI\ntest samples as entailing when the hypothesis is attested in the training text,\nregardless of the premise. We further show that named entity IDs are used as\n\"indices\" to access the memorized data. Second, we show that LLMs exploit a\nfurther corpus-based heuristic using the relative frequencies of words. We show\nthat LLMs score significantly worse on NLI test samples which do not conform to\nthese factors than those which do; we also discuss a tension between the two\nfactors, and a performance trade-off.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:24:44 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14553","submitter":"Andrew Lohn","authors":"Micah Musser, Andrew Lohn, James X. Dempsey, Jonathan Spring, Ram\n  Shankar Siva Kumar, Brenda Leong, Christina Liaghati, Cindy Martinez, Crystal\n  D. Grant, Daniel Rohrer, Heather Frase, Jonathan Elliott, John Bansemer,\n  Mikel Rodriguez, Mitt Regan, Rumman Chowdhury, Stefan Hermanek","title":"Adversarial Machine Learning and Cybersecurity: Risks, Challenges, and\n  Legal Implications","comments":null,"journal-ref":null,"doi":"10.51593/2022CA003","report-no":null,"categories":"cs.CR cs.AI cs.CY","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In July 2022, the Center for Security and Emerging Technology (CSET) at\nGeorgetown University and the Program on Geopolitics, Technology, and\nGovernance at the Stanford Cyber Policy Center convened a workshop of experts\nto examine the relationship between vulnerabilities in artificial intelligence\nsystems and more traditional types of software vulnerabilities. Topics\ndiscussed included the extent to which AI vulnerabilities can be handled under\nstandard cybersecurity processes, the barriers currently preventing the\naccurate sharing of information about AI vulnerabilities, legal issues\nassociated with adversarial attacks on AI systems, and potential areas where\ngovernment support could improve AI vulnerability management and mitigation.\n  This report is meant to accomplish two things. First, it provides a\nhigh-level discussion of AI vulnerabilities, including the ways in which they\nare disanalogous to other types of vulnerabilities, and the current state of\naffairs regarding information sharing and legal oversight of AI\nvulnerabilities. Second, it attempts to articulate broad recommendations as\nendorsed by the majority of participants at the workshop.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:27:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14554","submitter":"Anand Puthirath Balan","authors":"Lucas M. Sassi, Sathvik Ajay Iyengar, Anand B. Puthirath, Yuefei\n  Huang, Xingfu Li, Tanguy Terlier, Ali Mojibpour, Ana Paula C. Teixeira,\n  Palash Bharadwaj, Chandra Sekhar Tiwary, Robert Vajtai, Saikat Talapatra,\n  Boris Yakobson and Pulickel M. Ajayan","title":"Bottom-up Integration of TMDCs with Pre-Patterned Device Architectures\n  via Transfer-free Chemical Vapor Deposition","comments":"32 pages, 5 Figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci physics.app-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Two-dimensional (2D) transition metal dichalcogenides (TMDCs) remain a topic\nof immense interest. Specifically, given their low operational switching costs,\nthey find many niche applications in new computing architectures with the\npromise of continued miniaturization. However, challenges lie in Back End of\nLine (BEOL) integration temperature and time compliance regarding current\nrequirements for crystal growth. Additionally, deleterious and time-consuming\ntransfer processes and multiple steps involved in channel/contact engineering\ncan cripple device performance. This work demonstrates kinetics-governed\nin-situ growth regimes (surface or edge growth from gold) of WSe2 and provides\na mechanistic understanding of these regimes via energetics across various\nmaterial interfaces. As a proof-of-concept, field effect transistors (FET) with\nan in-situ grown WSe2 channel across Au contacts are fabricated, demonstrating\na 2D semiconductor transistor via a transfer-free method within the 450-600 C\n2h-time window requirement BEOL integration. We leverage directional edge\ngrowth to fabricate contacts with robust thickness-dependent Schottky-to-Ohmic\nbehavior. By transitioning between Au and SiO2 growth substrates in situ, this\nwork achieves strain-induced subthreshold swing of 140 mV/decade, relatively\nhigh mobility of 107 +- 19 cm2V-1s-1, and robust ON/OFF ratios 10^6 in the\nfabricated FETs.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:28:43 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14555","submitter":"Zhijing Jin","authors":"Yuxin Ren, Qipeng Guo, Zhijing Jin, Shauli Ravfogel, Mrinmaya Sachan,\n  Bernhard Sch\\\"olkopf, Ryan Cotterell","title":"All Roads Lead to Rome? Exploring the Invariance of Transformers'\n  Representations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  Transformer models bring propelling advances in various NLP tasks, thus\ninducing lots of interpretability research on the learned representations of\nthe models. However, we raise a fundamental question regarding the reliability\nof the representations. Specifically, we investigate whether transformers learn\nessentially isomorphic representation spaces, or those that are sensitive to\nthe random seeds in their pretraining process. In this work, we formulate the\nBijection Hypothesis, which suggests the use of bijective methods to align\ndifferent models' representation spaces. We propose a model based on invertible\nneural networks, BERT-INN, to learn the bijection more effectively than other\nexisting bijective methods such as the canonical correlation analysis (CCA). We\nshow the advantage of BERT-INN both theoretically and through extensive\nexperiments, and apply it to align the reproduced BERT embeddings to draw\ninsights that are meaningful to the interpretability research. Our code is at\nhttps://github.com/twinkle0331/BERT-similarity.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:30:43 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14556","submitter":"Tiziano Labruna","authors":"Tiziano Labruna, Sofia Brenna, Andrea Zaninello, Bernardo Magnini","title":"Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented\n  Dialogues and Annotations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Large pre-trained language models have exhibited unprecedented capabilities\nin producing high-quality text via prompting techniques. This fact introduces\nnew possibilities for data collection and annotation, particularly in\nsituations where such data is scarce, complex to gather, expensive, or even\nsensitive. In this paper, we explore the potential of these models to generate\nand annotate goal-oriented dialogues, and conduct an in-depth analysis to\nevaluate their quality. Our experiments employ ChatGPT, and encompass three\ncategories of goal-oriented dialogues (task-oriented, collaborative, and\nexplanatory), two generation modes (interactive and one-shot), and two\nlanguages (English and Italian). Based on extensive human-based evaluations, we\ndemonstrate that the quality of generated dialogues and annotations is on par\nwith those generated by humans.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:31:01 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14557","submitter":"Ryan Lau","authors":"Ryan M. Lau, Jason Wang, Matthew J. Hankins, Thayne Currie, Vincent\n  Deo, Izumi Endo, Olivier Guyon, Yinuo Han, Anthony P. Jones, Nemanja\n  Jovanovic, Julien Lozi, Anthony F. J. Moffat, Takashi Onaka, Garreth Ruane,\n  Andreas A. C. Sander, Samaporn Tinyanont, Peter G. Tuthill, Gerd Weigelt,\n  Peredur M. Williams, and Sebastien Vievard","title":"From Dust to Nanodust: Resolving Circumstellar Dust from the\n  Colliding-Wind Binary Wolf-Rayet (WR) 140","comments":"21 pages, 8 figures, Accepted for publication in ApJ","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.SR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Wolf-Rayet (WR) 140 is the archetypal periodic dust-forming colliding-wind\nbinary that hosts a carbon-rich WR (WC) star and an O-star companion with an\norbital period of 7.93 years and an orbital eccentricity of 0.9. Throughout the\npast several decades, multiple dust-formation episodes from WR 140 have been\nobserved that are linked to the binary orbit and occur near the time of\nperiastron passage. Given its predictable dust-formation episodes, WR 140\npresents an ideal astrophysical laboratory for investigating the formation and\nevolution of dust in the hostile environment around a massive binary system. In\nthis paper, we present near- and mid-infrared (IR) spectroscopic and imaging\nobservations of WR 140 with Subaru/SCExAO+CHARIS, Keck/NIRC2+PyWFS, and\nSubaru/COMICS taken between 2020 June and Sept that resolve the circumstellar\ndust emission linked to its most recent dust-formation episode in 2016 Dec. Our\nspectral energy distribution (SED) analysis of WR 140's resolved circumstellar\ndust emission reveals the presence of a hot ($T_\\mathrm{d}\\sim1000$ K) near-IR\ndust component that is co-spatial with the previously known and cooler\n($T_\\mathrm{d}\\sim500$ K) mid-IR dust component composed of $300-500$\n{\\AA}-sized dust grains. We attribute the hot near-IR dust emission to the\npresence of nano-sized (\"nanodust\") grains and suggest they were formed from\ngrain-grain collisions or the rotational disruption of the larger grain size\npopulation by radiative torques in the strong radiation field from the central\nbinary. Lastly, we speculate on the astrophysical implications of nanodust\nformation around colliding-wind WC binaries, which may present an early source\nof carbonaceous nanodust in the interstellar medium.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:32:32 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14558","submitter":"Vidushi Adlakha","authors":"Vidushi Adlakha and Eric Kuo","title":"Statistical causal inference methods for observational research in PER:\n  a primer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME physics.ed-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Recent critiques of Physics Education Research (PER) studies have revoiced\nthe critical issues when drawing causal inferences from observational data\nwhere no intervention is present. In response to a call for a \"causal reasoning\nprimer\", this paper discusses some of the fundamental issues underlying\nstatistical causal inference. In reviewing these issues, we discuss\nwell-established causal inference methods commonly applied in other fields and\ndiscuss their application to PER. Using simulated data sets, we illustrate (i)\nwhy analysis for causal inference should control for confounders but not\ncontrol for mediators and colliders and (ii) that multiple proposed causal\nmodels can fit a highly correlated data set. Finally, we discuss how these\ncausal inference methods can be used to represent and explain existing issues\nin quantitative PER. Throughout, we discuss a central issue: quantitative\nresults from observational studies cannot support a researcher's proposed\ncausal model over other alternative models. To address this issue, we propose\nan explicit role for observational studies in PER that draw statistical causal\ninferences: proposing future intervention studies and predicting their\noutcomes. Mirroring a broader connection between theoretical motivating\nexperiments in physics, observational studies in PER can make quantitative\npredictions of the causal effects of interventions, and future intervention\nstudies can test those predictions directly.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:33:44 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14559","submitter":"Klaudiusz Czudek","authors":"Klaudiusz Czudek","title":"Mixing of generic simple symmetric random walk on the circle","comments":"19 pages, no figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR math.DS","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Fix an irrational number $\\alpha$ and a smooth, positive function\n$\\mathfrak{p}$ on the circle. If a particle is placed at a point $x\\in \\mathbb\nR/\\mathbb Z$, then in the next step it jumps to $x+\\alpha$ with probability\n$\\mathfrak{p}(x)$ and to $x-\\alpha$ with probability $1-\\mathfrak{p}(x)$. Sinai\nproved that if $\\mathfrak{p}$ is asymmetric (in certain sense) or\n$\\mathfrak{p}$ is symmetric and $\\alpha$ is Diophantine then this random walk\nis mixing. Here we show it is mixing for every irrational frequency and generic\nsymmetric absolutely continuous $\\mathfrak{p}$. This can be rephrased as mixing\nof environment viewed from a particle. This partially answers a question posed\nby Dolgopyat, Fayad and Saprykina in 2019.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:43:19 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14560","submitter":"Margarite LaBorde","authors":"Margarite L. LaBorde","title":"A Menagerie of Symmetry Testing Quantum Algorithms","comments":"PhD Dissertation, Louisiana State University (2023). 188 pages, 27\n  figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph math-ph math.MP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This thesis aims to establish notions of symmetry for quantum states and\nchannels as well as describe algorithms to test for these properties on quantum\ncomputers. Ideally, the work will serve as a self-contained overview of the\nsubject. We begin by establishing the necessary mathematical background. We\nshow how to generate a notion of symmetry from a discrete, finite group and how\nthis generalizes to a continuous group. We then use these notions to\ninvestigate Hamiltonian symmetries. We propose quantum algorithms capable of\ntesting whether a Hamiltonian exhibits symmetry with respect to a group and\nshow that this algorithm is DQC1-Complete. We next discuss tests of symmetry\nfor quantum states. We prove that the acceptance probability of each algorithm\nis equal to the maximum symmetric fidelity of the state being tested and\nestablish various generalizations of the resource theory of asymmetry. In the\nnext chapter, we show that the analytical form of the acceptance probability of\nsuch a test is given by the cycle index polynomial of the symmetric group\n$S_k$. We derive a family of quantum separability tests, each of which is\ngenerated by a finite group; for all such algorithms, we show that the\nacceptance probability is determined by the cycle index polynomial of the\ngroup. Finally, we produce and analyze explicit circuit constructions for these\ntests, showing that the tests corresponding to the symmetric and cyclic groups\ncan be executed with $O(k^2)$ and $O(k\\log(k))$ controlled-SWAP gates,\nrespectively, where $k$ is the number of copies of the state. Finally, we\ninclude additional results not previously published; specifically, we give a\ntest for symmetry of a quantum state using density matrix exponentiation, a\nfurther result of Hamiltonian symmetry measurements when using Abelian groups,\nand an alternate Hamiltonian symmetry test construction for a block-encoded\nHamiltonian.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:55:02 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14561","submitter":"Yifan Qin","authors":"Yifan Qin, Zheyu Yan, Wujie Wen, Xiaobo Sharon Hu and Yiyu Shi","title":"Negative Feedback Training: A Novel Concept to Improve Robustness of\n  NVCiM DNN Accelerators","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.AR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Compute-in-Memory (CiM) utilizing non-volatile memory (NVM) devices presents\na highly promising and efficient approach for accelerating deep neural networks\n(DNNs). By concurrently storing network weights and performing matrix\noperations within the same crossbar structure, CiM accelerators offer DNN\ninference acceleration with minimal area requirements and exceptional energy\nefficiency. However, the stochasticity and intrinsic variations of NVM devices\noften lead to performance degradation, such as reduced classification accuracy,\ncompared to expected outcomes. Although several methods have been proposed to\nmitigate device variation and enhance robustness, most of them rely on overall\nmodulation and lack constraints on the training process. Drawing inspiration\nfrom the negative feedback mechanism, we introduce a novel training approach\nthat uses a multi-exit mechanism as negative feedback to enhance the\nperformance of DNN models in the presence of device variation. Our negative\nfeedback training method surpasses state-of-the-art techniques by achieving an\nimpressive improvement of up to 12.49% in addressing DNN robustness against\ndevice variation.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:56:26 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14562","submitter":"Yi Hu","authors":"Yi Hu, Chaoran Zhang, Edward Andert, Harshul Singh, Aviral\n  Shrivastava, James Laudon, Yanqi Zhou, Bob Iannucci, Carlee Joe-Wong","title":"GiPH: Generalizable Placement Learning for Adaptive Heterogeneous\n  Computing","comments":"to be published in Proceedings of Machine Learning and Systems 5\n  (MLSys 2023)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.SY eess.SY","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Careful placement of a computational application within a target device\ncluster is critical for achieving low application completion time. The problem\nis challenging due to its NP-hardness and combinatorial nature. In recent\nyears, learning-based approaches have been proposed to learn a placement policy\nthat can be applied to unseen applications, motivated by the problem of placing\na neural network across cloud servers. These approaches, however, generally\nassume the device cluster is fixed, which is not the case in mobile or edge\ncomputing settings, where heterogeneous devices move in and out of range for a\nparticular application. We propose a new learning approach called GiPH, which\nlearns policies that generalize to dynamic device clusters via 1) a novel graph\nrepresentation gpNet that efficiently encodes the information needed for\nchoosing a good placement, and 2) a scalable graph neural network (GNN) that\nlearns a summary of the gpNet information. GiPH turns the placement problem\ninto that of finding a sequence of placement improvements, learning a policy\nfor selecting this sequence that scales to problems of arbitrary size. We\nevaluate GiPH with a wide range of task graphs and device clusters and show\nthat our learned policy rapidly find good placements for new problem instances.\nGiPH finds placements with up to 30.5% lower completion times, searching up to\n3X faster than other search-based placement policies.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:02:21 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14563","submitter":"Aida Wofford","authors":"Aida Wofford, Andr\\'es Sixtos, Stephane Charlot, Gustavo Bruzual,\n  Fergus Cullen, Thomas M. Stanton, Svea Hern\\'andez, Linda J. Smith, Matthew\n  Hayes","title":"Extreme broad He\\2 emission at high and low redshifts: the dominant role\n  of VMS in NGC 3125-A1 and CDFS131717","comments":"17 pages, 17 figures, accepted in MNRAS","journal-ref":null,"doi":"10.1093/mnras/stad1622","report-no":null,"categories":"astro-ph.GA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Super star cluster (SSC) A1 (3.1E5 Msun) in NGC 3125 has one of the strongest\n(EW = 4.6 +/- 0.5 Ang) broad (FWHM = 1131 +\\- 40 km/s) He II 1640 emission\nlines in the nearby Universe and constitutes an important template for\ninterpreting observations of extreme He II emitters out to redshifts of z =\n2-3. We use Cosmic Origins Spectrograph (COS) observations of A1 to show that\nthere is no significant contamination of the He II line with nebular emission\nand that the line is redshifted by 121 +/-17 km/s relative to ISM lines. We\ncompare the COS G130M + G160M observations of A1 to recent binary BPASS and\nsingle-star Charlot & Bruzual (C&B) simple stellar population (SSP) models with\nVery Massive Stars (VMS) of up to 300 Msun. We suggest why BPASS models fail to\nreproduce A1's He II emission. On the other hand, a C&B model with Z = 0.008,\nage = 2.2 Myr, and VMS approaching the Eddington limit provides an excellent\nfit to the He II emission and fits reasonably well C III 1175, N V 1238,1241,\nand C IV 1548, 1551. We present O V 1371 line-profile predictions showing that\nthis line constitutes an important tracer of youth and VMS in galaxies.\nFinally, we discuss the presence of VMS in CDFS131717, a highly star-forming\nlow-metallicity galaxy located at z = 3.071, which has a tentative detection of\nO V absorption and strong broad He II emission. These features are rare and\nhint to the presence of short-lived VMS in the galaxy. Our results show the\neffect of the latest developments of stellar wind theory and the importance of\naccounting for VMS in models.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:02:32 GMT"}],"update_date":"2023-06-07"}
{"id":"2305.14564","submitter":"Simeng Sun","authors":"Simeng Sun, Yang Liu, Shuohang Wang, Chenguang Zhu, Mohit Iyyer","title":"PEARL: Prompting Large Language Models to Plan and Execute Actions Over\n  Long Documents","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Strategies such as chain-of-thought prompting improve the performance of\nlarge language models (LLMs) on complex reasoning tasks by decomposing input\nexamples into intermediate steps. However, it remains unclear how to apply such\nmethods to reason over long input documents, in which both the decomposition\nand the output of each intermediate step are non-trivial to obtain. In this\nwork, we propose PEARL, a prompting framework to improve reasoning over long\ndocuments, which consists of three stages: action mining, plan formulation, and\nplan execution. More specifically, given a question about a long document,\nPEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE,\nFIND_EVENT, FIND_RELATION) and then executes them over the document to obtain\nthe answer. Each stage of PEARL is implemented via zero-shot or few-shot\nprompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate\nPEARL on a challenging subset of the QuALITY dataset, which contains questions\nthat require complex reasoning over long narrative texts. PEARL outperforms\nzero-shot and chain-of-thought prompting on this dataset, and ablation\nexperiments show that each stage of PEARL is critical to its performance.\nOverall, PEARL is a first step towards leveraging LLMs to reason over long\ndocuments.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:06:04 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14565","submitter":"Justin Forlano","authors":"Andreia Chapouto, Justin Forlano","title":"Invariant measures for the periodic KdV and mKdV equations using\n  complete integrability","comments":"32 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We consider the real-valued defocusing modified Korteweg-de Vries equation\n(mKdV) on the circle. Based on the complete integrability of mKdV,\nKillip-Vi\\c{s}an-Zhang (2018) discovered a conserved quantity which they used\nto prove low regularity a priori bounds for solutions. It has been an open\nquestion if this conserved quantity can be used to define invariant measures\nsupported at fractional Sobolev regularities. Motivated by this question, we\nconstruct probability measures supported on $H^s(\\mathbb{T})$ for $0<s<1/2$\ninvariant under the mKdV flow. We then use the Miura transform to obtain\ninvariant measures for the Korteweg-de Vries equation, whose supports are\nrougher than the white noise measure.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:06:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14566","submitter":"Haoju Leng","authors":"Haoju Leng, Ruining Deng, Zuhayr Asad, R. Michael Womick, Haichun\n  Yang, Lipeng Wan, and Yuankai Huo","title":"An Accelerated Pipeline for Multi-label Renal Pathology Image\n  Segmentation at the Whole Slide Image Level","comments":null,"journal-ref":null,"doi":"10.1117/12.2653651","report-no":null,"categories":"eess.IV cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Deep-learning techniques have been used widely to alleviate the\nlabour-intensive and time-consuming manual annotation required for pixel-level\ntissue characterization. Our previous study introduced an efficient single\ndynamic network - Omni-Seg - that achieved multi-class multi-scale pathological\nsegmentation with less computational complexity. However, the patch-wise\nsegmentation paradigm still applies to Omni-Seg, and the pipeline is\ntime-consuming when providing segmentation for Whole Slide Images (WSIs). In\nthis paper, we propose an enhanced version of the Omni-Seg pipeline in order to\nreduce the repetitive computing processes and utilize a GPU to accelerate the\nmodel's prediction for both better model performance and faster speed. Our\nproposed method's innovative contribution is two-fold: (1) a Docker is released\nfor an end-to-end slide-wise multi-tissue segmentation for WSIs; and (2) the\npipeline is deployed on a GPU to accelerate the prediction, achieving better\nsegmentation quality in less time. The proposed accelerated implementation\nreduced the average processing time (at the testing stage) on a standard needle\nbiopsy WSI from 2.3 hours to 22 minutes, using 35 WSIs from the Kidney Tissue\nAtlas (KPMP) Datasets. The source code and the Docker have been made publicly\navailable at https://github.com/ddrrnn123/Omni-Seg.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:07:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14567","submitter":"Leo Feng","authors":"Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Yoshua Bengio,\n  Mohamed Osama Ahmed","title":"Constant Memory Attentive Neural Processes","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Neural Processes (NPs) are efficient methods for estimating predictive\nuncertainties. NPs comprise of a conditioning phase where a context dataset is\nencoded, a querying phase where the model makes predictions using the context\ndataset encoding, and an updating phase where the model updates its encoding\nwith newly received datapoints. However, state-of-the-art methods require\nadditional memory which scales linearly or quadratically with the size of the\ndataset, limiting their applications, particularly in low-resource settings. In\nthis work, we propose Constant Memory Attentive Neural Processes (CMANPs), an\nNP variant which only requires constant memory for the conditioning, querying,\nand updating phases. In building CMANPs, we propose Constant Memory Attention\nBlock (CMAB), a novel general-purpose attention block that can compute its\noutput in constant memory and perform updates in constant computation.\nEmpirically, we show CMANPs achieve state-of-the-art results on meta-regression\nand image completion tasks while being (1) significantly more memory efficient\nthan prior methods and (2) more scalable to harder settings.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:10:19 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14568","submitter":"Xiaohao Cai","authors":"Jiahui Liu, Xiaohao Cai, and Mahesan Niranjan","title":"GO-LDA: Generalised Optimal Linear Discriminant Analysis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.NA math.NA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Linear discriminant analysis (LDA) has been a useful tool in pattern\nrecognition and data analysis research and practice. While linearity of class\nboundaries cannot always be expected, nonlinear projections through pre-trained\ndeep neural networks have served to map complex data onto feature spaces in\nwhich linear discrimination has served well. The solution to binary LDA is\nobtained by eigenvalue analysis of within-class and between-class scatter\nmatrices. It is well known that the multiclass LDA is solved by an extension to\nthe binary LDA, a generalised eigenvalue problem, from which the largest\nsubspace that can be extracted is of dimension one lower than the number of\nclasses in the given problem. In this paper, we show that, apart from the first\nof the discriminant directions, the generalised eigenanalysis solution to\nmulticlass LDA does neither yield orthogonal discriminant directions nor\nmaximise discrimination of projected data along them. Surprisingly, to the best\nof our knowledge, this has not been noted in decades of literature on LDA. To\novercome this drawback, we present a derivation with a strict theoretical\nsupport for sequentially obtaining discriminant directions that are orthogonal\nto previously computed ones and maximise in each step the Fisher criterion. We\nshow distributions of projections along these axes and demonstrate that\ndiscrimination of data projected onto these discriminant directions has optimal\nseparation, which is much higher than those from the generalised eigenvectors\nof the multiclass LDA. Using a wide range of benchmark tasks, we present a\ncomprehensive empirical demonstration that on a number of pattern recognition\nand classification problems, the optimal discriminant subspaces obtained by the\nproposed method, referred to as GO-LDA (Generalised Optimal LDA), can offer\nsuperior accuracy.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:11:05 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14569","submitter":"Srijan Bansal","authors":"Srijan Bansal, Semih Yavuz, Bo Pang, Meghana Bhat, Yingbo Zhou","title":"Few-shot Unified Question Answering: Tuning Models or Prompts?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Question-answering (QA) tasks often investigate specific question types,\nknowledge domains, or reasoning skills, leading to specialized models catering\nto specific categories of QA tasks. While recent research has explored the idea\nof unified QA models, such models are usually explored for high-resource\nscenarios and require re-training to extend their capabilities. To overcome\nthese drawbacks, the paper explores the potential of two paradigms of tuning,\nmodel, and prompts, for unified QA under a low-resource setting. The paper\nprovides an exhaustive analysis of their applicability using 16 QA datasets,\nrevealing that prompt tuning can perform as well as model tuning in a few-shot\nsetting with a good initialization. The study also shows that parameter-sharing\nresults in superior few-shot performance, simple knowledge transfer techniques\nfor prompt initialization can be effective, and prompt tuning achieves a\nsignificant performance boost from pre-training in a low-resource regime. The\nresearch offers insights into the advantages and limitations of prompt tuning\nfor unified QA in a few-shot setting, contributing to the development of\neffective and efficient systems in low-resource scenarios.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:14:38 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14570","submitter":"Billie Goolsby","authors":"Tony G. Chen, Billie C. Goolsby, Guadalupe Bernal, Lauren A.\n  O'Connell, Mark R. Cutkosky","title":"Feed Me: Robotic Infiltration of Poison Frog Families","comments":"10 pages, 6 figures, to be presented at Living Machines 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  We present the design and operation of tadpole-mimetic robots prepared for a\nstudy of the parenting behaviors of poison frogs, which pair bond and raise\ntheir offspring. The mission of these robots is to convince poison frog parents\nthat they are tadpoles, which need to be fed. Tadpoles indicate this need, at\nleast in part, by wriggling with a characteristic frequency and amplitude.\nWhile the study is in progress, preliminary indications are that the TadBots\nhave passed their test, at least for father frogs. We discuss the design and\noperational requirements for producing convincing TadBots and provide some\ndetails of the study design and plans for future work.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:21:06 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14571","submitter":"Li Sun","authors":"Li Sun, Florian Luisier, Kayhan Batmanghelich, Dinei Florencio, Cha\n  Zhang","title":"From Characters to Words: Hierarchical Pre-trained Language Model for\n  Open-vocabulary Language Understanding","comments":"Accepted to ACL 2023 Main Conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Current state-of-the-art models for natural language understanding require a\npreprocessing step to convert raw text into discrete tokens. This process known\nas tokenization relies on a pre-built vocabulary of words or sub-word\nmorphemes. This fixed vocabulary limits the model's robustness to spelling\nerrors and its capacity to adapt to new domains. In this work, we introduce a\nnovel open-vocabulary language model that adopts a hierarchical two-level\napproach: one at the word level and another at the sequence level. Concretely,\nwe design an intra-word module that uses a shallow Transformer architecture to\nlearn word representations from their characters, and a deep inter-word\nTransformer module that contextualizes each word representation by attending to\nthe entire word sequence. Our model thus directly operates on character\nsequences with explicit awareness of word boundaries, but without biased\nsub-word or word-level vocabulary. Experiments on various downstream tasks show\nthat our method outperforms strong baselines. We also demonstrate that our\nhierarchical model is robust to textual corruption and domain shift.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:22:20 GMT"},{"version":"v2","created":"Tue, 30 May 2023 03:36:13 GMT"}],"update_date":"2023-05-31"}
{"id":"2305.14572","submitter":"Peter Petreczky","authors":"Raktim Abir, Igor Akushevich, Tolga Altinoluk, Daniele Paolo Anderle,\n  Fatma P. Aslan, Alessandro Bacchetta, Baha Balantekin, Joao Barata, Marco\n  Battaglieri, Carlos A. Bertulani, Guillaume Beuf, Chiara Bissolotti, Dani\\\"el\n  Boer, M. Boglione, Radja Boughezal, Eric Braaten, Nora Brambilla, Vladimir\n  Braun, Duane Byer, Francesco Giovanni Celiberto, Yang-Ting Chien, Ian C.\n  Clo\\\"et, Martha Constantinou, Wim Cosyn, Aurore Courtoy, Alexander Czajka,\n  Umberto D'Alesio, Giuseppe Bozzi, Igor Danilkin, Debasish Das, Daniel de\n  Florian, Andrea Delgado, J. P. B. C. de Melo, William Detmold, Michael\n  D\\\"oring, Adrian Dumitru, Miguel G. Echevarria, Robert Edwards, Gernot\n  Eichmann, Bruno El-Bennich, Michael Engelhardt, Cesar Fernandez-Ramirez,\n  Christian Fischer, Geofrey Fox, Adam Freese, Leonard Gamberg, Maria Vittoria\n  Garzelli, Francesco Giacosa, Gustavo Gil da Silveira, Derek Glazier, Victor\n  P. Goncalves, Silas Grossberndt, Feng-Kun Guo, Rajan Gupta, Yoshitaka Hatta,\n  Martin Hentschinski, Astrid Hiller Blin, Radja Boughezal, Timothy Hobbs,\n  Alexander Ilyichev, Jamal Jalilian-Marian, Chueng-Ryong Ji, Shuo Jia,\n  Zhong-Bo Kang, Bishnu Karki, Weiyao Ke, Vladimir Khachatryan, Dmitri\n  Kharzeev, Spencer R. Klein, Vladimir Korepin, Yuri Kovchegov, Brandon\n  Kriesten, Shunzo Kumano, Wai Kin Lai, Richard Lebed, Christopher Lee, Kyle\n  Lee, Hai Tao Li, Jifeng Liao, Huey-Wen Lin, Keh-Fei Liu, Simonetta Liuti,\n  C\\'edric Lorc\\'e, Magno V. T. Machado, Heikki Mantysaari, Vincent Mathieu,\n  Nilmani Mathur, Yacine Mehtar-Tani, Wally Melnitchouk, Emanuele Mereghetti,\n  Andreas Metz, Johannes K.L. Michel, Gerald Miller, Hamlet Mkrtchyan, Asmita\n  Mukherjee, Swagato Mukherjee, Piet Mulders, St\\'ephane Munier, Francesco\n  Murgia, P. M. Nadolsky, John W Negele, Duff Neill, Jan Nemchik, E. Nocera,\n  Vitalii Okorokov, Fredrick Olness, Barbara Pasquini, Chao Peng, Peter\n  Petreczky, Frank Petriello, Alessandro Pilloni, Bernard Pire, Cristian\n  Pisano, Daniel Pitonyak, Michal Praszalowicz, Alexei Prokudin, Jianwei Qiu,\n  Marco Radici, Kh\\'epani Raya, Felix Ringer, Jennifer Rittenhouse West,\n  Arkaitz Rodas, Simone Rodini, Juan Rojo, Farid Salazar, Elena Santopinto,\n  Misak Sargsian, Nobuo Sato, Bjoern Schenke, Stella Schindler, Gunar Schnell,\n  Peter Schweitzer, Ignazio Scimemi, Jorge Segovia, Kirill\n  Semenov-Tian-Shansky, Phiala Shanahan, Ding-Yu Shao, Matt Sievert, Andrea\n  Signori, Rajeev Singh, Vladi Skokov, Qin-Tao Song, Stanislav Srednyak, Iain\n  W. Stewart, Raza Sabbir Sufian, Eric Swanson, Sergey Syritsyn, Adam\n  Szczepaniak, Pawel Sznajder, Asli Tandogan, Yossathorn Tawabutr, A. Tawfik,\n  John Terry, Tobias Toll, Oleksandr Tomalak, Fidele Twagirayezu, Raju\n  Venugopalan, Ivan Vitev, Alexey Vladimirov, Werner Vogelsang, Ramona Vogt,\n  Gojko Vujanovic, Wouter Waalewijn, Xiang-Peng Wang, Bo-Wen Xiao, Hongxi Xing,\n  Yi-Bo Yang, Xiaojun Yao, Feng Yuan, Yong Zhao, Pia Zurita","title":"The case for an EIC Theory Alliance: Theoretical Challenges of the EIC","comments":"44 pages, ReVTeX, White Paper on EIC Theory Alliance","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph hep-ex","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We outline the physics opportunities provided by the Electron Ion Collider\n(EIC). These include the study of the parton structure of the nucleon and\nnuclei, the onset of gluon saturation, the production of jets and heavy flavor,\nhadron spectroscopy and tests of fundamental symmetries. We review the present\nstatus and future challenges in EIC theory that have to be addressed in order\nto realize this ambitious and impactful physics program, including how to\nengage a diverse and inclusive workforce. In order to address these many-fold\nchallenges, we propose a coordinated effort involving theory groups with\ndiffering expertise is needed. We discuss the scientific goals and scope of\nsuch an EIC Theory Alliance.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:22:39 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14573","submitter":"Yinxing (Allen) Zang","authors":"Allen Zang, Xinan Chen, Alexander Kolar, Joaquin Chung, Martin\n  Suchara, Tian Zhong, Rajkumar Kettimuthu","title":"Entanglement Distribution in Quantum Repeater with Purification and\n  Optimized Buffer Time","comments":"6 pages, 4 figures, IEEE INFOCOM'23 NetSciQCom 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Quantum repeater networks that allow long-distance entanglement distribution\nwill be the backbone of distributed quantum information processing. In this\npaper we explore entanglement distribution using quantum repeaters with\noptimized buffer time, equipped with noisy quantum memories and performing\nimperfect entanglement purification and swapping. We observe that increasing\nthe number of memories on end nodes leads to a higher entanglement distribution\nrate per memory and higher probability of high-fidelity entanglement\ndistribution, at least for the case with perfect operations. When imperfect\noperations are considered, however, we make the surprising observation that the\nper-memory entanglement rate decreases with increasing number of memories. Our\nresults suggest that building quantum repeaters that perform well under\nrealistic conditions requires careful modeling and design that takes into\nconsideration the operations and resources that are finite and imperfect.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:23:34 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14574","submitter":"Erin George","authors":"Erin George, Joyce Chew, Deanna Needell","title":"Detecting and Mitigating Indirect Stereotypes in Word Embeddings","comments":"15 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Societal biases in the usage of words, including harmful stereotypes, are\nfrequently learned by common word embedding methods. These biases manifest not\nonly between a word and an explicit marker of its stereotype, but also between\nwords that share related stereotypes. This latter phenomenon, sometimes called\n\"indirect bias,'' has resisted prior attempts at debiasing. In this paper, we\npropose a novel method called Biased Indirect Relationship Modification (BIRM)\nto mitigate indirect bias in distributional word embeddings by modifying biased\nrelationships between words before embeddings are learned. This is done by\nconsidering how the co-occurrence probability of a given pair of words changes\nin the presence of words marking an attribute of bias, and using this to\naverage out the effect of a bias attribute. To evaluate this method, we perform\na series of common tests and demonstrate that measures of bias in the word\nembeddings are reduced in exchange for minor reduction in the semantic quality\nof the embeddings. In addition, we conduct novel tests for measuring indirect\nstereotypes by extending the Word Embedding Association Test (WEAT) with new\ntest sets for indirect binary gender stereotypes. With these tests, we\ndemonstrate the presence of more subtle stereotypes not addressed by previous\nwork. The proposed method is able to reduce the presence of some of these new\nstereotypes, serving as a crucial next step towards non-stereotyped word\nembeddings.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:23:49 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14575","submitter":"Abhineet Singh","authors":"Abhineet Singh, Ila Jasra, Omar Mouhammed, Nidheesh Dadheech, Nilanjan\n  Ray, James Shapiro","title":"Towards Early Prediction of Human iPSC Reprogramming Success","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper presents advancements in automated early-stage prediction of the\nsuccess of reprogramming human induced pluripotent stem cells (iPSCs) as a\npotential source for regenerative cell therapies.The minuscule success rate of\niPSC-reprogramming of around $ 0.01% $ to $ 0.1% $ makes it labor-intensive,\ntime-consuming, and exorbitantly expensive to generate a stable iPSC line.\nSince that requires culturing of millions of cells and intense biological\nscrutiny of multiple clones to identify a single optimal clone. The ability to\nreliably predict which cells are likely to establish as an optimal iPSC line at\nan early stage of pluripotency would therefore be ground-breaking in rendering\nthis a practical and cost-effective approach to personalized medicine. Temporal\ninformation about changes in cellular appearance over time is crucial for\npredicting its future growth outcomes. In order to generate this data, we first\nperformed continuous time-lapse imaging of iPSCs in culture using an ultra-high\nresolution microscope. We then annotated the locations and identities of cells\nin late-stage images where reliable manual identification is possible. Next, we\npropagated these labels backwards in time using a semi-automated tracking\nsystem to obtain labels for early stages of growth. Finally, we used this data\nto train deep neural networks to perform automatic cell segmentation and\nclassification. Our code and data are available at\nhttps://github.com/abhineet123/ipsc_prediction.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:26:35 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14576","submitter":"Josip Juki\\'c","authors":"Josip Juki\\'c, Jan \\v{S}najder","title":"Parameter-Efficient Language Model Tuning with Active Learning in\n  Low-Resource Settings","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Pre-trained language models (PLMs) have ignited a surge in demand for\neffective fine-tuning techniques, particularly in low-resource domains and\nlanguages. Active learning (AL), a set of algorithms designed to decrease\nlabeling costs by minimizing label complexity, has shown promise in confronting\nthe labeling bottleneck. Concurrently, adapter modules, designed for\nparameter-efficient fine-tuning (PEFT), have showcased notable potential in\nlow-resource settings. However, the interplay between AL and adapter-based PEFT\nremains unexplored. In our study, we empirically investigate PEFT behavior with\nAL in low-resource settings for text classification tasks. Our findings affirm\nthe superiority of PEFT over full-fine tuning (FFT) in low-resource settings\nand demonstrate that this advantage persists in AL setups. Finally, we delve\ninto the properties of PEFT and FFT through the lens of forgetting dynamics and\ninstance-level representations, linking them to AL instance selection behavior\nand the stability of PEFT. Our research underscores the synergistic potential\nof AL, PEFT, and TAPT in low-resource settings, paving the way for advancements\nin efficient and effective fine-tuning.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:27:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14577","submitter":"Alex Wilf","authors":"Alex Wilf, Syeda Nahida Akter, Leena Mathur, Paul Pu Liang, Sheryl\n  Mathew, Mengrou Shou, Eric Nyberg, Louis-Philippe Morency","title":"Difference-Masking: Choosing What to Mask in Continued Pretraining","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Self-supervised learning (SSL) and the objective of masking-and-predicting in\nparticular have led to promising SSL performance on a variety of downstream\ntasks. However, while most approaches randomly mask tokens, there is strong\nintuition from the field of education that deciding what to mask can\nsubstantially improve learning outcomes. We introduce Difference-Masking, an\napproach that automatically chooses what to mask during continued pretraining\nby considering what makes an unlabelled target domain different from the\npretraining domain. Empirically, we find that Difference-Masking outperforms\nbaselines on continued pretraining settings across four diverse language and\nmultimodal video tasks. The cross-task applicability of Difference-Masking\nsupports the effectiveness of our framework for SSL pretraining in language,\nvision, and other domains.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:31:02 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14578","submitter":"Margarita Bugue\\~no","authors":"Margarita Bugue\\~no, Gerard de Melo","title":"Connecting the Dots: What Graph-Based Text Representations Work Best for\n  Text Classification using Graph Neural Networks?","comments":"17 pages, 2 figures, 15 tables. The Appendix starts on page 12","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Given the success of Graph Neural Networks (GNNs) for structure-aware machine\nlearning, numerous studies have explored their application to text\nclassification, as an alternative to traditional feature representation models.\nHowever, most studies considered just a specific domain and validated on data\nwith particular characteristics. This work presents an extensive empirical\ninvestigation of graph-based text representation methods proposed for text\nclassification, identifying practical implications and open challenges in the\nfield. We compare several GNN architectures as well as BERT across five\ndatasets, encompassing short and also long documents. The results show that: i)\ngraph performance is highly related to the textual input features and domain,\nii) despite its outstanding performance, BERT has difficulties converging when\ndealing with short texts, iii) graph methods are particularly beneficial for\nlonger documents.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:31:24 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14579","submitter":"Xiwen Li","authors":"Xiwen Li, Tristalee Mangin, Surojit Saha, Evan Blanchard, Dillon Tang,\n  Henry Poppe, Nathan Searle, Ouk Choi, Kerry Kelly, and Ross Whitaker","title":"Real-Time Idling Vehicles Detection Using Combined Audio-Visual Deep\n  Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Combustion vehicle emissions contribute to poor air quality and release\ngreenhouse gases into the atmosphere, and vehicle pollution has been associated\nwith numerous adverse health effects. Roadways with extensive waiting and/or\npassenger drop off, such as schools and hospital drop-off zones, can result in\nhigh incidence and density of idling vehicles. This can produce micro-climates\nof increased vehicle pollution. Thus, the detection of idling vehicles can be\nhelpful in monitoring and responding to unnecessary idling and be integrated\ninto real-time or off-line systems to address the resulting pollution. In this\npaper we present a real-time, dynamic vehicle idling detection algorithm. The\nproposed idle detection algorithm and notification rely on an algorithm to\ndetect these idling vehicles. The proposed method relies on a multi-sensor,\naudio-visual, machine-learning workflow to detect idling vehicles visually\nunder three conditions: moving, static with the engine on, and static with the\nengine off. The visual vehicle motion detector is built in the first stage, and\nthen a contrastive-learning-based latent space is trained for classifying\nstatic vehicle engine sound. We test our system in real-time at a hospital\ndrop-off point in Salt Lake City. This in-situ dataset was collected and\nannotated, and it includes vehicles of varying models and types. The\nexperiments show that the method can detect engine switching on or off\ninstantly and achieves 71.01 mean average precision (mAP).\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:35:43 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14580","submitter":"Lucas Gris","authors":"Lucas Rafael Stefanel Gris and Ricardo Marcacini and Arnaldo Candido\n  Junior and Edresson Casanova and Anderson Soares and Sandra Maria Alu\\'isio","title":"Evaluating OpenAI's Whisper ASR for Punctuation Prediction and Topic\n  Modeling of life histories of the Museum of the Person","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Automatic speech recognition (ASR) systems play a key role in applications\ninvolving human-machine interactions. Despite their importance, ASR models for\nthe Portuguese language proposed in the last decade have limitations in\nrelation to the correct identification of punctuation marks in automatic\ntranscriptions, which hinder the use of transcriptions by other systems,\nmodels, and even by humans. However, recently Whisper ASR was proposed by\nOpenAI, a general-purpose speech recognition model that has generated great\nexpectations in dealing with such limitations. This chapter presents the first\nstudy on the performance of Whisper for punctuation prediction in the\nPortuguese language. We present an experimental evaluation considering both\ntheoretical aspects involving pausing points (comma) and complete ideas\n(exclamation, question, and fullstop), as well as practical aspects involving\ntranscript-based topic modeling - an application dependent on punctuation marks\nfor promising performance. We analyzed experimental results from videos of\nMuseum of the Person, a virtual museum that aims to tell and preserve people's\nlife histories, thus discussing the pros and cons of Whisper in a real-world\nscenario. Although our experiments indicate that Whisper achieves\nstate-of-the-art results, we conclude that some punctuation marks require\nimprovements, such as exclamation, semicolon and colon.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:37:29 GMT"},{"version":"v2","created":"Fri, 26 May 2023 12:09:55 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.14581","submitter":"Smrithan Ravichandran","authors":"Smrithan Ravichandran, Andrew Longman, Marine Huault, Roberto Lera,\n  Calvin Z. He, Robert Fedosejevs, Luis Roso and Wendell T. Hill III","title":"Imaging electron angular distributions to assess a full-power\n  petawatt-class laser focus","comments":"10 pages, 6 figures, submitted to Physical Review A","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.ins-det physics.app-ph physics.optics","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  We present a novel technique to assess the focal volume of petawatt-class\nlasers at full power. Our approach exploits quantitative measurement of the\nangular distribution of electrons born in the focus via ionization of rarefied\ngas, which are accelerated forward and ejected ponderomotively by the field. We\nshow that a bivariate ($\\theta, \\phi$) angular distribution, which was obtained\nwith image plates, not only enables the peak intensity to be extracted, it also\nreflects nonideality of the focal-spot intensity distribution. In our prototype\ndemonstration at intensities of a few $\\times 10^{19}$ to a few $\\times\n10^{20}$ $\\mathrm{W/cm^2}$, an f/10 optic produced a focal spot in the paraxial\nregime, allowing a plane-wave parameterization of the peak intensity\n($\\tan{\\theta} \\propto 1/a_0$) to be compared with our measurements;\nqualitative agreement was found. Furthermore, we show that scintillation\ndetection of electrons is sensitive to real-time, shot-to-shot fluctuations,\nwhich means the technique would support single-shot measurement. Finally, we\nestimate that this approach should be usable with electrons up to intensities\n$\\sim 10^{21}\\ \\mathrm{W/cm^2}$ before angular measurements start to be less\neffective. However, angular measurements will again be possible at $\\sim\n10^{24}\\ \\mathrm{W/cm^2}$ when protons become relativistic within a single\ncycle.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:38:45 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14582","submitter":"Ziqi Zhao","authors":"Ziqi Zhao, Yucheng Shi, Shushan Wu, Fan Yang, Wenzhan Song, Ninghao\n  Liu","title":"Interpretation of Time-Series Deep Models: A Survey","comments":"18 pages, 3 figures, 1 table","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Deep learning models developed for time-series associated tasks have become\nmore widely researched nowadays. However, due to the unintuitive nature of\ntime-series data, the interpretability problem -- where we understand what is\nunder the hood of these models -- becomes crucial. The advancement of similar\nstudies in computer vision has given rise to many post-hoc methods, which can\nalso shed light on how to explain time-series models. In this paper, we present\na wide range of post-hoc interpretation methods for time-series models based on\nbackpropagation, perturbation, and approximation. We also want to bring focus\nonto inherently interpretable models, a novel category of interpretation where\nhuman-understandable information is designed within the models. Furthermore, we\nintroduce some common evaluation metrics used for the explanations, and propose\nseveral directions of future researches on the time-series interpretability\nproblem. As a highlight, our work summarizes not only the well-established\ninterpretation methods, but also a handful of fairly recent and under-developed\ntechniques, which we hope to capture their essence and spark future endeavours\nto innovate and improvise.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:43:26 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14583","submitter":"Alexander Hoyle","authors":"Alexander Hoyle, Rupak Sarkar, Pranav Goel, Philip Resnik","title":"Making the Implicit Explicit: Implicit Content as a First Class Citizen\n  in NLP","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  Language is multifaceted. A given utterance can be re-expressed in equivalent\nforms, and its implicit and explicit content support various logical and\npragmatic inferences. When processing an utterance, we consider these different\naspects, as mediated by our interpretive goals -- understanding that \"it's dark\nin here\" may be a veiled direction to turn on a light. Nonetheless, NLP methods\ntypically operate over the surface form alone, eliding this nuance.\n  In this work, we represent language with language, and direct an LLM to\ndecompose utterances into logical and plausible inferences. The reduced\ncomplexity of the decompositions makes them easier to embed, opening up novel\napplications. Variations on our technique lead to state-of-the-art improvements\non sentence embedding benchmarks, a substantive application in computational\npolitical science, and to a novel construct-discovery process, which we\nvalidate with human annotations.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:45:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14584","submitter":"Kangkang Duan","authors":"Kangkang Duan and Zhengbo Zou","title":"Learning from demonstrations: An intuitive VR environment for imitation\n  learning of construction robots","comments":"22 pages, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Construction robots are challenging the traditional paradigm of labor\nintensive and repetitive construction tasks. Present concerns regarding\nconstruction robots are focused on their abilities in performing complex tasks\nconsisting of several subtasks and their adaptability to work in unstructured\nand dynamic construction environments. Imitation learning (IL) has shown\nadvantages in training a robot to imitate expert actions in complex tasks and\nthe policy thereafter generated by reinforcement learning (RL) is more adaptive\nin comparison with pre-programmed robots. In this paper, we proposed a\nframework composed of two modules for imitation learning of construction\nrobots. The first module provides an intuitive expert demonstration collection\nVirtual Reality (VR) platform where a robot will automatically follow the\nposition, rotation, and actions of the expert's hand in real-time, instead of\nrequiring an expert to control the robot via controllers. The second module\nprovides a template for imitation learning using observations and actions\nrecorded in the first module. In the second module, Behavior Cloning (BC) is\nutilized for pre-training, Generative Adversarial Imitation Learning (GAIL) and\nProximal Policy Optimization (PPO) are combined to achieve a trade-off between\nthe strength of imitation vs. exploration. Results show that imitation\nlearning, especially when combined with PPO, could significantly accelerate\ntraining in limited training steps and improve policy performance.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:46:57 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14585","submitter":"Andrew Engel","authors":"Andrew Engel, Zhichao Wang, Natalie S. Frank, Ioana Dumitriu, Sutanay\n  Choudhury, Anand Sarwate, Tony Chiang","title":"Robust Explanations for Deep Neural Networks via Pseudo Neural Tangent\n  Kernel Surrogate Models","comments":"9 pages, 4 figures, 3 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  One of the ways recent progress has been made on explainable AI has been via\nexplain-by-example strategies, specifically, through data attribution tasks.\nThe feature spaces used to attribute decisions to training data, however, have\nnot been compared against one another as to whether they form a truly\nrepresentative surrogate model of the neural network (NN). Here, we demonstrate\nthe efficacy of surrogate linear feature spaces to neural networks through two\nmeans: (1) we establish that a normalized psuedo neural tangent kernel (pNTK)\nis more correlated to the neural network decision functions than embedding\nbased and influence based alternatives in both computer vision and large\nlanguage model architectures; (2) we show that the attributions created from\nthe normalized pNTK more accurately select perturbed training data in a data\npoisoning attribution task than these alternatives. Based on these\nobservations, we conclude that kernel linear models are effective surrogate\nmodels across multiple classification architectures and that pNTK-based kernels\nare the most appropriate surrogate feature space of all kernels studied.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:51:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14586","submitter":"Kangkang Duan","authors":"Kangkang Duan, Christine Wun Ki Suen, and Zhengbo Zou","title":"MARC: A multi-agent robots control framework for enhancing reinforcement\n  learning in construction tasks","comments":"19 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Letting robots emulate human behavior has always posed a challenge,\nparticularly in scenarios involving multiple robots. In this paper, we\npresented a framework aimed at achieving multi-agent reinforcement learning for\nrobot control in construction tasks. The construction industry often\nnecessitates complex interactions and coordination among multiple robots,\ndemanding a solution that enables effective collaboration and efficient task\nexecution. Our proposed framework leverages the principles of proximal policy\noptimization and developed a multi-agent version to enable the robots to\nacquire sophisticated control policies. We evaluated the effectiveness of our\nframework by learning four different collaborative tasks in the construction\nenvironments. The results demonstrated the capability of our approach in\nenabling multiple robots to learn and adapt their behaviors in complex\nconstruction tasks while effectively preventing collisions. Results also\nrevealed the potential of combining and exploring the advantages of\nreinforcement learning algorithms and inverse kinematics. The findings from\nthis research contributed to the advancement of multi-agent reinforcement\nlearning in the domain of construction robotics. By enabling robots to behave\nlike human counterparts and collaborate effectively, we pave the way for more\nefficient, flexible, and intelligent construction processes.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:52:31 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14587","submitter":"Hamed Rahimi","authors":"Hamed Rahimi, Jacob Louis Hoover, David Mimno, Hubert Naacke, Camelia\n  Constantin, Bernd Amann","title":"Contextualized Topic Coherence Metrics","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.IR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The recent explosion in work on neural topic modeling has been criticized for\noptimizing automated topic evaluation metrics at the expense of actual\nmeaningful topic identification. But human annotation remains expensive and\ntime-consuming. We propose LLM-based methods inspired by standard human topic\nevaluations, in a family of metrics called Contextualized Topic Coherence\n(CTC). We evaluate both a fully automated version as well as a semi-automated\nCTC that allows human-centered evaluation of coherence while maintaining the\nefficiency of automated methods. We evaluate CTC relative to five other metrics\non six topic models and find that it outperforms automated topic coherence\nmethods, works well on short documents, and is not susceptible to meaningless\nbut high-scoring topics.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:53:29 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14588","submitter":"Daniel Simig","authors":"Sebastian Cadavid-Sanchez, Khalil Kacem, Rafael Aparecido Martins\n  Frade, Johannes Boehm, Thomas Chaney, Danial Lashkari, Daniel Simig","title":"Evaluating end-to-end entity linking on domain-specific knowledge bases:\n  Learning about ancient technologies from museum collections","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  To study social, economic, and historical questions, researchers in the\nsocial sciences and humanities have started to use increasingly large\nunstructured textual datasets. While recent advances in NLP provide many tools\nto efficiently process such data, most existing approaches rely on generic\nsolutions whose performance and suitability for domain-specific tasks is not\nwell understood. This work presents an attempt to bridge this domain gap by\nexploring the use of modern Entity Linking approaches for the enrichment of\nmuseum collection data. We collect a dataset comprising of more than 1700 texts\nannotated with 7,510 mention-entity pairs, evaluate some off-the-shelf\nsolutions in detail using this dataset and finally fine-tune a recent\nend-to-end EL model on this data. We show that our fine-tuned model\nsignificantly outperforms other approaches currently available in this domain\nand present a proof-of-concept use case of this model. We release our dataset\nand our best model.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:53:58 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14589","submitter":"Xiaofeng Liu","authors":"Xiaofeng Liu, Jerry L. Prince, Fangxu Xing, Jiachen Zhuo, Reese\n  Timothy, Maureen Stone, Georges El Fakhri, Jonghye Woo","title":"Attentive Continuous Generative Self-training for Unsupervised Domain\n  Adaptive Medical Image Translation","comments":"Accepted to Medical Image Analysis","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV cs.LG physics.med-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Self-training is an important class of unsupervised domain adaptation (UDA)\napproaches that are used to mitigate the problem of domain shift, when applying\nknowledge learned from a labeled source domain to unlabeled and heterogeneous\ntarget domains. While self-training-based UDA has shown considerable promise on\ndiscriminative tasks, including classification and segmentation, through\nreliable pseudo-label filtering based on the maximum softmax probability, there\nis a paucity of prior work on self-training-based UDA for generative tasks,\nincluding image modality translation. To fill this gap, in this work, we seek\nto develop a generative self-training (GST) framework for domain adaptive image\ntranslation with continuous value prediction and regression objectives.\nSpecifically, we quantify both aleatoric and epistemic uncertainties within our\nGST using variational Bayes learning to measure the reliability of synthesized\ndata. We also introduce a self-attention scheme that de-emphasizes the\nbackground region to prevent it from dominating the training process. The\nadaptation is then carried out by an alternating optimization scheme with\ntarget domain supervision that focuses attention on the regions with reliable\npseudo-labels. We evaluated our framework on two cross-scanner/center,\ninter-subject translation tasks, including tagged-to-cine magnetic resonance\n(MR) image translation and T1-weighted MR-to-fractional anisotropy translation.\nExtensive validations with unpaired target domain data showed that our GST\nyielded superior synthesis performance in comparison to adversarial training\nUDA methods.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:57:44 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14590","submitter":"Sijia Wang","authors":"Pritika Ramu, Sijia Wang, Lalla Mouatadid, Joy Rimchala, Lifu Huang","title":"RE$^2$: Region-Aware Relation Extraction from Visually Rich Documents","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Current research in form understanding predominantly relies on large\npre-trained language models, necessitating extensive data for pre-training.\nHowever, the importance of layout structure (i.e., the spatial relationship\nbetween the entity blocks in the visually rich document) to relation extraction\nhas been overlooked. In this paper, we propose REgion-Aware Relation Extraction\n(RE$^2$) that leverages region-level spatial structure among the entity blocks\nto improve their relation prediction. We design an edge-aware graph attention\nnetwork to learn the interaction between entities while considering their\nspatial relationship defined by their region-level representations. We also\nintroduce a constraint objective to regularize the model towards consistency\nwith the inherent constraints of the relation extraction task. Extensive\nexperiments across various datasets, languages and domains demonstrate the\nsuperiority of our proposed approach.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 00:07:40 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14591","submitter":"Kexun Zhang","authors":"Kexun Zhang, Danqing Wang, Jingtao Xia, William Yang Wang, Lei Li","title":"ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.SE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large language models (LLMs) excel at implementing code from functionality\ndescriptions, but struggle with algorithmic problems that require not only\nimplementation but also identification of the suitable algorithm. Moreover,\nLLM-generated programs lack guaranteed correctness and require human\nverification. To address these challenges, we propose ALGO, a framework that\nsynthesizes Algorithmic programs with LLM-Generated Oracles to guide the\ncreation and verify their correctness. ALGO first generates a probably correct\nbut possibly slow reference oracle by prompting an LLM to exhaustively\nenumerate all the combinations of relevant variables. This oracle is then\nutilized to guide an arbitrary search strategy in exploring the algorithm space\nand to verify the algorithms synthesized. Our study shows that the\nLLM-generated oracles are correct for 88% of the cases. With the oracles as\nverifiers, ALGO can be integrated with any existing code generation model in a\nmodel-agnostic manner to enhance its performance. Experiments show that when\nequipped with ALGO, we achieve an 8x better one-submission pass rate over the\nCodex model and a 2.6x better one-submission pass rate over CodeT, the current\nstate-of-the-art model on CodeContests. We can also get 1.3x better pass rate\nover the ChatGPT Code Interpreter on unseen problems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 00:10:15 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14592","submitter":"Ruohao Guo","authors":"Ruohao Guo, Wei Xu, Alan Ritter","title":"Instruction Tuning with Lexicons for Zero-Shot Style Classification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Style is used to convey authors' intentions and attitudes. Despite the\nsuccess of large pre-trained language models on style classification, prior\nwork relies on fine-tuning with labeled examples. Prompting large language\nmodels to classify style without fine-tuning is challenging because language\nstyles can be difficult to define. In this study, we investigate the\neffectiveness of style lexicons as a means for instructing language models how\nto identify new styles that are unseen during training. Our experiments show\nthat lexicon-based instructions improve transfer zero-shot performance\nsignificantly. We will release our code and data.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 00:17:36 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14593","submitter":"Yuling Yao","authors":"Yuling Yao, Justin Domke","title":"Discriminative calibration","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG stat.CO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  To check the accuracy of Bayesian computations, it is common to use\nrank-based simulation-based calibration (SBC). However, SBC has drawbacks: The\ntest statistic is somewhat ad-hoc, interactions are difficult to examine,\nmultiple testing is a challenge, and the resulting p-value is not a divergence\nmetric. We propose to replace the marginal rank test with a flexible\nclassification approach that learns test statistics from data. This measure\ntypically has a higher statistical power than the SBC rank test and returns an\ninterpretable divergence measure of miscalibration, computed from\nclassification accuracy. This approach can be used with different data\ngenerating processes to address likelihood-free inference or traditional\ninference methods like Markov chain Monte Carlo or variational inference. We\nillustrate an automated implementation using neural networks and\nstatistically-inspired features, and validate the method with numerical and\nreal data experiments.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 00:18:48 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14594","submitter":"Salem Lahlou","authors":"Salem Lahlou, Joseph D. Viviano, Victor Schmidt","title":"torchgfn: A PyTorch GFlowNet library","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The increasing popularity of generative flow networks (GFlowNets or GFNs) is\naccompanied with a proliferation of code sources. This hinders the\nimplementation of new features, such as training losses, that can readily be\ncompared to existing ones, on a set of common environments. In addition to\nslowing down research in the field of GFlowNets, different code bases use\ndifferent conventions, that might be confusing for newcomers. `torchgfn` is a\nlibrary built on top of PyTorch, that aims at addressing both problems. It\nprovides user with a simple API for environments, and useful abstractions for\nsamplers and losses. Multiple examples are provided, replicating published\nresults. The code is available in https://github.com/saleml/torchgfn.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 00:20:59 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14595","submitter":"Serena Wang","authors":"Serena Wang, Stephen Bates, P. M. Aronow, Michael I. Jordan","title":"Operationalizing Counterfactual Metrics: Incentives, Ranking, and\n  Information Asymmetry","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CY cs.GT","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  From the social sciences to machine learning, it has been well documented\nthat metrics to be optimized are not always aligned with social welfare. In\nhealthcare, Dranove et al. [12] showed that publishing surgery mortality\nmetrics actually harmed the welfare of sicker patients by increasing provider\nselection behavior. Using a principal-agent model, we directly study the\nincentive misalignments that arise from such average treated outcome metrics,\nand show that the incentives driving treatment decisions would align with\nmaximizing total patient welfare if the metrics (i) accounted for\ncounterfactual untreated outcomes and (ii) considered total welfare instead of\naverage welfare among treated patients. Operationalizing this, we show how\ncounterfactual metrics can be modified to satisfy desirable properties when\nused for ranking. Extending to realistic settings when the providers observe\nmore about patients than the regulatory agencies do, we bound the decay in\nperformance by the degree of information asymmetry between the principal and\nthe agent. In doing so, our model connects principal-agent information\nasymmetry with unobserved heterogeneity in causal inference.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 00:24:38 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14596","submitter":"Sarah Wiegreffe","authors":"Sarah Wiegreffe, Matthew Finlayson, Oyvind Tafjord, Peter Clark,\n  Ashish Sabharwal","title":"Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  When large language models (LMs) are applied in zero- or few-shot settings to\ndiscriminative tasks such as multiple-choice questions, their attentiveness\n(i.e., probability mass) is spread across many vocabulary tokens that are not\nvalid choices. Such a spread across multiple surface forms with identical\nmeaning is thought to cause an underestimation of a model's true performance,\nreferred to as the \"surface form competition\" (SFC) hypothesis. This has\nmotivated the introduction of various probability normalization methods.\nHowever, many core questions remain unanswered. How do we measure SFC or\nattentiveness? Are there direct ways of increasing attentiveness on valid\nchoices? Does increasing attentiveness always improve task accuracy? We propose\na mathematical formalism for studying this phenomenon, provide a metric for\nquantifying attentiveness, and identify a simple method for increasing it --\nnamely, in-context learning with even just one example containing answer\nchoices. The formalism allows us to quantify SFC and bound its impact. Our\nexperiments on three diverse datasets and six LMs reveal several surprising\nfindings. For example, encouraging models to generate a valid answer choice\ncan, in fact, be detrimental to task performance for some LMs, and prior\nprobability normalization methods are less effective (sometimes even\ndetrimental) to instruction-tuned LMs. We conclude with practical insights for\neffectively using prompted LMs for multiple-choice tasks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 00:27:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14597","submitter":"Zhijing Jin","authors":"Yiwen Ding, Jiarui Liu, Zhiheng Lyu, Kun Zhang, Bernhard Schoelkopf,\n  Zhijing Jin, Rada Mihalcea","title":"Voices of Her: Analyzing Gender Differences in the AI Publication World","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  While several previous studies have analyzed gender bias in research, we are\nstill missing a comprehensive analysis of gender differences in the AI\ncommunity, covering diverse topics and different development trends. Using the\nAI Scholar dataset of 78K researchers in the field of AI, we identify several\ngender differences: (1) Although female researchers tend to have fewer overall\ncitations than males, this citation difference does not hold for all\nacademic-age groups; (2) There exist large gender homophily in co-authorship on\nAI papers; (3) Female first-authored papers show distinct linguistic styles,\nsuch as longer text, more positive emotion words, and more catchy titles than\nmale first-authored papers. Our analysis provides a window into the current\ndemographic trends in our AI community, and encourages more gender equality and\ndiversity in the future. Our code and data are at\nhttps://github.com/causalNLP/ai-scholar-gender.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 00:40:49 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14598","submitter":"Yutong Zhou","authors":"Yutong Zhou and Nobutaka Shimada","title":"Vision + Language Applications: A Survey","comments":"Accepted by GCV @CVPR2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.MM","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Text-to-image generation has attracted significant interest from researchers\nand practitioners in recent years due to its widespread and diverse\napplications across various industries. Despite the progress made in the domain\nof vision and language research, the existing literature remains relatively\nlimited, particularly with regard to advancements and applications in this\nfield. This paper explores a relevant research track within multimodal\napplications, including text, vision, audio, and others. In addition to the\nstudies discussed in this paper, we are also committed to continually updating\nthe latest relevant papers, datasets, application projects and corresponding\ninformation at https://github.com/Yutong-Zhou-cv/Awesome-Text-to-Image\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 00:42:06 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14599","submitter":"James Y. Huang","authors":"James Y. Huang, Wenlin Yao, Kaiqiang Song, Hongming Zhang, Muhao Chen,\n  Dong Yu","title":"Bridging Continuous and Discrete Spaces: Interpretable Sentence\n  Representation Learning via Compositional Operations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Traditional sentence embedding models encode sentences into vector\nrepresentations to capture useful properties such as the semantic similarity\nbetween sentences. However, in addition to similarity, sentence semantics can\nalso be interpreted via compositional operations such as sentence fusion or\ndifference. It is unclear whether the compositional semantics of sentences can\nbe directly reflected as compositional operations in the embedding space. To\nmore effectively bridge the continuous embedding and discrete text spaces, we\nexplore the plausibility of incorporating various compositional properties into\nthe sentence embedding space that allows us to interpret embedding\ntransformations as compositional sentence operations. We propose InterSent, an\nend-to-end framework for learning interpretable sentence embeddings that\nsupports compositional sentence operations in the embedding space. Our method\noptimizes operator networks and a bottleneck encoder-decoder model to produce\nmeaningful and interpretable sentence embeddings. Experimental results\ndemonstrate that our method significantly improves the interpretability of\nsentence embeddings on four textual generation tasks over existing approaches\nwhile maintaining strong performance on traditional semantic similarity tasks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 00:44:49 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14600","submitter":"Tao Li","authors":"Tao Li, Ghazaleh Kazeminejad, Susan W. Brown, Martha Palmer, Vivek\n  Srikumar","title":"Learning Semantic Role Labeling from Compatible Label Sequences","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper addresses the question of how to efficiently learn from disjoint,\ncompatible label sequences. We argue that the compatible structures between\ndisjoint label sets help model learning and inference. We verify this\nhypothesis on the task of semantic role labeling (SRL), specifically, tagging a\nsentence with two role sequences: VerbNet arguments and PropBank arguments.\nPrior work has shown that cross-task interaction improves performance. However,\nthe two tasks are still separately decoded, running the risk of generating\nstructurally inconsistent label sequences (as per lexicons like SEMLINK). To\neliminate this issue, we first propose a simple and effective setup that\njointly handles VerbNet and PropBank labels as one sequence. With this setup,\nwe show that enforcing SEMLINK constraints during decoding constantly improves\nthe overall F1. With special input constructions, our joint model infers\nVerbNet arguments from PropBank arguments with over 99% accuracy. We also\npropose a constrained marginal model that uses SEMLINK information during\ntraining to further benefit from the large amounts of PropBank-only data. Our\nmodels achieve state-of-the-art F1's on VerbNet and PropBank argument labeling\non the CoNLL05 dataset with strong out-of-domain generalization.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 00:46:02 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14601","submitter":"Chiyoung Song","authors":"Chiyoung Song, Dongjae Lee","title":"FaceFusion: Exploiting Full Spectrum of Multiple Datasets","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The size of training dataset is known to be among the most dominating aspects\nof training high-performance face recognition embedding model. Building a large\ndataset from scratch could be cumbersome and time-intensive, while combining\nmultiple already-built datasets poses the risk of introducing large amount of\nlabel noise. We present a novel training method, named FaceFusion. It creates a\nfused view of different datasets that is untainted by identity conflicts, while\nconcurrently training an embedding network using the view in an end-to-end\nfashion. Using the unified view of combined datasets enables the embedding\nnetwork to be trained against the entire spectrum of the datasets, leading to a\nnoticeable performance boost. Extensive experiments confirm superiority of our\nmethod, whose performance in public evaluation datasets surpasses not only that\nof using a single training dataset, but also that of previously known methods\nunder various training circumstances.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 00:51:04 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14602","submitter":"Siddhartha Sarkar","authors":"Siddhartha Sarkar, Mohamed El Hedi Bahri, Andrej Ko\\v{s}mrlj","title":"Statistical mechanics of nanotubes","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.stat-mech cond-mat.soft","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We investigate the effect of thermal fluctuations on the mechanical\nproperties of nanotubes by employing tools from statistical physics. For 2D\nsheets it was previously shown that thermal fluctuations effectively\nrenormalize elastic moduli beyond a characteristic temperature-dependent\nthermal length scale (a few nanometers for graphene at room temperature), where\nthe bending rigidity increases, while the in-plane elastic moduli reduce in a\nscale-dependent fashion with universal power law exponents. However, the\ncurvature of nanotubes produces new phenomena. In nanotubes, competition\nbetween stretching and bending costs associated with radial fluctuations\nintroduces a characteristic elastic length scale, which is proportional to the\ngeometric mean of the radius and effective thickness. Beyond elastic length\nscale, we find that the in-plane elastic moduli stop renormalizing in the axial\ndirection, while they continue to renormalize in the circumferential direction\nbeyond the elastic length scale albeit with different universal exponents. The\nbending rigidity, however, stops renormalizing in the circumferential direction\nat the elastic length scale. These results were verified using molecular\ndynamics simulations.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 00:55:40 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14603","submitter":"Li Zhang","authors":"Li Zhang, Hainiu Xu, Abhinav Kommula, Niket Tandon, Chris\n  Callison-Burch","title":"OpenPI2.0: An Improved Dataset for Entity Tracking in Texts","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Representing texts as information about entities has long been deemed\neffective in event reasoning. We propose OpenPI2.0, an improved dataset for\ntracking entity states in procedural texts. OpenPI2.0 features not only\ncanonicalized entities that facilitate evaluation, but also salience\nannotations including both manual labels and automatic predictions. Regarding\nentity salience, we provide a survey on annotation subjectivity, modeling\nfeasibility, and downstream applications in tasks such as question answering\nand classical planning.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 00:57:35 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14604","submitter":"Jason Milionis","authors":"Jason Milionis, Ciamac C. Moallemi, Tim Roughgarden","title":"Automated Market Making and Arbitrage Profits in the Presence of Fees","comments":"27 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"q-fin.MF math.OC q-fin.PM q-fin.PR q-fin.TR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We consider the impact of trading fees on the profits of arbitrageurs trading\nagainst an automated marker marker (AMM) or, equivalently, on the adverse\nselection incurred by liquidity providers due to arbitrage. We extend the model\nof Milionis et al. [2022] for a general class of two asset AMMs to both\nintroduce fees and discrete Poisson block generation times. In our setting, we\nare able to compute the expected instantaneous rate of arbitrage profit in\nclosed form. When the fees are low, in the fast block asymptotic regime, the\nimpact of fees takes a particularly simple form: fees simply scale down\narbitrage profits by the fraction of time that an arriving arbitrageur finds a\nprofitable trade.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 00:59:32 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14605","submitter":"Francisco Rodriguez","authors":"Francisco Rodr\\'iguez","title":"Estimating causal effects of sanctions impacts: what role for\n  country-level studies?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"econ.GN q-fin.EC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  This article reviews recent advances in addressing empirical identification\nissues in cross-country and country-level studies and their implications for\nthe identification of the effectiveness and consequences of economic sanctions.\nI argue that, given the difficulties in assessing causal relationships in\ncross-national data, country-level case studies can serve as a useful and\ninformative complement to cross-national regression studies. However, I also\nwarn that case studies pose a set of additional potential empirical pitfalls\nwhich can obfuscate rather than clarify the identification of causal mechanisms\nat work. Therefore, the most sensible way to read case study evidence is as a\ncomplement rather than as a substitute to cross-national research.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:00:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14606","submitter":"James Schmidt","authors":"James Schmidt","title":"Taylor Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Empirical risk minimization stands behind most optimization in supervised\nmachine learning. Under this scheme, labeled data is used to approximate an\nexpected cost (risk), and a learning algorithm updates model-defining\nparameters in search of an empirical risk minimizer, with the aim of thereby\napproximately minimizing expected cost. Parameter update is often done by some\nsort of gradient descent. In this paper, we introduce a learning algorithm to\nconstruct models for real analytic functions using neither gradient descent nor\nempirical risk minimization. Observing that such functions are defined by local\ninformation, we situate familiar Taylor approximation methods in the context of\nsampling data from a distribution, and prove a nonuniform learning result.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:10:58 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14607","submitter":"Aayushya Agarwal","authors":"Aayushya Agarwal, Larry Pileggi","title":"An Equivalent Circuit Approach to Distributed Optimization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY math.OC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Distributed optimization is an essential paradigm to solve large-scale\noptimization problems in modern applications where big-data and\nhigh-dimensionality creates a computational bottleneck. Distributed\noptimization algorithms that exhibit fast convergence allow us to fully utilize\ncomputing resources and effectively scale to larger optimization problems in a\nmyriad of areas ranging from machine learning to power systems. In this work,\nwe introduce a new centralized distributed optimization algorithm (ECADO)\ninspired by an equivalent circuit model of the distributed problem. The\nequivalent circuit (EC) model provides a physical analogy to derive new\ninsights to develop a fast-convergent algorithm. The main contributions of this\napproach are: 1) a weighting scheme based on a circuit-inspired aggregate\nsensitivity analysis, and 2) an adaptive step-sizing derived from a stable,\nBackward-Euler numerical integration. We demonstrate that ECADO exhibits faster\nconvergence compared to state-of-the art distributed optimization methods and\nprovably converges for nonconvex problems. We leverage the ECADO features to\nsolve convex and nonconvex optimization problems with large datasets such as:\ndistributing data for logistic regression, training a deep neural network model\nfor classification, and solving a high-dimensional problem security-constrained\noptimal power flow problem. Compared to state-of-the-art centralized methods,\nincluding ADMM, centralized gradient descent, and DANE, this new ECADO approach\nis shown to converge in fewer iterations.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:11:03 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14608","submitter":"Feiyang Wu","authors":"Feiyang Wu, Jingyang Ke, Anqi Wu","title":"Inverse Reinforcement Learning with the Average Reward Criterion","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We study the problem of Inverse Reinforcement Learning (IRL) with an\naverage-reward criterion. The goal is to recover an unknown policy and a reward\nfunction when the agent only has samples of states and actions from an\nexperienced agent. Previous IRL methods assume that the expert is trained in a\ndiscounted environment, and the discount factor is known. This work alleviates\nthis assumption by proposing an average-reward framework with efficient\nlearning algorithms. We develop novel stochastic first-order methods to solve\nthe IRL problem under the average-reward setting, which requires solving an\nAverage-reward Markov Decision Process (AMDP) as a subproblem. To solve the\nsubproblem, we develop a Stochastic Policy Mirror Descent (SPMD) method under\ngeneral state and action spaces that needs $\\mathcal{{O}}(1/\\varepsilon)$ steps\nof gradient computation. Equipped with SPMD, we propose the Inverse Policy\nMirror Descent (IPMD) method for solving the IRL problem with a\n$\\mathcal{O}(1/\\varepsilon^2)$ complexity. To the best of our knowledge, the\naforementioned complexity results are new in IRL. Finally, we corroborate our\nanalysis with numerical experiments using the MuJoCo benchmark and additional\ncontrol tasks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:12:08 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14609","submitter":"Yi-Ming Chen","authors":"Yiming Chen, and Sihui Wang, and Dong Xu","title":"Association of stroke lesion distributions with atrial fibrillation\n  detected after stroke","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"q-bio.NC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Background Atrial fibrillation is often missed by traditional intermittent\nelectrocardiogram monitoring after ischemic stroke due to its paroxysmal and\nasymptomatic nature. The knowledge of the unique characteristics of the\npopulation with atrial fibrillation detected after stroke (AFDAS) enables more\nischemic stroke patients to benefit from more aggressive anticoagulation\ntherapy and AF management. Method This is an observational, retrospective, MRI\nimaging-based single-center study. Patients with AFDAS were matched in 1:3\nratio with patients without AF (NoAF)and patients with known AF before\nstroke(KAF) in PSM model based on age, gender, and time from stroke onset to\nadmission. Multivariate logistic models were used to test the association of\nMRI-based stroke lesion distribution, other clinical parameters and AF. A\nbackward stepwise elimination regression was conducted to identify the most\nimportant variables. Results Compared to the NoAF group(n=103), the patients\nwith AFDAS (n=42) had more cortical involvement(p=0.016), as well as\ntemporal(p<0.001) and insular lobes(p=0.018) infraction. After performing a\nbackward stepwise elimination model in regression analysis, the temporal lobe\ninfraction(OR 0.274, 95%CI 0.090-0.838, p=0.023) remained independently\nassociated with the detection of AF. Compared to the KAF group(n=89), LAD(OR\n1.113, 95%CI 1.022-1.211, p=0.014), Number of lobes infarction(p=0.012),\n3-lobes involvement(OR 0.177, 95%CI 0.056-0.559, p=0.003), and left hemisphere\nlobe involvement(OR 5.966, 95%CI 2.273-15.817, p<0.001)) were independently\nassociated with AFDAS and KAF. Conclusions Ischemic stroke patients with AF\ndetected after stroke present more temporal lobe infraction and cortical\ninvolvement. These lesion distribution characteristics with clinical\ncharacteristics together may help in stratifying patients with long-term\ncardiac monitoring after stroke.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:14:08 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14610","submitter":"Bryan Li","authors":"Bryan Li, Chris Callison-Burch","title":"This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language\n  Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We introduce the notion of geopolitical bias -- a tendency to report\ndifferent geopolitical knowledge depending on the linguistic context. As a case\nstudy, we consider territorial disputes between countries. For example, for the\nwidely contested Spratly Islands, would an LM be more likely to say they belong\nto China if asked in Chinese, vs. to the Philippines if asked in Tagalog? To\nevaluate if such biases exist, we first collect a dataset of territorial\ndisputes from Wikipedia, then associate each territory with a set of\nmultilingual, multiple-choice questions. This dataset, termed BorderLines,\nconsists of 250 territories with questions in 45 languages. We pose these\nquestion sets to language models, and analyze geopolitical bias in their\nresponses through several proposed quantitative metrics. The metrics compare\nbetween responses in different question languages as well as to the actual\ngeopolitical situation. The phenomenon of geopolitical bias is a uniquely\ncross-lingual evaluation, contrasting with prior work's monolingual (mostly\nEnglish) focus on bias evaluation. Its existence shows that the knowledge of\nLMs, unlike multilingual humans, is inconsistent across languages.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:16:17 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14611","submitter":"Mulong Xie","authors":"Mulong Xie, Jiaming Ye, Zhenchang Xing, Lei Ma","title":"NiCro: Purely Vision-based, Non-intrusive Cross-Device and\n  Cross-Platform GUI Testing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  To ensure app compatibility and smoothness of user experience across diverse\ndevices and platforms, developers have to perform cross-device, cross-platform\ntesting of their apps, which is laborious. There comes a recently increasing\ntrend of using a record and replay approach to facilitate the testing process.\nHowever, the graphic user interface (GUI) of an app running on different\ndevices and platforms differs dramatically. This complicates the record and\nreplay process as the presence, appearance and layout of the GUI widgets in the\nrecording phase and replaying phase can be inconsistent. Existing techniques\nresort to instrumenting into the underlying system to obtain the app metadata\nfor widget identification and matching between various devices. But such\nintrusive practices are limited by the accessibility and accuracy of the\nmetadata on different platforms. On the other hand, several recent works\nattempt to derive the GUI information by analyzing the GUI image. Nevertheless,\ntheir performance is curbed by the applied preliminary visual approaches and\nthe failure to consider the divergence of the same GUI displayed on different\ndevices. To address the challenge, we propose a non-intrusive cross-device and\ncross-platform system NiCro. NiCro utilizes the state-of-the-art GUI widget\ndetector to detect widgets from GUI images and then analyses a set of\ncomprehensive information to match the widgets across diverse devices. At the\nsystem level, NiCro can interact with a virtual device farm and a robotic arm\nsystem to perform cross-device, cross-platform testing non-intrusively. We\nfirst evaluated NiCro by comparing its multi-modal widget and GUI matching\napproach with 4 commonly used matching techniques. Then, we further examined\nits overall performance on 8 various devices, using it to record and replay 107\ntest cases of 28 popular apps and the home page to show its effectiveness.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:19:05 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14612","submitter":"Xiong Zhao","authors":"Ziyu Gong, Xiong Zhao, Chen Yang","title":"Assessment of Anterior Cruciate Ligament Injury Risk Based on Human Key\n  Points Detection Algorithm","comments":"17 pages,and 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV stat.AP","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  This paper aims to detect the potential injury risk of the anterior cruciate\nligament (ACL) by proposing an ACL potential injury risk assessment algorithm\nbased on key points of the human body detected using computer vision\ntechnology. To obtain the key points data of the human body in each frame,\nOpenPose, an open source computer vision algorithm, was employed. The obtained\ndata underwent preprocessing and were then fed into an ACL potential injury\nfeature extraction model based on the Landing Error Evaluation System (LESS).\nThis model extracted several important parameters, including the knee flexion\nangle, the trunk flexion on the sagittal plane, trunk flexion angle on the\nfrontal plane, the ankle knee horizontal distance, and the ankle shoulder\nhorizontal distance. Each of these features was assigned a threshold interval,\nand a segmented evaluation function was utilized to score them accordingly. To\ncalculate the final score of the participant, the score values were input into\na weighted scoring model designed based on the Analytic Hierarchy Process\n(AHP). The AHP based model takes into account the relative importance of each\nfeature in the overall assessment. The results demonstrate that the proposed\nalgorithm effectively detects the potential risk of ACL injury. The proposed\nalgorithm demonstrates its effectiveness in detecting ACL injury risk, offering\nvaluable insights for injury prevention and intervention strategies in sports\nand related fields. Code is available at:\nhttps://github.com/ZiyuGong-proj/Assessment-of-ACL-Injury-Risk-Based-on-Openpose\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:22:26 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14613","submitter":"Jeremy Cole","authors":"Jeremy R. Cole, Michael J.Q. Zhang, Daniel Gillick, Julian Martin\n  Eisenschlos, Bhuwan Dhingra, and Jacob Eisenstein","title":"Selectively Answering Ambiguous Questions","comments":"10 pages, 5 figures, 2 pages of appendix","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Trustworthy language models should abstain from answering questions when they\ndo not know the answer. However, the answer to a question can be unknown for a\nvariety of reasons. Prior research has focused on the case in which the\nquestion is clear and the answer is unambiguous but possibly unknown. However,\nthe answer to a question can also be unclear due to uncertainty of the\nquestioner's intent or context. We investigate question answering from this\nperspective, focusing on answering a subset of questions with a high degree of\naccuracy, from a set of questions in which many are inherently ambiguous. In\nthis setting, we find that the most reliable approach to calibration involves\nquantifying repetition within a set of sampled model outputs, rather than the\nmodel's likelihood or self-verification as used in prior work. % We find this\nto be the case across different types of uncertainty, varying model scales and\nboth with or without instruction tuning. Our results suggest that\nsampling-based confidence scores help calibrate answers to relatively\nunambiguous questions, with more dramatic improvements on ambiguous questions.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:25:38 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14614","submitter":"Joseph M. Hellerstein","authors":"Joseph M. Hellerstein, Shadaj Laddad, Mae Milano, Conor Power, Mingwei\n  Samuel","title":"Invited Paper: Initial Steps Toward a Compiler for Distributed Programs","comments":null,"journal-ref":"The 5th workshop on Advanced tools, program- ming languages, and\n  PLatforms for Implementing and Evaluating algorithms for Distributed systems\n  (ApPLIED 2023), June 19, 2023, Orlando, FL, USA","doi":"10.1145/3584684.3597272","report-no":null,"categories":"cs.DC cs.DB cs.PL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In the Hydro project we are designing a compiler toolkit that can optimize\nfor the concerns of distributed systems, including scale-up and scale-down,\navailability, and consistency of outcomes across replicas. This invited paper\noverviews the project, and provides an early walk-through of the kind of\noptimization that is possible. We illustrate how type transformations as well\nas local program transformations can combine, step by step, to convert a\nsingle-node program into a variety of distributed design points that offer the\nsame semantics with different performance and deployment characteristics.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:28:01 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14615","submitter":"Hyungsung Yun","authors":"Ki-Ahm Lee and Hyungsung Yun","title":"Boundary Regularity for viscosity solutions of Fully nonlinear\n  degenerate/singular parabolic equations","comments":"30 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper, we establish the boundary regularity results for viscosity\nsolutions of fully nonlinear degenerate/singular parabolic equations of the\nform $$u_t - x_n^{\\gamma} F(D^2 u,x,t) = f,$$ where $\\gamma<1$. These equations\nare motivated by the porous media type equations. We show the boundary\n$C^{1,\\alpha}$-regularity of functions in their solutions class and the\nboundary $C^{2,\\alpha}$-regularity of solutions. As an application, we derive\nthe global regularity results and the solvability of the Cauchy-Dirichlet\nproblems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:29:06 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14616","submitter":"Pin-Er Chen","authors":"Pin-Er Chen, Hsin-Yu Chou, Po-Ya Angela Wang, Yu-Hsiang Tseng, Shu-Kai\n  Hsieh","title":"Exploring the Grounding Issues in Image Caption","comments":"10 pages, 10 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper explores the grounding issue concerning multimodal semantic\nrepresentation from a computational cognitive-linguistic view. Five perceptual\nproperties of groundedness are annotated and analyzed: Affordance, Perceptual\nsalience, Object number, Gaze cueing, and Ecological Niche Association (ENA).\nWe annotated selected images from the Flickr30k dataset with exploratory\nanalyses and statistical modeling of their captions. Our findings suggest that\na comprehensive understanding of an object or event requires cognitive\nattention, semantic distinctions in linguistic expression, and multimodal\nconstruction. During this construction process, viewers integrate situated\nmeaning and affordance into multimodal semantics, which is consolidated into\nimage captions used in the image-text dataset incorporating visual and textual\nelements. Our findings suggest that situated meaning and affordance grounding\nare critical for grounded natural language understanding systems to generate\nappropriate responses and show the potential to advance the understanding of\nhuman construal in diverse situations.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:30:50 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14617","submitter":"Sahithya Ravi","authors":"Sahithya Ravi, Raymond Ng, Vered Shwartz","title":"COMET-M: Reasoning about Multiple Events in Complex Sentences","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Understanding the speaker's intended meaning often involves drawing\ncommonsense inferences to reason about what is not stated explicitly. In\nmulti-event sentences, it requires understanding the relationships between\nevents based on contextual knowledge. We propose COMET-M (Multi-Event), an\nevent-centric commonsense model capable of generating commonsense inferences\nfor a target event within a complex sentence. COMET-M builds upon COMET\n(Bosselut et al., 2019), which excels at generating event-centric inferences\nfor simple sentences, but struggles with the complexity of multi-event\nsentences prevalent in natural text. To overcome this limitation, we curate a\nmulti-event inference dataset of 35K human-written inferences. We trained\nCOMET-M on the human-written inferences and also created baselines using\nautomatically labeled examples. Experimental results demonstrate the\nsignificant performance improvement of COMET-M over COMET in generating\nmulti-event inferences. Moreover, COMET-M successfully produces distinct\ninferences for each target event, taking the complete context into\nconsideration. COMET-M holds promise for downstream tasks involving natural\ntext such as coreference resolution, dialogue, and story understanding.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:35:01 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14618","submitter":"Wenting Zhao","authors":"Wenting Zhao and Justin T. Chiu and Claire Cardie and Alexander M.\n  Rush","title":"Abductive Commonsense Reasoning Exploiting Mutually Exclusive\n  Explanations","comments":"accepted at ACL'23","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Abductive reasoning aims to find plausible explanations for an event. This\nstyle of reasoning is critical for commonsense tasks where there are often\nmultiple plausible explanations. Existing approaches for abductive reasoning in\nnatural language processing (NLP) often rely on manually generated annotations\nfor supervision; however, such annotations can be subjective and biased.\nInstead of using direct supervision, this work proposes an approach for\nabductive commonsense reasoning that exploits the fact that only a subset of\nexplanations is correct for a given context. The method uses posterior\nregularization to enforce a mutual exclusion constraint, encouraging the model\nto learn the distinction between fluent explanations and plausible ones. We\nevaluate our approach on a diverse set of abductive reasoning datasets;\nexperimental results show that our approach outperforms or is comparable to\ndirectly applying pretrained language models in a zero-shot manner and other\nknowledge-augmented zero-shot methods.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:35:10 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14619","submitter":"Satoru Hayami","authors":"Satoru Hayami, Ryota Yambe","title":"Field-direction-dependent skyrmion crystals in noncentrosymmetric cubic\n  magnets: A comparison between point groups $(O,T)$ and $T_{\\rm d}$","comments":"11 pages, 11 figures, accepted for publication in PRB","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.str-el","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We investigate the instability toward a skyrmion crystal (SkX) in\nnoncentrosymmetric cubic magnets with an emphasis on a comparison between point\ngroups $(O,T)$ and $T_{\\rm d}$. By constructing low-temperature magnetic phase\ndiagrams under an external magnetic field for three directions based on\nnumerically simulated annealing, we find that the system under the point group\n$(O,T)$ exhibits different two types of SkXs depending on the field direction,\nwhile that under $T_{\\rm d}$ does not show such an instability. The difference\nbetween them is understood from the difference in the momentum-dependent\nDzyaloshinskii-Moriya interaction under each point group. Meanwhile, we show\nthat the system under $T_{\\rm d}$ leads to the SkX instability by considering\nan additional effect of the uniaxial strain, which lowers the symmetry to\n$D_{\\rm 2d}$. We obtain two different SkXs: N\\'eel-type and anti-type SkXs, the\nformer of which is stabilized in the presence of the interactions at the\nthree-dimensional ordering wave vectors. The present results provide rich\ntopological spin textures in the three-dimensional systems, which are sensitive\nto the magnetic-field direction and point-group symmetry.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:36:36 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14620","submitter":"ChongYang Liu","authors":"ChongYang Liu, XiaoMin Shen, Bin Zhou, Jun Gao","title":"Automated calculation of Jet fragmentation at NLO in QCD","comments":"35 pages, 15 figures, 3 tables; comments are welcome","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph hep-ex nucl-th","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We present FMNLO, a framework to combine general-purpose Monte Carlo\ngenerators and fragmentation functions (FFs). It is based on a hybrid scheme of\nphase-space slicing method and local subtraction method, and accurate to\nnext-to-leading order (NLO) in QCD. The new framework has been interfaced to\nMG5 aMC@NLO and made publicly available in this work. We demonstrate its unique\nability by giving theoretical predictions of various fragmentation measurements\nat the LHC, followed by comparison with the data. With the help of\ninterpolation techniques, FMNLO allows for fast calculation of fragmentation\nprocesses for a large number of different FFs, which makes it a promising tool\nfor future fits of FFs. As an example, we perform a NLO fit of parton\nfragmentation functions to unidentified charged hadrons using measurements at\nthe LHC. We find the ATLAS data from inclusive dijet production show a strong\nconstraining power. Notable disparities are found between our gluon FF and that\nof BKK, DSS and NNFF, indicating the necessities of additional constraints and\ndata for gluon fragmentation function.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:37:57 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14621","submitter":"Setareh Dabiri","authors":"Setareh Dabiri, Vasileios Lioutas, Berend Zwartsenberg, Yunpeng Liu,\n  Matthew Niedoba, Xiaoxuan Liang, Dylan Green, Justice Sefas, Jonathan Wilder\n  Lavington, Frank Wood, Adam Scibior","title":"Realistically distributing object placements in synthetic training data\n  improves the performance of vision-based object detection models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  When training object detection models on synthetic data, it is important to\nmake the distribution of synthetic data as close as possible to the\ndistribution of real data. We investigate specifically the impact of object\nplacement distribution, keeping all other aspects of synthetic data fixed. Our\nexperiment, training a 3D vehicle detection model in CARLA and testing on\nKITTI, demonstrates a substantial improvement resulting from improving the\nobject placement distribution.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:39:41 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14622","submitter":"Debaditya Shome","authors":"Debaditya Shome, Kuldeep Yadav","title":"EXnet: Efficient In-context Learning for Data-less Text classification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Large pre-trained language models (PLMs) have made significant progress in\nencoding world knowledge and spawned a new set of learning paradigms including\nzero-shot, few-shot, and in-context learning. Many language tasks can be\nmodeled as a set of prompts (for example, is this text about geography?) and\nlanguage models can provide binary answers, i.e., Yes or No. There is evidence\nto suggest that the next-word prediction used by many PLMs does not align well\nwith zero-shot paradigms. Therefore, PLMs are fine-tuned as a\nquestion-answering system. In-context learning extends zero-shot learning by\nincorporating prompts and examples, resulting in increased task accuracy. Our\npaper presents EXnet, a model specifically designed to perform in-context\nlearning without any limitations on the number of examples. We argue that\nin-context learning is an effective method to increase task accuracy, and\nproviding examples facilitates cross-task generalization, especially when it\ncomes to text classification tasks. With extensive experiments, we show that\neven our smallest model (15M parameters) generalizes to several unseen\nclassification tasks and domains.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:40:57 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14623","submitter":"Miaoran Li","authors":"Miaoran Li, Baolin Peng, Zhu Zhang","title":"Self-Checker: Plug-and-Play Modules for Fact-Checking with Large\n  Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  Fact-checking is an essential task in NLP that is commonly utilized for\nvalidating the factual accuracy of claims. Prior work has mainly focused on\nfine-tuning pre-trained languages models on specific datasets, which can be\ncomputationally intensive and time-consuming. With the rapid development of\nlarge language models (LLMs), such as ChatGPT and GPT-3, researchers are now\nexploring their in-context learning capabilities for a wide range of tasks. In\nthis paper, we aim to assess the capacity of LLMs for fact-checking by\nintroducing Self-Checker, a framework comprising a set of plug-and-play modules\nthat facilitate fact-checking by purely prompting LLMs in an almost zero-shot\nsetting. This framework provides a fast and efficient way to construct\nfact-checking systems in low-resource environments. Empirical results\ndemonstrate the potential of Self-Checker in utilizing LLMs for fact-checking.\nHowever, there is still significant room for improvement compared to SOTA\nfine-tuned models, which suggests that LLM adoption could be a promising\napproach for future fact-checking research.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:46:07 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14624","submitter":"Prashant Singh Dr","authors":"Ashish Kumar, Prashant Singh, Manoj K. Harbola","title":"Density Functional Theory of Material Design$:$ Fundamentals and\n  Applications$-II$","comments":"41 pages, 4 figures, 4 table","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This is the second and the final part of the review on density functional\ntheory (DFT), referred to as DFT-II. In the first review, DFT-I, we have\ndiscussed wavefunction-based methods, their complexity, and the basic of\ndensity functional theory. In DFT-II, we focus on fundamentals of DFT and their\nimplications for the betterment of the theory. We start our presentation with\nthe exact DFT result followed by the concept of exchange-correlation (xc) or\nFermi-Coulomb hole and its relation with xc energy functional. We also provide\nthe exact conditions for the xc-hole, xc-energy and xc-potential along with\ntheir physical interpretation. Next, we describe the extension of DFT for\nnon-integer numbers of electrons, the piecewise linearity of total energy and\ndiscontinuity of chemical potential at integer particle numbers, and derivative\ndiscontinuity of the xc potential, which has consequences on fundamental gap of\nsolids. After that, we present how one obtain more accurate xc energy\nfunctionals by going beyond LDA. We discuss the gradient expansion\napproximation (GEA), generalized gradient approximation (GGA), and hybrid\nfunctional approaches to designing better xc energy functionals that give\naccurate total energies but fail to predict properties like the ionization\npotential and the band gap. Thus, we describe different methods of modeling\nthese potentials and the results of their application for the calculation of\nthe band gaps of different solids to highlight accuracy of different xc\npotential. Finally, we conclude with a glimpse on orbital-free density\nfunctional theory and the machine learning approach .\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:48:26 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14625","submitter":"Shufan Wang","authors":"Shufan Wang, Yixiao Song, Andrew Drozdov, Aparna Garimella, Varun\n  Manjunatha, Mohit Iyyer","title":"KNN-LM Does Not Improve Open-ended Text Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper, we study the generation quality of interpolation-based\nretrieval-augmented language models (LMs). These methods, best exemplified by\nthe KNN-LM, interpolate the LM's predicted distribution of the next word with a\ndistribution formed from the most relevant retrievals for a given prefix. While\nthe KNN-LM and related methods yield impressive decreases in perplexity, we\ndiscover that they do not exhibit corresponding improvements in open-ended\ngeneration quality, as measured by both automatic evaluation metrics (e.g.,\nMAUVE) and human evaluations. Digging deeper, we find that interpolating with a\nretrieval distribution actually increases perplexity compared to a baseline\nTransformer LM for the majority of tokens in the WikiText-103 test set, even\nthough the overall perplexity is lower due to a smaller number of tokens for\nwhich perplexity dramatically decreases after interpolation. However, when\ndecoding a long sequence at inference time, significant improvements on this\nsmaller subset of tokens are washed out by slightly worse predictions on most\ntokens. Furthermore, we discover that the entropy of the retrieval distribution\nincreases faster than that of the base LM as the generated sequence becomes\nlonger, which indicates that retrieval is less reliable when using\nmodel-generated text as queries (i.e., is subject to exposure bias). We hope\nthat our analysis spurs future work on improved decoding algorithms and\ninterpolation strategies for retrieval-augmented language models.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:48:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14626","submitter":"Alexis Virelizier","authors":"Francesco Costantino, Nathan Geer, Bertrand Patureau-Mirand, Alexis\n  Virelizier","title":"Chromatic maps for finite tensor categories","comments":"14 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.QA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Chromatic maps for spherical tensor categories are instrumental tools to\nconstruct (non semisimple) invariants of 3-manifolds and their extension to\n(non compact) (2+1)-TQFTs. In this paper, we introduce left and right chromatic\nmaps for finite tensor categories and prove that such maps always exist. As a\ncorollary, we obtain that any spherical finite tensor category has a chromatic\nmap.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:53:26 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14627","submitter":"Tianyu Gao","authors":"Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen","title":"Enabling Large Language Models to Generate Text with Citations","comments":"Code and data are available at https://github.com/princeton-nlp/ALCE","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.IR cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large language models (LLMs) have emerged as a widely-used tool for\ninformation seeking, but their generated outputs are prone to hallucination. In\nthis work, we aim to enable LLMs to generate text with citations, improving\ntheir factual correctness and verifiability. Existing work mainly relies on\ncommercial search engines and human evaluation, making it challenging to\nreproduce and compare with different modeling approaches. We propose ALCE, the\nfirst benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a\ndiverse set of questions and retrieval corpora and requires building end-to-end\nsystems to retrieve supporting evidence and generate answers with citations. We\nbuild automatic metrics along three dimensions -- fluency, correctness, and\ncitation quality -- and demonstrate their strong correlation with human\njudgements. Our experiments with state-of-the-art LLMs and novel prompting\nstrategies show that current systems have considerable room for improvements --\nfor example, on the ELI5 dataset, even the best model has 49% of its\ngenerations lacking complete citation support. Our extensive analyses further\nhighlight promising future directions, including developing better retrievers,\nadvancing long-context LLMs, and improving the ability to synthesize\ninformation from multiple sources.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:53:49 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14628","submitter":"Chenglei Si","authors":"Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, Jordan\n  Boyd-Graber","title":"Mixture of Prompt Experts for Generalizable and Interpretable Question\n  Answering","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  One of the ultimate quests of question answering (QA) is to deploy a system\nthat can answer any type of question from the users, and refrain from answering\nwhen it does not know the answer. While recent advancements in scaling large\nlanguage models (LLMs) brought significant improvements on various QA datasets,\nit remains difficult for a single model to generalize across question types\nthat require distinct reasoning abilities. In this paper, we first provide\nempirical evidence that state-of-the-art LLMs such as Codex suffer from poor\ngeneralizability on question types beyond those seen in the prompt. To address\nthis, we propose a Mixture-of-Prompt-Experts (MOPE) system that ensembles\nmultiple specialized LLMs. We first implement each specialized model based on\nthe same backbone model (Codex) but with prompts optimized for different\nreasoning categories including factual, multihop, mathematical, and commonsense\nreasoning. By strategically selecting the best specialized model for each given\nquestion, our MOPE system significantly outperforms any single specialized\nmodel on a collection of 12 QA datasets from four reasoning types. Moreover,\nthe attribution and agreement among specialized expert models offer greater\ninterpretability, allowing for better selective question answering. Our human\nstudy further confirms that presenting the expert predictions and answer\nselection process helps annotators more accurately decide when to trust the\nsystem's output. We release all code and data to facilitate future work.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:00:51 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14629","submitter":"Zhesi Shen","authors":"Zhesi Shen, Liying Yang, Jinshan Wu","title":"Two indicators rule them all: Mean and standard deviation used to\n  calculate other journal indicators based on log-normal distribution of\n  citation counts","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Two journal-level indicators, respectively the mean ($m^i$) and the standard\ndeviation ($v^i$) are proposed to be the core indicators of each journal and we\nshow that quite several other indicators can be calculated from those two core\nindicators, assuming that yearly citation counts of papers in each journal\nfollows more or less a log-normal distribution. Those other journal-level\nindicators include journal h index, journal one-by-one-sample comparison\ncitation success index $S_j^i$, journal multiple-sample $K^i-K^j$ comparison\nsuccess rate $S_{j,K^j}^{i,K^i }$, and minimum representative sizes\n$\\kappa_j^i$ and $\\kappa_i^j$, the average ranking of all papers in a journal\nin a set of journals($R^t$). We find that those indicators are consistent with\nthose calculated directly using the raw citation data\n($C^i=\\{c_1^i,c_2^i,\\dots,c_{N^i}^i \\},\\forall i$) of journals. In addition to\nits theoretical significance, the ability to estimate other indicators from\ncore indicators has practical implications. This feature enables individuals\nwho lack access to raw citation count data to utilize other indicators by\nsimply using core indicators, which are typically easily accessible.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:02:05 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14630","submitter":"Sam Musker","authors":"Sam Musker, Ellie Pavlick","title":"Testing Causal Models of Word Meaning in GPT-3 and -4","comments":"Unabridged version. Code available at\n  https://github.com/smusker/Causal_Models_Of_Word_Meaning","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Large Language Models (LLMs) have driven extraordinary improvements in NLP.\nHowever, it is unclear how such models represent lexical concepts-i.e., the\nmeanings of the words they use. This paper evaluates the lexical\nrepresentations of GPT-3 and GPT-4 through the lens of HIPE theory, a theory of\nconcept representations which focuses on representations of words describing\nartifacts (such as \"mop\", \"pencil\", and \"whistle\"). The theory posits a causal\ngraph that relates the meanings of such words to the form, use, and history of\nthe objects to which they refer. We test LLMs using the same stimuli originally\nused by Chaigneau et al. (2004) to evaluate the theory in humans, and consider\na variety of prompt designs. Our experiments concern judgements about causal\noutcomes, object function, and object naming. We find no evidence that GPT-3\nencodes the causal structure hypothesized by HIPE, but do find evidence that\nGPT-4 encodes such structure. The results contribute to a growing body of\nresearch characterizing the representational capacity of large language models.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:03:23 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14631","submitter":"Ying Liu","authors":"BESIII Collaboration: M. Ablikim, M. N. Achasov, P. Adlarson, R.\n  Aliberti, A. Amoroso, M. R. An, Q. An, Y. Bai, O. Bakina, I. Balossino, Y.\n  Ban, V. Batozskaya, K. Begzsuren, N. Berger, M. Berlowski, M. Bertani, D.\n  Bettoni, F. Bianchi, E. Bianco, J. Bloms, A. Bortone, I. Boyko, R. A. Briere,\n  A. Brueggemann, H. Cai, X. Cai, A. Calcaterra, G. F. Cao, N. Cao, S. A.\n  Cetin, J. F. Chang, T. T. Chang, W. L. Chang, G. R. Che, G. Chelkov, C. Chen,\n  Chao Chen, G. Chen, H. S. Chen, M. L. Chen, S. J. Chen, S. M. Chen, T. Chen,\n  X. R. Chen, X. T. Chen, Y. B. Chen, Y. Q. Chen, Z. J. Chen, W. S. Cheng, S.\n  K. Choi, X. Chu, G. Cibinetto, S. C. Coen, F. Cossio, J. J. Cui, H. L. Dai,\n  J. P. Dai, A. Dbeyssi, R. E. de Boer, D. Dedovich, Z. Y. Deng, A. Denig, I.\n  Denysenko, M. Destefanis, F. De Mori, B. Ding, X. X. Ding, Y. Ding, Y. Ding,\n  J. Dong, L. Y. Dong, M. Y. Dong, X. Dong, S. X. Du, Z. H. Duan, P. Egorov,\n  Y.H. Y. Fan, Y. L. Fan, J. Fang, S. S. Fang, W. X. Fang, Y. Fang, R.\n  Farinelli, L. Fava, F. Feldbauer, G. Felici, C. Q. Feng, J. H. Feng, K\n  Fischer, M. Fritsch, C. Fritzsch, C. D. Fu, Y. W. Fu, H. Gao, Y. N. Gao, Yang\n  Gao, S. Garbolino, I. Garzia, P. T. Ge, Z. W. Ge, C. Geng, E. M. Gersabeck, A\n  Gilman, K. Goetzen, L. Gong, W. X. Gong, W. Gradl, S. Gramigna, M. Greco, M.\n  H. Gu, Y. T. Gu, C. Y Guan, Z. L. Guan, A. Q. Guo, L. B. Guo, M. J. Guo, R.\n  P. Guo, Y. P. Guo, A. Guskov, T. T. Han, W. Y. Han, X. Q. Hao, F. A. Harris,\n  K. K. He, K. L. He, F. H. H. Heinsius, C. H. Heinz, Y. K. Heng, C. Herold, T.\n  Holtmann, P. C. Hong, G. Y. Hou, X. T. Hou, Y. R. Hou, Z. L. Hou, H. M. Hu,\n  J. F. Hu, T. Hu, Y. Hu, G. S. Huang, K. X. Huang, L. Q. Huang, X. T. Huang,\n  Y. P. Huang, T. Hussain, N H\\\"usken, W. Imoehl, J. Jackson, S. Jaeger, S.\n  Janchiv, J. H. Jeong, Q. Ji, Q. P. Ji, X. B. Ji, X. L. Ji, Y. Y. Ji, X. Q.\n  Jia, Z. K. Jia, P. C. Jiang, S. S. Jiang, T. J. Jiang, X. S. Jiang, Y. Jiang,\n  J. B. Jiao, Z. Jiao, S. Jin, Y. Jin, M. Q. Jing, T. Johansson, X. K., S.\n  Kabana, N. Kalantar-Nayestanaki, X. L. Kang, X. S. Kang, R. Kappert, M.\n  Kavatsyuk, B. C. Ke, A. Khoukaz, R. Kiuchi, R. Kliemt, O. B. Kolcu, B. Kopf,\n  M. Kuessner, A. Kupsc, W. K\\\"uhn, J. J. Lane, P. Larin, A. Lavania, L.\n  Lavezzi, T. T. Lei, Z. H. Lei, H. Leithoff, M. Lellmann, T. Lenz, C. Li, C.\n  Li, C. H. Li, Cheng Li, D. M. Li, F. Li, G. Li, H. Li, H. B. Li, H. J. Li, H.\n  N. Li, Hui Li, J. R. Li, J. S. Li, J. W. Li, K. L. Li, Ke Li, L. J Li, L. K.\n  Li, Lei Li, M. H. Li, P. R. Li, Q. X. Li, S. X. Li, T. Li, W. D. Li, W. G.\n  Li, X. H. Li, X. L. Li, Xiaoyu Li, Y. G. Li, Z. J. Li, Z. X. Li, Z. Y. Li, C.\n  Liang, H. Liang, H. Liang, H. Liang, Y. F. Liang, Y. T. Liang, G. R. Liao, L.\n  Z. Liao, J. Libby, A. Limphirat, D. X. Lin, T. Lin, B. J. Liu, B. X. Liu, C.\n  Liu, C. X. Liu, D. Liu, F. H. Liu, Fang Liu, Feng Liu, G. M. Liu, H. Liu, H.\n  B. Liu, H. M. Liu, Huanhuan Liu, Huihui Liu, J. B. Liu, J. L. Liu, J. Y. Liu,\n  K. Liu, K. Y. Liu, Ke Liu, L. Liu, L. C. Liu, Lu Liu, M. H. Liu, P. L. Liu,\n  Q. Liu, S. B. Liu, T. Liu, W. K. Liu, W. M. Liu, X. Liu, Y. Liu, Y. B. Liu,\n  Z. A. Liu, Z. Q. Liu, X. C. Lou, F. X. Lu, H. J. Lu, J. G. Lu, X. L. Lu, Y.\n  Lu, Y. P. Lu, Z. H. Lu, C. L. Luo, M. X. Luo, T. Luo, X. L. Luo, X. R. Lyu,\n  Y. F. Lyu, F. C. Ma, H. L. Ma, J. L. Ma, L. L. Ma, M. M. Ma, Q. M. Ma, R. Q.\n  Ma, R. T. Ma, X. Y. Ma, Y. Ma, Y. M. Ma, F. E. Maas, M. Maggiora, S.\n  Maldaner, S. Malde, Q. A. Malik, A. Mangoni, Y. J. Mao, Z. P. Mao, S.\n  Marcello, Z. X. Meng, J. G. Messchendorp, G. Mezzadri, H. Miao, T. J. Min, R.\n  E. Mitchell, X. H. Mo, N. Yu. Muchnoi, Y. Nefedov, F. Nerling, I. B.\n  Nikolaev, Z. Ning, S. Nisar, Y. Niu, S. L. Olsen, Q. Ouyang, S. Pacetti, X.\n  Pan, Y. Pan, A. Pathak, P. Patteri, Y. P. Pei, M. Pelizaeus, H. P. Peng, K.\n  Peters, J. L. Ping, R. G. Ping, S. Plura, S. Pogodin, V. Prasad, F. Z. Qi, H.\n  Qi, H. R. Qi, M. Qi, T. Y. Qi, S. Qian, W. B. Qian, C. F. Qiao, J. J. Qin, L.\n  Q. Qin, X. P. Qin, X. S. Qin, Z. H. Qin, J. F. Qiu, S. Q. Qu, C. F. Redmer,\n  K. J. Ren, A. Rivetti, V. Rodin, M. Rolo, G. Rong, Ch. Rosner, S. N. Ruan, N.\n  Salone, A. Sarantsev, Y. Schelhaas, K. Schoenning, M. Scodeggio, K. Y. Shan,\n  W. Shan, X. Y. Shan, J. F. Shangguan, L. G. Shao, M. Shao, C. P. Shen, H. F.\n  Shen, W. H. Shen, X. Y. Shen, B. A. Shi, H. C. Shi, J. L. Shi, J. Y. Shi, Q.\n  Q. Shi, R. S. Shi, X. Shi, J. J. Song, T. Z. Song, W. M. Song, Y. J. Song, Y.\n  X. Song, S. Sosio, S. Spataro, F. Stieler, Y. J. Su, G. B. Sun, G. X. Sun, H.\n  Sun, H. K. Sun, J. F. Sun, K. Sun, L. Sun, S. S. Sun, T. Sun, W. Y. Sun, Y.\n  Sun, Y. J. Sun, Y. Z. Sun, Z. T. Sun, Y. X. Tan, C. J. Tang, G. Y. Tang, J.\n  Tang, Y. A. Tang, L. Y Tao, Q. T. Tao, M. Tat, J. X. Teng, V. Thoren, W. H.\n  Tian, W. H. Tian, Y. Tian, Z. F. Tian, I. Uman, S. J. Wang, B. Wang, B. L.\n  Wang, Bo Wang, C. W. Wang, D. Y. Wang, F. Wang, H. J. Wang, H. P. Wang, J. P.\n  Wang, K. Wang, L. L. Wang, M. Wang, Meng Wang, S. Wang, S. Wang, T. Wang, T.\n  J. Wang, W. Wang, W. Wang, W. H. Wang, W. P. Wang, X. Wang, X. F. Wang, X. J.\n  Wang, X. L. Wang, Y. Wang, Y. D. Wang, Y. F. Wang, Y. H. Wang, Y. N. Wang, Y.\n  Q. Wang, Yaqian Wang, Yi Wang, Z. Wang, Z. L. Wang, Z. Y. Wang, Ziyi Wang, D.\n  Wei, D. H. Wei, F. Weidner, S. P. Wen, C. W. Wenzel, U. Wiedner, G.\n  Wilkinson, M. Wolke, L. Wollenberg, C. Wu, J. F. Wu, L. H. Wu, L. J. Wu, X.\n  Wu, X. H. Wu, Y. Wu, Y. J. Wu, Z. Wu, L. Xia, X. M. Xian, T. Xiang, D. Xiao,\n  G. Y. Xiao, H. Xiao, S. Y. Xiao, Y. L. Xiao, Z. J. Xiao, C. Xie, X. H. Xie,\n  Y. Xie, Y. G. Xie, Y. H. Xie, Z. P. Xie, T. Y. Xing, C. F. Xu, C. J. Xu, G.\n  F. Xu, H. Y. Xu, Q. J. Xu, W. Xu, W. L. Xu, X. P. Xu, Y. C. Xu, Z. P. Xu, Z.\n  S. Xu, F. Yan, L. Yan, W. B. Yan, W. C. Yan, X. Q. Yan, H. J. Yang, H. L.\n  Yang, H. X. Yang, Tao Yang, Y. Yang, Y. F. Yang, Y. X. Yang, Yifan Yang, Z.\n  W. Yang, Z. P. Yao, M. Ye, M. H. Ye, J. H. Yin, Z. Y. You, B. X. Yu, C. X.\n  Yu, G. Yu, J. S. Yu, T. Yu, X. D. Yu, C. Z. Yuan, L. Yuan, S. C. Yuan, X. Q.\n  Yuan, Y. Yuan, Z. Y. Yuan, C. X. Yue, A. A. Zafar, F. R. Zeng, X. Zeng, Y.\n  Zeng, Y. J. Zeng, X. Y. Zhai, Y. C. Zhai, Y. H. Zhan, A. Q. Zhang, B. L.\n  Zhang, B. X. Zhang, D. H. Zhang, G. Y. Zhang, H. Zhang, H. H. Zhang, H. H.\n  Zhang, H. Q. Zhang, H. Y. Zhang, J. Zhang, J. J. Zhang, J. Q. Zhang, J. W.\n  Zhang, J. X. Zhang, J. Y. Zhang, J. Z. Zhang, Jianyu Zhang, Jiawei Zhang, L.\n  M. Zhang, L. Q. Zhang, Lei Zhang, P. Zhang, Q. Y. Zhang, Shuihan Zhang,\n  Shulei Zhang, X. D. Zhang, X. M. Zhang, X. Y. Zhang, Xuyan Zhang, Y. Zhang,\n  Y. T. Zhang, Y. H. Zhang, Yan Zhang, Yao Zhang, Z. H. Zhang, Z. L. Zhang, Z.\n  Y. Zhang, Z. Y. Zhang, G. Zhao, J. Zhao, J. Y. Zhao, J. Z. Zhao, Lei Zhao,\n  Ling Zhao, M. G. Zhao, S. J. Zhao, Y. B. Zhao, Y. X. Zhao, Z. G. Zhao, A.\n  Zhemchugov, B. Zheng, J. P. Zheng, W. J. Zheng, Y. H. Zheng, B. Zhong, X.\n  Zhong, H. Zhou, L. P. Zhou, X. Zhou, X. K. Zhou, X. R. Zhou, X. Y. Zhou, Y.\n  Z. Zhou, J. Zhu, K. Zhu, K. J. Zhu, L. Zhu, L. X. Zhu, S. H. Zhu, S. Q. Zhu,\n  T. J. Zhu, W. J. Zhu, Y. C. Zhu, Z. A. Zhu, J. H. Zou, J. Zu","title":"Determination of spin and parity of $D^{*}_{(s)}$ mesons","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ex","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The spin and parity of the charmed mesons $D_{s}^{*+}$, $D^{*0}$ and $D^{*+}$\nare determined for the first time to be $J^P=1^{-}$ with significances greater\nthan 10$\\sigma$ over other hypotheses of $2^{+}$ and $3^{-}$, using an $e^+e^-$\ncollision data sample with an integrated luminosity of 3.19 fb$^{-1}$ collected\nby the BESIII detector at a center-of-mass energy of 4.178 GeV. Different\nspin-parity hypotheses for $D_{s}^{*+}$, $D^{*0}$, and $D^{*+}$ mesons are\ntested via a helicity amplitude analysis of the processes $e^+e^-\\to\nD^{*+}_{s}D^{-}_{s}$, $D^{*0}D^{0}$ and $D^{*+}D^{-}$, with $D^{*+}_{s}\\to\nD^{+}_{s} \\gamma$, $D^{*0}\\to D^{0}\\pi^{0}$, and $D^{*+}\\to D^{+}\\pi^{0}$. The\nresults confirm the quark model predictions.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:05:34 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14632","submitter":"Rishi Sonthalia","authors":"Rishi Sonthalia and Anna Seigal and Guido Montufar","title":"Supermodular Rank: Set Function Decomposition and Optimization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.CO cs.CC cs.DM cs.LG math.OC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We define the supermodular rank of a function on a lattice. This is the\nsmallest number of terms needed to decompose it into a sum of supermodular\nfunctions. The supermodular summands are defined with respect to different\npartial orders. We characterize the maximum possible value of the supermodular\nrank and describe the functions with fixed supermodular rank. We analogously\ndefine the submodular rank. We use submodular decompositions to optimize set\nfunctions. Given a bound on the submodular rank of a set function, we formulate\nan algorithm that splits an optimization problem into submodular subproblems.\nWe show that this method improves the approximation ratio guarantees of several\nalgorithms for monotone set function maximization and ratio of set functions\nminimization, at a computation overhead that depends on the submodular rank.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:09:28 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14633","submitter":"Li Luo","authors":"Weideng Cui, Li Luo and Zheming Xu","title":"Asymptotic Schur algebras and cellularity of q-Schur algebras","comments":"28 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.RT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We prove that the q-Schur algebras of finite type introduced in [LW22] are\ncellular in the sense of Graham and Lehrer, which is a generalization of Geck's\ntheorem on the cellularity of Hecke algebras of finite type. Moreover, we study\nspecial modules of the associated asymptotic Schur algebras and left cell\nrepresentations of Schur algebras, which generalize Lusztig's work about\nspecial modules of asymptotic Hecke algebras and left cell representations of\nWeyl groups, respectively.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:12:31 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14634","submitter":"Prashant Singh Dr","authors":"Prashant Singh, Manoj K Harbola","title":"Density Functional Theory of Material Design: Fundamentals and\n  Applications -- I","comments":"44 pages, 3 figures, 5 tables","journal-ref":"Oxford Open Materials Science, Volume 1, Issue 1, 2021, itab018","doi":"10.1093/oxfmat/itab018","report-no":null,"categories":"cond-mat.mtrl-sci","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This article is part-I of a review of density-functional theory (DFT) that is\nthe most widely used method for calculating electronic structure of materials.\nThe accuracy and ease of numerical implementation of DFT methods has resulted\nin its extensive use for materials design and discovery and has thus ushered in\nthe new field of computational material science. In this article we start with\nan introduction to Schr\\\"odinger equation and methods of its solutions. After\npresenting exact results for some well-known systems, difficulties encountered\nin solving the equation for interacting electrons are described. How these\ndifficulties are handled using the variational principle for the energy to\nobtain approximate solutions of the Schr\\\"odinger equation is discussed. The\nresulting Hartree and Hartree-Fock theories are presented along with results\nthey give for atomic and solid-state systems. We then describe Thomas-Fermi\ntheory and its extensions which were the initial attempts to formulate\nmany-electron problem in terms of electronic density of a system. Having\ndescribed these theories, we introduce modern density functional theory by\ndiscussing Hohenberg-Kohn theorems that form its foundations. We then go on to\ndiscuss Kohn-Sham formulation of density-functional theory in its exact form.\nNext, local density approximation is introduced and solutions of Kohn-Sham\nequation for some representative systems, obtained using the local density\napproximation, are presented. We end part-I of the review describing the\ncontents of part-II.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:12:55 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14635","submitter":"Yan Zhou","authors":"Yan Zhou, Qingkai Fang, Yang Feng","title":"CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation","comments":"ACL 2023 main conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.SD eess.AS","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  End-to-end speech translation (ST) is the task of translating speech signals\nin the source language into text in the target language. As a cross-modal task,\nend-to-end ST is difficult to train with limited data. Existing methods often\ntry to transfer knowledge from machine translation (MT), but their performances\nare restricted by the modality gap between speech and text. In this paper, we\npropose Cross-modal Mixup via Optimal Transport CMOT to overcome the modality\ngap. We find the alignment between speech and text sequences via optimal\ntransport and then mix up the sequences from different modalities at a token\nlevel using the alignment. Experiments on the MuST-C ST benchmark demonstrate\nthat CMOT achieves an average BLEU of 30.0 in 8 translation directions,\noutperforming previous methods. Further analysis shows CMOT can adaptively find\nthe alignment between modalities, which helps alleviate the modality gap\nbetween speech and text. Code is publicly available at\nhttps://github.com/ictnlp/CMOT.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:13:48 GMT"},{"version":"v2","created":"Thu, 25 May 2023 08:55:41 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.14636","submitter":"Sakander Hayat","authors":"Jack H. Koolen, Mamoon Abdullah, Brhane Gebremichel, Sakander Hayat","title":"Distance-regular graphs with exactly one positive $q$-distance\n  eigenvalue","comments":"19 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.CO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, we study the $q$-distance matrix for a distance-regular graph\nand show that the $q$-distance matrix of a distance-regular graph with\nclassical parameters ($D, q, \\alpha, \\beta$) has exactly three distinct\neigenvalues, of which one is zero. Moreover, we study distance-regular graphs\nwhose $q$-distance matrix has exactly one positive eigenvalue.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:16:19 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14637","submitter":"Davit Soselia","authors":"Davit Soselia, Khalid Saifullah, and Tianyi Zhou","title":"Reinforcement Learning finetuned Vision-Code Transformer for UI-to-Code\n  Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Automated HTML/CSS code generation from screenshots is an important yet\nchallenging problem with broad applications in website development and design.\nIn this paper, we present a novel vision-code transformer approach that\nleverages an Encoder-Decoder architecture as well as explore actor-critic\nfine-tuning as a method for improving upon the baseline. For this purpose, two\nimage encoders are compared: Vision Transformer (ViT) and Document Image\nTransformer (DiT).\n  We propose an end-to-end pipeline that can generate high-quality code\nsnippets directly from screenshots, streamlining the website creation process\nfor developers. To train and evaluate our models, we created a synthetic\ndataset of 30,000 unique pairs of code and corresponding screenshots.\n  We evaluate the performance of our approach using a combination of automated\nmetrics such as MSE, BLEU, IoU, and a novel htmlBLEU score, where our models\ndemonstrated strong performance. We establish a strong baseline with the\nDiT-GPT2 model and show that actor-critic can be used to improve IoU score from\nthe baseline of 0.64 to 0.79 and lower MSE from 12.25 to 9.02. We achieved\nsimilar performance as when using larger models, with much lower computational\ncost.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:17:32 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14638","submitter":"Nikolay Yegovtsev","authors":"Nikolay Yegovtsev and Victor Gurarie","title":"Effective mass and interaction energy of heavy Bose polarons at\n  unitarity","comments":"5 pages, 0 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.quant-gas","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We study the motion of a heavy impurity immersed in a weakly interacting BEC\nusing the Gross-Pitaevskii equation (GPe). We construct a perturbative solution\nto the GPe in powers of impurity velocity in the case when the boson-impurity\npotential is tuned to unitarity and calculate the effective mass of the\npolaron. In addition, we calculate the interaction energy of two unitary\npolarons which are sufficiently far apart. Our formalism also reproduces the\nresults for both the mass and interaction energy obtained at weak\nboson-impurity coupling.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:17:44 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14639","submitter":"Ruizhe Chen","authors":"Ruizhe Chen (1), Sanjib Basu (2), Qian Shi (3) ((1) Johns Hopkins\n  University, (2) University of Illinois Chicago, (3) Mayo Clinic)","title":"Restricted Mean Survival Time Estimation Using Bayesian Nonparametric\n  Dependent Mixture Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME stat.AP","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Restricted mean survival time (RMST) is an intuitive summary statistic for\ntime-to-event random variables, and can be used for measuring treatment\neffects. Compared to hazard ratio, its estimation procedure is robust against\nthe non-proportional hazards assumption. We propose nonparametric Bayeisan\n(BNP) estimators for RMST using a dependent stick-breaking process prior\nmixture model that adjusts for mixed-type covariates. The proposed Bayesian\nestimators can yield both group-level causal estimate and subject-level\npredictions. Besides, we propose a novel dependent stick-breaking process prior\nthat on average results in narrower credible intervals while maintaining\nsimilar coverage probability compared to a dependent probit stick-breaking\nprocess prior. We conduct simulation studies to investigate the performance of\nthe proposed BNP RMST estimators compared to existing frequentist approaches\nand under different Bayesian modeling choices. The proposed framework is\napplied to estimate the treatment effect of an immuno therapy among KRAS\nwild-type colorectal cancer patients.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:18:14 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14640","submitter":"Simin Keykhosravi","authors":"Simin Keykhosravi and Ebrahim Bedeer","title":"Pilot Design and Doubly-Selective Channel Estimation for\n  Faster-than-Nyquist Signaling","comments":"In preparation to submit to a Journal","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT eess.SP math.IT","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Being capable of enhancing the spectral efficiency (SE), faster-than-Nyquist\n(FTN) signaling is a promising approach for wireless communication systems.\nThis paper investigates the doubly-selective (i.e., time- and\nfrequency-selective) channel estimation and data detection of FTN signaling. We\nconsider the intersymbol interference (ISI) resulting from both the FTN\nsignaling and the frequency-selective channel and adopt an efficient frame\nstructure with reduced overhead. We propose a novel channel estimation\ntechnique of FTN signaling based on the least sum of squared errors (LSSE)\napproach to estimate the complex channel coefficients at the pilot locations\nwithin the frame. In particular, we find the optimal pilot sequence that\nminimizes the mean square error (MSE) of the channel estimation. To address the\ntime-selective nature of the channel, we use a low-complexity linear\ninterpolation to track the complex channel coefficients at the data symbols\nlocations within the frame. To detect the data symbols of FTN signaling, we\nadopt a turbo equalization technique based on a linear soft-input soft-output\n(SISO) minimum mean square error (MMSE) equalizer. Simulation results show that\nthe MSE of the proposed FTN signaling channel estimation employing the designed\noptimal pilot sequence is lower than its counterpart designed for conventional\nNyquist transmission. The bit error rate (BER) of the FTN signaling employing\nthe proposed optimal pilot sequence shows improvement compared to the FTN\nsignaling employing the conventional Nyquist pilot sequence. Additionally, for\nthe same SE, the proposed FTN signaling channel estimation employing the\ndesigned optimal pilot sequence shows better performance when compared to\ncompeting techniques from the literature.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:18:32 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14641","submitter":"Zhe Wang","authors":"Zhe Wang, ZhiJie He, Ding Liu","title":"Graph Analysis Using a GPU-based Parallel Algorithm: Quantum Clustering","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The article introduces a new method for applying Quantum Clustering to graph\nstructures. Quantum Clustering (QC) is a novel density-based unsupervised\nlearning method that determines cluster centers by constructing a potential\nfunction. In this method, we use the Graph Gradient Descent algorithm to find\nthe centers of clusters. GPU parallelization is utilized for computing\npotential values. We also conducted experiments on five widely used datasets\nand evaluated using four indicators. The results show superior performance of\nthe method. Finally, we discuss the influence of $\\sigma$ on the experimental\nresults.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:22:00 GMT"},{"version":"v2","created":"Sun, 28 May 2023 10:20:47 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.14642","submitter":"Lingbing Guo","authors":"Lingbing Guo, Weiqing Wang, Zhuo Chen, Ningyu Zhang, Zequn Sun, Yixuan\n  Lai, Qiang Zhang, and Huajun Chen","title":"Newton-Cotes Graph Neural Networks: On the Time Evolution of Dynamic\n  Systems","comments":"Under review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CE","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Reasoning system dynamics is one of the most important analytical approaches\nfor many scientific studies. With the initial state of a system as input, the\nrecent graph neural networks (GNNs)-based methods are capable of predicting the\nfuture state distant in time with high accuracy. Although these methods have\ndiverse designs in modeling the coordinates and interacting forces of the\nsystem, we show that they actually share a common paradigm that learns the\nintegration of the velocity over the interval between the initial and terminal\ncoordinates. However, their integrand is constant w.r.t. time. Inspired by this\nobservation, we propose a new approach to predict the integration based on\nseveral velocity estimations with Newton-Cotes formulas and prove its\neffectiveness theoretically. Extensive experiments on several benchmarks\nempirically demonstrate consistent and significant improvement compared with\nthe state-of-the-art methods.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:23:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14643","submitter":"Manabu Tsujimoto","authors":"Manabu Tsujimoto, Kaveh Delfanazari, Takanari Kashiwagi, Toshiaki\n  Hattori, and Kazuo Kadowaki","title":"Terahertz imaging system with on-chip superconducting Josephson plasma\n  emitters for nondestructive testing","comments":"6 pages, 8 figures, 1 table","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.supr-con physics.app-ph physics.optics","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Compared with adjacent microwaves and infrared frequencies, terahertz (THz)\nfrequency offers numerous advantages for imaging applications. The unique THz\nspectral signatures of chemicals allow the development of THz imaging systems\nfor nondestructive tests and the evaluation of biological objects, materials,\ncomponents, circuits, and systems, which are especially useful in the security,\nmedical, material, pharmaceutical, aeronautical, and electronics industries.\nHowever, technological advancements have been hindered owing to the lack of\npower-efficient and compact THz sources. Here, we use high-temperature\nsuperconducting monolithic sources known as Josephson plasma emitters\n(JPEs)-which are compact, chip-integrated coherent and monochromatic sources of\nbroadly tunable THz waves-and report the art of non-destructive imaging of\nconcealed metallic surgical blades, floppy disks, dandelion leaves, and slices\nof pork meat in the THz spectral range. The quality of the images, exhibiting\nhigh-contrast differentiation between metallic and non-metallic parts, making\ndifferent features of objects visible, and targeting different powders,\ndemonstrates the viability of this THz imaging system for nondestructive,\ncontactless, quick, and accurate environmental monitoring, security, medicine,\nmaterials, and quantum science and technology applications.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:23:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14644","submitter":"Hemanth Manjunatha","authors":"Hemanth Manjunatha, Andrey Pak, Dimitar Filev, Panagiotis Tsiotras","title":"KARNet: Kalman Filter Augmented Recurrent Neural Network for Learning\n  World Models in Autonomous Driving Tasks","comments":"arXiv admin note: substantial text overlap with arXiv:2205.08712","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.RO","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Autonomous driving has received a great deal of attention in the automotive\nindustry and is often seen as the future of transportation. The development of\nautonomous driving technology has been greatly accelerated by the growth of\nend-to-end machine learning techniques that have been successfully used for\nperception, planning, and control tasks. An important aspect of autonomous\ndriving planning is knowing how the environment evolves in the immediate future\nand taking appropriate actions. An autonomous driving system should effectively\nuse the information collected from the various sensors to form an abstract\nrepresentation of the world to maintain situational awareness. For this\npurpose, deep learning models can be used to learn compact latent\nrepresentations from a stream of incoming data. However, most deep learning\nmodels are trained end-to-end and do not incorporate any prior knowledge (e.g.,\nfrom physics) of the vehicle in the architecture. In this direction, many works\nhave explored physics-infused neural network (PINN) architectures to infuse\nphysics models during training. Inspired by this observation, we present a\nKalman filter augmented recurrent neural network architecture to learn the\nlatent representation of the traffic flow using front camera images only. We\ndemonstrate the efficacy of the proposed model in both imitation and\nreinforcement learning settings using both simulated and real-world datasets.\nThe results show that incorporating an explicit model of the vehicle (states\nestimated using Kalman filtering) in the end-to-end learning significantly\nincreases performance.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:27:34 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14645","submitter":"Minqian Liu","authors":"Xiaochu Li, Minqian Liu, Zhiyang Xu, Lifu Huang","title":"Iteratively Improving Biomedical Entity Linking and Event Extraction via\n  Hard Expectation-Maximization","comments":"15 pages, 3 figures, 10 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Biomedical entity linking and event extraction are two crucial tasks to\nsupport text understanding and retrieval in the biomedical domain. These two\ntasks intrinsically benefit each other: entity linking disambiguates the\nbiomedical concepts by referring to external knowledge bases and the domain\nknowledge further provides additional clues to understand and extract the\nbiological processes, while event extraction identifies a key trigger and\nentities involved to describe each biological process which also captures the\nstructural context to better disambiguate the biomedical entities. However,\nprevious research typically solves these two tasks separately or in a pipeline,\nleading to error propagation. What's more, it's even more challenging to solve\nthese two tasks together as there is no existing dataset that contains\nannotations for both tasks. To solve these challenges, we propose joint\nbiomedical entity linking and event extraction by regarding the event\nstructures and entity references in knowledge bases as latent variables and\nupdating the two task-specific models in a hard Expectation-Maximization (EM)\nfashion: (1) predicting the missing variables for each partially annotated\ndataset based on the current two task-specific models, and (2) updating the\nparameters of each model on the corresponding pseudo completed dataset.\nExperimental results on two benchmark datasets: Genia 2011 for event extraction\nand BC4GO for entity linking, show that our joint framework significantly\nimproves the model for each individual task and outperforms the strong\nbaselines for both tasks. We will make the code and model checkpoints publicly\navailable once the paper is accepted.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:30:31 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14646","submitter":"Wuhyun Sohn","authors":"Wuhyun Sohn, James R. Fergusson, E. P. S. Shellard","title":"High-resolution CMB bispectrum estimator with flexible modal basis","comments":"35 pages, 12 figures, public codes at\n  https://github.com/Wuhyun/CMB-BEST and https://github.com/Wuhyun/Tetraquad","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.CO","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  We present a new independent pipeline for the CMB bispectrum estimation of\nprimordial non-Gaussianity and release a public code for constraining\nbispectrum shapes of interest based on the Planck 2018 temperature and\npolarization data. The estimator combines the strengths of the conventional KSW\nand Modal estimators at the cost of increased computational complexity, which\nhas been made manageable through intensive algorithmic and implementation\noptimization. We also detail some methodological advances in numerical\nintegration over a tetrapyd - domain where the bispectrum is defined on - via\nnew quadrature rules. The pipeline has been validated both internally and\nagainst Planck. As a proof-of-concept example, we constrain some highly\noscillatory models that were out of reach in conventional analyses using a\ntargeted basis with a fixed oscillation frequency, and no significant evidence\nfor primordial non-Gaussianity of these shapes is found. The methodology and\ncode developed in this work will be directly applicable to future surveys where\nwe expect a notable boost in sensitivity.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:33:34 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14647","submitter":"Qi Zeng","authors":"Qi Zeng, Mankeerat Sidhu, Hou Pong Chan, Lu Wang, Heng Ji","title":"Meta-review Generation with Checklist-guided Iterative Introspection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Opinions in the scientific domain can be divergent, leading to controversy or\nconsensus among reviewers. However, current opinion summarization datasets\nmostly focus on product review domains, which do not account for this\nvariability under the assumption that the input opinions are non-controversial.\nTo address this gap, we propose the task of scientific opinion summarization,\nwhere research paper reviews are synthesized into meta-reviews. To facilitate\nthis task, we introduce a new ORSUM dataset covering 10,989 paper meta-reviews\nand 40,903 paper reviews from 39 conferences. Furthermore, we propose the\nChecklist-guided Iterative Introspection (CGI$^2$) approach, which breaks down\nthe task into several stages and iteratively refines the summary under the\nguidance of questions from a checklist. We conclude that (1) human-written\nsummaries are not always reliable since many do not follow the guideline, and\n(2) the combination of task decomposition and iterative self-refinement shows\npromising discussion involvement ability and can be applied to other complex\ntext generation using black-box LLM.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:33:35 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14648","submitter":"Paul Crowell","authors":"James M. Etheridge, Joseph Dill, Connor P. Dempsey, Mihir Pendharkar,\n  Javier Garcia-Barriocanal, Guichuan Yu, Vlad S. Pribiag, Paul A. Crowell,\n  Chris J. Palmstr{\\o}m","title":"Competing Uniaxial Anisotropies in Epitaxial Fe Thin Films Grown on\n  InAs(001)","comments":"5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We report on the interplay of two uniaxial magnetic anisotropies in epitaxial\nFe thin films of varying thickness grown on InAs(001) as observed in\nferromagnetic resonance experiments. One anisotropy originates from the Fe/InAs\ninterface while the other originates from in-plane shear strain resulting from\nthe anisotropic relaxation of the Fe film. X-ray diffraction was used to\nmeasure the in-plane lattice constants of the Fe films, confirming the\ncorrelation between the onset of film relaxation and the corresponding shear\nstrain inferred from ferromagnetic resonance data. These results are relevant\nfor ongoing efforts to develop spintronic and quantum devices utilizing large\nspin-orbit coupling in III-V semiconductors.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:34:41 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14649","submitter":"Yushu Chen","authors":"Yushu Chen, Shengzhuo Liu, Jinzhe Yang, Hao Jing, Wenlai Zhao, and\n  Guangwen Yang","title":"A Joint Time-frequency Domain Transformer for Multivariate Time Series\n  Forecasting","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  To enhance predicting performance while minimizing computational demands,\nthis paper introduces a joint time-frequency domain Transformer (JTFT) for\nmultivariate forecasting. The method exploits the sparsity of time series in\nthe frequency domain using a small number of learnable frequencies to extract\ntemporal dependencies effectively. Alongside the frequency domain\nrepresentation, a fixed number of the most recent data points are directly\nencoded in the time domain, bolstering the learning of local relationships and\nmitigating the adverse effects of non-stationarity. JTFT achieves linear\ncomplexity since the length of the internal representation remains independent\nof the input sequence length. Additionally, a low-rank attention layer is\nproposed to efficiently capture cross-dimensional dependencies and prevent\nperformance degradation due to the entanglement of temporal and channel-wise\nmodeling. Experiments conducted on six real-world datasets demonstrate that\nJTFT outperforms state-of-the-art methods.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:37:23 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14650","submitter":"Yuri Sales","authors":"Yuri S. Ribeiro, Fazal E-Asim, Andr\\'e L.F de Almeida, Behrooz Makki,\n  Gabor Fodor","title":"Low-Complexity Joint Active and Passive Beamforming Design for\n  IRS-Assisted MIMO","comments":"5 pages, 3 figures, submited to wireless communication letters (under\n  review)","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  In this letter, we consider an intelligent reflecting surface (IRS)-assisted\nmultiple input multiple output (MIMO) communication and we optimize the joint\nactive and passive beamforming by exploiting the geometrical structure of the\npropagation channels. Due to the inherent Kronecker product structure of the\nchannel matrix, the global beamforming optimization problem is split into lower\ndimensional horizontal and vertical sub-problems. Based on this factorization\nproperty, we propose two closed-form methods for passive and active beamforming\ndesigns, at the IRS, the base station, and user equipment, respectively. The\nfirst solution is a singular value decomposition (SVD)-based algorithm\nindependently applied on the factorized channels, while the second method\nresorts to a third-order rank-one tensor approximation along each domain.\nSimulation results show that exploiting the channel Kronecker structures yields\na significant improvement in terms of computational complexity at the expense\nof negligible spectral efficiency (SE) loss. We also show that under imperfect\nchannel estimation, the tensor-based solution shows better SE than the\nbenchmark and proposed SVD-based solutions.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:39:15 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14651","submitter":"Lingbing Guo","authors":"Lingbing Guo, Zhuo Chen, Jiaoyan Chen, and Huajun Chen","title":"Revisit and Outstrip Entity Alignment: A Perspective of Generative\n  Models","comments":"Under review","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.IR cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Recent embedding-based methods have achieved great successes on exploiting\nentity alignment from knowledge graph (KG) embeddings of multiple modals. In\nthis paper, we study embedding-based entity alignment (EEA) from a perspective\nof generative models. We show that EEA is a special problem where the main\nobjective is analogous to that in a typical generative model, based on which we\ntheoretically prove the effectiveness of the recently developed generative\nadversarial network (GAN)-based EEA methods. We then reveal that their\nincomplete objective limits the capacity on both entity alignment and entity\nsynthesis (i.e., generating new entities). We mitigate this problem by\nintroducing a generative EEA (abbr., GEEA) framework with the proposed mutual\nvariational autoencoder (M-VAE) as the generative model. M-VAE can convert an\nentity from one KG to another and generate new entities from random noise\nvectors. We demonstrate the power of GEEA with theoretical analysis and\nempirical experiments on both entity alignment and entity synthesis tasks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:39:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14652","submitter":"Shaoxiang Wu","authors":"Shaoxiang Wu, Damai Dai, Ziwei Qin, Tianyu Liu, Binghuai Lin, Yunbo\n  Cao, Zhifang Sui","title":"Denoising Bottleneck with Mutual Information Maximization for Video\n  Multimodal Fusion","comments":"Accept at ACL2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Video multimodal fusion aims to integrate multimodal signals in videos, such\nas visual, audio and text, to make a complementary prediction with multiple\nmodalities contents. However, unlike other image-text multimodal tasks, video\nhas longer multimodal sequences with more redundancy and noise in both visual\nand audio modalities. Prior denoising methods like forget gate are coarse in\nthe granularity of noise filtering. They often suppress the redundant and noisy\ninformation at the risk of losing critical information. Therefore, we propose a\ndenoising bottleneck fusion (DBF) model for fine-grained video multimodal\nfusion. On the one hand, we employ a bottleneck mechanism to filter out noise\nand redundancy with a restrained receptive field. On the other hand, we use a\nmutual information maximization module to regulate the filter-out module to\npreserve key information within different modalities. Our DBF model achieves\nsignificant improvement over current state-of-the-art baselines on multiple\nbenchmarks covering multimodal sentiment analysis and multimodal summarization\ntasks. It proves that our model can effectively capture salient features from\nnoisy and redundant video, audio, and text inputs. The code for this paper is\npublicly available at https://github.com/WSXRHFG/DBF.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:39:43 GMT"},{"version":"v2","created":"Thu, 25 May 2023 02:48:40 GMT"},{"version":"v3","created":"Wed, 31 May 2023 08:20:33 GMT"}],"update_date":"2023-06-01"}
{"id":"2305.14653","submitter":"Jo\\~ao Souto-Maior","authors":"Jo\\~ao M. Souto-Maior","title":"Hoarding without hoarders: unpacking the emergence of opportunity\n  hoarding within schools","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.soc-ph cs.MA econ.TH","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Sociologists of education increasingly highlight the role of opportunity\nhoarding in the formation of Black-White educational inequalities. Informed by\nthis literature, this article unpacks the necessary and sufficient conditions\nunder which the hoarding of educational resources emerges within schools. It\ndevelops a qualitatively informed agent-based model which captures Black and\nWhite students' competition for a valuable school resource: advanced\ncoursework. In contrast to traditional accounts -- which explain the emergence\nof hoarding through the actions of Whites that keep valuable resources within\nWhite communities -- simulations, perhaps surprisingly, show hoarding to arise\neven when Whites do not play the role of hoarders of resources. Behind this\nresult is the fact that a structural inequality (i.e., racial differences in\nsocial class) -- and not action-driven hoarding -- is the necessary condition\nfor hoarding to emerge. Findings, therefore, illustrate that common\naction-driven understandings of opportunity hoarding can overlook the\nstructural foundations behind this important phenomenon. Policy implications\nare discussed.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:49:38 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14654","submitter":"Atil Iscen","authors":"Ken Caluwaerts, Atil Iscen, J. Chase Kew, Wenhao Yu, Tingnan Zhang,\n  Daniel Freeman, Kuang-Huei Lee, Lisa Lee, Stefano Saliceti, Vincent Zhuang,\n  Nathan Batchelor, Steven Bohez, Federico Casarini, Jose Enrique Chen, Omar\n  Cortes, Erwin Coumans, Adil Dostmohamed, Gabriel Dulac-Arnold, Alejandro\n  Escontrela, Erik Frey, Roland Hafner, Deepali Jain, Bauyrjan Jyenis, Yuheng\n  Kuang, Edward Lee, Linda Luu, Ofir Nachum, Ken Oslund, Jason Powell, Diego\n  Reyes, Francesco Romano, Feresteh Sadeghi, Ron Sloat, Baruch Tabanpour,\n  Daniel Zheng, Michael Neunert, Raia Hadsell, Nicolas Heess, Francesco Nori,\n  Jeff Seto, Carolina Parada, Vikas Sindhwani, Vincent Vanhoucke, and Jie Tan","title":"Barkour: Benchmarking Animal-level Agility with Quadruped Robots","comments":"17 pages, 19 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Animals have evolved various agile locomotion strategies, such as sprinting,\nleaping, and jumping. There is a growing interest in developing legged robots\nthat move like their biological counterparts and show various agile skills to\nnavigate complex environments quickly. Despite the interest, the field lacks\nsystematic benchmarks to measure the performance of control policies and\nhardware in agility. We introduce the Barkour benchmark, an obstacle course to\nquantify agility for legged robots. Inspired by dog agility competitions, it\nconsists of diverse obstacles and a time based scoring mechanism. This\nencourages researchers to develop controllers that not only move fast, but do\nso in a controllable and versatile way. To set strong baselines, we present two\nmethods for tackling the benchmark. In the first approach, we train specialist\nlocomotion skills using on-policy reinforcement learning methods and combine\nthem with a high-level navigation controller. In the second approach, we\ndistill the specialist skills into a Transformer-based generalist locomotion\npolicy, named Locomotion-Transformer, that can handle various terrains and\nadjust the robot's gait based on the perceived environment and robot states.\nUsing a custom-built quadruped robot, we demonstrate that our method can\ncomplete the course at half the speed of a dog. We hope that our work\nrepresents a step towards creating controllers that enable robots to reach\nanimal-level agility.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:49:43 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14655","submitter":"Yu Ling","authors":"Yu Ling, Weimin Tan and Bo Yan","title":"Learning Survival Distribution with Implicit Survival Function","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Survival analysis aims at modeling the relationship between covariates and\nevent occurrence with some untracked (censored) samples. In implementation,\nexisting methods model the survival distribution with strong assumptions or in\na discrete time space for likelihood estimation with censorship, which leads to\nweak generalization. In this paper, we propose Implicit Survival Function (ISF)\nbased on Implicit Neural Representation for survival distribution estimation\nwithout strong assumptions,and employ numerical integration to approximate the\ncumulative distribution function for prediction and optimization. Experimental\nresults show that ISF outperforms the state-of-the-art methods in three public\ndatasets and has robustness to the hyperparameter controlling estimation\nprecision.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:51:29 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14656","submitter":"Hao Sun","authors":"Yilong Xu, Yang Liu, Hao Sun","title":"RSRM: Reinforcement Symbolic Regression Machine","comments":"18 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.SC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In nature, the behaviors of many complex systems can be described by\nparsimonious math equations. Automatically distilling these equations from\nlimited data is cast as a symbolic regression process which hitherto remains a\ngrand challenge. Keen efforts in recent years have been placed on tackling this\nissue and demonstrated success in symbolic regression. However, there still\nexist bottlenecks that current methods struggle to break when the discrete\nsearch space tends toward infinity and especially when the underlying math\nformula is intricate. To this end, we propose a novel Reinforcement Symbolic\nRegression Machine (RSRM) that masters the capability of uncovering complex\nmath equations from only scarce data. The RSRM model is composed of three key\nmodules: (1) a Monte Carlo tree search (MCTS) agent that explores optimal math\nexpression trees consisting of pre-defined math operators and variables, (2) a\nDouble Q-learning block that helps reduce the feasible search space of MCTS via\nproperly understanding the distribution of reward, and (3) a modulated sub-tree\ndiscovery block that heuristically learns and defines new math operators to\nimprove representation ability of math expression trees. Biding of these\nmodules yields the state-of-the-art performance of RSRM in symbolic regression\nas demonstrated by multiple sets of benchmark examples. The RSRM model shows\nclear superiority over several representative baseline models.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:51:45 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14657","submitter":"Yiduo Guo","authors":"Yiduo Guo, Bing Liu, Dongyan Zhao","title":"Dealing with Cross-Task Class Discrimination in Online Continual\n  Learning","comments":"Accepted by CVPR2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Existing continual learning (CL) research regards catastrophic forgetting\n(CF) as almost the only challenge. This paper argues for another challenge in\nclass-incremental learning (CIL), which we call cross-task class discrimination\n(CTCD),~i.e., how to establish decision boundaries between the classes of the\nnew task and old tasks with no (or limited) access to the old task data. CTCD\nis implicitly and partially dealt with by replay-based methods. A replay method\nsaves a small amount of data (replay data) from previous tasks. When a batch of\ncurrent task data arrives, the system jointly trains the new data and some\nsampled replay data. The replay data enables the system to partially learn the\ndecision boundaries between the new classes and the old classes as the amount\nof the saved data is small. However, this paper argues that the replay approach\nalso has a dynamic training bias issue which reduces the effectiveness of the\nreplay data in solving the CTCD problem. A novel optimization objective with a\ngradient-based adaptive method is proposed to dynamically deal with the problem\nin the online CL process. Experimental results show that the new method\nachieves much better results in online CL.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:52:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14658","submitter":"Yongkang Liu","authors":"Yongkang Liu and Shi Feng and Daling Wang and Yifei Zhang and Hinrich\n  Sch\\\"utze","title":"Evaluate What You Can't Evaluate: Unassessable Generated Responses\n  Quality","comments":"preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  LLMs (large language models) such as ChatGPT have shown remarkable language\nunderstanding and generation capabilities. Although reference-free evaluators\nbased on LLMs show better human alignment than traditional reference-based\nevaluators, there are many challenges in using reference-free evaluators based\non LLMs. Reference-free evaluators are more suitable for open-ended examples\nwith different semantics responses. But not all examples are open-ended. For\nclosed-ended examples with unique correct semantic response, reference-free\nevaluators will still consider it high quality when giving a response that is\ninconsistent with the facts and the semantic of reference. In order to\ncomprehensively evaluate the reliability of evaluators based on LLMs, we\nconstruct two adversarial meta-evaluation dialogue generation datasets\nKdConv-ADV and DSTC7-ADV based on KdConv and DSTC7-AVSD, respectively. Compared\nto previous meta-evaluation benchmarks, KdConv-ADV and DSTC7-ADV are much more\nchallenging since they requires evaluators to be able to reasonably evaluate\nclosed-ended examples with the help of external knowledge or even its own\nknowledge. Empirical results show that the ability of LLMs to identify\nunreasonable responses is insufficient. There are risks in using eference-free\nevaluators based on LLMs to evaluate the quality of dialogue responses.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:52:48 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14659","submitter":"Ishani Mondal","authors":"Ishani Mondal, Michelle Yuan, Anandhavelu N, Aparna Garimella, Francis\n  Ferraro, Andrew Blair-Stanek, Benjamin Van Durme, Jordan Boyd-Graber","title":"InteractiveIE: Towards Assessing the Strength of Human-AI Collaboration\n  in Improving the Performance of Information Extraction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Learning template based information extraction from documents is a crucial\nyet difficult task. Prior template-based IE approaches assume foreknowledge of\nthe domain templates; however, real-world IE do not have pre-defined schemas\nand it is a figure-out-as you go phenomena. To quickly bootstrap templates in a\nreal-world setting, we need to induce template slots from documents with zero\nor minimal supervision. Since the purpose of question answering intersect with\nthe goal of information extraction, we use automatic question generation to\ninduce template slots from the documents and investigate how a tiny amount of a\nproxy human-supervision on-the-fly (termed as InteractiveIE) can further boost\nthe performance. Extensive experiments on biomedical and legal documents, where\nobtaining training data is expensive, reveal encouraging trends of performance\nimprovement using InteractiveIE over AI-only baseline.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:53:22 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14660","submitter":"Anna Martin-Boyle","authors":"Anna Martin-Boyle, Andrew Head, Kyle Lo, Risham Sidhu, Marti A.\n  Hearst, and Dongyeop Kang","title":"Complex Mathematical Symbol Definition Structures: A Dataset and Model\n  for Coordination Resolution in Definition Extraction","comments":"9 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Mathematical symbol definition extraction is important for improving\nscholarly reading interfaces and scholarly information extraction (IE).\nHowever, the task poses several challenges: math symbols are difficult to\nprocess as they are not composed of natural language morphemes; and scholarly\npapers often contain sentences that require resolving complex coordinate\nstructures. We present SymDef, an English language dataset of 5,927 sentences\nfrom full-text scientific papers where each sentence is annotated with all\nmathematical symbols linked with their corresponding definitions. This dataset\nfocuses specifically on complex coordination structures such as \"respectively\"\nconstructions, which often contain overlapping definition spans. We also\nintroduce a new definition extraction method that masks mathematical symbols,\ncreates a copy of each sentence for each symbol, specifies a target symbol, and\npredicts its corresponding definition spans using slot filling. Our experiments\nshow that our definition extraction model significantly outperforms RoBERTa and\nother strong IE baseline systems by 10.9 points with a macro F1 score of 84.82.\nWith our dataset and model, we can detect complex definitions in scholarly\ndocuments to make scientific writing more readable.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:53:48 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14661","submitter":"Zhedong Zhang Dr","authors":"Joel Jiahao Fan, Zhe-Yu Jeff Ou, Zhedong Zhang","title":"Entangled Photons Enabled Ultrafast Stimulated Raman Spectroscopy for\n  Molecular Dynamics","comments":"7 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph physics.chem-ph","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Quantum entanglement has emerged as a great resource for interactions between\nmolecules and radiation. We propose a new paradigm of stimulated Raman\nscattering with entangled photons. A quantum ultrafast Raman spectroscopy is\ndeveloped for condensed-phase molecules, to monitor the exciton populations and\ncoherences. Analytic results are obtained, showing a time-frequency scale not\nattainable by classical light. The Raman signal presents an unprecedented\nselectivity of molecular correlation functions, as a result of the\nHong-Ou-Mandel interference. This is a typical quantum nature, advancing the\nspectroscopy for clarity. Our work suggests a new scheme of optical signals and\nspectroscopy, with potential to unveil advanced information about complex\nmaterials.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:57:43 GMT"},{"version":"v2","created":"Thu, 25 May 2023 03:15:28 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.14662","submitter":"Honglin Wen","authors":"Honglin Wen","title":"Probabilistic Wind Power Forecasting with Missing Values via Adaptive\n  Quantile Regression","comments":"submitted to IEEE Transactions on Sustainable Energy","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.AP cs.SY eess.SY","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Missing values challenge the probabilistic wind power forecasting at both\nparameter estimation and operational forecasting stages. In this paper, we\nillustrate that we are allowed to estimate forecasting functions for each\nmissing patterns conveniently, and propose an adaptive quantile regression\nmodel whose parameters can adapt to missing patterns. For that, we particularly\ndesign a feature extraction block within the quantile regression model, where\nparameters are set as a function of missingness pattern and only account for\nobserved values. To avoidthe quantile-crossing phenomena, we design a\nmulti-task model to ensure the monotonicity of quantiles, where higher\nquantiles are derived by the addition between lower quantiles and non-negative\nincrements modeled by neural networks. The proposed approach is\ndistribution-free and applicable to both missing-at-random and\nmissing-not-at-random cases. Case studies demonstrate that the proposed\napproach achieves the state-of-the-art in terms of the continuous ranked\nprobability score.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:58:32 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14663","submitter":"Naihao Deng","authors":"Naihao Deng, Siyang Liu, Xinliang Frederick Zhang, Winston Wu, Lu\n  Wang, Rada Mihalcea","title":"You Are What You Annotate: Towards Better Models through Annotator\n  Representations","comments":"19 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  Annotator disagreement is ubiquitous in natural language processing (NLP)\ntasks. There are multiple reasons for such disagreements, including the\nsubjectivity of the task, difficult cases, unclear guidelines, and so on.\nRather than simply aggregating labels to obtain data annotations, we instead\npropose to explicitly account for the annotator idiosyncrasies and leverage\nthem in the modeling process. We create representations for the annotators\n(annotator embeddings) and their annotations (annotation embeddings) with\nlearnable matrices associated with each. Our approach significantly improves\nmodel performance on various NLP benchmarks by adding fewer than 1% model\nparameters. By capturing the unique tendencies and subjectivity of individual\nannotators, our embeddings help democratize AI and ensure that AI models are\ninclusive of diverse viewpoints.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:06:13 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14664","submitter":"Michael McGuigan","authors":"Michael McGuigan","title":"Two Matrix Model, the Riemann Hypothesis and Master Matrix Obstruction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NT hep-th math-ph math.MP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We identify the Riemann Xi function as the Baker-Akhiezer function for a\n(p,1) two matrix model as p goes to infinity. We solve the two matrix model\nusing biorthogonal polynomials and study the zeros of the polynomials in the\ndouble scaling limit as N goes to infinity. We find zeros off the critical line\nat finite N which possibly go to infinity as N goes to infinity. We study other\nBaker-Akhiezer functions whose zeros are known to be on a critical line using\nthe two matrix model technique and find the zeros on the critical line in those\ncases. We study other L-functions using the two matrix model and compare the\nbiorthogonal method with other approaches to the two matrix model such as the\nmaster matrix approach and saddle point method. In cases where there are zeros\noff the critical line the master matrix approach encounters an obstruction to\nthe solution to a quenched master matrix.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:09:32 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14665","submitter":"Kuan-Ming Hung","authors":"Kuan-Ming Hung and Tung-Ho Shieh","title":"Pairing interactions induce antiferromagnetism in cuprate\n  superconductors","comments":"31 pages, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.supr-con cond-mat.str-el","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In cuprate superconductors, superconductivity is consistently accompanied by\nantiferromagnetism. This raises the question of a potential causal link between\nsuperconductivity and antiferromagnetic mechanisms. In this study, we consider\nthe non-local Coulomb interaction and solve the single-particle Green function\nexactly. The exact solution shows the existence of strong inter-spin\ncorrelations in the absence of superexchange, eliminating the directional\ndegrees of freedom for spin quantization in the Hatsugai-Kohmoto (HK) model.\nStrong Coulomb interactions force the energy band splitting into infinitely\nequidistant sub-bands weighted by a Poisson distribution, exhibiting boson\nproperties (incoherent pairs of fermions) and non-Fermi liquids. Attractive\npairing interactions caused by electron-phonon coupling open a pairing gap at\nthe Fermi surface. Based on the Pauli exclusion principle, paired particles\nwith the same spin quantization direction completely occupy the state below the\ngap, resulting in pairing insulators, superconducting instability, and\nespecially antiferromagnetism. A pseudogap occurs when the higher split\nsub-band overlaps in energy with the lowest sub-band. The critical temperature\nTc of superconductivity exhibits a multi-dome structure and peaks around the\nmaximum density of states. The maximum Tc depends on splitting sub-bands and\nparticle occupancy, consistent with experiments. This study concludes that the\nnon-local effect preserves the Hubbard and HK models' characteristics while\nrenormalizing the system to antiferromagnetism in pairing insulators and Pauli\nparamagnetism in weak semimetals.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:15:35 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14666","submitter":"Tian Xia","authors":"Tian Xia and Luca Scardovi","title":"An input-output framework for stability and synchronization analysis of\n  networks of infinite-dimensional linear systems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper extends a synchronization criterion established for networks of\nfinite-dimensional linear systems to the infinite-dimensional setting. Our\nresult, established in the general framework of input-output relations,\nrequires an additional input-output stability property, compared to the\nfinite-dimensional counterpart. We establish conditions for this property to\nhold for a large class of infinite-dimensional systems including Abstract\nCauchy Problems, parabolic PDEs, and time-delay ordinary differential\nequations.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:17:06 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14667","submitter":"Feng Wang","authors":"Feng Wang and Chuan-Fu Yang","title":"Ambarzumyan-type theorem for vectorial Sturm-Liouville operator with\n  impulses","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.SP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We consider the vector-impulsive Sturm-Liouville problem with Neumann\nconditions. The Ambarzumyan$^{\\textbf{,}}$s theorem for the problem is proved,\nwhich states that if the eigenvalues of the problem coincide with those of the\nzero potential, then the potential is zero.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:19:02 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14668","submitter":"Guofeng Zhang","authors":"Artur Jesslen, Guofeng Zhang, Angtian Wang, Alan Yuille, Adam\n  Kortylewski","title":"Robust 3D-aware Object Classification via Discriminative\n  Render-and-Compare","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In real-world applications, it is essential to jointly estimate the 3D object\npose and class label of objects, i.e., to perform 3D-aware classification.While\ncurrent approaches for either image classification or pose estimation can be\nextended to 3D-aware classification, we observe that they are inherently\nlimited: 1) Their performance is much lower compared to the respective\nsingle-task models, and 2) they are not robust in out-of-distribution (OOD)\nscenarios. Our main contribution is a novel architecture for 3D-aware\nclassification, which builds upon a recent work and performs comparably to\nsingle-task models while being highly robust. In our method, an object category\nis represented as a 3D cuboid mesh composed of feature vectors at each mesh\nvertex. Using differentiable rendering, we estimate the 3D object pose by\nminimizing the reconstruction error between the mesh and the feature\nrepresentation of the target image. Object classification is then performed by\ncomparing the reconstruction losses across object categories. Notably, the\nneural texture of the mesh is trained in a discriminative manner to enhance the\nclassification performance while also avoiding local optima in the\nreconstruction loss. Furthermore, we show how our method and feed-forward\nneural networks can be combined to scale the render-and-compare approach to\nlarger numbers of categories. Our experiments on PASCAL3D+, occluded-PASCAL3D+,\nand OOD-CV show that our method outperforms all baselines at 3D-aware\nclassification by a wide margin in terms of performance and robustness.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:20:09 GMT"},{"version":"v2","created":"Mon, 5 Jun 2023 17:39:03 GMT"}],"update_date":"2023-06-06"}
{"id":"2305.14669","submitter":"Yexing Song","authors":"Yexing Song, Meilin Wang, Xiaoyu Xian, Zhijing Yang, Yuming Fan, Yukai\n  Shi","title":"NegVSR: Augmenting Negatives for Generalized Noise Modeling in\n  Real-World Video Super-Resolution","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The capability of video super-resolution (VSR) to synthesize high-resolution\n(HR) video from ideal datasets has been demonstrated in many works. However,\napplying the VSR model to real-world video with unknown and complex degradation\nremains a challenging task. First, existing degradation metrics in most VSR\nmethods are not able to effectively simulate real-world noise and blur. On the\ncontrary, simple combinations of classical degradation are used for real-world\nnoise modeling, which led to the VSR model often being violated by\nout-of-distribution noise. Second, many SR models focus on noise simulation and\ntransfer. Nevertheless, the sampled noise is monotonous and limited. To address\nthe aforementioned problems, we propose a Negatives augmentation strategy for\ngeneralized noise modeling in Video Super-Resolution (NegVSR) task.\nSpecifically, we first propose sequential noise generation toward real-world\ndata to extract practical noise sequences. Then, the degeneration domain is\nwidely expanded by negative augmentation to build up various yet challenging\nreal-world noise sets. We further propose the augmented negative guidance loss\nto learn robust features among augmented negatives effectively. Extensive\nexperiments on real-world datasets (e.g., VideoLQ and FLIR) show that our\nmethod outperforms state-of-the-art methods with clear margins, especially in\nvisual quality.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:23:35 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14670","submitter":"Ben Kane","authors":"Ben Kane and Zichen Yang","title":"On finiteness theorems for sums of generalized polygonal numbers","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, we consider mixed sums of generalized polygonal numbers.\nSpecifically, we obtain a finiteness condition for universality of such sums;\nthis means that it suffices to check representability of a finite subset of the\npositive integers in order to conclude that the sum of generalized polygonal\nnumbers represents every positive integer. The sub-class of sums of generalized\npolygonal numbers which we consider is those sums of $m_j$-gonal numbers for\nwhich $\\operatorname{lcm}(m_1-2,\\dots,m_{r}-2)\\leq \\mathfrak{M}$ and we obtain\na bound on the asymptotic growth of a constant $\\Gamma_{\\mathfrak{M}}$ such\nthat it suffices to check the representability condition for $n\\leq\n\\Gamma_{\\mathfrak{M}}$.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:24:48 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14671","submitter":"Hao Zou","authors":"Hao Zou, Zae Myung Kim, Dongyeop Kang","title":"Diffusion Models in NLP: A Survey","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This survey paper provides a comprehensive review of the use of diffusion\nmodels in natural language processing (NLP). Diffusion models are a class of\nmathematical models that aim to capture the diffusion of information or signals\nacross a network or manifold. In NLP, diffusion models have been used in a\nvariety of applications, such as natural language generation, sentiment\nanalysis, topic modeling, and machine translation. This paper discusses the\ndifferent formulations of diffusion models used in NLP, their strengths and\nlimitations, and their applications. We also perform a thorough comparison\nbetween diffusion models and alternative generative models, specifically\nhighlighting the autoregressive (AR) models, while also examining how diverse\narchitectures incorporate the Transformer in conjunction with diffusion models.\nCompared to AR models, diffusion models have significant advantages for\nparallel generation, text interpolation, token-level controls such as syntactic\nstructures and semantic contents, and robustness. Exploring further\npermutations of integrating Transformers into diffusion models would be a\nvaluable pursuit. Also, the development of multimodal diffusion models and\nlarge-scale diffusion language models with notable capabilities for few-shot\nlearning would be important directions for the future advance of diffusion\nmodels in NLP.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:25:32 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14672","submitter":"Melissa Dell","authors":"Xinmei Yang and Abhishek Arora and Shao-Yu Jheng and Melissa Dell","title":"Quantifying Character Similarity with Vision Transformers","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CV econ.GN q-fin.EC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Record linkage is a bedrock of quantitative social science, as analyses often\nrequire linking data from multiple, noisy sources. Off-the-shelf string\nmatching methods are widely used, as they are straightforward and cheap to\nimplement and scale. Not all character substitutions are equally probable, and\nfor some settings there are widely used handcrafted lists denoting which string\nsubstitutions are more likely, that improve the accuracy of string matching.\nHowever, such lists do not exist for many settings, skewing research with\nlinked datasets towards a few high-resource contexts that are not\nrepresentative of the diversity of human societies. This study develops an\nextensible way to measure character substitution costs for OCR'ed documents, by\nemploying large-scale self-supervised training of vision transformers (ViT)\nwith augmented digital fonts. For each language written with the CJK script, we\ncontrastively learn a metric space where different augmentations of the same\ncharacter are represented nearby. In this space, homoglyphic characters - those\nwith similar appearance such as ``O'' and ``0'' - have similar vector\nrepresentations. Using the cosine distance between characters' representations\nas the substitution cost in an edit distance matching algorithm significantly\nimproves record linkage compared to other widely used string matching methods,\nas OCR errors tend to be homoglyphic in nature. Homoglyphs can plausibly\ncapture character visual similarity across any script, including low-resource\nsettings. We illustrate this by creating homoglyph sets for 3,000 year old\nancient Chinese characters, which are highly pictorial. Fascinatingly, a ViT is\nable to capture relationships in how different abstract concepts were\nconceptualized by ancient societies, that have been noted in the archaeological\nliterature.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:25:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14673","submitter":"Xiao Liang","authors":"Xiao Liang, Shan Lin, Fei Liu, Dimitri Schreiber, and Michael Yip","title":"ORRN: An ODE-based Recursive Registration Network for Deformable\n  Respiratory Motion Estimation with Lung 4DCT Images","comments":"Accepted by IEEE Transactions on Biomedical Engineering","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Deformable Image Registration (DIR) plays a significant role in quantifying\ndeformation in medical data. Recent Deep Learning methods have shown promising\naccuracy and speedup for registering a pair of medical images. However, in 4D\n(3D + time) medical data, organ motion, such as respiratory motion and heart\nbeating, can not be effectively modeled by pair-wise methods as they were\noptimized for image pairs but did not consider the organ motion patterns\nnecessary when considering 4D data. This paper presents ORRN, an Ordinary\nDifferential Equations (ODE)-based recursive image registration network. Our\nnetwork learns to estimate time-varying voxel velocities for an ODE that models\ndeformation in 4D image data. It adopts a recursive registration strategy to\nprogressively estimate a deformation field through ODE integration of voxel\nvelocities. We evaluate the proposed method on two publicly available lung 4DCT\ndatasets, DIRLab and CREATIS, for two tasks: 1) registering all images to the\nextreme inhale image for 3D+t deformation tracking and 2) registering extreme\nexhale to inhale phase images. Our method outperforms other learning-based\nmethods in both tasks, producing the smallest Target Registration Error of\n1.24mm and 1.26mm, respectively. Additionally, it produces less than 0.001\\%\nunrealistic image folding, and the computation speed is less than 1 second for\neach CT volume. ORRN demonstrates promising registration accuracy, deformation\nplausibility, and computation efficiency on group-wise and pair-wise\nregistration tasks. It has significant implications in enabling fast and\naccurate respiratory motion estimation for treatment planning in radiation\ntherapy or robot motion planning in thoracic needle insertion.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:26:26 GMT"},{"version":"v2","created":"Thu, 25 May 2023 04:56:19 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.14674","submitter":"Kangfu Mei","authors":"Kangfu Mei and Mo Zhou and Vishal M. Patel","title":"T1: Scaling Diffusion Probabilistic Fields to High-Resolution on Unified\n  Visual Modalities","comments":"for project page, see https://t1-diffusion-model.github.io","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Diffusion Probabilistic Field (DPF) models the distribution of continuous\nfunctions defined over metric spaces. While DPF shows great potential for\nunifying data generation of various modalities including images, videos, and 3D\ngeometry, it does not scale to a higher data resolution. This can be attributed\nto the ``scaling property'', where it is difficult for the model to capture\nlocal structures through uniform sampling. To this end, we propose a new model\ncomprising of a view-wise sampling algorithm to focus on local structure\nlearning, and incorporating additional guidance, e.g., text description, to\ncomplement the global geometry. The model can be scaled to generate\nhigh-resolution data while unifying multiple modalities. Experimental results\non data generation in various modalities demonstrate the effectiveness of our\nmodel, as well as its potential as a foundation framework for scalable\nmodality-unified visual content generation.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:32:03 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14675","submitter":"Yiheng Jiang","authors":"Yiheng Jiang and Yuanbo Xu","title":"Revenge of MLP in Sequential Recommendation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Sequential recommendation models sequences of historical user-item\ninteractive behaviors (or referred as token) to better infer dynamic\npreferences. Fueled by the improved neural network architectures such as RNN,\nCNN and Transformer, this field has enjoyed rapid performance boost in the past\nyears. Recent progress on all-MLP models lights on an efficient method with\nless intensive computation, token-mixing MLP, to learn the transformation\npatterns among historical behaviors. However, due to the inherent\nfully-connection design that allows the unrestricted cross-token communication\nand ignores the chronological order, we find that directly applying\ntoken-mixing MLP into sequential recommendation leads to subpar performance. In\nthis paper, we present a purely MLP-based sequential recommendation\narchitecture TriMLP with a novel \\underline{Tri}angular Mixer where the\nmodified \\underline{MLP} endows tokens with ordered interactions. As the\ncross-token interaction in MLP is actually matrix multiplication, Triangular\nMixer drops the lower-triangle neurons in the weight matrix and thus blocks the\nconnections from future tokens, which prevents information leakage and improves\nprediction capability under the standard auto-regressive training fashion. To\nfurther model long and short-term preferences on fine-grained level, the mixer\nadopts a dual-branch structure based on the delicate MLP described above,\nnamely global and local mixing, to separately capture the sequential long-range\ndependencies and local patterns. Empirical study on 9 different scale datasets\n(contain 50K\\textasciitilde20M behaviors) of various benchmarks, including\nMovieLens, Amazon and Tenrec, demonstrates that TriMLP attains promising and\nstable accuracy/efficiency trade-off, i.e., averagely surpasses several\nstate-of-the-art baselines by 5.32\\% and saves 8.44\\% inference time cost.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:32:31 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14676","submitter":"Woojeong Jin","authors":"Woojeong Jin, Subhabrata Mukherjee, Yu Cheng, Yelong Shen, Weizhu\n  Chen, Ahmed Hassan Awadallah, Damien Jose, Xiang Ren","title":"GRILL: Grounded Vision-language Pre-training via Aligning Text and Image\n  Regions","comments":"Preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Generalization to unseen tasks is an important ability for few-shot learners\nto achieve better zero-/few-shot performance on diverse tasks. However, such\ngeneralization to vision-language tasks including grounding and generation\ntasks has been under-explored; existing few-shot VL models struggle to handle\ntasks that involve object grounding and multiple images such as visual\ncommonsense reasoning or NLVR2. In this paper, we introduce GRILL, GRounded\nvIsion Language aLigning, a novel VL model that can be generalized to diverse\ntasks including visual question answering, captioning, and grounding tasks with\nno or very few training instances. Specifically, GRILL learns object grounding\nand localization by exploiting object-text alignments, which enables it to\ntransfer to grounding tasks in a zero-/few-shot fashion. We evaluate our model\non various zero-/few-shot VL tasks and show that it consistently surpasses the\nstate-of-the-art few-shot methods.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:33:21 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14677","submitter":"Zhongjie Duan","authors":"Zhongjie Duan, Chengyu Wang, Cen Chen, Jun Huang and Weining Qian","title":"Optimal Linear Subspace Search: Learning to Construct Fast and\n  High-Quality Schedulers for Diffusion Models","comments":"13 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In recent years, diffusion models have become the most popular and powerful\nmethods in the field of image synthesis, even rivaling human artists in\nartistic creativity. However, the key issue currently limiting the application\nof diffusion models is its extremely slow generation process. Although several\nmethods were proposed to speed up the generation process, there still exists a\ntrade-off between efficiency and quality. In this paper, we first provide a\ndetailed theoretical and empirical analysis of the generation process of the\ndiffusion models based on schedulers. We transform the designing problem of\nschedulers into the determination of several parameters, and further transform\nthe accelerated generation process into an expansion process of the linear\nsubspace. Based on these analyses, we consequently propose a novel method\ncalled Optimal Linear Subspace Search (OLSS), which accelerates the generation\nprocess by searching for the optimal approximation process of the complete\ngeneration process in the linear subspaces spanned by latent variables. OLSS is\nable to generate high-quality images with a very small number of steps. To\ndemonstrate the effectiveness of our method, we conduct extensive comparative\nexperiments on open-source diffusion models. Experimental results show that\nwith a given number of steps, OLSS can significantly improve the quality of\ngenerated images. Using an NVIDIA A100 GPU, we make it possible to generate a\nhigh-quality image by Stable Diffusion within only one second without other\noptimization techniques.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:33:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14678","submitter":"Zhonghai Zhao","authors":"Zhonghai Zhao, Yang Xu, Jia Liu, Li Zhao, Yulong Shen and Norio\n  Shiratori","title":"Marriage Matching-based Instant Parking Spot Sharing in Internet of\n  Vehicles","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The rapid development and integration of automotive manufacturing, sensor,\nand communication technologies have facilitated the emergence of the Internet\nof Vehicles (IoV). However, the explosive growing demand for parking spots has\nbecome a challenging issue to be addressed in IoV. In this paper, we propose a\nnovel Smart Parking System (SPS) for IoV by applying the matching game\napproach. Specifically, the proposed SPS consists of three types of entities:\ndrivers, parking spot owners (PSOs), and a certificate authority (CA). Drivers\nand PSOs send parking requests and parking spot information to the CA,\nrespectively. The CA is responsible for identifying legitimate system users,\nmatching drivers with PSOs, and recording their transaction information. We\nfirst design rational utility functions for drivers and PSOs, and then\nestablish preference lists for them based on real-time conditions. With these\ninformation, we further develop an improved marriage matching (MM) algorithm to\nachieve stable matching results for drivers' requests and parking spot\nallocation. Simulation results demonstrate that the proposed MM-based SPS not\nonly ensures stable matching results with the objective of distance\nminimization but also achieves an overall utility close to that of the optimal\nalgorithm.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:37:25 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14679","submitter":"Masahiro Kojima","authors":"Masahiro Kojima","title":"Dynamic Borrowing Method for Historical Information Using a Frequentist\n  Approach for Hybrid Control Design","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME stat.ML","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Information borrowing from historical data is gaining attention in clinical\ntrials of rare and pediatric diseases, where statistical power may be\ninsufficient for confirmation of efficacy if the sample size is small. Although\nBayesian information borrowing methods are well established, test-then-pool and\nequivalence-based test-then-pool methods have recently been proposed as\nfrequentist methods to determine whether historical data should be used for\nstatistical hypothesis testing. Depending on the results of the hypothesis\ntesting, historical data may not be usable. This paper proposes a dynamic\nborrowing method for historical information based on the similarity between\ncurrent and historical data. In our proposed method of dynamic information\nborrowing, as in Bayesian dynamic borrowing, the amount of borrowing ranges\nfrom 0% to 100%. We propose two methods using the density function of the\nt-distribution and a logistic function as a similarity measure. We evaluate the\nperformance of the proposed methods through Monte Carlo simulations. We\ndemonstrate the usefulness of borrowing information by reanalyzing actual\nclinical trial data.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:38:26 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14680","submitter":"Zhichao Liu","authors":"Zhichao Liu, Zhouyu Lu, Ali-akbar Agha-mohammadi and Konstantinos\n  Karydis","title":"Contact-Prioritized Planning of Impact-Resilient Aerial Robots with an\n  Integrated Compliant Arm","comments":"To appear in TMECH/AIM FS. Video https://youtu.be/Ks1tMMtjfGg","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The article develops an impact-resilient aerial robot (s-ARQ) equipped with a\ncompliant arm to sense contacts and reduce collision impact and featuring a\nreal-time contact force estimator and a non-linear motion controller to handle\ncollisions while performing aggressive maneuvers and stabilize from high-speed\nwall collisions. Further, a new collision-inclusive planning method that aims\nto prioritize contacts to facilitate aerial robot navigation in cluttered\nenvironments is proposed. A range of simulated and physical experiments\ndemonstrate key benefits of the robot and the contact-prioritized (CP) planner.\nExperimental results show that the compliant robot has only a $4\\%$ weight\nincrease but around $40\\%$ impact reduction in drop tests and wall collision\ntests. s-ARQ can handle collisions while performing aggressive maneuvers and\nstabilize from high-speed wall collisions at $3.0$ m/s with a success rate of\n$100\\%$. Our proposed compliant robot and contact-prioritized planning method\ncan accelerate computation time while having shorter trajectory time and larger\nclearances compared to A$^\\ast$ and RRT$^\\ast$ planners with velocity\nconstraints. Online planning tests in partially-known environments further\ndemonstrate the preliminary feasibility of our method to apply in practical use\ncases.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:41:10 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14681","submitter":"James Michaelov","authors":"James A. Michaelov, Benjamin K. Bergen","title":"Emergent inabilities? Inverse scaling over the course of pretraining","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Does inverse scaling only occur as a function of model parameter size, or can\nit also occur over the course of training? We carry out an exploratory study\ninvestigating whether, over the course of training on the language modeling\ntask, the performance of language models at specific tasks can decrease while\ngeneral performance remains high. We find that for two tasks from the Inverse\nScaling Challenge - quote-repetition and redefine-math - this is indeed the\ncase. Specifically, we find that for Pythia (Biderman et al., 2023) models with\na higher number of parameters, performance decreases over the course of\ntraining at these two tasks, despite these models showing standard (positive)\nscaling overall. This highlights the importance of testing model performance at\nall relevant benchmarks any time they are trained on additional data, even if\ntheir overall performance improves.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:42:43 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14682","submitter":"Jian Wu","authors":"Jian Wu, Yicheng Xu, Yan Gao, Jian-Guang Lou, B\\\"orje F. Karlsson,\n  Manabu Okumura","title":"TACR: A Table-alignment-based Cell-selection and Reasoning Model for\n  Hybrid Question-Answering","comments":"Accepted at Findings of ACL 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Hybrid Question-Answering (HQA), which targets reasoning over tables and\npassages linked from table cells, has witnessed significant research in recent\nyears. A common challenge in HQA and other passage-table QA datasets is that it\nis generally unrealistic to iterate over all table rows, columns, and linked\npassages to retrieve evidence. Such a challenge made it difficult for previous\nstudies to show their reasoning ability in retrieving answers. To bridge this\ngap, we propose a novel Table-alignment-based Cell-selection and Reasoning\nmodel (TACR) for hybrid text and table QA, evaluated on the HybridQA and\nWikiTableQuestions datasets. In evidence retrieval, we design a\ntable-question-alignment enhanced cell-selection method to retrieve\nfine-grained evidence. In answer reasoning, we incorporate a QA module that\ntreats the row containing selected cells as context. Experimental results over\nthe HybridQA and WikiTableQuestions (WTQ) datasets show that TACR achieves\nstate-of-the-art results on cell selection and outperforms fine-grained\nevidence retrieval baselines on HybridQA, while achieving competitive\nperformance on WTQ. We also conducted a detailed analysis to demonstrate that\nbeing able to align questions to tables in the cell-selection stage can result\nin important gains from experiments of over 90\\% table row and column selection\naccuracy, meanwhile also improving output explainability.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:42:44 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14683","submitter":"Lachlan MacDonald","authors":"Lachlan Ewen MacDonald and Jack Valmadre and Simon Lucey","title":"On progressive sharpening, flat minima and generalisation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We present a new approach to understanding the relationship between loss\ncurvature and generalisation in deep learning. Specifically, we use existing\nempirical analyses of the spectrum of deep network loss Hessians to ground an\nansatz tying together the loss Hessian and the input-output Jacobian of a deep\nneural network. We then prove a series of theoretical results which quantify\nthe degree to which the input-output Jacobian of a model approximates its\nLipschitz norm over a data distribution, and deduce a novel generalisation\nbound in terms of the empirical Jacobian. We use our ansatz, together with our\ntheoretical results, to give a new account of the recently observed progressive\nsharpening phenomenon, as well as the generalisation properties of flat minima.\nExperimental evidence is provided to validate our claims.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:44:50 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14684","submitter":"Zehong Zhou","authors":"Zehong Zhou, Fei Zhou, Guoping Qiu","title":"Collaborative Auto-encoding for Blind Image Quality Assessment","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV eess.IV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Blind image quality assessment (BIQA) is a challenging problem with important\nreal-world applications. Recent efforts attempting to exploit powerful\nrepresentations by deep neural networks (DNN) are hindered by the lack of\nsubjectively annotated data. This paper presents a novel BIQA method which\novercomes this fundamental obstacle. Specifically, we design a pair of\ncollaborative autoencoders (COAE) consisting of a content autoencoder (CAE) and\na distortion autoencoder (DAE) that work together to extract content and\ndistortion representations, which are shown to be highly descriptive of image\nquality. While the CAE follows a standard codec procedure, we introduce the\nCAE-encoded feature as an extra input to the DAE's decoder for reconstructing\ndistorted images, thus effectively forcing DAE's encoder to extract distortion\nrepresentations. The self-supervised learning framework allows the COAE\nincluding two feature extractors to be trained by almost unlimited amount of\ndata, thus leaving limited samples with annotations to finetune a BIQA model.\nWe will show that the proposed BIQA method achieves state-of-the-art\nperformance and has superior generalization capability over other learning\nbased models. The codes are available at:\nhttps://github.com/Macro-Zhou/NRIQA-VISOR/.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:45:03 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14685","submitter":"Shi Yu","authors":"Shi Yu (1), Chenghao Fan (2), Chenyan Xiong (3), David Jin (4),\n  Zhiyuan Liu (1), Zhenghao Liu (5) ((1) Tsinghua University, (2) Huazhong\n  University of Science and Technology, (3) Microsoft Research, (4)\n  Massachusetts Institute of Technology, (5) Northeastern University)","title":"Fusion-in-T5: Unifying Document Ranking Signals for Improved Information\n  Retrieval","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Common IR pipelines are typically cascade systems that may involve multiple\nrankers and/or fusion models to integrate different information step-by-step.\nIn this paper, we propose a novel re-ranker named Fusion-in-T5 (FiT5), which\nintegrates document text information, retrieval features, and global document\ninformation into a single unified model using templated-based input and global\nattention. Experiments on passage ranking benchmarks MS MARCO and TREC DL show\nthat FiT5 significantly improves ranking performance over prior pipelines.\nAnalyses find that through global attention, FiT5 is able to jointly utilize\nthe ranking features via gradually attending to related documents, and thus\nimprove the detection of subtle nuances between them. Our code will be\nopen-sourced.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:45:35 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14686","submitter":"Yu Chen","authors":"Yu Chen, Jin Cheng, Shuai Lu, Masahiro Yamamoto","title":"Harmonic Measures and Numerical Computation of Cauchy Problems for\n  Laplace Equations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  It is well known that Cauchy problem for Laplace equations is an ill-posed\nproblem in Hadamard's sense. Small deviations in Cauchy data may lead to large\nerrors in the solutions. It is observed that if a bound is imposed on the\nsolution, there exists a conditional stability estimate. This gives a\nreasonable way to construct stable algorithms. However, it is impossible to\nhave good results at all points in the domain. Although numerical methods for\nCauchy problems for Laplace equations have been widely studied for quite a long\ntime, there are still some unclear points, for example, how to evaluate the\nnumerical solutions, which means whether we can approximate the Cauchy data\nwell and keep the bound of the solution, and at which points the numerical\nresults are reliable? In this paper, we will prove the conditional stability\nestimate which is quantitatively related to harmonic measures. The harmonic\nmeasure can be used as an indicate function to pointwisely evaluate the\nnumerical result, which further enables us to find a reliable subdomain where\nthe local convergence rate is higher than a certain order.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:46:38 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14687","submitter":"Yuqing Fu","authors":"Bocong Chen, Yuqing Fu and Hongwei Liu","title":"Improved upper bounds on the number of non-zero weights of cyclic codes","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT math.IT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Let C be an arbitrary simple-root cyclic code and let G be the subgroup of\nAut(C) (the automorphism group of C) generated by the multiplier, the cyclic\nshift and the scalar multiplications. To the best of our knowledge, the\nsubgroup G is the largest subgroup of Aut(C). In this paper, an explicit\nformula, in some cases an upper bound, for the number of orbits of G on C\\{0}\nis established. An explicit upper bound on the number of non-zero weights of C\nis consequently derived and a necessary and sufficient condition for the code C\nmeeting the bound is exhibited. Many examples are presented to show that our\nnew upper bounds are tight and are strictly less than the upper bounds in [Chen\nand Zhang, IEEE-TIT, 2023]. In addition, for two special classes of cyclic\ncodes, smaller upper bounds on the number of non-zero weights of such codes are\nobtained by replacing G with larger subgroups of the automorphism groups of\nthese codes. As a byproduct, our main results suggest a new way to find\nfew-weight cyclic codes.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:50:11 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14688","submitter":"Benfeng Xu","authors":"Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong\n  Zhang, Zhendong Mao","title":"ExpertPrompting: Instructing Large Language Models to be Distinguished\n  Experts","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The answering quality of an aligned large language model (LLM) can be\ndrastically improved if treated with proper crafting of prompts. In this paper,\nwe propose ExpertPrompting to elicit the potential of LLMs to answer as\ndistinguished experts. We first utilize In-Context Learning to automatically\nsynthesize detailed and customized descriptions of the expert identity for each\nspecific instruction, and then ask LLMs to provide answer conditioned on such\nagent background. Based on this augmented prompting strategy, we produce a new\nset of instruction-following data using GPT-3.5, and train a competitive\nopen-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation\nto show that 1) the expert data is of significantly higher quality than vanilla\nanswers, and 2) ExpertLLaMA outperforms existing open-source opponents and\nachieves 96\\% of the original ChatGPT's capability. All data and the\nExpertLLaMA model will be made publicly available at\n\\url{https://github.com/OFA-Sys/ExpertLLaMA}.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:51:31 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14689","submitter":"Rishi Sonthalia","authors":"Rishi Sonthalia and Xinyue Li and Bochao Gu","title":"Under-Parameterized Double Descent for Ridge Regularized Least Squares\n  Denoising of Data on a Line","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG math.ST stat.TH","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The relationship between the number of training data points, the number of\nparameters in a statistical model, and the generalization capabilities of the\nmodel has been widely studied. Previous work has shown that double descent can\noccur in the over-parameterized regime, and believe that the standard\nbias-variance trade-off holds in the under-parameterized regime. In this paper,\nwe present a simple example that provably exhibits double descent in the\nunder-parameterized regime. For simplicity, we look at the ridge regularized\nleast squares denoising problem with data on a line embedded in high-dimension\nspace. By deriving an asymptotically accurate formula for the generalization\nerror, we observe sample-wise and parameter-wise double descent with the peak\nin the under-parameterized regime rather than at the interpolation point or in\nthe over-parameterized regime.\n  Further, the peak of the sample-wise double descent curve corresponds to a\npeak in the curve for the norm of the estimator, and adjusting $\\mu$, the\nstrength of the ridge regularization, shifts the location of the peak. We\nobserve that parameter-wise double descent occurs for this model for small\n$\\mu$. For larger values of $\\mu$, we observe that the curve for the norm of\nthe estimator has a peak but that this no longer translates to a peak in the\ngeneralization error. Moreover, we study the training error for this problem.\nThe considered problem setup allows for studying the interaction between two\nregularizers. We provide empirical evidence that the model implicitly favors\nusing the ridge regularizer over the input data noise regularizer. Thus, we\nshow that even though both regularizers regularize the same quantity, i.e., the\nnorm of the estimator, they are not equivalent.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:52:48 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14690","submitter":"Tongtong Fang","authors":"Tongtong Fang, Nan Lu, Gang Niu, Masashi Sugiyama","title":"Generalizing Importance Weighting to A Universal Solver for Distribution\n  Shift Problems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Distribution shift (DS) may have two levels: the distribution itself changes,\nand the support (i.e., the set where the probability density is non-zero) also\nchanges. When considering the support change between the training and test\ndistributions, there can be four cases: (i) they exactly match; (ii) the\ntraining support is wider (and thus covers the test support); (iii) the test\nsupport is wider; (iv) they partially overlap. Existing methods are good at\ncases (i) and (ii), while cases (iii) and (iv) are more common nowadays but\nstill under-explored. In this paper, we generalize importance weighting (IW), a\ngolden solver for cases (i) and (ii), to a universal solver for all cases.\nSpecifically, we first investigate why IW may fail in cases (iii) and (iv);\nbased on the findings, we propose generalized IW (GIW) that could handle cases\n(iii) and (iv) and would reduce to IW in cases (i) and (ii). In GIW, the test\nsupport is split into an in-training (IT) part and an out-of-training (OOT)\npart, and the expected risk is decomposed into a weighted classification term\nover the IT part and a standard classification term over the OOT part, which\nguarantees the risk consistency of GIW. Then, the implementation of GIW\nconsists of three components: (a) the split of validation data is carried out\nby the one-class support vector machine, (b) the first term of the empirical\nrisk can be handled by any IW algorithm given training data and IT validation\ndata, and (c) the second term just involves OOT validation data. Experiments\ndemonstrate that GIW is a universal solver for DS problems, outperforming IW\nmethods in cases (iii) and (iv).\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:53:15 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14691","submitter":"Jiajia Li","authors":"Jiajia Li, Dong Chen, Xinda Qi, Zhaojian Li, Yanbo Huang, Daniel\n  Morris, Xiaobo Tan","title":"Label-Efficient Learning in Agriculture: A Comprehensive Review","comments":"34 pages, 23 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  The past decade has witnessed many great successes of machine learning (ML)\nand deep learning (DL) applications in agricultural systems, including weed\ncontrol, plant disease diagnosis, agricultural robotics, and precision\nlivestock management. Despite tremendous progresses, one downside of such ML/DL\nmodels is that they generally rely on large-scale labeled datasets for\ntraining, and the performance of such models is strongly influenced by the size\nand quality of available labeled data samples. In addition, collecting,\nprocessing, and labeling such large-scale datasets is extremely costly and\ntime-consuming, partially due to the rising cost in human labor. Therefore,\ndeveloping label-efficient ML/DL methods for agricultural applications has\nreceived significant interests among researchers and practitioners. In fact,\nthere are more than 50 papers on developing and applying deep-learning-based\nlabel-efficient techniques to address various agricultural problems since 2016,\nwhich motivates the authors to provide a timely and comprehensive review of\nrecent label-efficient ML/DL methods in agricultural applications. To this end,\nwe first develop a principled taxonomy to organize these methods according to\nthe degree of supervision, including weak supervision (i.e., active learning\nand semi-/weakly- supervised learning), and no supervision (i.e., un-/self-\nsupervised learning), supplemented by representative state-of-the-art\nlabel-efficient ML/DL methods. In addition, a systematic review of various\nagricultural applications exploiting these label-efficient algorithms, such as\nprecision agriculture, plant phenotyping, and postharvest quality assessment,\nis presented. Finally, we discuss the current problems and challenges, as well\nas future research directions. A well-classified paper list can be accessed at\nhttps://github.com/DongChen06/Label-efficient-in-Agriculture.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:53:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14692","submitter":"Rahul Krishna Yandrapally","authors":"Rahulkrishna Yandrapally, Saurabh Sinha, Rachel Tzoref-Brill, Ali\n  Mesbah","title":"Carving UI Tests to Generate API Tests and API Specification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Modern web applications make extensive use of API calls to update the UI\nstate in response to user events or server-side changes. For such applications,\nAPI-level testing can play an important role, in-between unit-level testing and\nUI-level (or end-to-end) testing. Existing API testing tools require API\nspecifications (e.g., OpenAPI), which often may not be available or, when\navailable, be inconsistent with the API implementation, thus limiting the\napplicability of automated API testing to web applications. In this paper, we\npresent an approach that leverages UI testing to enable API-level testing for\nweb applications. Our technique navigates the web application under test and\nautomatically generates an API-level test suite, along with an OpenAPI\nspecification that describes the application's server-side APIs (for REST-based\nweb applications). A key element of our solution is a dynamic approach for\ninferring API endpoints with path parameters via UI navigation and directed API\nprobing. We evaluated the technique for its accuracy in inferring API\nspecifications and the effectiveness of the \"carved\" API tests. Our results on\nseven open-source web applications show that the technique achieves 98%\nprecision and 56% recall in inferring endpoints. The carved API tests, when\nadded to test suites generated by two automated REST API testing tools,\nincrease statement coverage by 52% and 29% and branch coverage by 99% and 75%,\non average. The main benefits of our technique are: (1) it enables API-level\ntesting of web applications in cases where existing API testing tools are\ninapplicable and (2) it creates API-level test suites that cover server-side\ncode efficiently while exercising APIs as they would be invoked from an\napplication's web UI, and that can augment existing API test suites.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:53:34 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14693","submitter":"Xiaoyang Song","authors":"Xiaoyang Song, Akshat Gupta, Kiyan Mohebbizadeh, Shujie Hu, Anant\n  Singh","title":"Have Large Language Models Developed a Personality?: Applicability of\n  Self-Assessment Tests in Measuring Personality in LLMs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Have Large Language Models (LLMs) developed a personality? The short answer\nis a resounding \"We Don't Know!\". In this paper, we show that we do not yet\nhave the right tools to measure personality in language models. Personality is\nan important characteristic that influences behavior. As LLMs emulate\nhuman-like intelligence and performance in various tasks, a natural question to\nask is whether these models have developed a personality. Previous works have\nevaluated machine personality through self-assessment personality tests, which\nare a set of multiple-choice questions created to evaluate personality in\nhumans. A fundamental assumption here is that human personality tests can\naccurately measure personality in machines. In this paper, we investigate the\nemergence of personality in five LLMs of different sizes ranging from 1.5B to\n30B. We propose the Option-Order Symmetry property as a necessary condition for\nthe reliability of these self-assessment tests. Under this condition, the\nanswer to self-assessment questions is invariant to the order in which the\noptions are presented. We find that many LLMs personality test responses do not\npreserve option-order symmetry. We take a deeper look at LLMs test responses\nwhere option-order symmetry is preserved to find that in these cases, LLMs do\nnot take into account the situational statement being tested and produce the\nexact same answer irrespective of the situation being tested. We also identify\nthe existence of inherent biases in these LLMs which is the root cause of the\naforementioned phenomenon and makes self-assessment tests unreliable. These\nobservations indicate that self-assessment tests are not the correct tools to\nmeasure personality in LLMs. Through this paper, we hope to draw attention to\nthe shortcomings of current literature in measuring personality in LLMs and\ncall for developing tools for machine personality measurement.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:53:43 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14694","submitter":"Keith Paarporn","authors":"Keith Paarporn, Shouhuai Xu","title":"Analysis of Contagion Dynamics with Active Cyber Defenders","comments":"3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper, we analyze the infection spreading dynamics of malware in a\npopulation of cyber nodes (i.e., computers or devices). Unlike most prior\nstudies where nodes are reactive to infections, in our setting some nodes are\nactive defenders meaning that they are able to clean up malware infections of\ntheir neighboring nodes, much like how spreading malware exploits the network\nconnectivity properties in order to propagate. We formulate these dynamics as\nan Active Susceptible-Infected-Susceptible (A-SIS) compartmental model of\ncontagion. We completely characterize the system's asymptotic behavior by\nestablishing conditions for the global asymptotic stability of the\ninfection-free equilibrium and for an endemic equilibrium state. We show that\nthe presence of active defenders counter-acts infectious spreading, effectively\nincreasing the epidemic threshold on parameters for which an endemic state\nprevails. Leveraging this characterization, we investigate a general class of\nproblems for finding optimal investments in active cyber defense capabilities\ngiven limited resources. We show that this class of problems has unique\nsolutions under mild assumptions. We then analyze an Active\nSusceptible-Infected-Recovered (A-SIR) compartmental model, where the peak\ninfection level of any trajectory is explicitly derived.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:57:15 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14695","submitter":"Fei Wang","authors":"Fei Wang, Wenjie Mo, Yiwei Wang, Wenxuan Zhou, Muhao Chen","title":"A Causal View of Entity Bias in (Large) Language Models","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Entity bias widely affects pretrained (large) language models, causing them\nto excessively rely on (biased) parametric knowledge to make unfaithful\npredictions. Although causality-inspired methods have shown great potential to\nmitigate entity bias, it is hard to precisely estimate the parameters of\nunderlying causal models in practice. The rise of black-box LLMs also makes the\nsituation even worse, because of their inaccessible parameters and uncalibrated\nlogits. To address these problems, we propose a specific structured causal\nmodel (SCM) whose parameters are comparatively easier to estimate. Building\nupon this SCM, we propose causal intervention techniques to mitigate entity\nbias for both white-box and black-box settings. The proposed causal\nintervention perturbs the original entity with neighboring entities. This\nintervention reduces specific biasing information pertaining to the original\nentity while still preserving sufficient common predictive information from\nsimilar entities. When evaluated on the relation extraction task, our\ntraining-time intervention significantly improves the F1 score of RoBERTa by\n5.7 points on EntRED, in which spurious shortcuts between entities and labels\nare removed. Meanwhile, our in-context intervention effectively reduces the\nknowledge conflicts between parametric knowledge and contextual knowledge in\nGPT-3.5 and improves the F1 score by 9.14 points on a challenging test set\nderived from Re-TACRED.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:59:18 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14696","submitter":"Dheeraj Mekala","authors":"Dheeraj Mekala, Adithya Samavedhi, Chengyu Dong, Jingbo Shang","title":"SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to\n  Rank","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Deep neural classifiers trained with cross-entropy loss (CE loss) often\nsuffer from poor calibration, necessitating the task of out-of-distribution\n(OOD) detection. Traditional supervised OOD detection methods require expensive\nmanual annotation of in-distribution and OOD samples. To address the annotation\nbottleneck, we introduce SELFOOD, a self-supervised OOD detection method that\nrequires only in-distribution samples as supervision. We cast OOD detection as\nan inter-document intra-label (IDIL) ranking problem and train the classifier\nwith our pairwise ranking loss, referred to as IDIL loss. Specifically, given a\nset of in-distribution documents and their labels, for each label, we train the\nclassifier to rank the softmax scores of documents belonging to that label to\nbe higher than the scores of documents that belong to other labels. Unlike CE\nloss, our IDIL loss function reaches zero when the desired confidence ranking\nis achieved and gradients are backpropagated to decrease probabilities\nassociated with incorrect labels rather than continuously increasing the\nprobability of the correct label. Extensive experiments with several\nclassifiers on multiple classification datasets demonstrate the effectiveness\nof our method in both coarse- and fine-grained settings.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:01:27 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14697","submitter":"Heyi Liang","authors":"Yan Fang, Artem M. Rumyantsev, Angelika E. Neitzel, Heyi Liang,\n  William T. Heller, Paul F. Nealey, Matthew V. Tirrel, Juan J. de Pablo","title":"Scattering Evidence of Positional Charge Correlations in Polyelectrolyte\n  Complexes","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.soft","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Polyelectrolyte complexation plays an important role in materials science and\nbiology. The internal structure of the resultant polyelectrolyte complex (PEC)\nphase dictates properties such as physical state, response to external stimuli,\nand dynamics. Small-angle scattering experiments with X-rays and neutrons have\nrevealed structural similarities between PECs and semidilute solutions of\nneutral polymers, where the total scattering function exhibits an\nOrnstein-Zernike form. In spite of consensus among different theoretical\npredictions, the existence of positional correlations between polyanion and\npolycation charges has not been confirmed experimentally. Here, we present\nsmall-angle neutron scattering profiles where the polycation scattering length\ndensity is matched to that of the solvent to extract positional correlations\namong anionic monomers. The polyanion scattering functions exhibit a peak at\nthe inverse polymer screening radius of Coulomb interactions, $q^{*} \\approx\n0.2 {\\AA}^{-1}$. This peak, attributed to Coulomb repulsions between the\nfragments of polyanions and their attractions to polycations, is even more\npronounced in the calculated charge scattering function that quantifies\npositional correlations of all polymer charges within the PEC. Screening of\nelectrostatic interactions by adding salt leads to the gradual disappearance of\nthis correlation peak, and the scattering functions regain an Ornstein-Zernike\nform. Experimental scattering results are consistent with those calculated from\nthe random phase approximation, a scaling analysis, and molecular simulations.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:05:09 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14698","submitter":"Francisco Rodriguez","authors":"Dorothy Kronick and Francisco Rodr\\'iguez","title":"Political Conflict and Economic Growth in Post-Independence Venezuela","comments":"28 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"econ.GN q-fin.EC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Venezuela has suffered three economic catastrophes since independence: one\neach in the nineteenth, twentieth, and twenty-first centuries. Prominent\nexplanations for this trilogy point to the interaction of class conflict and\nresource dependence. We turn attention to intra-class conflict, arguing that\nthe most destructive policy choices stemmed not from the rich defending\nthemselves against the masses but rather from pitched battles among elites.\nOthers posit that Venezuelan political institutions failed to sustain growth\nbecause they were insufficiently inclusive; we suggest in addition that they\ninadequately mediated intra-elite conflict.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:05:56 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14699","submitter":"Dylan Zhang","authors":"Shizhuo Dylan Zhang, Curt Tigges, Stella Biderman, Maxim Raginsky,\n  Talia Ringer","title":"Can Transformers Learn to Solve Problems Recursively?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.LO cs.PL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Neural networks have in recent years shown promise for helping software\nengineers write programs and even formally verify them. While semantic\ninformation plays a crucial part in these processes, it remains unclear to what\ndegree popular neural architectures like transformers are capable of modeling\nthat information. This paper examines the behavior of neural networks learning\nalgorithms relevant to programs and formal verification proofs through the lens\nof mechanistic interpretability, focusing in particular on structural\nrecursion. Structural recursion is at the heart of tasks on which symbolic\ntools currently outperform neural models, like inferring semantic relations\nbetween datatypes and emulating program behavior. We evaluate the ability of\ntransformer models to learn to emulate the behavior of structurally recursive\nfunctions from input-output examples. Our evaluation includes empirical and\nconceptual analyses of the limitations and capabilities of transformer models\nin approximating these functions, as well as reconstructions of the ``shortcut\"\nalgorithms the model learns. By reconstructing these algorithms, we are able to\ncorrectly predict 91 percent of failure cases for one of the approximated\nfunctions. Our work provides a new foundation for understanding the behavior of\nneural networks that fail to solve the very tasks they are trained for.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:08:37 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14700","submitter":"Zihui Wu","authors":"Zihui Wu, Haichang Gao, Bingqian Zhou, Ping Wang","title":"AdvFunMatch: When Consistent Teaching Meets Adversarial Robustness","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  \\emph{Consistent teaching} is an effective paradigm for implementing\nknowledge distillation (KD), where both student and teacher models receive\nidentical inputs, and KD is treated as a function matching task (FunMatch).\nHowever, one limitation of FunMatch is that it does not account for the\ntransfer of adversarial robustness, a model's resistance to adversarial\nattacks. To tackle this problem, we propose a simple but effective strategy\ncalled Adversarial Function Matching (AdvFunMatch), which aims to match\ndistributions for all data points within the $\\ell_p$-norm ball of the training\ndata, in accordance with consistent teaching. Formulated as a min-max\noptimization problem, AdvFunMatch identifies the worst-case instances that\nmaximizes the KL-divergence between teacher and student model outputs, which we\nrefer to as \"mismatched examples,\" and then matches the outputs on these\nmismatched examples. Our experimental results show that AdvFunMatch effectively\nproduces student models with both high clean accuracy and robustness.\nFurthermore, we reveal that strong data augmentations (\\emph{e.g.},\nAutoAugment) are beneficial in AdvFunMatch, whereas prior works have found them\nless effective in adversarial training. Code is available at\n\\url{https://gitee.com/zihui998/adv-fun-match}.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:09:08 GMT"},{"version":"v2","created":"Thu, 25 May 2023 02:46:26 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.14701","submitter":"Tom McCoy","authors":"R. Thomas McCoy and Thomas L. Griffiths","title":"Modeling rapid language learning by distilling Bayesian priors into\n  artificial neural networks","comments":"21 pages plus references; 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Humans can learn languages from remarkably little experience. Developing\ncomputational models that explain this ability has been a major challenge in\ncognitive science. Bayesian models that build in strong inductive biases -\nfactors that guide generalization - have been successful at explaining how\nhumans might generalize from few examples in controlled settings but are\nusually too restrictive to be tractably applied to more naturalistic data. By\ncontrast, neural networks have flexible representations that allow them to\nlearn well from naturalistic data but require many more examples than humans\nreceive. We show that learning from limited naturalistic data is possible with\nan approach that combines the strong inductive biases of a Bayesian model with\nthe flexible representations of a neural network. This approach works by\ndistilling a Bayesian model's biases into a neural network. Like a Bayesian\nmodel, the resulting system can learn formal linguistic patterns from a small\nnumber of examples. Like a neural network, it can also learn aspects of English\nsyntax from a corpus of natural language - and it outperforms a standard neural\nnetwork at acquiring the linguistic phenomena of recursion and priming.\nBridging the divide between Bayesian models and neural networks makes it\npossible to handle a broader range of learning scenarios than either approach\ncan handle on its own.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:11:59 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14702","submitter":"Kaiqiang Song","authors":"Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Hassan Foroosh,\n  Fei Liu","title":"Analyzing Influential Factors in Human Preference Judgments via GPT-4","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Pairwise human judgments are pivotal in guiding large language models (LLMs)\nto generate outputs that align with human preferences. They are also often used\nin summarization evaluation, complementing existing automatic metrics. Despite\ntheir significance, however, there has been limited research probing these\npairwise human judgments. The collective impact and respective weights of\nfactors such as informativeness, coherence, fluency, and factual consistency\nremain elusive. The impact of hidden factors on the final judgment is also\nunclear. In this paper, we conduct an in-depth examination of a dataset of\npairwise human judgments released by OpenAI. Utilizing the Bradley-Terry-Luce\nmodel, we identify key factors that could potentially influence human\njudgments. Our research uncovers the inherent preferences embedded in human\njudgments and suggests strategies to boost sample efficiency. Finally, we\nprovide insights on the construction of balanced datasets for human judgment\nevaluations, a crucial step in shaping the behaviors of future LLMs.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:13:15 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14703","submitter":"Ting Wang","authors":"Ting Wang, Petr Plechac, Jaroslaw Knap","title":"Generative diffusion learning for parametric partial differential\n  equations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We develop a class of data-driven generative models that approximate the\nsolution operator for parameter-dependent partial differential equations (PDE).\nWe propose a novel probabilistic formulation of the operator learning problem\nbased on recently developed generative denoising diffusion probabilistic models\n(DDPM) in order to learn the input-to-output mapping between problem parameters\nand solutions of the PDE. To achieve this goal we modify DDPM to supervised\nlearning in which the solution operator for the PDE is represented by a class\nof conditional distributions. The probabilistic formulation combined with DDPM\nallows for an automatic quantification of confidence intervals for the learned\nsolutions. Furthermore, the framework is directly applicable for learning from\na noisy data set. We compare computational performance of the developed method\nwith the Fourier Network Operators (FNO). Our results show that our method\nachieves comparable accuracy and recovers the noise magnitude when applied to\ndata sets with outputs corrupted by additive noise.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:15:34 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14704","submitter":"Zezhong Zhang","authors":"Zezhong Zhang and Ted Yuan","title":"An Evaluation on Practical Batch Bayesian Sampling Algorithms for Online\n  Adaptive Traffic Experimentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.AP stat.ME","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  To speed up online testing, adaptive traffic experimentation through\nmulti-armed bandit algorithms is rising as an essential complementary\nalternative to the fixed horizon A/B testing. Based on recent research on best\narm identification and statistical inference with adaptively collected data,\nthis paper derives and evaluates four Bayesian batch bandit algorithms (NB-TS,\nWB-TS, NB-TTTS, WB-TTTS), which are combinations of two ways of weighting\nbatches (Naive Batch and Weighted Batch) and two Bayesian sampling strategies\n(Thompson Sampling and Top-Two Thompson Sampling) to adaptively determine\ntraffic allocation. These derived Bayesian sampling algorithms are practically\nbased on summary batch statistics of a reward metric for pilot experiments,\nwhere one of the combination WB-TTTS in this paper seems to be newly discussed.\nThe comprehensive evaluation on the four Bayesian sampling algorithms covers\ntrustworthiness, sensitivity and regret of a testing methodology. Moreover, the\nevaluation includes 4 real-world eBay experiments and 40 reproducible synthetic\nexperiments to reveal the learnings, which covers both stationary and\nnon-stationary situations. Our evaluation reveals that, (a) There exist false\npositives inflation with equivalent best arms, while seldom discussed in\nliteratures; (b) To control false positives, connections between convergence of\nposterior optimal probabilities and neutral posterior reshaping are discovered;\n(c) WB-TTTS shows competitive recall, higher precision, and robustness against\nnon-stationary trend; (d) NB-TS outperforms on minimizing regret trials except\non precision and robustness; (e) WB-TTTS is a promising alternative if regret\nof A/B Testing is affordable, otherwise NB-TS is still a powerful choice with\nregret consideration for pilot experiments.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:16:56 GMT"},{"version":"v2","created":"Thu, 25 May 2023 21:38:01 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.14705","submitter":"Sheng Shen","authors":"Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei,\n  Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, Tu Vu, Yuexin Wu,\n  Wuyang Chen, Albert Webson, Yunxuan Li, Vincent Zhao, Hongkun Yu, Kurt\n  Keutzer, Trevor Darrell, Denny Zhou","title":"Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse\n  Mixture of Experts","comments":"Preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The explosive growth of language models and their applications have led to an\nincreased demand for efficient and scalable methods. In this paper, we\nintroduce Flan-MoE, a set of Instruction-Finetuned Sparse Mixture-of-Expert\n(MoE) models. We show that naively finetuning MoE models on a task-specific\ndataset (in other words, no instruction-finetuning) often yield worse\nperformance compared to dense models of the same computational complexity.\nHowever, our Flan-MoE outperforms dense models under multiple experiment\nsettings: instruction-finetuning only and instruction-finetuning followed by\ntask-specific finetuning. This shows that instruction-finetuning is an\nessential stage for MoE models. Specifically, our largest model, Flan-MoE-32B,\nsurpasses the performance of Flan-PaLM-62B on four benchmarks, while utilizing\nonly one-third of the FLOPs. The success of Flan-MoE encourages rethinking the\ndesign of large-scale, high-performance language models, under the setting of\ntask-agnostic learning.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:22:26 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14706","submitter":"Yushan Su","authors":"Yushan Su, Vishvak Murahari, Karthik Narasimhan, Kai Li","title":"PruMUX: Augmenting Data Multiplexing with Model Compression","comments":"Findings of the Association for Computational Linguistics (ACL 2023)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  As language models increase in size by the day, methods for efficient\ninference are critical to leveraging their capabilities for various\napplications. Prior work has investigated techniques like model pruning,\nknowledge distillation, and data multiplexing to increase model throughput\nwithout sacrificing accuracy. In this paper, we combine two such methods --\nstructured pruning and data multiplexing -- to compound the speedup gains\nobtained by either method. Our approach, PruMUX, obtains up to 7.5-29.5X\nthroughput improvement over BERT-base model with accuracy threshold from 80% to\n74%. We further study various combinations of parameters (such as sparsity and\nmultiplexing factor) in the two techniques to provide a comprehensive analysis\nof the tradeoff between accuracy and throughput in the resulting models. We\nthen propose Auto-PruMUX, a meta-level model that can predict the\nhigh-performance parameters for pruning and multiplexing given a desired\naccuracy loss budget, providing a practical method to leverage the combination\neffectively.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:22:38 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14707","submitter":"Dhananjay Ashok","authors":"Dhananjay Ashok, Atharva Kulkarni, Hai Pham, Barnab\\'as P\\'oczos","title":"The student becomes the master: Matching GPT3 on Scientific Factual\n  Error Correction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Due to the prohibitively high cost of creating error correction datasets,\nmost Factual Claim Correction methods rely on a powerful verification model to\nguide the correction process. This leads to a significant drop in performance\nin domains like Scientific Claim Correction, where good verification models do\nnot always exist. In this work, we introduce a claim correction system that\nmakes no domain assumptions and does not require a verifier but is able to\noutperform existing methods by an order of magnitude -- achieving 94%\ncorrection accuracy on the SciFact dataset, and 62.5% on the SciFact-Open\ndataset, compared to the next best methods 0.5% and 1.50% respectively. Our\nmethod leverages the power of prompting with LLMs during training to create a\nrichly annotated dataset that can be used for fully supervised training and\nregularization. We additionally use a claim-aware decoding procedure to improve\nthe quality of corrected claims. Our method is competitive with the very LLM\nthat was used to generate the annotated dataset -- with GPT3.5 achieving 89.5%\nand 60% correction accuracy on SciFact and SciFact-Open, despite using 1250\ntimes as many parameters as our model.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:24:16 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14708","submitter":"Yichen Chi","authors":"Yichen Chi, Junhao Gu, Jiamiao Zhang, Wenming Yang, Yapeng Tian","title":"EgoVSR: Towards High-Quality Egocentric Video Super-Resolution","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Due to the limitations of capture devices and scenarios, egocentric videos\nfrequently have low visual quality, mainly caused by high compression and\nsevere motion blur. With the increasing application of egocentric videos, there\nis an urgent need to enhance the quality of these videos through\nsuper-resolution. However, existing Video Super-Resolution (VSR) works,\nfocusing on third-person view videos, are actually unsuitable for handling\nblurring artifacts caused by rapid ego-motion and object motion in egocentric\nvideos. To this end, we propose EgoVSR, a VSR framework specifically designed\nfor egocentric videos. We explicitly tackle motion blurs in egocentric videos\nusing a Dual Branch Deblur Network (DB$^2$Net) in the VSR framework. Meanwhile,\na blurring mask is introduced to guide the DB$^2$Net learning, and can be used\nto localize blurred areas in video frames. We also design a MaskNet to predict\nthe mask, as well as a mask loss to optimize the mask estimation. Additionally,\nan online motion blur synthesis model for common VSR training data is proposed\nto simulate motion blurs as in egocentric videos. In order to validate the\neffectiveness of our proposed method, we introduce an EgoVSR dataset containing\na large amount of fast-motion egocentric video sequences. Extensive experiments\ndemonstrate that our EgoVSR model can efficiently super-resolve low-quality\negocentric videos and outperform strong comparison baselines. Our code,\npre-trained models, and data will be released.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:25:51 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14709","submitter":"Gabriele Farina","authors":"Gabriele Farina and Julien Grand-Cl\\'ement and Christian Kroer and\n  Chung-Wei Lee and Haipeng Luo","title":"Regret Matching+: (In)Stability and Fast Convergence in Games","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GT cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Regret Matching+ (RM+) and its variants are important algorithms for solving\nlarge-scale games. However, a theoretical understanding of their success in\npractice is still a mystery. Moreover, recent advances on fast convergence in\ngames are limited to no-regret algorithms such as online mirror descent, which\nsatisfy stability. In this paper, we first give counterexamples showing that\nRM+ and its predictive version can be unstable, which might cause other players\nto suffer large regret. We then provide two fixes: restarting and chopping off\nthe positive orthant that RM+ works in. We show that these fixes are sufficient\nto get $O(T^{1/4})$ individual regret and $O(1)$ social regret in normal-form\ngames via RM+ with predictions. We also apply our stabilizing techniques to\nclairvoyant updates in the uncoupled learning setting for RM+ and prove\ndesirable results akin to recent works for Clairvoyant online mirror descent.\nOur experiments show the advantages of our algorithms over vanilla RM+-based\nalgorithms in matrix and extensive-form games.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:26:21 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14710","submitter":"Muhao Chen","authors":"Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, Muhao Chen","title":"Instructions as Backdoors: Backdoor Vulnerabilities of Instruction\n  Tuning for Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CR cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Instruction-tuned models are trained on crowdsourcing datasets with task\ninstructions to achieve superior performance. However, in this work we raise\nsecurity concerns about this training paradigm. Our studies demonstrate that an\nattacker can inject backdoors by issuing very few malicious instructions among\nthousands of gathered data and control model behavior through data poisoning,\nwithout even the need of modifying data instances or labels themselves. Through\nsuch instruction attacks, the attacker can achieve over 90% attack success rate\nacross four commonly used NLP datasets, and cause persistent backdoors that are\neasily transferred to 15 diverse datasets zero-shot. In this way, the attacker\ncan directly apply poisoned instructions designed for one dataset on many other\ndatasets. Moreover, the poisoned model cannot be cured by continual learning.\nLastly, instruction attacks show resistance to existing inference-time defense.\nThese findings highlight the need for more robust defenses against data\npoisoning attacks in instructiontuning models and underscore the importance of\nensuring data quality in instruction crowdsourcing.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:27:21 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14711","submitter":"Haoyi Qiu","authors":"Haoyi Qiu, Zi-Yi Dou, Tianlu Wang, Asli Celikyilmaz, Nanyun Peng","title":"Gender Biases in Automatic Evaluation Metrics: A Case Study on Image\n  Captioning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Pretrained model-based evaluation metrics have demonstrated strong\nperformance with high correlations with human judgments in various natural\nlanguage generation tasks such as image captioning. Despite the impressive\nresults, their impact on fairness is under-explored -- it is widely\nacknowledged that pretrained models can encode societal biases, and utilizing\nthem for evaluation purposes may inadvertently manifest and potentially amplify\nbiases. In this paper, we conduct a systematic study in gender biases of\nmodel-based evaluation metrics with a focus on image captioning tasks.\nSpecifically, we first identify and quantify gender biases in different\nevaluation metrics regarding profession, activity, and object concepts. Then,\nwe demonstrate the negative consequences of using these biased metrics, such as\nfavoring biased generation models in deployment and propagating the biases to\ngeneration models through reinforcement learning. We also present a simple but\neffective alternative to reduce gender biases by combining n-gram\nmatching-based and pretrained model-based evaluation metrics.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:27:40 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14712","submitter":"Mingyang Yi","authors":"Mingyang Yi, Jiacheng Sun, Zhenguo Li","title":"On the Generalization of Diffusion Model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The diffusion probabilistic generative models are widely used to generate\nhigh-quality data. Though they can synthetic data that does not exist in the\ntraining set, the rationale behind such generalization is still unexplored. In\nthis paper, we formally define the generalization of the generative model,\nwhich is measured by the mutual information between the generated data and the\ntraining set. The definition originates from the intuition that the model which\ngenerates data with less correlation to the training set exhibits better\ngeneralization ability. Meanwhile, we show that for the empirical optimal\ndiffusion model, the data generated by a deterministic sampler are all highly\nrelated to the training set, thus poor generalization. This result contradicts\nthe observation of the trained diffusion model's (approximating empirical\noptima) extrapolation ability (generating unseen data). To understand this\ncontradiction, we empirically verify the difference between the sufficiently\ntrained diffusion model and the empirical optima. We found, though obtained\nthrough sufficient training, there still exists a slight difference between\nthem, which is critical to making the diffusion model generalizable. Moreover,\nwe propose another training objective whose empirical optimal solution has no\npotential generalization problem. We empirically show that the proposed\ntraining objective returns a similar model to the original one, which further\nverifies the generalization ability of the trained diffusion model.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:27:57 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14713","submitter":"Yixiong Yan","authors":"Yixiong Yan, Liangzhu Cheng, Yongxu Li, Xinjuan Tuo","title":"Streaming Object Detection on Fisheye Cameras for Automatic Parking","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Fisheye cameras are widely employed in automatic parking, and the video\nstream object detection (VSOD) of the fisheye camera is a fundamental\nperception function to ensure the safe operation of vehicles. In past research\nwork, the difference between the output of the deep learning model and the\nactual situation at the current moment due to the existence of delay of the\nperception system is generally ignored. But the environment will inevitably\nchange within the delay time which may cause a potential safety hazard. In this\npaper, we propose a real-time detection framework equipped with a dual-flow\nperception module (dynamic and static flows) that can predict the future and\nalleviate the time-lag problem. Meanwhile, we use a new scheme to evaluate\nlatency and accuracy. The standard bounding box is unsuitable for the object in\nfisheye camera images due to the strong radial distortion of the fisheye camera\nand the primary detection objects of parking perception are vehicles and\npedestrians, so we adopt the rotate bounding box and propose a new periodic\nangle loss function to regress the angle of the box, which is the simple and\naccurate representation method of objects. The instance segmentation ground\ntruth is used to supervise the training. Experiments demonstrate the\neffectiveness of our approach. Code is released at:\nhttps://gitee.com/hiyanyx/fisheye-streaming-perception.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:30:25 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14714","submitter":"Ximo Gallud Cidoncha","authors":"Ximo Gallud and Paulo C. Lozano","title":"The limited effect of electric conductivity on the ion current\n  evaporated from electrospray sources","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.app-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Electrohydrodynamic modeling and experimental observations indicate that the\nion evaporation current emitted by passively-fed electrospray sources is\nindependent of the electrical conductivity of the ionic liquid. This contrasts\nwith cone-jet electrosprays, in which current depends on conductivity at fixed\nflow rates. The current in the pure ionic regime is controlled by the bias\nvoltage and flow impedance, where a low viscosity is key for its enhancement.\nIn addition to clarifying the role of electric conductivity in ionic liquid ion\nsources, this observation provides guidance to the selection of liquid\nproperties for a particular application.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:32:49 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14715","submitter":"Daehee Park","authors":"Daehee Park, Hobin Ryu, Yunseo Yang, Jegyeong Cho, Jiwon Kim, Kuk-Jin\n  Yoon","title":"Leveraging Future Relationship Reasoning for Vehicle Trajectory\n  Prediction","comments":"ICLR 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Understanding the interaction between multiple agents is crucial for\nrealistic vehicle trajectory prediction. Existing methods have attempted to\ninfer the interaction from the observed past trajectories of agents using\npooling, attention, or graph-based methods, which rely on a deterministic\napproach. However, these methods can fail under complex road structures, as\nthey cannot predict various interactions that may occur in the future. In this\npaper, we propose a novel approach that uses lane information to predict a\nstochastic future relationship among agents. To obtain a coarse future motion\nof agents, our method first predicts the probability of lane-level waypoint\noccupancy of vehicles. We then utilize the temporal probability of passing\nadjacent lanes for each agent pair, assuming that agents passing adjacent lanes\nwill highly interact. We also model the interaction using a probabilistic\ndistribution, which allows for multiple possible future interactions. The\ndistribution is learned from the posterior distribution of interaction obtained\nfrom ground truth future trajectories. We validate our method on popular\ntrajectory prediction datasets: nuScenes and Argoverse. The results show that\nthe proposed method brings remarkable performance gain in prediction accuracy,\nand achieves state-of-the-art performance in long-term prediction benchmark\ndataset.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:33:28 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14716","submitter":"Yueqi Song","authors":"Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, Fahim Faisal,\n  Alissa Ostapenko, Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya,\n  Yulia Tsvetkov, Antonios Anastasopoulos and Graham Neubig","title":"GlobalBench: A Benchmark for Global Progress in Natural Language\n  Processing","comments":"Preprint, 9 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Despite the major advances in NLP, significant disparities in NLP system\nperformance across languages still exist. Arguably, these are due to uneven\nresource allocation and sub-optimal incentives to work on less resourced\nlanguages. To track and further incentivize the global development of equitable\nlanguage technology, we introduce GlobalBench. Prior multilingual benchmarks\nare static and have focused on a limited number of tasks and languages. In\ncontrast, GlobalBench is an ever-expanding collection that aims to dynamically\ntrack progress on all NLP datasets in all languages. Rather than solely\nmeasuring accuracy, GlobalBench also tracks the estimated per-speaker utility\nand equity of technology across all languages, providing a multi-faceted view\nof how language technology is serving people of the world. Furthermore,\nGlobalBench is designed to identify the most under-served languages, and\nrewards research efforts directed towards those languages. At present, the most\nunder-served languages are the ones with a relatively high population, but\nnonetheless overlooked by composite multilingual benchmarks (like Punjabi,\nPortuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190\nlanguages, and has 1,128 system submissions spanning 62 languages.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:36:32 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14717","submitter":"Linhan Zhang","authors":"Linhan Zhang, Qian Chen, Wen Wang, Yuxin Jiang, Bing Li, Wei Wang, Xin\n  Cao","title":"Exploiting Correlations Between Contexts and Definitions with Multiple\n  Definition Modeling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Definition modeling is an important task in advanced natural language\napplications such as understanding and conversation. Since its introduction, it\nfocus on generating one definition for a target word or phrase in a given\ncontext, which we refer to as Single Definition Modeling (SDM). However, this\napproach does not adequately model the correlations and patterns among\ndifferent contexts and definitions of words. In addition, the creation of a\ntraining dataset for SDM requires significant human expertise and effort. In\nthis paper, we carefully design a new task called Multiple Definition Modeling\n(MDM) that pool together all contexts and definition of target words. We\ndemonstrate the ease of creating a model as well as multiple training sets\nautomatically. % In the experiments, we demonstrate and analyze the benefits of\nMDM, including improving SDM's performance by using MDM as the pretraining task\nand its comparable performance in the zero-shot setting.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:38:29 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14718","submitter":"Ashutosh Baheti","authors":"Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap,\n  Mark Riedl","title":"Improving Language Models with Advantage-based Offline Policy Gradients","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Improving language model generations according to some user-defined quality\nor style constraints is challenging. Typical approaches include learning on\nadditional human-written data, filtering ``low-quality'' data using heuristics\nand/or using reinforcement learning with human feedback (RLHF). However,\nfiltering can remove valuable training signals, whereas data collection and\nRLHF constantly require additional human-written or LM exploration data which\ncan be costly to obtain. A natural question to ask is ``Can we leverage RL to\noptimize LM utility on existing crowd-sourced and internet data?''\n  To this end, we present Left-over Lunch RL (LoL-RL), a simple training\nalgorithm that uses offline policy gradients for learning language generation\ntasks as a 1-step RL game. LoL-RL can finetune LMs to optimize arbitrary\nclassifier-based or human-defined utility functions on any sequence-to-sequence\ndata. Experiments with five different language generation tasks using models of\nvarying sizes and multiple rewards show that models trained with LoL-RL can\nconsistently outperform the best supervised learning models. We also release\nour experimental code. https://github.com/abaheti95/LoL-RL\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:42:17 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14719","submitter":"Michael Kranzlein","authors":"Michael Kranzlein, Nathan Schneider, Kevin Tobia","title":"CuRIAM: Corpus re Interpretation and Metalanguage in U.S. Supreme Court\n  Opinions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Most judicial decisions involve the interpretation of legal texts; as such,\njudicial opinion requires the use of language as a medium to comment on or draw\nattention to other language. Language used this way is called metalanguage. We\ndevelop an annotation schema for categorizing types of legal metalanguage and\napply our schema to a set of U.S. Supreme Court opinions, yielding a corpus\ntotaling 59k tokens. We remark on several patterns observed in the kinds of\nmetalanguage used by the justices.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:47:55 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14720","submitter":"Dongxu Li","authors":"Dongxu Li, Junnan Li, Steven C.H. Hoi","title":"BLIP-Diffusion: Pre-trained Subject Representation for Controllable\n  Text-to-Image Generation and Editing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Subject-driven text-to-image generation models create novel renditions of an\ninput subject based on text prompts. Existing models suffer from lengthy\nfine-tuning and difficulties preserving the subject fidelity. To overcome these\nlimitations, we introduce BLIP-Diffusion, a new subject-driven image generation\nmodel that supports multimodal control which consumes inputs of subject images\nand text prompts. Unlike other subject-driven generation models, BLIP-Diffusion\nintroduces a new multimodal encoder which is pre-trained to provide subject\nrepresentation. We first pre-train the multimodal encoder following BLIP-2 to\nproduce visual representation aligned with the text. Then we design a subject\nrepresentation learning task which enables a diffusion model to leverage such\nvisual representation and generates new subject renditions. Compared with\nprevious methods such as DreamBooth, our model enables zero-shot subject-driven\ngeneration, and efficient fine-tuning for customized subject with up to 20x\nspeedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with\nexisting techniques such as ControlNet and prompt-to-prompt to enable novel\nsubject-driven generation and editing applications. Code and models will be\nreleased at\nhttps://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion. Project\npage at https://dxli94.github.io/BLIP-Diffusion-website/.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:51:04 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14721","submitter":"Manil T Mohan","authors":"Manil T. Mohan","title":"Approximations of 2D and 3D Stochastic Convective Brinkman-Forchheimer\n  Extended Darcy Equations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  In this article, we consider two- and three- dimensional stochastic\nconvective Brinkman-Forchheimer extended Darcy (CBFeD) equations\n\\begin{equation*} \\frac{\\partial \\boldsymbol{u}}{\\partial t}-\\mu\n\\Delta\\boldsymbol{u}+(\\boldsymbol{u}\\cdot\\nabla)\\boldsymbol{u}+\\alpha|\\boldsymbol{u}|^{q-1}\\boldsymbol{u}+\\beta|\\boldsymbol{u}|^{r-1}\\boldsymbol{u}+\\nabla\np=\\boldsymbol{f},\\ \\nabla\\cdot\\boldsymbol{u}=0, \\end{equation*} on a torus,\nwhere $\\mu,\\beta>0$, $\\alpha\\in\\mathbb{R}$, $r\\in[1,\\infty)$ and $q\\in[1,r)$.\nThe goal is to show that the solutions of 2D and 3D stochastic CBFeD equations\ndriven by Brownian motion can be approximated by 2D and 3D stochastic CBFeD\nequations forced by pure jump noise/random kicks on on the state space\n$\\mathrm{D}([0,T];\\mathbb{H})$. The results are established for\n$d=2,r\\in[1,\\infty)$ and $d=3,r\\in[3,\\infty)$ with $2\\beta\\mu\\geq 1$ for\n$d=r=3,$ and by using less regular assumptions on the noise coefficient.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:53:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14722","submitter":"Hao Chen","authors":"Hao Chen, Haotian Zhang, Keyan Chen, Chenyao Zhou, Song Chen, Zhengxia\n  Zhou, Zhenwei Shi","title":"Remote Sensing Image Change Detection Towards Continuous Bitemporal\n  Resolution Differences","comments":"19 pages, 11 figures. Submitted to the IEEE for a possible\n  publication","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Most contemporary supervised Remote Sensing (RS) image Change Detection (CD)\napproaches are customized for equal-resolution bitemporal images. Real-world\napplications raise the need for cross-resolution change detection, aka, CD\nbased on bitemporal images with different spatial resolutions. Current\ncross-resolution methods that are trained with samples of a fixed resolution\ndifference (resolution ratio between the high-resolution (HR) image and the\nlow-resolution (LR) one) may fit a certain ratio but lack adaptation to other\nresolution differences. Toward continuous cross-resolution CD, we propose\nscale-invariant learning to enforce the model consistently predicting HR\nresults given synthesized samples of varying bitemporal resolution differences.\nConcretely, we synthesize blurred versions of the HR image by random\ndownsampled reconstructions to reduce the gap between HR and LR images. We\nintroduce coordinate-based representations to decode per-pixel predictions by\nfeeding the coordinate query and corresponding multi-level embedding features\ninto an MLP that implicitly learns the shape of land cover changes, therefore\nbenefiting recognizing blurred objects in the LR image. Moreover, considering\nthat spatial resolution mainly affects the local textures, we apply\nlocal-window self-attention to align bitemporal features during the early\nstages of the encoder. Extensive experiments on two synthesized and one\nreal-world different-resolution CD datasets verify the effectiveness of the\nproposed method. Our method significantly outperforms several vanilla CD\nmethods and two cross-resolution CD methods on the three datasets both in\nin-distribution and out-of-distribution settings. The empirical results suggest\nthat our method could yield relatively consistent HR change predictions\nregardless of varying resolution difference ratios. Our code will be public.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:57:24 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14723","submitter":"Hiroshi Sato","authors":"Hiroshi Sato, Ryo Masumura, Tsubasa Ochiai, Marc Delcroix, Takafumi\n  Moriya, Takanori Ashihara, Kentaro Shinayama, Saki Mizuno, Mana Ihori,\n  Tomohiro Tanaka, Nobukatsu Hojo","title":"Downstream Task Agnostic Speech Enhancement with Self-Supervised\n  Representation Loss","comments":"4 pages , 2 figures, Accepted to Interspeech 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.AS cs.SD","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Self-supervised learning (SSL) is the latest breakthrough in speech\nprocessing, especially for label-scarce downstream tasks by leveraging massive\nunlabeled audio data. The noise robustness of the SSL is one of the important\nchallenges to expanding its application. We can use speech enhancement (SE) to\ntackle this issue. However, the mismatch between the SE model and SSL models\npotentially limits its effect. In this work, we propose a new SE training\ncriterion that minimizes the distance between clean and enhanced signals in the\nfeature representation of the SSL model to alleviate the mismatch. We expect\nthat the loss in the SSL domain could guide SE training to preserve or enhance\nvarious levels of characteristics of the speech signals that may be required\nfor high-level downstream tasks. Experiments show that our proposal improves\nthe performance of an SE and SSL pipeline on five downstream tasks with noisy\ninput while maintaining the SE performance.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:00:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14724","submitter":"Tuhin Chakrabarty Mr","authors":"Tuhin Chakrabarty, Arkadiy Saakyan, Olivia Winn, Artemis Panagopoulou,\n  Yue Yang, Marianna Apidianaki, Smaranda Muresan","title":"I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create\n  Visual Metaphors","comments":"ACL 2023 (Findings)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CV cs.HC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Visual metaphors are powerful rhetorical devices used to persuade or\ncommunicate creative ideas through images. Similar to linguistic metaphors,\nthey convey meaning implicitly through symbolism and juxtaposition of the\nsymbols. We propose a new task of generating visual metaphors from linguistic\nmetaphors. This is a challenging task for diffusion-based text-to-image models,\nsuch as DALL$\\cdot$E 2, since it requires the ability to model implicit meaning\nand compositionality. We propose to solve the task through the collaboration\nbetween Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3\n(davinci-002) with Chain-of-Thought prompting generates text that represents a\nvisual elaboration of the linguistic metaphor containing the implicit meaning\nand relevant objects, which is then used as input to the diffusion-based\ntext-to-image models.Using a human-AI collaboration framework, where humans\ninteract both with the LLM and the top-performing diffusion model, we create a\nhigh-quality dataset containing 6,476 visual metaphors for 1,540 linguistic\nmetaphors and their associated visual elaborations. Evaluation by professional\nillustrators shows the promise of LLM-Diffusion Model collaboration for this\ntask.To evaluate the utility of our Human-AI collaboration framework and the\nquality of our dataset, we perform both an intrinsic human-based evaluation and\nan extrinsic evaluation using visual entailment as a downstream task.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:01:10 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14725","submitter":"Barry Menglong Yao","authors":"Barry Menglong Yao, Yu Chen, Qifan Wang, Sijia Wang, Minqian Liu,\n  Zhiyang Xu, Licheng Yu, Lifu Huang","title":"AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes","comments":"12 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We propose attribute-aware multimodal entity linking, where the input is a\nmention described with a text and image, and the goal is to predict the\ncorresponding target entity from a multimodal knowledge base (KB) where each\nentity is also described with a text description, a visual image and a set of\nattributes and values. To support this research, we construct AMELI, a\nlarge-scale dataset consisting of 18,472 reviews and 35,598 products. To\nestablish baseline performance on AMELI, we experiment with the current\nstate-of-the-art multimodal entity linking approaches and our enhanced\nattribute-aware model and demonstrate the importance of incorporating the\nattribute information into the entity linking process. To be best of our\nknowledge, we are the first to build benchmark dataset and solutions for the\nattribute-aware multimodal entity linking task. Datasets and codes will be made\npublicly available.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:01:48 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14726","submitter":"Dan Iter","authors":"Dan Iter, Reid Pryzant, Ruochen Xu, Shuohang Wang, Yang Liu, Yichong\n  Xu, Chenguang Zhu","title":"In-Context Demonstration Selection with Cross Entropy Difference","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Large language models (LLMs) can use in-context demonstrations to improve\nperformance on zero-shot tasks. However, selecting the best in-context examples\nis challenging because model performance can vary widely depending on the\nselected examples. We present a cross-entropy difference (CED) method for\nselecting in-context demonstrations. Our method is based on the observation\nthat the effectiveness of in-context demonstrations negatively correlates with\nthe perplexity of the test example by a language model that was finetuned on\nthat demonstration. We utilize parameter efficient finetuning to train small\nmodels on training data that are used for computing the cross-entropy\ndifference between a test example and every candidate in-context demonstration.\nThis metric is used to rank and select in-context demonstrations independently\nfor each test input. We evaluate our method on a mix-domain dataset that\ncombines 8 benchmarks, representing 4 text generation tasks, showing that CED\nfor in-context demonstration selection can improve performance for a variety of\nLLMs.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:04:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14727","submitter":"Pierre Senellart","authors":"Angelo Saadeh and Pierre Senellart and St\\'ephane Bressan","title":"Confidential Truth Finding with Multi-Party Computation (Extended\n  Version)","comments":"15-page extended version of a paper published at DEXA 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.DB","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Federated knowledge discovery and data mining are challenged to assess the\ntrustworthiness of data originating from autonomous sources while protecting\nconfidentiality and privacy. Truth-finding algorithms help corroborate data\nfrom disagreeing sources. For each query it receives, a truth-finding algorithm\npredicts a truth value of the answer, possibly updating the trustworthiness\nfactor of each source. Few works, however, address the issues of\nconfidentiality and privacy. We devise and present a secure\nsecret-sharing-based multi-party computation protocol for pseudo-equality tests\nthat are used in truth-finding algorithms to compute additions depending on a\ncondition. The protocol guarantees confidentiality of the data and privacy of\nthe sources. We also present variants of truth-finding algorithms that would\nmake the computation faster when executed using secure multi-party computation.\nWe empirically evaluate the performance of the proposed protocol on two\nstate-of-the-art truth-finding algorithms, Cosine, and 3-Estimates, and compare\nthem with that of the baseline plain algorithms. The results confirm that the\nsecret-sharing-based secure multi-party algorithms are as accurate as the\ncorresponding baselines but for proposed numerical approximations that\nsignificantly reduce the efficiency loss incurred.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:06:03 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14728","submitter":"Victoria Lin","authors":"Victoria Lin, Louis-Philippe Morency","title":"SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language\n  Representations","comments":"Accepted to Findings of ACL 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Although deep language representations have become the dominant form of\nlanguage featurization in recent years, in many settings it is important to\nunderstand a model's decision-making process. This necessitates not only an\ninterpretable model but also interpretable features. In particular, language\nmust be featurized in a way that is interpretable while still characterizing\nthe original text well. We present SenteCon, a method for introducing human\ninterpretability in deep language representations. Given a passage of text,\nSenteCon encodes the text as a layer of interpretable categories in which each\ndimension corresponds to the relevance of a specific category. Our empirical\nevaluations indicate that encoding language with SenteCon provides high-level\ninterpretability at little to no cost to predictive performance on downstream\ntasks. Moreover, we find that SenteCon outperforms existing interpretable\nlanguage representations with respect to both its downstream performance and\nits agreement with human characterizations of the text.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:06:28 GMT"},{"version":"v2","created":"Thu, 1 Jun 2023 20:55:25 GMT"}],"update_date":"2023-06-05"}
{"id":"2305.14729","submitter":"Yuto Sano","authors":"Nobuyuki Shukuno, Yuto Sano, Makoto Tsubota","title":"Faraday Waves in Bose-Einstein Condensates -- The Excitation by the\n  Modulation of the Interaction and the Potential","comments":"7 pages, 14 figures","journal-ref":"J. Phys. Soc. Jpn. 92, 064602 (2023)","doi":"10.7566/JPSJ.92.064602","report-no":null,"categories":"cond-mat.quant-gas physics.flu-dyn","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We numerically study the dynamics of Faraday waves for Bose-Einstein\ncondensates(BECs) trapped by anisotropic potentials using the three-dimensional\nGross-Pitaevskii equation. In previous studies, Faraday waves were excited by\nperiodic modulation of the interaction or potential; in contrast, this study\nsystematically addresses the excitations of the two methods. When the\ninteraction is modulated with a modulation frequency resonant with Faraday\nwaves, the breathing mode along the tight confinement direction is excited, and\nthe Faraday waves appear in the direction of weak confinement. A modulation\nfrequency that is not resonant with Faraday waves does not excite Faraday\nwaves. Thus, the dynamics depend on modulation frequencies. The behavior of the\ntotal energy and its decomposition characterize the dynamics. The excitation of\nFaraday waves depends on the anisotropy of the potentials as well; Faraday\nwaves are excited only for elongated BECs. We compare the differences of the\ndynamics in modulation methods. There are no qualitative differences between\nthe modulation of the interaction and potential. When the interaction and\npotential are simultaneously modulated, Faraday waves are excited but they do\nnot necessarily work additively. To understand this phenomenon as a dynamical\nsystem, we choose a few dynamical variables and follow their trajectory in a\nphase space. The trajectory characteristics of Faraday waves and the breathing\nmode show that the methods of modulation are not very relevant; determining the\ntarget mode to excite is important.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:06:28 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14730","submitter":"Junrui Xiao","authors":"Junrui Xiao, Zhikai Li, Lianwei Yang, Qingyi Gu","title":"BinaryViT: Towards Efficient and Accurate Binary Vision Transformers","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Vision Transformers (ViTs) have emerged as the fundamental architecture for\nmost computer vision fields, but the considerable memory and computation costs\nhinders their application on resource-limited devices. As one of the most\npowerful compression methods, binarization reduces the computation of the\nneural network by quantizing the weights and activation values as $\\pm$1.\nAlthough existing binarization methods have demonstrated excellent performance\non Convolutional Neural Networks (CNNs), the full binarization of ViTs is still\nunder-studied and suffering a significant performance drop. In this paper, we\nfirst argue empirically that the severe performance degradation is mainly\ncaused by the weight oscillation in the binarization training and the\ninformation distortion in the activation of ViTs. Based on these analyses, we\npropose $\\textbf{BinaryViT}$, an accurate full binarization scheme for ViTs,\nwhich pushes the quantization of ViTs to the limit. Specifically, we propose a\nnovel gradient regularization scheme (GRS) for driving a bimodal distribution\nof the weights to reduce oscillation in binarization training. Moreover, we\ndesign an activation shift module (ASM) to adaptively tune the activation\ndistribution to reduce the information distortion caused by binarization.\nExtensive experiments on ImageNet dataset show that our BinaryViT consistently\nsurpasses the strong baseline by 2.05% and improve the accuracy of fully\nbinarized ViTs to a usable level. Furthermore, our method achieves impressive\nsavings of 16.2$\\times$ and 17.7$\\times$ in model size and OPs compared to the\nfull-precision DeiT-S. The codes and models will be released on github.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:06:59 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14731","submitter":"Peyman Gholami","authors":"Peyman Gholami and Robert Xiao","title":"AutoDepthNet: High Frame Rate Depth Map Reconstruction using Commodity\n  Depth and RGB Cameras","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Depth cameras have found applications in diverse fields, such as computer\nvision, artificial intelligence, and video gaming. However, the high latency\nand low frame rate of existing commodity depth cameras impose limitations on\ntheir applications. We propose a fast and accurate depth map reconstruction\ntechnique to reduce latency and increase the frame rate in depth cameras. Our\napproach uses only a commodity depth camera and color camera in a hybrid camera\nsetup; our prototype is implemented using a Kinect Azure depth camera at 30 fps\nand a high-speed RGB iPhone 11 Pro camera captured at 240 fps. The proposed\nnetwork, AutoDepthNet, is an encoder-decoder model that captures frames from\nthe high-speed RGB camera and combines them with previous depth frames to\nreconstruct a stream of high frame rate depth maps. On GPU, with a 480 x 270\noutput resolution, our system achieves an inference time of 8 ms, enabling\nreal-time use at up to 200 fps with parallel processing. AutoDepthNet can\nestimate depth values with an average RMS error of 0.076, a 44.5% improvement\ncompared to an optical flow-based comparison method. Our method can also\nimprove depth map quality by estimating depth values for missing and\ninvalidated pixels. The proposed method can be easily applied to existing depth\ncameras and facilitates the use of depth cameras in applications that require\nhigh-speed depth estimation. We also showcase the effectiveness of the\nframework in upsampling different sparse datasets e.g. video object\nsegmentation. As a demonstration of our method, we integrated our framework\ninto existing body tracking systems and demonstrated the robustness of the\nproposed method in such applications.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:09:43 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14732","submitter":"Xu Shen","authors":"Jacopo Guanetti, Yeojun Kim, Xu Shen, Joel Donham, Santosh Alexander,\n  Bruce Wootton, Francesco Borrelli","title":"Increasing Electric Vehicles Utilization in Transit Fleets using\n  Learning, Predictions, Optimization, and Automation","comments":"Accepted at the 35th IEEE Intelligent Vehicles Symposium (IV 2023)","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This work presents a novel hierarchical approach to increase Battery Electric\nBuses (BEBs) utilization in transit fleets. The proposed approach relies on\nthree key components. A learning-based BEB digital twin cloud platform is used\nto accurately predict BEB charge consumption on a per vehicle, per driver, and\nper route basis, and accurately predict the time-to-charge BEB batteries to any\nlevel. These predictions are then used by a Predictive Block Assignment module\nto maximize the BEB fleet utilization. This module computes the optimal BEB\ndaily assignment and charge management strategy. A Depot Parking and Charging\nQueue Management module is used to autonomously park and charge the vehicles\nbased on their charging demands. The paper discusses the technical approach and\nbenefits of each level in architecture and concludes with a realistic\nsimulations study. The study shows that if our approach is employed, BEB fleet\nutilization can increase by 50% compared to state-of-the-art methods.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:10:13 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14733","submitter":"Andrew Cameron","authors":"A. D. Cameron, M. Bailes, D. J. Champion, P. C. C. Freire, M. Kramer,\n  M. A. McLaughlin, C. Ng, A. Possenti, A. Ridolfi, T. M. Tauris, H. M. Wahl,\n  N. Wex","title":"New constraints on the kinematic, relativistic and evolutionary\n  properties of the PSR J1757$-$1854 double neutron star system","comments":"23 pages, 16 figures, 7 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.HE gr-qc","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  PSR J1757$-$1854 is one of the most relativistic double neutron star binary\nsystems known in our Galaxy, with an orbital period of\n$P_\\text{b}=4.4\\,\\text{hr}$ and an orbital eccentricity of $e=0.61$. As such,\nit has promised to be an outstanding laboratory for conducting tests of\nrelativistic gravity. We present the results of a 6-yr campaign with the 100-m\nGreen Bank and 64-m Parkes radio telescopes, designed to capitalise on this\npotential. We identify secular changes in the profile morphology and\npolarisation of PSR J1757$-$1854, confirming the presence of geodetic\nprecession and allowing the constraint of viewing geometry solutions consistent\nwith General Relativity. We also update PSR J1757$-$1854's timing, including\nnew constraints of the pulsar's proper motion, post-Keplerian parameters and\ncomponent masses. We conclude that the radiative test of gravity provided by\nPSR J1757$-$1854 is fundamentally limited to a precision of 0.3 per cent due to\nthe pulsar's unknown distance. A search for pulsations from the companion\nneutron star is also described, with negative results. We provide an updated\nevaluation of the system's evolutionary history, finding strong support for a\nlarge kick velocity of $w\\ge280\\,\\text{km s}^{-1}$ following the second\nprogenitor supernova. Finally, we reassess PSR J1757$-$1854's potential to\nprovide new relativistic tests of gravity. We conclude that a 3-$\\sigma$\nconstraint of the change in the projected semi-major axis ($\\dot{x}$)\nassociated with Lense-Thirring precession is expected no earlier than 2031.\nMeanwhile, we anticipate a 3-$\\sigma$ measurement of the relativistic orbital\ndeformation parameter $\\delta_\\theta$ as soon as 2026.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:12:49 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14734","submitter":"Bashar Alhafni","authors":"Bashar Alhafni, Go Inoue, Christian Khairallah, Nizar Habash","title":"Advancements in Arabic Grammatical Error Detection and Correction: An\n  Empirical Investigation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Grammatical error correction (GEC) is a well-explored problem in English with\nmany existing models and datasets. However, research on GEC in morphologically\nrich languages has been limited due to challenges such as data scarcity and\nlanguage complexity. In this paper, we present the first results on Arabic GEC\nby using two newly developed Transformer-based pretrained sequence-to-sequence\nmodels. We address the task of multi-class Arabic grammatical error detection\n(GED) and present the first results on multi-class Arabic GED. We show that\nusing GED information as auxiliary input in GEC models improves GEC performance\nacross three datasets spanning different genres. Moreover, we also investigate\nthe use of contextual morphological preprocessing in aiding GEC systems. Our\nmodels achieve state-of-the-art results on two Arabic GEC shared tasks datasets\nand establish a strong benchmark on a newly created dataset.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:12:58 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14735","submitter":"Vyoma Raman","authors":"Vyoma Raman, Eve Fleisig, Dan Klein","title":"Centering the Margins: Outlier-Based Identification of Harmed\n  Populations in Toxicity Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  A standard method for measuring the impacts of AI on marginalized communities\nis to determine performance discrepancies between specified demographic groups.\nThese approaches aim to address harms toward vulnerable groups, but they\nobscure harm patterns faced by intersectional subgroups or shared across\ndemographic groups. We instead operationalize \"the margins\" as data points that\nare statistical outliers due to having demographic attributes distant from the\n\"norm\" and measure harms toward these outliers. We propose a Group-Based\nPerformance Disparity Index (GPDI) that measures the extent to which a\nsubdivision of a dataset into subgroups identifies those facing increased\nharms. We apply our approach to detecting disparities in toxicity detection and\nfind that text targeting outliers is 28% to 86% more toxic for all types of\ntoxicity examined. We also discover that model performance is consistently\nworse for demographic outliers, with disparities in error between outliers and\nnon-outliers ranging from 28% to 71% across toxicity types. Our outlier-based\nanalysis has comparable or higher GPDI than traditional subgroup-based\nanalyses, suggesting that outlier analysis enhances identification of subgroups\nfacing greater harms. Finally, we find that minoritized racial and religious\ngroups are most associated with outliers, which suggests that outlier analysis\nis particularly beneficial for identifying harms against those groups.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:15:36 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14736","submitter":"Krishna Chaitanya Kalagarla","authors":"Krishna C. Kalagarla, Dhruva Kartik, Dongming Shen, Rahul Jain,\n  Ashutosh Nayyar and Pierluigi Nuzzo","title":"Optimal Control of Logically Constrained Partially Observable and\n  Multi-Agent Markov Decision Processes","comments":"arXiv admin note: substantial text overlap with arXiv:2203.09038","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.FL cs.SY eess.SY","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Autonomous systems often have logical constraints arising, for example, from\nsafety, operational, or regulatory requirements. Such constraints can be\nexpressed using temporal logic specifications. The system state is often\npartially observable. Moreover, it could encompass a team of multiple agents\nwith a common objective but disparate information structures and constraints.\nIn this paper, we first introduce an optimal control theory for partially\nobservable Markov decision processes (POMDPs) with finite linear temporal logic\nconstraints. We provide a structured methodology for synthesizing policies that\nmaximize a cumulative reward while ensuring that the probability of satisfying\na temporal logic constraint is sufficiently high. Our approach comes with\nguarantees on approximate reward optimality and constraint satisfaction. We\nthen build on this approach to design an optimal control framework for\nlogically constrained multi-agent settings with information asymmetry. We\nillustrate the effectiveness of our approach by implementing it on several case\nstudies.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:15:36 GMT"},{"version":"v2","created":"Fri, 26 May 2023 21:40:54 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.14737","submitter":"Valentin Vergara Hidd","authors":"Valent\\'in Vergara Hidd, Mailun Zhang, Simone Centellegher, Sam G. B.\n  Roberts, Bruno Lepri, Eduardo L\\'opez","title":"The Rhythms of Transient Relationships: Allocating time between weekdays\n  and weekends","comments":"15 pages, 4 figures. Submitted for review at EPJ Data Science","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.soc-ph cs.SI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  People maintain a variety of different types of social relationships with\nothers. Among them, transient relationships constitute a substantial portion of\ncommunication, yet receive little attention in the literature. They are\ncharacterized by a limited lifetime, which recent research has found can be\neffectively predicted by call volume, suggesting that available personal time\nfor communication is a key driver for maintaining these relationships. Whilst\nthere are examples of research on the circadian rhythms of communication over a\n24-hour period, little is known about patterns of communication during\ndifferent days of the week. In this study, we use mobile phone datasets in the\nUK and Italy to analyze the allocation of time to transient relationships at\ndistinct parts of the week focusing on the differences between relationships\nwith more contact during weekdays (Monday to Friday) or weekends. We find more\nrelationships are created during weekdays, with an overall greater proportion\nof them receiving more contact during these days of the week in the long term.\nHowever, the smaller group of relationships that receive more phone calls\nduring the weekend tend to remain longer in their corresponding ego network\n(have a longer relationship lifetime). We uncover a sorting process by which\nsome ties are moved from weekdays to weekends and vice versa, mostly in the\nfirst half of the relationship. This process also leads to higher mutual\ninformation between lifetime and the part of the week in which the relationship\nis pursued. This suggests that there is an evaluation process early in the\nrelationship that leads to a decision on how to allocate time to different\ntypes of transient ties. Overall, these results demonstrate that the part of\nthe week that concentrates the bulk of mobile communication provides\ninformation about key aspects of the ego-alter relationship.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:17:07 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14738","submitter":"Jaekwan Jeon","authors":"Jaekwan Jeon, Dongsoo Shin","title":"Deformations of weighted homogeneous surface singularities with big\n  central node","comments":"We welcome any comments","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AG","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  We prove Koll\\'{a}r conjecture for weighted homogeneous surface singularities\nwith big central node. More precisely, we show that every irreducible component\nof the deformation space of the singularity is parametrized by a certain\npartial resolution which is known as a $P$-resolution.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:18:21 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14739","submitter":"Weijia Shi","authors":"Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke\n  Zettlemoyer, Scott Wen-tau Yih","title":"Trusting Your Evidence: Hallucinate Less with Context-aware Decoding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Language models (LMs) often struggle to pay enough attention to the input\ncontext, and generate texts that are unfaithful or contain hallucinations. To\nmitigate this issue, we present context-aware decoding (CAD), which follows a\ncontrastive output distribution that amplifies the difference between the\noutput probabilities when a model is used with and without context. Our\nexperiments show that CAD, without additional training, significantly improves\nthe faithfulness of different LM families, including OPT, GPT, LLaMA and\nFLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality\nmetrics). Furthermore, CAD is particularly effective in overriding a model's\nprior knowledge when it contradicts the provided context, leading to\nsubstantial improvements in tasks where resolving the knowledge conflict is\nessential.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:19:15 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14740","submitter":"Yuxi Xie","authors":"Yuxi Xie and Guanzhen Li and Min-Yen Kan","title":"ECHo: Event Causality Inference via Human-centric Reasoning","comments":"Please find data and code at https://github.com/YuxiXie/ECHo","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We introduce ECHo, a diagnostic dataset of event causality inference grounded\nin visual-and-linguistic social scenarios. ECHo employs real-world\nhuman-centric deductive information collected from crime drama, bridging the\ngap in multimodal reasoning towards higher social intelligence through the\nelicitation of intermediate Theory-of-Mind (ToM). We propose a unified\nframework aligned with the Chain-of-Thought (CoT) paradigm to assess the\nreasoning capability of current AI systems. This ToM-enhanced CoT pipeline can\naccommodate and integrate various large foundation models in zero-shot\nvisual-and-linguistic understanding. With this framework, we scrutinize the\nadvanced large language and multimodal models via three complementary\nhuman-centric ECHo tasks. Further analysis demonstrates ECHo as a challenging\ndataset to expose imperfections and inconsistencies in reasoning.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:21:13 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14741","submitter":"Naoya Ando","authors":"Naoya Ando","title":"Sections of time-like twistor spaces with light-like or zero covariant\n  derivatives","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.DG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The conformal Gauss maps of time-like minimal surfaces in $E^3_1$ give\nsections of the time-like twistor spaces associated with the pull-back bundles\nsuch that the covariant derivatives are fully light-like, that is, these are\neither light-like or zero, and do not vanish at any point. For an oriented\nneutral $4n$-manifold $(M, h)$, if $J$ is an $h$-reversing almost paracomplex\nstructure of $M$ such that $\\nabla J$ is locally given by the tensor product of\na nowhere zero 1-form and an almost nilpotent structure related to $J$, then we\nwill see that $\\nabla J$ is valued in a light-like $2n$-dimensional\ndistribution $\\mathcal{D}$ such that $(M , h, \\mathcal{D} )$ is a Walker\nmanifold and that the square norm $\\parallel\\!\\nabla J\\!\\parallel^2$ of $\\nabla\nJ$ vanishes. We will obtain examples of $h$-reversing almost paracomplex\nstructures of $E^{4n}_{2n}$ as above. In addition, we will obtain all the pairs\nof $h$-reversing almost paracomplex structures of $E^4_2$ such that each pair\ngives sections of the two time-like twistor spaces with fully light-like\ncovariant derivatives.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:25:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14742","submitter":"Dongxu Yue","authors":"Dongxu Yue, Qin Guo, Munan Ning, Jiaxi Cui, Yuesheng Zhu, Li Yuan","title":"ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space\n  Manipulation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Editing real facial images is a crucial task in computer vision with\nsignificant demand in various real-world applications. While GAN-based methods\nhave showed potential in manipulating images especially when combined with\nCLIP, these methods are limited in their ability to reconstruct real images due\nto challenging GAN inversion capability. Despite the successful image\nreconstruction achieved by diffusion-based methods, there are still challenges\nin effectively manipulating fine-gained facial attributes with textual\ninstructions.To address these issues and facilitate convenient manipulation of\nreal facial images, we propose a novel approach that conduct text-driven image\nediting in the semantic latent space of diffusion model. By aligning the\ntemporal feature of the diffusion model with the semantic condition at\ngenerative process, we introduce a stable manipulation strategy, which perform\nprecise zero-shot manipulation effectively. Furthermore, we develop an\ninteractive system named ChatFace, which combines the zero-shot reasoning\nability of large language models to perform efficient manipulations in\ndiffusion semantic latent space. This system enables users to perform complex\nmulti-attribute manipulations through dialogue, opening up new possibilities\nfor interactive image editing. Extensive experiments confirmed that our\napproach outperforms previous methods and enables precise editing of real\nfacial images, making it a promising candidate for real-world applications.\nProject page: https://dongxuyue.github.io/chatface/\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:28:37 GMT"},{"version":"v2","created":"Mon, 5 Jun 2023 10:34:05 GMT"}],"update_date":"2023-06-06"}
{"id":"2305.14743","submitter":"Hanming Ma","authors":"Hanming Ma, Dilip Bhoi, Jun Gouchi, Hiroyasu Sato, Toru Shigeoka,\n  J.-G. Cheng, and Yoshiya Uwatoko","title":"Investigation of the atomic coordinates of CeNiC$_2$ under pressure:\n  switching of the Ce-Ce first nearest neighbor direction","comments":"13 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.str-el cond-mat.mtrl-sci cond-mat.other cond-mat.supr-con","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  When pressurized, the heavy fermion compound CeNiC$_2$ reveals a rich\nelectronic phase diagram and shows unconventional superconductivity with a\ntransition temperature $T_c$ $\\sim$ 3.7 K, the highest among Ce-based heavy\nfermion superconductors [S. Katano et al., Phys. Rev. B. 99, 100501(R) (2019)].\nUnderstanding of this appearance of superconductivity in the vicinity of\nmagnetic quantum critical point is still lacking. Given that physical\nproperties of CeNiC$_2$ are sensitive to subtle changes in the interatomic\ndistances, information on atomic coordinates may offer essential insights into\nthe local lattice arrangements, thus the mechanisms behind the exotic phases\nand phase transitions. However, extraction of precise information on the atomic\ncoordinates under pressure remains a challenge. To find a correlation between\nthe local lattice environments and exotic physical properties in CeNiC$_2$, we\ninvestigate its crystal structure from ambient pressure to 18.6 GPa via single\ncrystal X-ray diffraction. The pressure dependence of lattice parameters\nreveals anisotropic linear compressibility, $|\\kappa|$, following the\nrelationship $|\\kappa_{a}|$ (3.70$\\times$10$^{-3}$ GPa$^{-1}$) $>$\n$|\\kappa_{c}|$ (1.97$\\times$10$^{-3}$ GPa$^{-1}$) $>$ $|\\kappa_{b}|$\n(1.39$\\times$10$^{-3}$ GPa$^{-1}$), and a large bulk modulus, B$_0$ $\\sim$ 134\nGPa. Although the atomic coordinates between Ce and Ni remain unchanged under\napplied pressure, direction of the first nearest and the second nearest\nneighbors between both the Ce-Ce and Ni-Ni atoms switch $\\sim$ 7 GPa. Notably,\nthis is the same pressure that antiferromagnetic ordering temperature reaches\nmaximum in the pressure temperature phase diagram of CeNiC$_2$. Our results\nsuggest that the direction of nearest neighbors interchange might play a key\nrole in the suppression of magnetic order and the enhancement of Kondo effect.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:29:27 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14744","submitter":"Liangzu Peng","authors":"Liangzu Peng and Ren\\'e Vidal","title":"Block Coordinate Descent on Smooth Manifolds","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Block coordinate descent is an optimization paradigm that iteratively updates\none block of variables at a time, making it quite amenable to big data\napplications due to its scalability and performance. Its convergence behavior\nhas been extensively studied in the (block-wise) convex case, but it is much\nless explored in the non-convex case. In this paper we analyze the convergence\nof block coordinate methods on non-convex sets and derive convergence rates on\nsmooth manifolds under natural or weaker assumptions than prior work. Our\nanalysis applies to many non-convex problems (e.g., generalized PCA, optimal\ntransport, matrix factorization, Burer-Monteiro factorization, outlier-robust\nestimation, alternating projection, maximal coding rate reduction, neural\ncollapse, adversarial attacks, homomorphic sensing), either yielding novel\ncorollaries or recovering previously known results.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:32:49 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14745","submitter":"Ziaullah Momand","authors":"Hamida Ashna, Ziaullah Momand","title":"Applications of Machine Learning in Detecting Afghan Fake Banknotes","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Fake currency, unauthorized imitation money lacking government approval,\nconstitutes a form of fraud. Particularly in Afghanistan, the prevalence of\nfake currency poses significant challenges and detrimentally impacts the\neconomy. While banks and commercial establishments employ authentication\nmachines, the public lacks access to such systems, necessitating a program that\ncan detect counterfeit banknotes accessible to all. This paper introduces a\nmethod using image processing to identify counterfeit Afghan banknotes by\nanalyzing specific security features. Extracting first and second order\nstatistical features from input images, the WEKA machine learning tool was\nemployed to construct models and perform classification with Random Forest,\nPART, and Na\\\"ive Bayes algorithms. The Random Forest algorithm achieved\nexceptional accuracy of 99% in detecting fake Afghan banknotes, indicating the\nefficacy of the proposed method as a solution for identifying counterfeit\ncurrency.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:39:46 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14746","submitter":"Minh-Ngoc Tran","authors":"Nhat-Minh Nguyen and Minh-Ngoc Tran and Christopher Drovandi and David\n  Nott","title":"Wasserstein Gaussianization and Efficient Variational Bayes for Robust\n  Bayesian Synthetic Likelihood","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.CO stat.ML","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The Bayesian Synthetic Likelihood (BSL) method is a widely-used tool for\nlikelihood-free Bayesian inference. This method assumes that some summary\nstatistics are normally distributed, which can be incorrect in many\napplications. We propose a transformation, called the Wasserstein\nGaussianization transformation, that uses a Wasserstein gradient flow to\napproximately transform the distribution of the summary statistics into a\nGaussian distribution. BSL also implicitly requires compatibility between\nsimulated summary statistics under the working model and the observed summary\nstatistics. A robust BSL variant which achieves this has been developed in the\nrecent literature. We combine the Wasserstein Gaussianization transformation\nwith robust BSL, and an efficient Variational Bayes procedure for posterior\napproximation, to develop a highly efficient and reliable approximate Bayesian\ninference method for likelihood-free problems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:42:56 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14747","submitter":"Peter Hamm","authors":"Jan Helbing and Peter Hamm","title":"Versatile Femtosecond Laser Synchronization for Multiple-Timescale\n  Transient IR Spectroscopy","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.optics physics.chem-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Several ways to electronically synchronize different types of amplified\nfemtosecond laser systems are presented, based on a single freely programmable\nelectronics hardware: Arbitrary-detuning asynchronous optical sampling, as well\nas actively locking two femtosecond laser oscillators, albeit not necessarily\nto the same round-trip frequency. They allow us to rapidly probe a very wide\nrange of timescales, from picoseconds to potentially seconds, in a single\ntransient absorption experiment without the need to move any delay stage.\nExperiments become possible that address a largely unexplored aspect of many\nphotochemical reactions, in particular in the context of photo-catalysis as\nwell as photoactive proteins, where an initial femtosecond trigger very often\ninitiates a long-lasting cascade of follow-up processes. The approach is very\nversatile, and allows us to synchronize very different lasers, such as a Ti:Sa\namplifier and a 100~kHz Yb-laser system. The jitter of the synchronisation, and\ntherewith the time-resolution in the transient experiment, lies in the range\nfrom 1~ps to 3~ps, depending on the method. For illustration, transient IR\nmeasurements of the excited state solvation and decay of a metal carbonyl\ncomplex as well as the full reaction cycle of bacteriorhodopsin are shown. The\npros and cons of the various methods are discussed, with regard to the\nscientific question one might want to address, and also with regard to the\nlaser systems that might be already existent in a laser lab.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:46:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14748","submitter":"Dan Lin","authors":"Dan Lin, Jiajing Wu, Qishuang Fu, Yunmei Yu, Kaixin Lin, Zibin Zheng,\n  Shuo Yang","title":"Towards Understanding Crypto Money Laundering in Web3 Through the Lenses\n  of Ethereum Heists","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.SI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  With the overall momentum of the blockchain industry, crypto-based crimes are\nbecoming more and more prevalent. After committing a crime, the main goal of\ncybercriminals is to obfuscate the source of the illicit funds in order to\nconvert them into cash and get away with it. Many studies have analyzed money\nlaundering in the field of the traditional financial sector and\nblockchain-based Bitcoin. But so far, little is known about the characteristics\nof crypto money laundering in the blockchain-based Web3 ecosystem. To fill this\ngap, and considering that Ethereum is the largest platform on Web3, in this\npaper, we systematically study the behavioral characteristics and economic\nimpact of money laundering accounts through the lenses of Ethereum heists.\nBased on a very small number of tagged accounts of exchange hackers, DeFi\nexploiters, and scammers, we mine untagged money laundering groups through\nheuristic transaction tracking methods, to carve out a full picture of security\nincidents. By analyzing account characteristics and transaction networks, we\nobtain many interesting findings about crypto money laundering in Web3,\nobserving the escalating money laundering methods such as creating counterfeit\ntokens and masquerading as speculators. Finally, based on these findings we\nprovide inspiration for anti-money laundering to promote the healthy\ndevelopment of the Web3 ecosystem.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:46:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14749","submitter":"Chaitanya K. Joshi","authors":"Chaitanya K. Joshi, Arian R. Jamasb, Ramon Vi\\~nas, Charles Harris,\n  Simon Mathis, Pietro Li\\`o","title":"Multi-State RNA Design with Geometric Multi-Graph Neural Networks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG q-bio.BM q-bio.QM","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Computational RNA design has broad applications across synthetic biology and\ntherapeutic development. Fundamental to the diverse biological functions of RNA\nis its conformational flexibility, enabling single sequences to adopt a variety\nof distinct 3D states. Currently, computational biomolecule design tasks are\noften posed as inverse problems, where sequences are designed based on adopting\na single desired structural conformation. In this work, we propose gRNAde, a\ngeometric RNA design pipeline that operates on sets of 3D RNA backbone\nstructures to explicitly account for and reflect RNA conformational diversity\nin its designs. We demonstrate the utility of gRNAde for improving native\nsequence recovery over single-state approaches on a new large-scale 3D RNA\ndesign dataset, especially for multi-state and structurally diverse RNAs. Our\ncode is available at https://github.com/chaitjo/geometric-rna-design\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:46:56 GMT"},{"version":"v2","created":"Thu, 25 May 2023 14:53:11 GMT"},{"version":"v3","created":"Sun, 28 May 2023 22:44:27 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.14750","submitter":"Nishant Balepur","authors":"Nishant Balepur, Jie Huang, Samraj Moorjani, Hari Sundaram, Kevin\n  Chen-Chuan Chang","title":"Mastering the ABCDs of Complex Questions: Answer-Based Claim\n  Decomposition for Fine-grained Self-Evaluation","comments":"In progress preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  When answering complex questions, large language models (LLMs) may produce\nanswers that do not satisfy all criteria of the question. While existing\nself-evaluation techniques aim to detect if such answers are correct, these\ntechniques are unable to determine which criteria of the question are satisfied\nby the generated answers. To address this issue, we propose answer-based claim\ndecomposition (ABCD), a prompting strategy that decomposes questions into a\nseries of true/false claims that can be used to verify which criteria of the\ninput question an answer satisfies. Using the decomposed ABCD claims, we\nperform fine-grained self-evaluation. Through preliminary experiments on three\ndatasets, including a newly-collected challenge dataset ObscureQA, we find that\nGPT-3.5 has some ability to determine to what extent its answer satisfies the\ncriteria of the input question, and can give insights into the errors and\nknowledge gaps of the model.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:53:11 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14751","submitter":"Tianyu Liu","authors":"Zefan Cai, Xin Zheng, Tianyu Liu, Xu Wang, Haoran Meng, Jiaqi Han,\n  Gang Yuan, Binghuai Lin, Baobao Chang and Yunbo Cao","title":"DialogVCS: Robust Natural Language Understanding in Dialogue System\n  Upgrade","comments":"work in progress. The first three authors contribute equally","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In the constant updates of the product dialogue systems, we need to retrain\nthe natural language understanding (NLU) model as new data from the real users\nwould be merged into the existent data accumulated in the last updates. Within\nthe newly added data, new intents would emerge and might have semantic\nentanglement with the existing intents, e.g. new intents that are semantically\ntoo specific or generic are actually subset or superset of some existing\nintents in the semantic space, thus impairing the robustness of the NLU model.\nAs the first attempt to solve this problem, we setup a new benchmark consisting\nof 4 Dialogue Version Control dataSets (DialogVCS). We formulate the intent\ndetection with imperfect data in the system update as a multi-label\nclassification task with positive but unlabeled intents, which asks the models\nto recognize all the proper intents, including the ones with semantic\nentanglement, in the inference. We also propose comprehensive baseline models\nand conduct in-depth analyses for the benchmark, showing that the semantically\nentangled intents can be effectively recognized with an automatic workflow.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:53:38 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14752","submitter":"Norbert Tihanyi Dr.","authors":"Yiannis Charalambous, Norbert Tihanyi, Ridhi Jain, Youcheng Sun,\n  Mohamed Amine Ferrag, Lucas C. Cordeiro","title":"A New Era in Software Security: Towards Self-Healing Software via Large\n  Language Models and Formal Verification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE cs.AI cs.FL cs.LG","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  In this paper we present a novel solution that combines the capabilities of\nLarge Language Models (LLMs) with Formal Verification strategies to verify and\nautomatically repair software vulnerabilities. Initially, we employ Bounded\nModel Checking (BMC) to locate the software vulnerability and derive a\ncounterexample. The counterexample provides evidence that the system behaves\nincorrectly or contains a vulnerability. The counterexample that has been\ndetected, along with the source code, are provided to the LLM engine. Our\napproach involves establishing a specialized prompt language for conducting\ncode debugging and generation to understand the vulnerability's root cause and\nrepair the code. Finally, we use BMC to verify the corrected version of the\ncode generated by the LLM. As a proof of concept, we create ESBMC-AI based on\nthe Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trained\nTransformer model, specifically gpt-3.5-turbo, to detect and fix errors in C\nprograms. Our experimentation involved generating a dataset comprising 1000 C\ncode samples, each consisting of 20 to 50 lines of code. Notably, our proposed\nmethod achieved an impressive success rate of up to 80% in repairing vulnerable\ncode encompassing buffer overflow and pointer dereference failures. We assert\nthat this automated approach can effectively incorporate into the software\ndevelopment lifecycle's continuous integration and deployment (CI/CD) process.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:54:10 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14753","submitter":"Barbara Dietz","authors":"Weihua Zhang, Xiaodong Zhang, and Barbara Dietz","title":"Time-reversal Invariance Violation and Quantum Chaos Induced by\n  Magnetization in Ferrite-Loaded Resonators","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"nlin.CD","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We investigate the fluctuation properties in the eigenfrequency spectra of\nflat cylindrical microwave cavities that are homogeneously filled with\nmagnetized ferrite. These studies are motivated by experiments in which only\nsmall pieces of ferrite were embedded in the cavity and magnetized with an\nexternal static magnetic field to induce partial time-reversal (T ) invariance\nviolation. We use two different shapes of the cavity, one exhibiting an\nintegrable wave dynamics, the other one a chaotic one. We demonstrate that in\nthe frequency region where only transverse-magnetic modes exist, the\nmagnetization of the ferrites has no effect on the wave dynamics and does not\ninduce T -invariance violation whereas it is fully violated above the cutoff\nfrequency of the first transverse-electric mode. Above all, independently of\nthe shape of the resonator, it induces a chaotic wave dynamics in that\nfrequency range in the sense that for both resonator geometries the spectral\nproperties coincide with those of quantum systems with a chaotic classical\ndynamics and same invariance properties under application of the generalized T\noperator associated with the resonator geometry.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:55:44 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14754","submitter":"Cheng-Te Li","authors":"Yi-Zhan Xu, Chih-Yao Chen, Cheng-Te Li","title":"SUVR: A Search-based Approach to Unsupervised Visual Representation\n  Learning","comments":"ICASSP 2023","journal-ref":null,"doi":"10.1109/ICASSP49357.2023.10096936","report-no":null,"categories":"cs.CV cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Unsupervised learning has grown in popularity because of the difficulty of\ncollecting annotated data and the development of modern frameworks that allow\nus to learn from unlabeled data. Existing studies, however, either disregard\nvariations at different levels of similarity or only consider negative samples\nfrom one batch. We argue that image pairs should have varying degrees of\nsimilarity, and the negative samples should be allowed to be drawn from the\nentire dataset. In this work, we propose Search-based Unsupervised Visual\nRepresentation Learning (SUVR) to learn better image representations in an\nunsupervised manner. We first construct a graph from the image dataset by the\nsimilarity between images, and adopt the concept of graph traversal to explore\npositive samples. In the meantime, we make sure that negative samples can be\ndrawn from the full dataset. Quantitative experiments on five benchmark image\nclassification datasets demonstrate that SUVR can significantly outperform\nstrong competing methods on unsupervised embedding learning. Qualitative\nexperiments also show that SUVR can produce better representations in which\nsimilar images are clustered closer together than unrelated images in the\nlatent space.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:57:58 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14755","submitter":"Akhila Yerukola","authors":"Akhila Yerukola, Xuhui Zhou, Maarten Sap","title":"Don't Take This Out of Context! On the Need for Contextual Models and\n  Evaluations for Stylistic Rewriting","comments":"may 24 submission","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Most existing stylistic text rewriting methods operate on a sentence level,\nbut ignoring the broader context of the text can lead to generic, ambiguous,\nand incoherent rewrites. In this paper, we propose the integration of preceding\ntextual context into both the rewriting and evaluation stages of stylistic text\nrewriting, focusing on formality, toxicity, and sentiment transfer tasks. We\nconduct a comparative evaluation of rewriting through few-shot prompting of\nGPT-3.5 and GPT NeoX, comparing non-contextual rewrites to contextual rewrites.\nOur experiments show that humans often prefer contextual rewrites over\nnon-contextual ones, but automatic metrics (e.g., BLEU, sBERT) do not. To\nbridge this gap, we propose context-infused versions of common automatic\nmetrics, and show that these better reflect human preferences. Overall, our\npaper highlights the importance of integrating preceding textual context into\nboth the rewriting and evaluation stages of stylistic text rewriting.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:58:17 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14756","submitter":"Aditya Lahiri","authors":"Aditya Lahiri, Naigam Shah, Shivaank Agarwal, Vignesh Nandakumar","title":"Deterministic Algorithmic Approaches to Solve Generalised Wordle","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DS","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Wordle is a single-player word-based game where the objective is to guess the\n5-letter word in a maximum of 6 tries. The game was released to the public in\nOctober 2021 and has since gained popularity with people competing against each\nother to maintain daily streaks and guess the word in a minimum number of\ntries. There have been works using probabilistic and reinforcement learning\nbased approaches to solve the game. Our work aims to formulate and analyze\ndeterministic algorithms that can solve the game and minimize the number of\nturns required to guess the word and do so for any generalized setting of the\ngame. As a simplifying assumption, for our analysis of all the algorithms we\npresent, we assume that all letters will be unique in any word which is part of\nour vocabulary. We propose two algorithms to play Wordle - one a greedy based\napproach, and other based on Cliques. The Greedy approach is applicable for\nboth hard and easy modes of Wordle, while the Clique formation based approach\nonly works on the Easy mode. We present our analysis on both approaches one by\none, next.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:58:31 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14757","submitter":"Salvatore Giorgi","authors":"Salvatore Giorgi, Shreya Havaldar, Farhan Ahmed, Zuhaib Akhtar,\n  Shalaka Vaidya, Gary Pan, Lyle H. Ungar, H. Andrew Schwartz, Joao Sedoc","title":"Human-Centered Metrics for Dialog System Evaluation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  We present metrics for evaluating dialog systems through a\npsychologically-grounded \"human\" lens: conversational agents express a\ndiversity of both states (short-term factors like emotions) and traits\n(longer-term factors like personality) just as people do. These interpretable\nmetrics consist of five measures from established psychology constructs that\ncan be applied both across dialogs and on turns within dialogs: emotional\nentropy, linguistic style and emotion matching, as well as agreeableness and\nempathy. We compare these human metrics against 6 state-of-the-art automatic\nmetrics (e.g. BARTScore and BLEURT) on 7 standard dialog system data sets. We\nalso introduce a novel data set, the Three Bot Dialog Evaluation Corpus, which\nconsists of annotated conversations from ChatGPT, GPT-3, and BlenderBot. We\ndemonstrate the proposed human metrics offer novel information, are\nuncorrelated with automatic metrics, and lead to increased accuracy beyond\nexisting automatic metrics for predicting crowd-sourced dialog judgements. The\ninterpretability and unique signal of our proposed human-centered framework\nmake it a valuable tool for evaluating and improving dialog systems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:02:32 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14758","submitter":"Tianlun Zheng","authors":"Tianlun Zheng, Zhineng Chen, BingChen Huang, Wei Zhang and Yu-Gang\n  Jiang","title":"MRN: Multiplexed Routing Network for Incremental Multilingual Text\n  Recognition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Traditional Multilingual Text Recognition (MLTR) usually targets a fixed set\nof languages and thus struggles to handle newly added languages or adapt to\never-changing class distributions. In this paper, we introduce the Incremental\nMultilingual Text Recognition (IMLTR) task in the incremental learning setting,\nwhere new language data comes in batches. Compared to generic incremental\nlearning, IMLTR is even more challenging as it suffers from rehearsal-imbalance\n(uneven distribution of sample characters in the rehearsal set). To address\nthis issue, we propose a Multiplexed Routing Network (MRN), where a series of\nrecognizers is trained for each language. Subsequently, a language predictor is\nadopted to weigh the recognizers for voting. Since the recognizers are derived\nfrom the original model, MRN effectively reduces the reliance on older data and\nis better suited for rehearsal-imbalance. We extensively evaluate MRN on MLT17\nand MLT19 datasets, outperforming existing state-of-the-art methods by a large\nmargin, i.e., accuracy improvement ranging from 10.3% to 27.4% under different\nsettings.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:03:34 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14759","submitter":"Xinlei Zhang","authors":"Xin-Lei Zhang, Heng Xiao, Xiaodong Luo, Guowei He","title":"Combining direct and indirect sparse data for learning generalizable\n  turbulence models","comments":"42 pages, 16 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.flu-dyn","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Learning turbulence models from observation data is of significant interest\nin discovering a unified model for a broad range of practical flow\napplications. Either the direct observation of Reynolds stress or the indirect\nobservation of velocity has been used to improve the predictive capacity of\nturbulence models. In this work, we propose combining the direct and indirect\nsparse data to train neural network-based turbulence models. The\nbackpropagation technique and the observation augmentation approach are used to\ntrain turbulence models with different observation data in a unified\nensemble-based framework. These two types of observation data can explore\nsynergy to constrain the model training in different observation spaces, which\nenables learning generalizable models from very sparse data. The present method\nis tested in secondary flows in a square duct and separated flows over periodic\nhills. Both cases demonstrate that combining direct and indirect observations\nis able to improve the generalizability of the learned model in similar flow\nconfigurations, compared to using only indirect data. The ensemble-based method\ncan serve as a practical tool for model learning from different types of\nobservations due to its non-intrusive and derivative-free nature.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:07:45 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14760","submitter":"Heming Xia","authors":"Shoujie Tong, Heming Xia, Damai Dai, Tianyu Liu, Binghuai Lin, Yunbo\n  Cao, Zhifang Sui","title":"Bi-Drop: Generalizable Fine-tuning for Pre-trained Language Models via\n  Adaptive Subnetwork Optimization","comments":"Work in progress. Co-first authors with equal contributions","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Pretrained language models have achieved remarkable success in a variety of\nnatural language understanding tasks. Nevertheless, finetuning large pretrained\nmodels on downstream tasks is susceptible to overfitting if the training set is\nlimited, which will lead to diminished performance. In this work, we propose a\ndynamic fine-tuning strategy for pretrained language models called Bi-Drop. It\nutilizes the gradient information of various sub-models generated by dropout to\nupdate the model parameters selectively. Experiments on the GLUE benchmark show\nthat Bi-Drop outperforms previous fine-tuning methods by a considerable margin,\nand exhibits consistent superiority over vanilla fine-tuning across various\npretrained models. Furthermore, empirical results indicate that Bi-Drop yields\nsubstantial improvements in the multiple task or domain transfer, data\nimbalance, and low-resource scenarios, demonstrating superb generalization\nability and robustness.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:09:26 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14761","submitter":"Parsa Kavehzadeh","authors":"Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, Shafiq Joty","title":"UniChart: A Universal Vision-language Pretrained Model for Chart\n  Comprehension and Reasoning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Charts are very popular for analyzing data, visualizing key insights and\nanswering complex reasoning questions about data. To facilitate chart-based\ndata analysis using natural language, several downstream tasks have been\nintroduced recently such as chart question answering and chart summarization.\nHowever, most of the methods that solve these tasks use pretraining on language\nor vision-language tasks that do not attempt to explicitly model the structure\nof the charts (e.g., how data is visually encoded and how chart elements are\nrelated to each other). To address this, we first build a large corpus of\ncharts covering a wide variety of topics and visual styles. We then present\nUniChart, a pretrained model for chart comprehension and reasoning. UniChart\nencodes the relevant text, data, and visual elements of charts and then uses a\nchart-grounded text decoder to generate the expected output in natural\nlanguage. We propose several chart-specific pretraining tasks that include: (i)\nlow-level tasks to extract the visual elements (e.g., bars, lines) and data\nfrom charts, and (ii) high-level tasks to acquire chart understanding and\nreasoning skills. We find that pretraining the model on a large corpus with\nchart-specific low- and high-level tasks followed by finetuning on three\ndown-streaming tasks results in state-of-the-art performance on three\ndownstream tasks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:11:17 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14762","submitter":"Taishi Kurahashi","authors":"Taishi Kurahashi and Yuta Sato","title":"The finite frame property of some extensions of the pure logic of\n  necessitation","comments":"16 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.LO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We study the finite frame property of some extensions of Fitting, Marek, and\nTruszczy\\'nski's pure logic of necessitation $\\mathbf{N}$. For any natural\nnumbers $m, n$, we introduce the logic $\\mathbf{N}^+\\mathbf{A}_{m, n}$ by\nadding the single axiom scheme $\\Box^n \\varphi \\to \\Box^m \\varphi$ and the rule\n$\\dfrac{\\neg \\Box \\varphi}{\\neg \\Box \\Box \\varphi}$ (Ros$^\\Box$) into\n$\\mathbf{N}$. We prove the finite frame property of $\\mathbf{N}^+\\mathbf{A}_{m,\nn}$ with respect to Fitting, Marek, and Truszczy\\'nski's relational semantics.\nWe also prove that the Ros$^\\Box$ rule is necessary for the completeness of\n$\\mathbf{N}^+\\mathbf{A}_{0, n}$ for $n \\ge 2$.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:12:02 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14763","submitter":"Vered Shwartz","authors":"Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin\n  Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz","title":"Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in\n  Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The escalating debate on AI's capabilities warrants developing reliable\nmetrics to assess machine \"intelligence\". Recently, many anecdotal examples\nwere used to suggest that newer large language models (LLMs) like ChatGPT and\nGPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached\nconflicting conclusions regarding those abilities. We investigate the extent of\nLLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs\nexhibit certain N-ToM abilities, this behavior is far from being robust. We\nfurther examine the factors impacting performance on N-ToM tasks and discover\nthat LLMs struggle with adversarial examples, indicating reliance on shallow\nheuristics rather than robust ToM abilities. We caution against drawing\nconclusions from anecdotal examples, limited benchmark testing, and using\nhuman-designed psychological tests to evaluate models.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:14:31 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14764","submitter":"Naoya Mamada","authors":"Naoya Mamada, Masaichiro Mizumaki, Ichiro Akai, and Toru Aonishi","title":"Detection of Non-uniformity in Parameters for Magnetic Domain Pattern\n  Generation by Machine Learning","comments":"27 pages, 11 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We attempt to estimate the spatial distribution of heterogeneous physical\nparameters involved in the formation of magnetic domain patterns of\npolycrystalline thin films by using convolutional neural networks. We propose a\nmethod to obtain a spatial map of physical parameters by estimating the\nparameters from patterns within a small subregion window of the full magnetic\ndomain and subsequently shifting this window. To enhance the accuracy of\nparameter estimation in such subregions, we employ employ large-scale models\nutilized for natural image classification and exploit the benefits of\npretraining. Using a model with high estimation accuracy on these subregions,\nwe conduct inference on simulation data featuring spatially varying parameters\nand demonstrate the capability to detect such parameter variations.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:15:27 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14765","submitter":"Insung Kong","authors":"Insung Kong, Dongyoon Yang, Jongjin Lee, Ilsang Ohn, Gyuseung Baek,\n  Yongdai Kim","title":"Masked Bayesian Neural Networks : Theoretical Guarantee and its\n  Posterior Inference","comments":"30 pages, ICML 2023 proceedings. arXiv admin note: substantial text\n  overlap with arXiv:2206.00853","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Bayesian approaches for learning deep neural networks (BNN) have been\nreceived much attention and successfully applied to various applications.\nParticularly, BNNs have the merit of having better generalization ability as\nwell as better uncertainty quantification. For the success of BNN, search an\nappropriate architecture of the neural networks is an important task, and\nvarious algorithms to find good sparse neural networks have been proposed. In\nthis paper, we propose a new node-sparse BNN model which has good theoretical\nproperties and is computationally feasible. We prove that the posterior\nconcentration rate to the true model is near minimax optimal and adaptive to\nthe smoothness of the true model. In particular the adaptiveness is the first\nof its kind for node-sparse BNNs. In addition, we develop a novel MCMC\nalgorithm which makes the Bayesian inference of the node-sparse BNN model\nfeasible in practice.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:16:11 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14766","submitter":"Xiao Liu","authors":"Hao Sun, Xiao Liu, Yeyun Gong, Anlei Dong, Jingwen Lu, Yan Zhang,\n  Daxin Jiang, Linjun Yang, Rangan Majumder, Nan Duan","title":"BeamSearchQA: Large Language Models are Strong Zero-Shot QA Solver","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Open-domain question answering is a crucial task that often requires\naccessing external information. Existing methods typically adopt a single-turn\nretrieve-then-read approach, where relevant documents are first retrieved, and\nquestions are then answered based on the retrieved information. However, there\nare cases where answering a question requires implicit knowledge that is not\ndirectly retrievable from the question itself. In this work, we propose a novel\nquestion-answering pipeline called BeamSearchQA. Our approach leverages large\nlanguage models to iteratively generate new questions about the original\nquestion, enabling an iterative reasoning process. By iteratively refining and\nexpanding the scope of the question, our method aims to capture and utilize\nhidden knowledge that may not be directly obtainable through retrieval. We\nevaluate our approach on the widely-used open-domain NQ and WebQ datasets. The\nexperimental results demonstrate that BeamSearchQA significantly outperforms\nother zero-shot baselines, indicating its effectiveness in tackling the\nchallenges of open-domain question answering.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:16:44 GMT"},{"version":"v2","created":"Thu, 1 Jun 2023 07:53:11 GMT"}],"update_date":"2023-06-02"}
{"id":"2305.14767","submitter":"Andi Wang","authors":"Andi Wang, Hao Yan, and Juan Du","title":"Interpretation and visualization of distance covariance through additive\n  decomposition of correlations formula","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Distance covariance is a widely used statistical methodology for testing the\ndependency between two groups of variables. Despite the appealing properties of\nconsistency and superior testing power, the testing results of distance\ncovariance are often hard to be interpreted. This paper presents an elementary\ninterpretation of the mechanism of distance covariance through an additive\ndecomposition of correlations formula. Based on this formula, a visualization\nmethod is developed to provide practitioners with a more intuitive explanation\nof the distance covariance score.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:17:38 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14768","submitter":"Zhengkai Jiang","authors":"Zhengkai Jiang and Liang Liu and Jiangning Zhang and Yabiao Wang and\n  Mingang Chen and Chengjie Wang","title":"Dual Path Transformer with Partition Attention","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper introduces a novel attention mechanism, called dual attention,\nwhich is both efficient and effective. The dual attention mechanism consists of\ntwo parallel components: local attention generated by Convolutional Neural\nNetworks (CNNs) and long-range attention generated by Vision Transformers\n(ViTs). To address the high computational complexity and memory footprint of\nvanilla Multi-Head Self-Attention (MHSA), we introduce a novel Multi-Head\nPartition-wise Attention (MHPA) mechanism. The partition-wise attention\napproach models both intra-partition and inter-partition attention\nsimultaneously. Building on the dual attention block and partition-wise\nattention mechanism, we present a hierarchical vision backbone called\nDualFormer. We evaluate the effectiveness of our model on several computer\nvision tasks, including image classification on ImageNet, object detection on\nCOCO, and semantic segmentation on Cityscapes. Specifically, the proposed\nDualFormer-XS achieves 81.5\\% top-1 accuracy on ImageNet, outperforming the\nrecent state-of-the-art MPViT-XS by 0.6\\% top-1 accuracy with much higher\nthroughput.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:17:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14769","submitter":"Raul Fuentes-Azcatl Dr.","authors":"Ra\\'ul Fuentes-Azcatl, Minerva Gonz\\'alez-Melchor","title":"Novel force field of [Bmim][Nf$_2$T] and its tranferability in a mixture\n  with water","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.soft physics.chem-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this work, a new force field is presented for the ionic liquid\n1-butyl-3-methylimidazolium-bis(trifluoromethylsulfonyl)imide, [Bmim][Nf$_2$T].\nAs a part of the \\epsilon force field, the acronym IL/\\epsilon is used to refer\nto this ionic liquid. This new force field reproduces the dielectric constant,\nthe density, and the entalphy of vaporization, with an error of less than 3%,\nbeing posible to generate new force fields for ionic liquids, based on the\nflexibility of the molecule. In addition, a study of the [Bmim][Nf$_2$T]-H$_2$O\nmixture is performed from pure IL to pure water, passing through various\nconcentrations.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:18:57 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14770","submitter":"Manya Wadhwa","authors":"Manya Wadhwa, Jifan Chen, Junyi Jessy Li, Greg Durrett","title":"Using Natural Language Explanations to Rescale Human Judgments","comments":"Data available at\n  https://github.com/ManyaWadhwa/explanation_based_rescaling","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The rise of large language models (LLMs) has brought a critical need for\nhigh-quality human-labeled data, particularly for processes like human feedback\nand evaluation. A common practice is to label data via consensus annotation\nover the judgments of multiple crowdworkers. However, different annotators may\nhave different interpretations of labeling schemes unless given extensive\ntraining, and for subjective NLP tasks, even trained expert annotators can\ndiverge heavily. We show that these nuances can be captured by high quality\nnatural language explanations, and propose a method to rescale ordinal\nannotation in the presence of disagreement using LLMs. Specifically, we feed\nLikert ratings and corresponding natural language explanations into an LLM and\nprompt it to produce a numeric score. This score should reflect the underlying\nassessment of the example by the annotator. The presence of explanations allows\nthe LLM to homogenize ratings across annotators in spite of scale usage\ndifferences. We explore our technique in the context of a document-grounded\nquestion answering task on which large language models achieve near-human\nperformance. Among questions where annotators identify incompleteness in the\nanswers, our rescaling improves correlation between nearly all annotator pairs,\nimproving pairwise correlation on these examples by an average of 0.2 Kendall's\ntau.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:19:14 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14771","submitter":"Xiaochuang Han","authors":"Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov, Marjan Ghazvininejad","title":"SSD-2: Scaling and Inference-time Fusion of Diffusion Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Diffusion-based language models (LMs) have been shown to be competent\ngenerative models that are easy to control at inference and are a promising\nalternative to autoregressive LMs. While autoregressive LMs have benefited\nimmensely from scaling and instruction-based learning, existing studies on\ndiffusion LMs have been conducted on a relatively smaller scale. Starting with\na recently proposed diffusion model SSD-LM, in this work we explore methods to\nscale it from 0.4B to 13B parameters, proposing several techniques to improve\nits training and inference efficiency. We call the new model SSD-2. We further\nshow that this model can be easily finetuned to follow instructions. Finally,\nleveraging diffusion models' capability at inference-time control, we show that\nSSD-2 facilitates novel ensembles with 100x smaller models that can be\ncustomized and deployed by individual users. We find that compared to\nautoregressive models, the collaboration between diffusion models is more\neffective, leading to higher-quality and more relevant model responses due to\ntheir ability to incorporate bi-directional contexts.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:22:14 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14772","submitter":"Benjamin Newman","authors":"Benjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, Kyle Lo","title":"A Controllable QA-based Framework for Decontextualization","comments":"11 pages, 3 figures, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Many real-world applications require surfacing extracted snippets to users,\nwhether motivated by assistive tools for literature surveys or document\ncross-referencing, or needs to mitigate and recover from model generated\ninaccuracies., Yet, these passages can be difficult to consume when divorced\nfrom their original document context. In this work, we explore the limits of\nLLMs to perform decontextualization of document snippets in user-facing\nscenarios, focusing on two real-world settings - question answering and\ncitation context previews for scientific documents. We propose a\nquestion-answering framework for decontextualization that allows for better\nhandling of user information needs and preferences when determining the scope\nof rewriting. We present results showing state-of-the-art LLMs under our\nframework remain competitive with end-to-end approaches. We also explore\nincorporating user preferences into the system, finding our framework allows\nfor controllability.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:23:02 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14773","submitter":"Hogyun Kim","authors":"Hogyun Kim, Gilhwan Kang, Seokhwan Jeong, Seungjun Ma and Younggun Cho","title":"Robust Imaging Sonar-based Place Recognition and Localization in\n  Underwater Environments","comments":"7 pages, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Place recognition using SOund Navigation and Ranging (SONAR) images is an\nimportant task for simultaneous localization and mapping(SLAM) in underwater\nenvironments. This paper proposes a robust and efficient imaging SONAR based\nplace recognition, SONAR context, and loop closure method. Unlike previous\nmethods, our approach encodes geometric information based on the\ncharacteristics of raw SONAR measurements without prior knowledge or training.\nWe also design a hierarchical searching procedure for fast retrieval of\ncandidate SONAR frames and apply adaptive shifting and padding to achieve\nrobust matching on rotation and translation changes. In addition, we can derive\nthe initial pose through adaptive shifting and apply it to the iterative\nclosest point (ICP) based loop closure factor. We evaluate the performance of\nSONAR context in the various underwater sequences such as simulated open water,\nreal water tank, and real underwater environments. The proposed approach shows\nthe robustness and improvements of place recognition on various datasets and\nevaluation metrics. Supplementary materials are available at\nhttps://github.com/sparolab/sonar_context.git.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:23:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14774","submitter":"Jordi Boronat","authors":"M. C. Gordillo and J. Boronat","title":"Phases of $^4$He and H$_2$ adsorbed on a single carbon nanotube","comments":null,"journal-ref":"Phys. Rev. B 107, 174518 (2023)","doi":"10.1103/PhysRevB.107.174518","report-no":null,"categories":"cond-mat.other","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Using a diffusion Monte Carlo (DMC) technique, we calculated the phase\ndiagrams of $^4$He and H$_2$ adsorbed on a single (5,5) carbon nanotube, one of\nthe narrowest that can be obtained experimentally. For a single monolayer, when\nthe adsorbate density increases, both species undergo a series of first order\nsolid-solid phase transitions between incommensurate arrangements. Remarkably,\nthe $^4$He lowest-density solid phase shows supersolid behavior in contrast\nwith the normal solid that we found for H$_2$. The nature of the second-layer\nis also different for both adsorbates. Contrarily to what happens on graphite,\nthe second-layer of $^4$He on that tube is a liquid, at least up to the density\ncorresponding to a third-layer promotion on a flat substrate. However, the\nsecond-layer of H$_2$ is a solid that, at its lowest stable density, has a\nsmall but observable superfluid fraction.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:24:01 GMT"}],"update_date":"2023-05-31"}
{"id":"2305.14775","submitter":"Amirhossein Kazemnejad","authors":"Amirhossein Kazemnejad, Mehdi Rezagholizadeh, Prasanna Parthasarathi,\n  Sarath Chandar","title":"Measuring the Knowledge Acquisition-Utilization Gap in Pretrained\n  Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  While pre-trained language models (PLMs) have shown evidence of acquiring\nvast amounts of knowledge, it remains unclear how much of this parametric\nknowledge is actually usable in performing downstream tasks. We propose a\nsystematic framework to measure parametric knowledge utilization in PLMs. Our\nframework first extracts knowledge from a PLM's parameters and subsequently\nconstructs a downstream task around this extracted knowledge. Performance on\nthis task thus depends exclusively on utilizing the model's possessed\nknowledge, avoiding confounding factors like insufficient signal. As an\ninstantiation, we study factual knowledge of PLMs and measure utilization\nacross 125M to 13B parameter PLMs. We observe that: (1) PLMs exhibit two gaps -\nin acquired vs. utilized knowledge, (2) they show limited robustness in\nutilizing knowledge under distribution shifts, and (3) larger models close the\nacquired knowledge gap but the utilized knowledge gap remains. Overall, our\nstudy provides insights into PLMs' capabilities beyond their acquired\nknowledge.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:26:11 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14776","submitter":"Yuchen Ding","authors":"Yuchen Ding and Lilu Zhao","title":"Solution to a problem of Luca, Menares and Pizarro-Madariaga","comments":"some minor typos are corrected","journal-ref":null,"doi":null,"report-no":null,"categories":"math.NT","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Let $k\\ge 2$ be a positive integer and $P^+(n)$ the greatest prime factor of\na positive integer $n$ with convention $P^+(1)=1$. For any $\\theta\\in\n\\left[\\frac 1{2k},\\frac{17}{32k}\\right)$, set\n$$T_{k,\\theta}(x)=\\sum_{\\substack{p_1\\cdot\\cdot\\cdot p_k\\le x\\\\\nP^+(\\gcd(p_1-1,...,p_k-1))\\ge (p_1\\cdot\\cdot\\cdot p_k)^\\theta}}1,$$ where the\n$p'$s are primes. It is proved that\n$$T_{k,\\theta}(x)\\ll_{k}\\frac{x^{1-\\theta(k-1)}}{(\\log x)^2},$$ which, together\nwith the lower bound $$T_{k,\\theta}(x)\\gg_{k}\\frac{x^{1-\\theta(k-1)}}{(\\log\nx)^2}$$ obtained by Wu in 2019, answer a 2015 problem of Luca, Menares and\nPizarro-Madariaga on the exact order of magnitude of $T_{k,\\theta}(x)$.\n  A main novelty in the proof is that, instead of using the Brun--Titchmarsh\ntheorem to estimate the $k^{th}$ movement of primes in arithmetic progressions,\nwe transform the movement to an estimation involving taking primes\nsimultaneously by linear shifts of primes.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:29:29 GMT"},{"version":"v2","created":"Mon, 5 Jun 2023 06:42:01 GMT"}],"update_date":"2023-06-06"}
{"id":"2305.14777","submitter":"Jaemoo Choi","authors":"Jaemoo Choi, Jaewoong Choi, Myungjoo Kang","title":"Generative Modeling through the Semi-dual Formulation of Unbalanced\n  Optimal Transport","comments":"23 pages, 15 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Optimal Transport (OT) problem investigates a transport map that bridges two\ndistributions while minimizing a given cost function. In this regard, OT\nbetween tractable prior distribution and data has been utilized for generative\nmodeling tasks. However, OT-based methods are susceptible to outliers and face\noptimization challenges during training. In this paper, we propose a novel\ngenerative model based on the semi-dual formulation of Unbalanced Optimal\nTransport (UOT). Unlike OT, UOT relaxes the hard constraint on distribution\nmatching. This approach provides better robustness against outliers, stability\nduring training, and faster convergence. We validate these properties\nempirically through experiments. Moreover, we study the theoretical upper-bound\nof divergence between distributions in UOT. Our model outperforms existing\nOT-based generative models, achieving FID scores of 2.97 on CIFAR-10 and 5.80\non CelebA-HQ-256.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:31:05 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14778","submitter":"Xiyuan Wang","authors":"Xiyuan Wang, Fangyuan Wang, Bo Xu, Liang Xu, Jing Xiao","title":"P-vectors: A Parallel-Coupled TDNN/Transformer Network for Speaker\n  Verification","comments":"Accepted by INTERSPEECH 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.AS cs.SD","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Typically, the Time-Delay Neural Network (TDNN) and Transformer can serve as\na backbone for Speaker Verification (SV). Both of them have advantages and\ndisadvantages from the perspective of global and local feature modeling. How to\neffectively integrate these two style features is still an open issue. In this\npaper, we explore a Parallel-coupled TDNN/Transformer Network (p-vectors) to\nreplace the serial hybrid networks. The p-vectors allows TDNN and Transformer\nto learn the complementary information from each other through Soft Feature\nAlignment Interaction (SFAI) under the premise of preserving local and global\nfeatures. Also, p-vectors uses the Spatial Frequency-channel Attention (SFA) to\nenhance the spatial interdependence modeling for input features. Finally, the\noutputs of dual branches of p-vectors are combined by Embedding Aggregation\nLayer (EAL). Experiments show that p-vectors outperforms MACCIF-TDNN and\nMFA-Conformer with relative improvements of 11.5% and 13.9% in EER on\nVoxCeleb1-O.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:33:38 GMT"},{"version":"v2","created":"Thu, 25 May 2023 07:40:15 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.14779","submitter":"Nikita Srivatsan","authors":"Nikita Srivatsan, Sofia Samaniego, Omar Florez, Taylor\n  Berg-Kirkpatrick","title":"Text Conditional Alt-Text Generation for Twitter Images","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this work we present an approach for generating alternative text (or\nalt-text) descriptions for images shared on social media, specifically Twitter.\nThis task is more than just a special case of image captioning, as alt-text is\nboth more literally descriptive and context-specific. Also critically, images\nposted to Twitter are often accompanied by user-written text that despite not\nnecessarily describing the image may provide useful context that if properly\nleveraged can be informative -- e.g. the tweet may name an uncommon object in\nthe image that the model has not previously seen. We address this with a CLIP\nprefix model that extracts an embedding of the image and passes it to a mapping\nnetwork that outputs a short sequence in word embedding space, or a ``prefix'',\nto which we also concatenate the text from the tweet itself. This lets the\nmodel condition on both visual and textual information from the post. The\ncombined multimodal prefix is then fed as a prompt to a pretrained language\nmodel which autoregressively completes the sequence to generate the alt-text.\nWhile prior work has used similar methods for captioning, ours is the first to\nour knowledge that incorporates textual information from the associated social\nmedia post into the prefix as well, and we further demonstrate through\nablations that utility of these two information sources stacks. We put forward\na new dataset scraped from Twitter and evaluate on it across a variety of\nautomated metrics as well as human evaluation, and show that our approach of\nconditioning on both tweet text and visual information significantly\noutperforms prior work.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:35:26 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14780","submitter":"Bo Xue","authors":"Bo Xue, Kayode Adedotun Oyesina and Alex M. H. Wong","title":"Electromagnetic Near-Field Mutual Coupling Suppression with Active Janus\n  Sources","comments":"21 pages, 6 figures, 1 table","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.app-ph physics.class-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Electric dipoles and magnetic dipoles are the most fundamental particles in\nelectromagnetic theory. Huygens and Janus sources, formed by the orthogonal\ncombination of electric and magnetic dipoles, both show good directionality in\nthe near field. Although the Huygens source has been widely used in antennas\nand metasurfaces, the applications of Janus source are heretofore limited. In\nthis paper we report the first physical construction of an active Janus source.\nThrough full-wave simulations within the PPW environment, we show that our\nsource achieves the directional electromagnetic near-field and quasi-isotropic\nfar-field requisite of the Janus source. Using this fact, we demonstrate that\ntwo active Janus sources in close proximity (about 0.10 to 0.25 wavelengths)\nachieve a near 1000-fold reduced mutual coupling compared to electric dipole\nsources. The achievement of strong mutual coupling suppression and\nquasi-isotropic radiation make the Janus source an ideal candidate for\nconsideration in future compact MIMO communication systems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:37:12 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.14781","submitter":"Qingyuan Liu","authors":"Qingyuan Liu, Zhengchao Huang, Hao Ye, Dexian Huang, Chao Shang","title":"Accelerated Nonconvex ADMM with Self-Adaptive Penalty for\n  Rank-Constrained Model Identification","comments":"7 pages, 4 figures. Submitted to 62nd IEEE Conference on Decision and\n  Control (CDC 2023)","journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC cs.SY eess.SY","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The alternating direction method of multipliers (ADMM) has been widely\nadopted in low-rank approximation and low-order model identification tasks;\nhowever, the performance of nonconvex ADMM is highly reliant on the choice of\npenalty parameter. To accelerate ADMM for solving rankconstrained\nidentification problems, this paper proposes a new self-adaptive strategy for\nautomatic penalty update. Guided by first-order analysis of the increment of\nthe augmented Lagrangian, the self-adaptive penalty updating enables effective\nand balanced minimization of both primal and dual residuals and thus ensures a\nstable convergence. Moreover, improved efficiency can be obtained within the\nAnderson acceleration scheme. Numerical examples show that the proposed\nstrategy significantly accelerates the convergence of nonconvex ADMM while\nalleviating the critical reliance on tedious tuning of penalty parameters.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:38:36 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14782","submitter":"Pengyuan Lu","authors":"Pengyuan Lu and Michele Caprio and Eric Eaton and Insup Lee","title":"Zero-shot Task Preference Addressing Enabled by Imprecise Bayesian\n  Continual Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Like generic multi-task learning, continual learning has the nature of\nmulti-objective optimization, and therefore faces a trade-off between the\nperformance of different tasks. That is, to optimize for the current task\ndistribution, it may need to compromise performance on some tasks to improve on\nothers. This means there exist multiple models that are each optimal at\ndifferent times, each addressing a distinct task-performance trade-off.\nResearchers have discussed how to train particular models to address specific\npreferences on these trade-offs. However, existing algorithms require\nadditional sample overheads -- a large burden when there are multiple, possibly\ninfinitely many, preferences. As a response, we propose Imprecise Bayesian\nContinual Learning (IBCL). Upon a new task, IBCL (1) updates a knowledge base\nin the form of a convex hull of model parameter distributions and (2) obtains\nparticular models to address preferences with zero-shot. That is, IBCL does not\nrequire any additional training overhead to construct preference-addressing\nmodels from its knowledge base. We show that models obtained by IBCL have\nguarantees in identifying the preferred parameters. Moreover, experiments show\nthat IBCL is able to locate the Pareto set of parameters given a preference,\nmaintain similar to better performance than baseline methods, and significantly\nreduce training overhead via zero-shot preference addressing.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:39:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14783","submitter":"Zihong Liang","authors":"Zihong Liang, Xiaojun Quan, Qifan Wang","title":"Disentangled Phonetic Representation for Chinese Spelling Correction","comments":"Accepted to ACL 2023 Main Conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Chinese Spelling Correction (CSC) aims to detect and correct erroneous\ncharacters in Chinese texts. Although efforts have been made to introduce\nphonetic information (Hanyu Pinyin) in this task, they typically merge phonetic\nrepresentations with character representations, which tends to weaken the\nrepresentation effect of normal texts. In this work, we propose to disentangle\nthe two types of features to allow for direct interaction between textual and\nphonetic information. To learn useful phonetic representations, we introduce a\npinyin-to-character objective to ask the model to predict the correct\ncharacters based solely on phonetic information, where a separation mask is\nimposed to disable attention from phonetic input to text. To avoid overfitting\nthe phonetics, we further design a self-distillation module to ensure that\nsemantic information plays a major role in the prediction. Extensive\nexperiments on three CSC benchmarks demonstrate the superiority of our method\nin using phonetic information.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:39:12 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14784","submitter":"Ameet Deshpande","authors":"Ameet Deshpande, Tanmay Rajpurohit, Karthik Narasimhan, Ashwin Kalyan","title":"Anthropomorphization of AI: Opportunities and Risks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.CY cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Anthropomorphization is the tendency to attribute human-like traits to\nnon-human entities. It is prevalent in many social contexts -- children\nanthropomorphize toys, adults do so with brands, and it is a literary device.\nIt is also a versatile tool in science, with behavioral psychology and\nevolutionary biology meticulously documenting its consequences. With widespread\nadoption of AI systems, and the push from stakeholders to make it human-like\nthrough alignment techniques, human voice, and pictorial avatars, the tendency\nfor users to anthropomorphize it increases significantly. We take a dyadic\napproach to understanding this phenomenon with large language models (LLMs) by\nstudying (1) the objective legal implications, as analyzed through the lens of\nthe recent blueprint of AI bill of rights and the (2) subtle psychological\naspects customization and anthropomorphization. We find that anthropomorphized\nLLMs customized for different user bases violate multiple provisions in the\nlegislative blueprint. In addition, we point out that anthropomorphization of\nLLMs affects the influence they can have on their users, thus having the\npotential to fundamentally change the nature of human-AI interaction, with\npotential for manipulation and negative influence. With LLMs being\nhyper-personalized for vulnerable groups like children and patients among\nothers, our work is a timely and important contribution. We propose a\nconservative strategy for the cautious use of anthropomorphization to improve\ntrustworthiness of AI systems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:39:45 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14785","submitter":"Yoav Goldberg","authors":"Victoria Basmov, Yoav Goldberg, Reut Tsarfaty","title":"ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  This paper sheds light on the limitations of ChatGPT's understanding\ncapabilities, focusing on simple inference tasks that are typically easy for\nhumans but appear to be challenging for the model. Specifically, we target (i)\ngrammatically-specified entailments, (ii) premises with evidential adverbs of\nuncertainty, and (iii) monotonicity entailments. We present expert-designed\nevaluation sets for these inference types and conduct experiments in a\nzero-shot setup. Our results show that the model struggles with these types of\ninferences, exhibiting moderate to low accuracy. Moreover, while ChatGPT\ndemonstrates knowledge of the underlying linguistic concepts when prompted\ndirectly, it often fails to incorporate this knowledge to make correct\ninferences. Even more strikingly, further experiments show that embedding the\npremise under presupposition triggers or non-factive verbs causes the model to\npredict entailment more frequently {regardless} of the correct semantic label.\nOverall these results suggest that, despite GPT's celebrated language\nunderstanding capacity, ChatGPT has blindspots with respect to certain types of\nentailment, and that certain entailment-cancelling features act as ``blinds''\novershadowing the semantics of the embedded premise. Our analyses emphasize the\nneed for further research into the linguistic comprehension and reasoning\ncapabilities of LLMs, in order to improve their reliability, and establish\ntheir trustworthiness for real-world applications.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:41:09 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14786","submitter":"Takuya Aoyama","authors":"Takuya Aoyama, Kenya Ohgushi","title":"Piezomagnetic Properties in Altermagnetic MnTe","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci cond-mat.str-el","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We examined the piezomagnetic effect in an antiferromagnet composed of MnTe,\nwhich is a candidate material for altermagnetism with a high critical\ntemperature. We observed that the magnetization develops with the application\nof stress and revealed that the piezomagnetic coefficient Q is\n1.38$\\times10^{-8}$ ${\\mu}$B/MPa at 300 K. The poling-field dependence of\nmagnetization indicates that the antiferromagnetic domain can be controlled\nusing the piezomagnetic effect. We demonstrate that the piezomagnetic effect is\nsuitable for detecting and controlling the broken time reversal symmetry in\naltermagnets.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:41:53 GMT"},{"version":"v2","created":"Thu, 1 Jun 2023 01:16:44 GMT"}],"update_date":"2023-06-02"}
{"id":"2305.14787","submitter":"Michael Baltaxe","authors":"Michael Baltaxe, Tomer Pe'er, Dan Levi","title":"Polarimetric Imaging for Perception","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Autonomous driving and advanced driver-assistance systems rely on a set of\nsensors and algorithms to perform the appropriate actions and provide alerts as\na function of the driving scene. Typically, the sensors include color cameras,\nradar, lidar and ultrasonic sensors. Strikingly however, although light\npolarization is a fundamental property of light, it is seldom harnessed for\nperception tasks. In this work we analyze the potential for improvement in\nperception tasks when using an RGB-polarimetric camera, as compared to an RGB\ncamera. We examine monocular depth estimation and free space detection during\nthe middle of the day, when polarization is independent of subject heading, and\nshow that a quantifiable improvement can be achieved for both of them using\nstate-of-the-art deep neural networks, with a minimum of architectural changes.\nWe also present a new dataset composed of RGB-polarimetric images, lidar scans,\nGNSS / IMU readings and free space segmentations that further supports\ndeveloping perception algorithms that take advantage of light polarization.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:42:27 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14788","submitter":"Alexander Wettig","authors":"Alexis Chevalier, Alexander Wettig, Anirudh Ajith, Danqi Chen","title":"Adapting Language Models to Compress Contexts","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Transformer-based language models (LMs) are powerful and widely-applicable\ntools, but their usefulness is constrained by a finite context window and the\nexpensive computational cost of processing long text documents. We propose to\nadapt pre-trained LMs into AutoCompressors. These models are capable of\ncompressing long contexts into compact summary vectors, which are then\naccessible to the model as soft prompts. Summary vectors are trained with an\nunsupervised objective, whereby long documents are processed in segments and\nsummary vectors from all previous segments are used in language modeling. We\nfine-tune OPT models on sequences of up to 30,720 tokens and show that\nAutoCompressors can utilize long contexts to improve perplexity. We evaluate\nAutoCompressors on in-context learning by compressing task demonstrations. We\nfind that summary vectors are good substitutes for plain-text demonstrations,\nincreasing accuracy while reducing inference cost. Finally, we explore the\nbenefits of pre-computing summary vectors for large corpora by applying summary\nvectors to retrieval-augmented language modeling. Overall, AutoCompressors\nemerge as a simple and inexpensive solution for extending the context window of\nLMs while speeding up inference over long contexts.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:42:44 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14789","submitter":"Xinyi Yuan","authors":"Junyi Xie, Xinyi Yuan","title":"Partial Heights and the Geometric Bombieri-Lang Conjecture","comments":"61 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.NT math.AG","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  We prove the geometric Bombieri-Lang conjecture for projective varieties\nwhich have finite morphisms to abelian varieties over function fields of\ncharacteristic 0. Our proof is complex analytic, which applies the classical\nBrody lemma to construct entire curves on complex varieties. Our key\ningredients includes a new notion of partial height and its non-degeneracy in a\nsuitable sense. The non-degeneracy is required in the application of the Brody\nlemma.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:43:05 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14790","submitter":"Feng Jiang","authors":"Feng Jiang, Weihao Liu, Xiaomin Chu, Peifeng Li, Qiaoming Zhu, Haizhou\n  Li","title":"Advancing Topic Segmentation and Outline Generation in Chinese Texts:\n  The Paragraph-level Topic Representation, Corpus, and Benchmark","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Topic segmentation and outline generation strive to divide a document into\ncoherent topic sections and generate corresponding subheadings. Such a process\nunveils the discourse topic structure of a document that benefits quickly\ngrasping and understanding the overall context of the document from a higher\nlevel. However, research and applications in this field have been restrained\ndue to the lack of proper paragraph-level topic representations and\nlarge-scale, high-quality corpora in Chinese compared to the success achieved\nin English. Addressing these issues, we introduce a hierarchical\nparagraph-level topic structure representation with title, subheading, and\nparagraph that comprehensively models the document discourse topic structure.\nIn addition, we ensure a more holistic representation of topic distribution\nwithin the document by using sentences instead of keywords to represent\nsub-topics. Following this representation, we construct the largest Chinese\nParagraph-level Topic Structure corpus (CPTS), four times larger than the\npreviously largest one. We also employ a two-stage man-machine collaborative\nannotation method to ensure the high quality of the corpus both in form and\nsemantics. Finally, we validate the computability of CPTS on two fundamental\ntasks (topic segmentation and outline generation) by several strong baselines,\nand its efficacy has been preliminarily confirmed on the downstream task:\ndiscourse parsing. The representation, corpus, and benchmark we established\nwill provide a solid foundation for future studies.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:43:23 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14791","submitter":"Yongqi Li","authors":"Yongqi Li, Mayi Xu, Xin Miao, Shen Zhou, Tieyun Qian","title":"Large Language Models as Counterfactual Generator: Strengths and\n  Weaknesses","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Large language models (LLMs) have demonstrated remarkable performance in a\nrange of natural language understanding and generation tasks. Yet, their\nability to generate counterfactuals, which can be used for areas like data\naugmentation, remains under-explored. This study aims to investigate the\ncounterfactual generation capabilities of LLMs and analysis factors that\ninfluence this ability. First, we evaluate how effective are LLMs in\ncounterfactual generation through data augmentation experiments for small\nlanguage models (SLMs) across four tasks: sentiment analysis, natural language\ninference, named entity recognition, and relation extraction. While LLMs show\npromising enhancements in various settings, they struggle in complex tasks due\nto their self-limitations and the lack of logical guidance to produce\ncounterfactuals that align with commonsense. Second, our analysis reveals the\npivotal role of providing accurate task definitions and detailed step-by-step\ninstructions to LLMs in generating counterfactuals. Interestingly, we also find\nthat LLMs can generate reasonable counterfactuals even with unreasonable\ndemonstrations, which illustrates that demonstrations are primarily to regulate\nthe output format.This study provides the first comprehensive insight into\ncounterfactual generation abilities of LLMs, and offers a novel perspective on\nutilizing LLMs for data augmentation to enhance SLMs.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:44:32 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14792","submitter":"Tianyu Li","authors":"Tianyu Li, Jungdam Won, Alexander Clegg, Jeonghwan Kim, Akshara Rai,\n  Sehoon Ha","title":"ACE: Adversarial Correspondence Embedding for Cross Morphology Motion\n  Retargeting from Human to Nonhuman Characters","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.GR","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Motion retargeting is a promising approach for generating natural and\ncompelling animations for nonhuman characters. However, it is challenging to\ntranslate human movements into semantically equivalent motions for target\ncharacters with different morphologies due to the ambiguous nature of the\nproblem. This work presents a novel learning-based motion retargeting\nframework, Adversarial Correspondence Embedding (ACE), to retarget human\nmotions onto target characters with different body dimensions and structures.\nOur framework is designed to produce natural and feasible robot motions by\nleveraging generative-adversarial networks (GANs) while preserving high-level\nmotion semantics by introducing an additional feature loss. In addition, we\npretrain a robot motion prior that can be controlled in a latent embedding\nspace and seek to establish a compact correspondence. We demonstrate that the\nproposed framework can produce retargeted motions for three different\ncharacters -- a quadrupedal robot with a manipulator, a crab character, and a\nwheeled manipulator. We further validate the design choices of our framework by\nconducting baseline comparisons and a user study. We also showcase sim-to-real\ntransfer of the retargeted motions by transferring them to a real Spot robot.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:44:32 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14793","submitter":"Nikhita Vedula","authors":"Zhuoer Wang, Marcus Collins, Nikhita Vedula, Simone Filice, Shervin\n  Malmasi, Oleg Rokhlenko","title":"Faithful Low-Resource Data-to-Text Generation through Cycle Training","comments":"19 pages, 4 figures, ACL 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Methods to generate text from structured data have advanced significantly in\nrecent years, primarily due to fine-tuning of pre-trained language models on\nlarge datasets. However, such models can fail to produce output faithful to the\ninput data, particularly on out-of-domain data. Sufficient annotated data is\noften not available for specific domains, leading us to seek an unsupervised\napproach to improve the faithfulness of output text. Since the problem is\nfundamentally one of consistency between the representations of the structured\ndata and text, we evaluate the effectiveness of cycle training in this work.\nCycle training uses two models which are inverses of each other: one that\ngenerates text from structured data, and one which generates the structured\ndata from natural language text. We show that cycle training, when initialized\nwith a small amount of supervised data (100 samples in our case), achieves\nnearly the same performance as fully supervised approaches for the data-to-text\ngeneration task on the WebNLG, E2E, WTQ, and WSQL datasets. We perform\nextensive empirical analysis with automated evaluation metrics and a newly\ndesigned human evaluation schema to reveal different cycle training strategies'\neffectiveness of reducing various types of generation errors. Our code is\npublicly available at https://github.com/Edillower/CycleNLG.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:44:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14794","submitter":"Chengyu Dong","authors":"Chengyu Dong, Zihan Wang, Jingbo Shang","title":"Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak\n  Supervision for Text Classification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Recent advances in weakly supervised text classification mostly focus on\ndesigning sophisticated methods to turn high-level human heuristics into\nquality pseudo-labels. In this paper, we revisit the seed matching-based\nmethod, which is arguably the simplest way to generate pseudo-labels, and show\nthat its power was greatly underestimated. We show that the limited performance\nof seed matching is largely due to the label bias injected by the simple\nseed-match rule, which prevents the classifier from learning reliable\nconfidence for selecting high-quality pseudo-labels. Interestingly, simply\ndeleting the seed words present in the matched input texts can mitigate the\nlabel bias and help learn better confidence. Subsequently, the performance\nachieved by seed matching can be improved significantly, making it on par with\nor even better than the state-of-the-art. Furthermore, to handle the case when\nthe seed words are not made known, we propose to simply delete the word tokens\nin the input text randomly with a high deletion ratio. Remarkably, seed\nmatching equipped with this random deletion method can often achieve even\nbetter performance than that with seed deletion.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:45:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14795","submitter":"Zexuan Zhong","authors":"Zexuan Zhong, Zhengxuan Wu, Christopher D. Manning, Christopher Potts,\n  Danqi Chen","title":"MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop\n  Questions","comments":"Our code and datasets are available at\n  https://github.com/princeton-nlp/MQuAKE","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The information stored in large language models (LLMs) falls out of date\nquickly, and retraining from scratch is often not an option. This has recently\ngiven rise to a range of techniques for injecting new facts through updating\nmodel weights. Current evaluation paradigms are extremely limited, mainly\nvalidating the recall of edited facts, but changing one fact should cause\nrippling changes to the model's related beliefs. If we edit the UK Prime\nMinister to now be Rishi Sunak, then we should get a different answer to Who is\nmarried to the British Prime Minister? In this work, we present a benchmark\nMQuAKE (Multi-hop Question Answering for Knowledge Editing) comprising\nmulti-hop questions that assess whether edited models correctly answer\nquestions where the answer should change as an entailed consequence of edited\nfacts. While we find that current knowledge-editing approaches can recall\nedited facts accurately, they fail catastrophically on the constructed\nmulti-hop questions. We thus propose a simple memory-based approach, MeLLo,\nwhich stores all edited facts externally while prompting the language model\niteratively to generate answers that are consistent with the edited facts.\nWhile MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up\nto 175B) and outperforms previous model editors by a large margin.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:48:41 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14796","submitter":"J. C. J. Koelemeij","authors":"J. C. J. Koelemeij, H. Dun, C. E. V. Diouf, E. F. Dierikx, G. J. M.\n  Janssen and C. C. J. M. Tiberius","title":"A hybrid optical-wireless network for decimetre-level terrestrial\n  positioning","comments":"38 pages, 9 figures, 3 tables","journal-ref":"Nature 611 (2022) 473-478","doi":"10.1038/s41586-022-05315-7","report-no":null,"categories":"physics.app-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Global navigation satellite systems (GNSS) are widely used for navigation and\ntime distribution, features indispensable for critical infrastructure such as\nmobile communication networks, as well as emerging technologies like automated\ndriving and sustainable energy grids. While GNSS can provide centimetre-level\nprecision, GNSS receivers are prone to many-metre errors due to multipath\npropagation and obstructed view of the sky, which occur especially in urban\nareas where accurate positioning is needed most. Moreover, the vulnerabilities\nof GNSS, combined with the lack of a back-up system, pose a severe risk to\nGNSS-dependent technologies. Here, we demonstrate a terrestrial positioning\nsystem which is independent of GNSS and offers superior performance through a\nconstellation of radio transmitters, connected and time-synchronised at the\nsub-nanosecond level through a fibre-optic Ethernet network. Employing optical\nand wireless transmission schemes similar to those encountered in mobile\ncommunication networks, and exploiting spectrally efficient virtual wideband\nsignals, the detrimental effects of multipath propagation are mitigated, thus\nenabling robust decimetre-level positioning and sub-nanosecond timing in a\nmultipath-prone outdoor environment. This work provides a glimpse of a future\nin which telecommunication networks provide not only connectivity, but also\nGNSS-independent timing and positioning services with unprecedented accuracy\nand reliability.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:50:46 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14797","submitter":"Xiao Li","authors":"Xiao Li, Igor Gilitschenski, Guy Rosman, Sertac Karaman, Daniela Rus","title":"Multi-Abstractive Neural Controller: An Efficient Hierarchical Control\n  Architecture for Interactive Driving","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  As learning-based methods make their way from perception systems to\nplanning/control stacks, robot control systems have started to enjoy the\nbenefits that data-driven methods provide. Because control systems directly\naffect the motion of the robot, data-driven methods, especially black box\napproaches, need to be used with caution considering aspects such as stability\nand interpretability. In this paper, we describe a differentiable and\nhierarchical control architecture. The proposed representation, called\n\\textit{multi-abstractive neural controller}, uses the input image to control\nthe transitions within a novel discrete behavior planner (referred to as the\nvisual automaton generative network, or \\textit{vAGN}). The output of a vAGN\ncontrols the parameters of a set of dynamic movement primitives which provides\nthe system controls. We train this neural controller with real-world driving\ndata via behavior cloning and show improved explainability, sample efficiency,\nand similarity to human driving.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:51:55 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14798","submitter":"Junyi Liu","authors":"Ying Cui, Junyi Liu, Jong-Shi Pang","title":"The Minimization of Piecewise Functions: Pseudo Stationarity","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  There are many significant applied contexts that require the solution of\ndiscontinuous optimization problems in finite dimensions. Yet these problems\nare very difficult, both computationally and analytically. With the functions\nbeing discontinuous and a minimizer (local or global) of the problems, even if\nit exists, being impossible to verifiably compute, a foremost question is what\nkind of ''stationary solutions'' one can expect to obtain; these solutions\nprovide promising candidates for minimizers; i.e., their defining conditions\nare necessary for optimality. Motivated by recent results on sparse\noptimization, we introduce in this paper such a kind of solution, termed\n''pseudo B- (for Bouligand) stationary solution'', for a broad class of\ndiscontinuous piecewise continuous optimization problems with objective and\nconstraint defined by indicator functions of the positive real axis composite\nwith functions that are possibly nonsmooth. We present two approaches for\ncomputing such a solution. One approach is based on lifting the problem to a\nhigher dimension via the epigraphical formulation of the indicator functions;\nthis requires the addition of some auxiliary variables. The other approach is\nbased on certain continuous (albeit not necessarily differentiable) piecewise\napproximations of the indicator functions and the convergence to a pseudo\nB-stationary solution of the original problem is established. The conditions\nfor convergence are discussed and illustrated by an example.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:52:21 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14799","submitter":"Hoang Nguyen","authors":"Hoang Tien Nguyen, Young-Jin Kim, and Dae-Hyun Choi","title":"Sample-Efficient Learning for a Surrogate Model of Three-Phase\n  Distribution System","comments":"Under review in IEEE PES Letter","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  A surrogate model that accurately predicts distribution system voltages is\ncrucial for reliable smart grid planning and operation. This letter proposes a\nfixed-point data-driven surrogate modeling method that employs a limited\ndataset to learn the power-voltage relationship of an unbalanced three-phase\ndistribution system. The proposed surrogate model is designed using a\nfixed-point load-flow equation, and the stochastic gradient descent method with\nan automatic differentiation technique is employed to update the parameters of\nthe surrogate model using complex power and voltage samples. Numerical examples\nin IEEE 13-bus, 37-bus, and 123-bus systems demonstrate that the proposed\nsurrogate model can outperform surrogate models based on the deep neural\nnetwork and Gaussian process regarding prediction accuracy and sample\nefficiency\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:52:25 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14800","submitter":"Yongliang Wu","authors":"Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, Xin Geng","title":"Exploring Diverse In-Context Configurations for Image Captioning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  After discovering that Language Models (LMs) can be good in-context few-shot\nlearners, numerous strategies have been proposed to optimize in-context\nsequence configurations. Recently, researchers in Vision-Language (VL) domains\nalso develop their few-shot learners, while they only use the simplest way,\ni.e., randomly sampling, to configure in-context image-text pairs. In order to\nexplore the effects of varying configurations on VL in-context learning, we\ndevised four strategies for image selection and four for caption assignment to\nconfigure in-context image-text pairs for image captioning. Here Image\nCaptioning is used as the case study since it can be seen as the\nvisually-conditioned LM. Our comprehensive experiments yield two\ncounter-intuitive but valuable insights, highlighting the distinct\ncharacteristics of VL in-context learning due to multi-modal synergy, as\ncompared to the NLP case.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:52:47 GMT"},{"version":"v2","created":"Fri, 26 May 2023 13:17:45 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.14801","submitter":"Elvira Zappale","authors":"Giuliano Gargiulo and Elvira Zappale","title":"A sufficient condition for the lower semicontinuity of nonlocal supremal\n  functionals in the vectorial case","comments":"to appear in European Journal of Mathematics","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP math.OC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this note we present a sufficient condition ensuring lower semicontinuity\nfor nonlocal supremal functionals of the type $$W^{1,\\infty}(\\Omega;\\mathbb\nR^d)\\ni u \\mapsto \\sup{\\rm ess}_{(x,y)\\in \\Omega} W(x,y, \\nabla u(x),\\nabla\nu(y)),$$ where $\\Omega$ is a bounded open subset of $\\mathbb R^N$ and $W:\\Omega\n\\times \\Omega \\times \\mathbb R^{d \\times N}\\times \\mathbb R^{d \\times N} \\to\n\\mathbb R$.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:54:39 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14802","submitter":"Harvey Yiyun Fu","authors":"Harvey Yiyun Fu, Qinyuan Ye, Albert Xu, Xiang Ren, Robin Jia","title":"Estimating Large Language Model Capabilities without Labeled Test Data","comments":"14 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Large Language Models (LLMs) have exhibited an impressive ability to perform\nin-context learning (ICL) from only a few examples, but the success of ICL\nvaries widely from task to task. Thus, it is important to quickly determine\nwhether ICL is applicable to a new task, but directly evaluating ICL accuracy\ncan be expensive in situations where test data is expensive to annotate -- the\nexact situations where ICL is most appealing. In this paper, we propose the\ntask of ICL accuracy estimation, in which we predict the accuracy of an LLM\nwhen doing in-context learning on a new task given only unlabeled data for that\ntask. To perform ICL accuracy estimation, we propose a method that trains a\nmeta-model using LLM confidence scores as features. We compare our method to\nseveral strong accuracy estimation baselines on a new benchmark that covers 4\nLLMs and 3 task collections. On average, the meta-model improves over all\nbaselines and achieves the same estimation performance as directly evaluating\non 40 labeled test examples per task, across the total 12 settings. We\nencourage future work to improve on our methods and evaluate on our ICL\naccuracy estimation benchmark to deepen our understanding of when ICL works.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:55:09 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14803","submitter":"Gilles Dowek","authors":"Gilles Dowek (LOGICAL)","title":"Preliminary investigations on induction over real numbers","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The induction principle for natural numbers expresses that when a property\nholds for some natural number a and is hereditary, then it holds for all\nnumbers greater than or equal to a. We present a similar principle for real\nnumbers.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:58:16 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14804","submitter":"Jean-Marie MALHERBE","authors":"Jean-Marie Malherbe, Florence Cornu, Isabelle Bual\\'e","title":"Solex observations for the BASS2000 database, a collaboration PRO-AM","comments":"arXiv admin note: text overlap with arXiv:2301.11105","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.IM astro-ph.SR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Systematic observations of the chromosphere and the photosphere started in\nMeudon Observatory 115 years ago with Deslandres spectroheliograph. An\nexceptional collection of more than 100 000 monochromatic images in CaII K and\nH$\\alpha$ spanning more than 10 solar cycles is proposed to the international\ncommunity by the BASS2000 solar database. We started in 2023 a ''PRO-AM''\ncollaboration between professional and amateur astronomers with the Solar\nExplorer (SOLEX), a compact and high quality spectroheliograph designed by\nChristian Buil, in order to record images every day, and several times per day,\nowing to tens of observing stations in various places. This paper summarizes\nthe scientific objectives and provides practical and technical information to\namateurs willing to join the observing network.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:59:47 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14805","submitter":"Kailiang Wu","authors":"Chaoyi Cai, Jianxian Qiu, Kailiang Wu","title":"Provably convergent Newton-Raphson methods for recovering primitive\n  variables with applications to physical-constraint-preserving Hermite WENO\n  schemes for relativistic hydrodynamics","comments":"49 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA astro-ph.IM cs.NA physics.comp-ph physics.flu-dyn","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The relativistic hydrodynamics (RHD) equations have three crucial intrinsic\nphysical constraints on the primitive variables: positivity of pressure and\ndensity, and subluminal fluid velocity. However, numerical simulations can\nviolate these constraints, leading to nonphysical results or even simulation\nfailure. Designing genuinely physical-constraint-preserving (PCP) schemes is\nvery difficult, as the primitive variables cannot be explicitly reformulated\nusing conservative variables due to relativistic effects. In this paper, we\npropose three efficient Newton--Raphson (NR) methods for robustly recovering\nprimitive variables from conservative variables. Importantly, we rigorously\nprove that these NR methods are always convergent and PCP, meaning they\npreserve the physical constraints throughout the NR iterations. The discovery\nof these robust NR methods and their PCP convergence analyses are highly\nnontrivial and technical. As an application, we apply the proposed NR methods\nto design PCP finite volume Hermite weighted essentially non-oscillatory\n(HWENO) schemes for solving the RHD equations. Our PCP HWENO schemes\nincorporate high-order HWENO reconstruction, a PCP limiter, and\nstrong-stability-preserving time discretization. We rigorously prove the PCP\nproperty of the fully discrete schemes using convex decomposition techniques.\nMoreover, we suggest the characteristic decomposition with rescaled\neigenvectors and scale-invariant nonlinear weights to enhance the performance\nof the HWENO schemes in simulating large-scale RHD problems. Several demanding\nnumerical tests are conducted to demonstrate the robustness, accuracy, and high\nresolution of the proposed PCP HWENO schemes and to validate the efficiency of\nour NR methods.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:59:54 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14806","submitter":"Shuyang Cao","authors":"Shuyang Cao and Lu Wang","title":"AWESOME: GPU Memory-constrained Long Document Summarization using Memory\n  Mechanism and Global Salient Content","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Long document summarization systems are critical for domains with lengthy and\njargonladen text, yet they present significant challenges to researchers and\ndevelopers with limited computing resources. Existing solutions mainly focus on\nefficient attentions or divide-and-conquer strategies. The former reduces\ntheoretical time complexity, but is still memory-heavy. The latter methods\nsacrifice global context, leading to uninformative and incoherent summaries.\nThis work aims to leverage the memory-efficient nature of divide-and-conquer\nmethods while preserving global context. Concretely, our framework AWESOME uses\ntwo novel mechanisms: (1) External memory mechanisms track previously encoded\ndocument segments and their corresponding summaries, to enhance global document\nunderstanding and summary coherence. (2) Global salient content is further\nidentified beforehand to augment each document segment to support its\nsummarization. Extensive experiments on diverse genres of text, including\ngovernment reports, transcripts, scientific papers, and novels, show that\nAWESOME produces summaries with improved informativeness, faithfulness, and\ncoherence than competitive baselines on longer documents, while having a\nsimilar or smaller GPU memory footprint.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:00:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14807","submitter":"Gil Weinberg","authors":"Gil Weinberg, Uri Weiss, Ori Katz","title":"Image scanning lensless fiber-bundle endomicroscopy","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.optics physics.app-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Fiber-based confocal endomicroscopy has shown great promise for\nminimally-invasive deep-tissue imaging. Despite its advantages, confocal\nfiber-bundle endoscopy inherently suffers from undersampling due to the spacing\nbetween fiber cores, and low collection efficiency when the target is not in\nproximity to the distal fiber facet. Here, we demonstrate an adaptation of\nimage-scanning microscopy (ISM) to lensless fiber bundle endoscopy, doubling\nthe spatial sampling frequency and significantly improving collection\nefficiency. Our approach only requires replacing the confocal detector with a\ncamera. It improves the spatial resolution for targets placed at a distance\nfrom the fiber tip, and addresses the fundamental challenge of\naliasing/pixelization artifacts.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:01:22 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14808","submitter":"Yuwei Zhang","authors":"Yuwei Zhang and Zhi Jin and Zejun Wang and Ying Xing and Ge Li","title":"SAGA: Summarization-Guided Assert Statement Generation","comments":"Preprint, to appear in the Journal of Computer Science and Technology\n  (JCST)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Generating meaningful assert statements is one of the key challenges in\nautomated test case generation, which requires understanding the intended\nfunctionality of the tested code. Recently, deep learning-based models have\nshown promise in improving the performance of assert statement generation.\nHowever, existing models only rely on the test prefixes along with their\ncorresponding focal methods, yet ignore the developer-written summarization.\nBased on our observations, the summarization contents usually express the\nintended program behavior or contain parameters that will appear directly in\nthe assert statement. Such information will help existing models address their\ncurrent inability to accurately predict assert statements. This paper presents\na novel summarization-guided approach for automatically generating assert\nstatements. To derive generic representations for natural language (i.e.,\nsummarization) and programming language (i.e., test prefixes and focal\nmethods), we leverage a pre-trained language model as the reference\narchitecture and fine-tune it on the task of assert statement generation. To\nthe best of our knowledge, the proposed approach makes the first attempt to\nleverage the summarization of focal methods as the guidance for making the\ngenerated assert statements more accurate. We demonstrate the effectiveness of\nour approach on two real-world datasets when compared with state-of-the-art\nmodels.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:03:21 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14809","submitter":"Khondokar Fida Hasan","authors":"Keyvan Ansari and Khondokar Fida Hasan","title":"Proposition of Augmenting V2X Roadside Unit to Enhance Cooperative\n  Awareness of Heterogeneously Connected Road Users","comments":"13 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.NI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Intelligent transportation and autonomous mobility solutions rely on\ncooperative awareness developed by exchanging proximity and mobility data among\nroad users. To maintain pervasive awareness on roads, all vehicles and\nvulnerable road users must be identified, either cooperatively, where road\nusers equipped with wireless capabilities of Vehicle-to-Everything (V2X) radios\ncan communicate with one another, or passively, where users without V2X\ncapabilities are detected by means other than V2X communications. This\nnecessitates the establishment of a communications channel among all\nV2X-enabled road users, regardless of whether their underlying V2X technology\nis compatible or not. At the same time, for cooperative awareness to realize\nits full potential, non-V2X-enabled road users must also be communicated with\nwhere possible or, leastwise, be identified passively. However, the question is\nwhether current V2X technologies can provide such a welcoming heterogeneous\nroad environment for all parties, including varying V2X-enabled and\nnon-V2X-enabled road users? This paper investigates the roles of a\npropositional concept named Augmenting V2X Roadside Unit (A-RSU) in enabling\nheterogeneous vehicular networks to support and benefit from pervasive\ncooperative awareness. To this end, this paper explores the efficacy of A-RSU\nin establishing pervasive cooperative awareness and investigates the\ncapabilities of the available communication networks using secondary data. The\nprimary findings suggest that A-RSU is a viable solution for accommodating all\ntypes of road users regardless of their V2X capabilities.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:03:32 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14810","submitter":"Jiongnan Liu","authors":"Jiongnan Liu, Zhicheng Dou, Guoyu Tang, Sulong Xu","title":"JDsearch: A Personalized Product Search Dataset with Real Queries and\n  Full Interactions","comments":"Accepted to SIGIR 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Recently, personalized product search attracts great attention and many\nmodels have been proposed. To evaluate the effectiveness of these models,\nprevious studies mainly utilize the simulated Amazon recommendation dataset,\nwhich contains automatically generated queries and excludes cold users and tail\nproducts. We argue that evaluating with such a dataset may yield unreliable\nresults and conclusions, and deviate from real user satisfaction. To overcome\nthese problems, in this paper, we release a personalized product search dataset\ncomprised of real user queries and diverse user-product interaction types\n(clicking, adding to cart, following, and purchasing) collected from JD.com, a\npopular Chinese online shopping platform. More specifically, we sample about\n170,000 active users on a specific date, then record all their interacted\nproducts and issued queries in one year, without removing any tail users and\nproducts. This finally results in roughly 12,000,000 products, 9,400,000 real\nsearches, and 26,000,000 user-product interactions. We study the\ncharacteristics of this dataset from various perspectives and evaluate\nrepresentative personalization models to verify its feasibility. The dataset\ncan be publicly accessed at Github: https://github.com/rucliujn/JDsearch.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:06:21 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14811","submitter":"Magali Hersant","authors":"Magali Hersant (CREN)","title":"Doing mathematics in kindergarten: Under what conditions?","comments":"in French language","journal-ref":"Grand N, Revue de math{\\'e}matiques, de sciences et technologie\n  pour les ma{\\^i}tres de l'enseignement primaire, 2022, 110, pp.4-16","doi":null,"report-no":null,"categories":"math.HO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this article, we identify the didactic conditions for learning mathematics\nin kindergarten. To do so, we rely on the framework of the theory of didactic\nsituations (Brousseau, 1998) and the notion of problem-situation (Douady,\n1984). We first explain what constitutes for us the stakes of teaching\nmathematics in kindergarten and then, based on examples, we highlight the\nconditions related to the stakes of the pupils' activity, to the\ncharacteristics of the situations proposed to the pupils and to the teacher's\ninterventions.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:08:17 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14812","submitter":"Hyun-Chul Kim","authors":"Samson Clymton and Hyun-Chul Kim","title":"Two-pole structure of the $b_1$(1235) axial-vector meson","comments":"10 pages and 6 figures. The version submitted to PRD","journal-ref":null,"doi":null,"report-no":"INHA-NTG-03/2023","categories":"hep-ph hep-ex","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We investigate the dynamical generation of the $b_1$ meson in the $\\pi\\omega$\ninteraction, using the fully off-mass-shell coupled-channel formalism with the\n$\\pi\\omega$, $\\eta\\rho$, $\\pi\\phi$, and $K\\bar{K}^*$ channels included. We\nfirst construct the Feynman amplitudes for the sixteen different kernel\namplitudes, considering only the $t$ and $u$ channels. Solving the coupled\nintegral equation, we obtain the transition amplitude for the $\\pi\\omega$\ninteraction. We select the axial-vector and isovector channels from the\npartial-wave expansion and single out the two poles corresponding to the $b_1$\nmesons: $(1306-i70)$ MeV and $(1356-i65)$ MeV. They are located below the\n$K\\bar{K}^*$ threshold. The first pole lies below the $\\eta\\rho$ threshold by\nabout 10 MeV, whereas the second one emerges above it by about 40 MeV. We\nanalyze the effects of the two poles and background contributions to the\n$\\pi\\omega$ total cross section by using a toy model.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:08:29 GMT"},{"version":"v2","created":"Fri, 26 May 2023 08:16:23 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.14813","submitter":"Yuhang Zang","authors":"Yuhang Zang, Kaiyang Zhou, Chen Huang, Chen Change Loy","title":"Semi-Supervised and Long-Tailed Object Detection with CascadeMatch","comments":"International Journal of Computer Vision (IJCV), 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  This paper focuses on long-tailed object detection in the semi-supervised\nlearning setting, which poses realistic challenges, but has rarely been studied\nin the literature. We propose a novel pseudo-labeling-based detector called\nCascadeMatch. Our detector features a cascade network architecture, which has\nmulti-stage detection heads with progressive confidence thresholds. To avoid\nmanually tuning the thresholds, we design a new adaptive pseudo-label mining\nmechanism to automatically identify suitable values from data. To mitigate\nconfirmation bias, where a model is negatively reinforced by incorrect\npseudo-labels produced by itself, each detection head is trained by the\nensemble pseudo-labels of all detection heads. Experiments on two long-tailed\ndatasets, i.e., LVIS and COCO-LT, demonstrate that CascadeMatch surpasses\nexisting state-of-the-art semi-supervised approaches -- across a wide range of\ndetection architectures -- in handling long-tailed object detection. For\ninstance, CascadeMatch outperforms Unbiased Teacher by 1.9 AP Fix on LVIS when\nusing a ResNet50-based Cascade R-CNN structure, and by 1.7 AP Fix when using\nSparse R-CNN with a Transformer encoder. We also show that CascadeMatch can\neven handle the challenging sparsely annotated object detection problem.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:09:25 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14814","submitter":"Samuel Vaiter","authors":"Nicolas Keriven (CNRS, IRISA), Samuel Vaiter (CNRS, LJAD)","title":"What functions can Graph Neural Networks compute on random graphs? The\n  role of Positional Encoding","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We aim to deepen the theoretical understanding of Graph Neural Networks\n(GNNs) on large graphs, with a focus on their expressive power. Existing\nanalyses relate this notion to the graph isomorphism problem, which is mostly\nrelevant for graphs of small sizes, or studied graph classification or\nregression tasks, while prediction tasks on nodes are far more relevant on\nlarge graphs. Recently, several works showed that, on very general random\ngraphs models, GNNs converge to certains functions as the number of nodes\ngrows. In this paper, we provide a more complete and intuitive description of\nthe function space generated by equivariant GNNs for node-tasks, through\ngeneral notions of convergence that encompass several previous examples. We\nemphasize the role of input node features, and study the impact of node\nPositional Encodings (PEs), a recent line of work that has been shown to yield\nstate-of-the-art results in practice. Through the study of several examples of\nPEs on large random graphs, we extend previously known universality results to\nsignificantly more general models. Our theoretical results hint at some\nnormalization tricks, which is shown numerically to have a positive impact on\nGNN generalization on synthetic and real data. Our proofs contain new\nconcentration inequalities of independent interest.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:09:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14815","submitter":"Dung Thai","authors":"Dung Thai, Dhruv Agarwal, Mudit Chaudhary, Rajarshi Das, Manzil\n  Zaheer, Jay-Yoon Lee, Hannaneh Hajishirzi, Andrew McCallum","title":"Machine Reading Comprehension using Case-based Reasoning","comments":"9 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.IR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We present an accurate and interpretable method for answer extraction in\nmachine reading comprehension that is reminiscent of case-based reasoning (CBR)\nfrom classical AI. Our method (CBR-MRC) builds on the hypothesis that\ncontextualized answers to similar questions share semantic similarities with\neach other. Given a target question, CBR-MRC retrieves a set of similar\nquestions from a memory of observed cases and predicts an answer by selecting\nthe span in the target context that is most similar to the contextualized\nrepresentations of answers in the retrieved cases. The semi-parametric nature\nof our approach allows CBR-MRC to attribute a prediction to the specific set of\ncases used during inference, making it a desirable choice for building reliable\nand debuggable QA systems. We show that CBR-MRC achieves high test accuracy\ncomparable with large reader models, outperforming baselines by 11.5 and 8.4 EM\non NaturalQuestions and NewsQA, respectively. Further, we also demonstrate the\nability of CBR-MRC in identifying not just the correct answer tokens but also\nthe span with the most relevant supporting evidence. Lastly, we observe that\ncontexts for certain question types show higher lexical diversity than others\nand find CBR-MRC to be robust to these variations while performance using\nfully-parametric methods drops.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:09:56 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14816","submitter":"Wenhao Zhan","authors":"Wenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D. Lee, Wen Sun","title":"Provable Offline Reinforcement Learning with Human Feedback","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG math.ST stat.ML stat.TH","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  In this paper, we investigate the problem of offline reinforcement learning\nwith human feedback where feedback is available in the form of preference\nbetween trajectory pairs rather than explicit rewards. Our proposed algorithm\nconsists of two main steps: (1) estimate the implicit reward using Maximum\nLikelihood Estimation (MLE) with general function approximation from offline\ndata and (2) solve a distributionally robust planning problem over a confidence\nset around the MLE. We consider the general reward setting where the reward can\nbe defined over the whole trajectory and provide a novel guarantee that allows\nus to learn any target policy with a polynomial number of samples, as long as\nthe target policy is covered by the offline data. This guarantee is the first\nof its kind with general function approximation. To measure the coverage of the\ntarget policy, we introduce a new single-policy concentrability coefficient,\nwhich can be upper bounded by the per-trajectory concentrability coefficient.\nWe also establish lower bounds that highlight the necessity of such\nconcentrability and the difference from standard RL, where state-action-wise\nrewards are directly observed. We further extend and analyze our algorithm when\nthe feedback is given over action pairs.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:11:26 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14817","submitter":"Subhadeep Gupta","authors":"Jun Hui See Toh, Mengxin Du, Xinxin Tang, Ying Su, Tristan Rojo,\n  Carson O. Patterson, Nicolas R. Williams, Chuanwei Zhang and Subhadeep Gupta","title":"Evidence for a Many-Body Anderson Metal-Insulator Transition using\n  Kicked Quantum Gases","comments":"15 pages, 11 figures, 2 tables, including supplementary materials","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.quant-gas physics.atom-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Understanding the interplay of interactions and disorder in quantum transport\nposes long-standing fundamental challenges for theory and experiment. Despite\nremarkable advances using ultracold atomic platforms combining atomic\ninteractions with spatially disordered lattices, many-body effects on quantum\ntransport phenomena in high-dimensional disordered systems, such as the\nthree-dimensional (d = 3) Anderson metal-insulator transition (MIT), have\nlargely remained unexplored. Here we utilize a momentum space lattice platform\nusing quasi-periodically kicked ultracold atomic gases as a quantum simulator\nto experimentally investigate the role of many-body interactions in the d = 3\nAnderson MIT. We observe interaction-driven sub-diffusive delocalization and\nfind a divergence of the delocalization onset time as kick strength approaches\nthe many-body phase boundary. By modifying the kick quasi-periodicity, we\ndemonstrate interaction-driven sub-diffusion in d = 2 and d = 4. Our numerical\nsimulations using a mean-field treatment exhibit an interaction-induced shift\nof the d = 3 transition boundary and many-body delocalization dynamics, that\nare both in qualitative agreement with experimental observations. However,\nthere are significant quantitative deviations between experiment and mean-field\ntheory which increase with higher interaction strengths, calling for further\nstudy of the underlying many-body physics.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:12:11 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14818","submitter":"Raunak Shah","authors":"Koyel Mukherjee, Raunak Shah, Shiv Kumar Saini, Karanpreet Singh,\n  Khushi, Harsh Kesarwani, Kavya Barnwal, Ayush Chauhan","title":"Towards Optimizing Storage Costs on the Cloud","comments":"The first two authors contributed equally. 12 pages, Accepted to the\n  International Conference on Data Engineering (ICDE) 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DB","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We study the problem of optimizing data storage and access costs on the cloud\nwhile ensuring that the desired performance or latency is unaffected. We first\npropose an optimizer that optimizes the data placement tier (on the cloud) and\nthe choice of compression schemes to apply, for given data partitions with\ntemporal access predictions. Secondly, we propose a model to learn the\ncompression performance of multiple algorithms across data partitions in\ndifferent formats to generate compression performance predictions on the fly,\nas inputs to the optimizer. Thirdly, we propose to approach the data\npartitioning problem fundamentally differently than the current default in most\ndata lakes where partitioning is in the form of ingestion batches. We propose\naccess pattern aware data partitioning and formulate an optimization problem\nthat optimizes the size and reading costs of partitions subject to access\npatterns.\n  We study the various optimization problems theoretically as well as\nempirically, and provide theoretical bounds as well as hardness results. We\npropose a unified pipeline of cost minimization, called SCOPe that combines the\ndifferent modules. We extensively compare the performance of our methods with\nrelated baselines from the literature on TPC-H data as well as enterprise\ndatasets (ranging from GB to PB in volume) and show that SCOPe substantially\nimproves over the baselines. We show significant cost savings compared to\nplatform baselines, of the order of 50% to 83% on enterprise Data Lake datasets\nthat range from terabytes to petabytes in volume.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:12:25 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14819","submitter":"gong chen","authors":"Chen Gong (LJLL), Yvon Maday (LJLL, IUF)","title":"Directed Message Passing Based on Attention for Prediction of Molecular\n  Properties","comments":"Computational Materials Science, In press","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT math.IT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Molecular representation learning (MRL) has long been crucial in the fields\nof drug discovery and materials science, and it has made significant progress\ndue to the development of natural language processing (NLP) and graph neural\nnetworks (GNNs). NLP treats the molecules as one dimensional sequential tokens\nwhile GNNs treat them as two dimensional topology graphs. Based on different\nmessage passing algorithms, GNNs have various performance on detecting chemical\nenvironments and predicting molecular properties. Herein, we propose Directed\nGraph Attention Networks (D-GATs): the expressive GNNs with directed bonds. The\nkey to the success of our strategy is to treat the molecular graph as directed\ngraph and update the bond states and atom states by scaled dot-product\nattention mechanism. This allows the model to better capture the sub-structure\nof molecular graph, i.e., functional groups. Compared to other GNNs or Message\nPassing Neural Networks (MPNNs), D-GATs outperform the state-of-the-art on 13\nout of 15 important molecular property prediction benchmarks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:13:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14820","submitter":"Kailiang Wu","authors":"Shengrong Ding, Kailiang Wu","title":"A new discretely divergence-free positivity-preserving high-order finite\n  volume method for ideal MHD equations","comments":"26 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA astro-ph.IM cs.NA physics.comp-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper proposes and analyzes a novel efficient high-order finite volume\nmethod for the ideal magnetohydrodynamics (MHD). As a distinctive feature, the\nmethod simultaneously preserves a discretely divergence-free (DDF) constraint\non the magnetic field and the positivity-preserving (PP) property, which\nensures the positivity of density, pressure, and internal energy. To enforce\nthe DDF condition, we design a new discrete projection approach that projects\nthe reconstructed point values at the cell interface into a DDF space, without\nusing any approximation polynomials. This projection method is highly\nefficient, easy to implement, and particularly suitable for standard high-order\nfinite volume WENO methods, which typically return only the point values in the\nreconstruction. Moreover, we also develop a new finite volume framework for\nconstructing provably PP schemes for the ideal MHD system. The framework\ncomprises the discrete projection technique, a suitable approximation to the\nGodunov--Powell source terms, and a simple PP limiter. We provide rigorous\nanalysis of the PP property of the proposed finite volume method, demonstrating\nthat the DDF condition and the proper approximation to the source terms\neliminate the impact of magnetic divergence terms on the PP property. The\nanalysis is challenging due to the internal energy function's nonlinearity and\nthe intricate relationship between the DDF and PP properties. To address these\nchallenges, the recently developed geometric quasilinearization approach is\nadopted, which transforms a nonlinear constraint into a family of linear\nconstraints. Finally, we validate the effectiveness of the proposed method\nthrough several benchmark and demanding numerical examples. The results\ndemonstrate that the proposed method is robust, accurate, and highly effective,\nconfirming the significance of the proposed DDF projection and PP techniques.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:18:29 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14821","submitter":"Attila Szolnoki","authors":"Attila Szolnoki and Xiaojie Chen","title":"Emerging solutions from the battle of defensive alliances","comments":"11 pages, 8 figures, accepted for publication in Scientific Reports","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.soc-ph cond-mat.stat-mech cs.GT q-bio.PE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Competing strategies in an evolutionary game model, or species in a\nbiosystem, can easily form a larger unit which protects them from the invasion\nof an external actor. Such a defensive alliance may have two, three, four or\neven more members. But how effective can be such formation against an\nalternative group composed by other competitors? To address this question we\nstudy a minimal model where a two-member and a four-member alliances fight in a\nsymmetric and balanced way. By presenting representative phase diagrams, we\nsystematically explore the whole parameter range which characterizes the inner\ndynamics of the alliances and the intensity of their interactions. The group\nformed by a pair, who can exchange their neighboring positions, prevail in the\nmajority of the parameter region. The rival quartet can only win if their inner\ncyclic invasion rate is significant while the mixing rate of the pair is\nextremely low. At specific parameter values, when neither of the alliances is\nstrong enough, new four-member solutions emerge where a\nrock-paper-scissors-like trio is extended by the other member of the pair.\nThese new solutions coexist hence all six competitors can survive. The\nevolutionary process is accompanied by serious finite-size effects which can be\nmitigated by appropriately chosen prepared initial states.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:21:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14822","submitter":"Roi Livni","authors":"Niva Elkin-Koren and Uri Hacohen and Roi Livni and Shay Moran","title":"Can Copyright be Reduced to Privacy?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CR","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  There is an increasing concern that generative AI models may produce outputs\nthat are remarkably similar to the copyrighted input content on which they are\ntrained. This worry has escalated as the quality and complexity of generative\nmodels have immensely improved, and the availability of large datasets\ncontaining copyrighted material has increased. Researchers are actively\nexploring strategies to mitigate the risk of producing infringing samples, and\na recent line of work suggests to employ techniques such as differential\nprivacy and other forms of algorithmic stability to safeguard copyrighted\ncontent.\n  In this work, we examine the question whether algorithmic stability\ntechniques such as differential privacy are suitable to ensure the responsible\nuse of generative models without inadvertently violating copyright laws. We\nargue that there are fundamental differences between privacy and copyright that\nshould not be overlooked. In particular we highlight that although algorithmic\nstability may be perceived as a practical tool to detect copying, it does not\nnecessarily equate to copyright protection. Therefore, if it is adopted as\nstandard for copyright infringement, it may undermine copyright law intended\npurposes.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:22:41 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14823","submitter":"Ian Roberts","authors":"Ian D. Roberts and Toby Brown and Nikki Zabel and Christine D. Wilson\n  and Aeree Chung and Laura C. Parker and Dhruv Bisaria and Alessandro Boselli\n  and Barbara Catinella and Ryan Chown and Luca Cortese and Timothy A. Davis\n  and Sara Ellison and Maria Jesus Jimenez-Donaire and Bumhyun Lee and Rory\n  Smith and Kristine Spekkens and Adam R.H. Stevens and Mallory Thorp and\n  Vincente Villanueva and Adam B. Watts and Charlotte Welker and Hyein Yoon","title":"VERTICO VI: Cold-gas asymmetries in Virgo cluster galaxies","comments":"15 pages, 8 figures, 1 table, accepted for publication in A&A","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.GA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We analyze cold-gas distributions in Virgo cluster galaxies using resolved\nCO(2-1) (tracing molecular hydrogen, H2) and HI observations from the Virgo\nEnvironment Traced In CO (VERTICO) and the VLA Imaging of Virgo in Atomic Gas\n(VIVA) surveys. From a theoretical perspective, it is expected that\nenvironmental processes in clusters will have a stronger influence on diffuse\natomic gas compared to the relatively dense molecular gas component, and that\nthese environmental perturbations can compress the cold interstellar medium in\ncluster galaxies leading to elevated star formation. In this work we\nobservationally test these predictions for star-forming satellite galaxies\nwithin the Virgo cluster. We divide our Virgo galaxy sample into HI-normal,\nHI-tailed, and HI-truncated classes and show, unsurprisingly, that the\nHI-tailed galaxies have the largest quantitative HI asymmetries. We also\ncompare to a control sample of non-cluster galaxies and find that Virgo\ngalaxies, on average, have HI asymmetries that are 40 +/- 10 per cent larger\nthan the control. There is less separation between control, HI-normal,\nHI-tailed, and HI-truncated galaxies in terms of H2 asymmetries, and on\naverage, Virgo galaxies have H2 asymmetries that are only marginally (20 +/- 10\nper cent) larger than the control sample. We find a weak correlation between HI\nand H2 asymmetries over our entire sample, but a stronger correlation for those\nspecific galaxies being strongly impacted by environmental perturbations.\nFinally, we divide the discs of the HI-tailed Virgo galaxies into a leading\nhalf and trailing half according to the observed tail direction. We find\nevidence for excess molecular gas mass on the leading halves of the disc. This\nexcess molecular gas on the leading half is accompanied by an excess in star\nformation rate such that the depletion time is, on average, unchanged.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:27:21 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14824","submitter":"Michael J.Q. Zhang","authors":"Michael J.Q. Zhang and Eunsol Choi","title":"Mitigating Temporal Misalignment by Discarding Outdated Facts","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  While large language models are able to retain vast amounts of world\nknowledge seen during pretraining, such knowledge is prone to going out of date\nand is nontrivial to update. Furthermore, these models are often used under\ntemporal misalignment, tasked with answering questions about the present,\ndespite having only been trained on data collected in the past. To mitigate the\neffects of temporal misalignment, we propose fact duration prediction: the task\nof predicting how long a given fact will remain true. In our experiments, we\ndemonstrate how identifying facts that are prone to rapid change can help\nmodels avoid from reciting outdated information and identify which predictions\nrequire seeking out up-to-date knowledge sources. We also show how modeling\nfact duration improves calibration for knowledge-intensive tasks, such as\nopen-retrieval question answering, under temporal misalignment by discarding\nvolatile facts. Our data and code will be released publicly at\nhttps://github.com/mikejqzhang/mitigating_misalignment.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:30:08 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14825","submitter":"Xiaojuan Tang","authors":"Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu,\n  Yitao Liang, Muhan Zhang","title":"Large Language Models are In-Context Semantic Reasoners rather than\n  Symbolic Reasoners","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The emergent few-shot reasoning capabilities of Large Language Models (LLMs)\nhave excited the natural language and machine learning community over recent\nyears. Despite of numerous successful applications, the underlying mechanism of\nsuch in-context capabilities still remains unclear. In this work, we\nhypothesize that the learned \\textit{semantics} of language tokens do the most\nheavy lifting during the reasoning process. Different from human's symbolic\nreasoning process, the semantic representations of LLMs could create strong\nconnections among tokens, thus composing a superficial logical chain. To test\nour hypothesis, we decouple semantics from the language reasoning process and\nevaluate three kinds of reasoning abilities, i.e., deduction, induction and\nabduction. Our findings reveal that semantics play a vital role in LLMs'\nin-context reasoning -- LLMs perform significantly better when semantics are\nconsistent with commonsense but struggle to solve symbolic or\ncounter-commonsense reasoning tasks by leveraging in-context new knowledge. The\nsurprising observations question whether modern LLMs have mastered the\ninductive, deductive and abductive reasoning abilities as in human\nintelligence, and motivate research on unveiling the magic existing within the\nblack-box LLMs. On the whole, our analysis provides a novel perspective on the\nrole of semantics in developing and evaluating language models' reasoning\nabilities. Code is available at {\\url{https://github.com/XiaojuanTang/ICSR}}.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:33:34 GMT"},{"version":"v2","created":"Thu, 8 Jun 2023 16:38:51 GMT"}],"update_date":"2023-06-09"}
{"id":"2305.14826","submitter":"Xuhong Wang","authors":"Xuhong Wang, Ding Wang, Liang Chen and Yilun Lin","title":"Building Transportation Foundation Model via Generative Graph\n  Transformer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Efficient traffic management is crucial for maintaining urban mobility,\nespecially in densely populated areas where congestion, accidents, and delays\ncan lead to frustrating and expensive commutes. However, existing prediction\nmethods face challenges in terms of optimizing a single objective and\nunderstanding the complex composition of the transportation system. Moreover,\nthey lack the ability to understand the macroscopic system and cannot\nefficiently utilize big data. In this paper, we propose a novel approach,\nTransportation Foundation Model (TFM), which integrates the principles of\ntraffic simulation into traffic prediction. TFM uses graph structures and\ndynamic graph generation algorithms to capture the participatory behavior and\ninteraction of transportation system actors. This data-driven and model-free\nsimulation method addresses the challenges faced by traditional systems in\nterms of structural complexity and model accuracy and provides a foundation for\nsolving complex transportation problems with real data. The proposed approach\nshows promising results in accurately predicting traffic outcomes in an urban\ntransportation setting.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:34:15 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14827","submitter":"Mujeen Sung","authors":"Mujeen Sung, James Gung, Elman Mansimov, Nikolaos Pappas, Raphael Shu,\n  Salvatore Romeo, Yi Zhang, Vittorio Castelli","title":"Pre-training Intent-Aware Encoders for Zero- and Few-Shot Intent\n  Classification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Intent classification (IC) plays an important role in task-oriented dialogue\nsystems as it identifies user intents from given utterances. However, models\ntrained on limited annotations for IC often suffer from a lack of\ngeneralization to unseen intent classes. We propose a novel pre-training method\nfor text encoders that uses contrastive learning with intent psuedo-labels to\nproduce embeddings that are well-suited for IC tasks. By applying this\npre-training strategy, we also introduce the pre-trained intent-aware encoder\n(PIE). Specifically, we first train a tagger to identify key phrases within\nutterances that are crucial for interpreting intents. We then use these\nextracted phrases to create examples for pre-training a text encoder in a\ncontrastive manner. As a result, our PIE model achieves up to 5.4% and 4.0%\nhigher accuracy than the previous state-of-the-art pre-trained sentence encoder\nfor the N-way zero- and one-shot settings on four IC datasets.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:34:32 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14828","submitter":"Prashant Krishnan","authors":"Prashant Krishnan, Zilong Wang, Yangkun Wang and Jingbo Shang","title":"Towards Few-shot Entity Recognition in Document Images: A Graph Neural\n  Network Approach Robust to Image Manipulation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CV","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  Recent advances of incorporating layout information, typically bounding box\ncoordinates, into pre-trained language models have achieved significant\nperformance in entity recognition from document images. Using coordinates can\neasily model the absolute position of each token, but they might be sensitive\nto manipulations in document images (e.g., shifting, rotation or scaling),\nespecially when the training data is limited in few-shot settings. In this\npaper, we propose to further introduce the topological adjacency relationship\namong the tokens, emphasizing their relative position information.\nSpecifically, we consider the tokens in the documents as nodes and formulate\nthe edges based on the topological heuristics from the k-nearest bounding\nboxes. Such adjacency graphs are invariant to affine transformations including\nshifting, rotations and scaling. We incorporate these graphs into the\npre-trained language model by adding graph neural network layers on top of the\nlanguage model embeddings, leading to a novel model LAGER. Extensive\nexperiments on two benchmark datasets show that LAGER significantly outperforms\nstrong baselines under different few-shot settings and also demonstrate better\nrobustness to manipulations.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:34:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14829","submitter":"Jonathan Kua","authors":"Shiva Raj Pokhrel, Jonathan Kua, Deol Satish, Phil Williams, Arkady\n  Zaslavsky, Seng W. Loke, Jinho Choi","title":"On Correlated Knowledge Distillation for Monitoring Human Pose with\n  Radios","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV eess.SP","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  In this work, we propose and develop a simple experimental testbed to study\nthe feasibility of a novel idea by coupling radio frequency (RF) sensing\ntechnology with Correlated Knowledge Distillation (CKD) theory towards\ndesigning lightweight, near real-time and precise human pose monitoring\nsystems. The proposed CKD framework transfers and fuses pose knowledge from a\nrobust \"Teacher\" model to a parameterized \"Student\" model, which can be a\npromising technique for obtaining accurate yet lightweight pose estimates. To\nassure its efficacy, we implemented CKD for distilling logits in our integrated\nSoftware Defined Radio (SDR)-based experimental setup and investigated the\nRF-visual signal correlation. Our CKD-RF sensing technique is characterized by\ntwo modes -- a camera-fed Teacher Class Network (e.g., images, videos) with an\nSDR-fed Student Class Network (e.g., RF signals). Specifically, our CKD model\ntrains a dual multi-branch teacher and student network by distilling and fusing\nknowledge bases. The resulting CKD models are then subsequently used to\nidentify the multimodal correlation and teach the student branch in reverse.\nInstead of simply aggregating their learnings, CKD training comprised multiple\nparallel transformations with the two domains, i.e., visual images and RF\nsignals. Once trained, our CKD model can efficiently preserve privacy and\nutilize the multimodal correlated logits from the two different neural networks\nfor estimating poses without using visual signals/video frames (by using only\nthe RF signals).\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:34:49 GMT"},{"version":"v2","created":"Tue, 30 May 2023 13:14:05 GMT"}],"update_date":"2023-05-31"}
{"id":"2305.14830","submitter":"Bin Chen","authors":"Bin Chen, Weidong Wang, Xia Zhao, Peibiao Zhao","title":"An inverse Gauss curvature flow and its application to p-capacitary\n  Orlicz-Minkowski problem","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In [Calc. Var., 57:5 (2018)], Hong-Ye-Zhang proposed the $p$-capacitary\nOrlicz-Minkowski problem and proved the existence of convex solutions to this\nproblem by variational method for $p\\in(1,n)$.\n  However, the smoothness and uniqueness of solutions are still open.\n  Notice that the $p$-capacitary Orlicz-Minkowski problem can be converted\nequivalently to a Monge-Amp\\`{e}re type equation in smooth case:\n  \\begin{align}\\label{0.1}\n  f\\phi(h_K)|\\nabla\\Psi|^p=\\tau G\n  \\end{align}\n  for $p\\in(1,n)$ and some constant $\\tau>0$, where $f$ is a positive function\ndefined on the unit sphere $\\mathcal{S}^{n-1}$, $\\phi$ is a continuous positive\nfunction defined in $(0,+\\infty)$, and $G$ is the Gauss curvature.\n  In this paper, we confirm the existence of smooth solutions to $p$-capacitary\nOrlicz-Minkowski problem with $p\\in(1,n)$ for the first time by a class of\ninverse Gauss curvature flows, which converges smoothly to the solution of\nEquation (\\ref{0.1}).\n  Furthermore, we prove the uniqueness result for Equation (\\ref{0.1}) in a\nspecial case.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:36:23 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14831","submitter":"Zhiwen Yan","authors":"Zhiwen Yan, Chen Li, Gim Hee Lee","title":"OD-NeRF: Efficient Training of On-the-Fly Dynamic Neural Radiance Fields","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Dynamic neural radiance fields (dynamic NeRFs) have demonstrated impressive\nresults in novel view synthesis on 3D dynamic scenes. However, they often\nrequire complete video sequences for training followed by novel view synthesis,\nwhich is similar to playing back the recording of a dynamic 3D scene. In\ncontrast, we propose OD-NeRF to efficiently train and render dynamic NeRFs\non-the-fly which instead is capable of streaming the dynamic scene. When\ntraining on-the-fly, the training frames become available sequentially and the\nmodel is trained and rendered frame-by-frame. The key challenge of efficient\non-the-fly training is how to utilize the radiance field estimated from the\nprevious frames effectively. To tackle this challenge, we propose: 1) a NeRF\nmodel conditioned on the multi-view projected colors to implicitly track\ncorrespondence between the current and previous frames, and 2) a transition and\nupdate algorithm that leverages the occupancy grid from the last frame to\nsample efficiently at the current frame. Our algorithm can achieve an\ninteractive speed of 6FPS training and rendering on synthetic dynamic scenes\non-the-fly, and a significant speed-up compared to the state-of-the-art on\nreal-world dynamic scenes.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:36:47 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14832","submitter":"Karol Capala","authors":"Karol Capa{\\l}a, Bart{\\l}omiej Dybiec","title":"Optimization of escape kinetics by reflecting and resetting","comments":"7 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.stat-mech","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Stochastic restarting is a strategy of starting anew. Incorporation of the\nresetting to the random walks can result in the decrease of the mean first\npassage time, due to the ability to limit unfavorably meandering, sub-optimal\ntrajectories. In the following manuscript we examine how stochastic resetting\ninfluences escape dynamics from the $(-\\infty,1)$ interval in the presence of\nthe single-well power-law $|x|^\\kappa$ potentials with $\\kappa>0$. Examination\nof the mean first passage time is complemented by the analysis of the\ncoefficient of variation, which provides a robust and reliable indicator\nassessing efficiency of stochastic resetting. The restrictive nature of\nresetting is compared to placing a reflective boundary in the system at hand.\nIn particular, for each potential, the position of the reflecting barrier\ngiving the same mean first passage time as the optimal resetting rate is\ndetermined. Finally, in addition to reflecting, we compare effectiveness of\nother resetting strategies with respect to optimization of the mean first\npassage time.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:38:04 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14833","submitter":"Jake Shipley","authors":"Jake O. Shipley","title":"Stable photon orbits in stationary axisymmetric spacetimes with an\n  electromagnetic field and a cosmological constant","comments":"6 pages, 1 figure","journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Stable light rings, which are associated with spacetime instabilities, are\nknown to exist in four-dimensional stationary axisymmetric spacetimes that\nsolve the Einstein$\\unicode{x2013}$Maxwell equations (so-called electrovacuum\nsolutions, with Faraday tensor $F_{\\mu \\nu} \\neq 0$); however, they are not\npermitted in pure vacuum ($F_{\\mu \\nu} = 0$). In this work, we extend this\nresult to spacetimes with a non-zero cosmological constant $\\Lambda$. In\nparticular, we demonstrate that stable light rings are permitted in\n$\\Lambda$-electrovacuum ($F_{\\mu \\nu} \\neq 0$, $\\Lambda \\neq 0$), but ruled out\nin $\\Lambda$-vacuum ($F_{\\mu \\nu} = 0$, $\\Lambda \\neq 0$).\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:38:08 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14834","submitter":"Jeremy Smallwood","authors":"Jeremy L. Smallwood","title":"Formation of the warped debris disc around $\\beta$ Pictoris","comments":"10 pages, 8 figures, accepted for publication in MNRAS","journal-ref":null,"doi":"10.1093/mnras/stad1586","report-no":null,"categories":"astro-ph.EP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In light of the recent confirmation of an eccentric orbit giant planet,\n$\\beta$ Pic c, I revisit the formation and evolution of the warped debris disc\nin the system. $\\beta$ Pic c is interior to $\\beta$ Pic b, and the debris disc\nis exterior to both planets. Previous $N$-body simulations have shown that\n$\\beta$ Pic b is responsible for exciting the inclination of the debris disc.\nWith hydrodynamical simulations, I model a protoplanetary gas disc misaligned\nwith the planets. I find that the gas disc does not exhibit significant long\nlasting inclination excitation from the planets even for the observed disc\nsize. The warp that is excited by the planets propagates through the entire\ndisc with a timescale much less than the gas disc lifetime. Therefore, the\nobserved warp in the debris disc must be produced after the gas disc has\ndispersed. With analytical secular theory calculations, I show that two secular\nresonances are exterior to $\\beta$ Pic b, located at $\\sim 20\\, \\rm au$ and\n$\\sim 25\\, \\rm au$. This agrees with my $N$-body simulations that show that\nthese secular resonances shape the inner edge of the $\\beta$ Pic debris disc at\na radius that agrees with observations.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:38:17 GMT"}],"update_date":"2023-05-31"}
{"id":"2305.14835","submitter":"Haopeng Zhang","authors":"Haopeng Zhang, Xiao Liu, Jiawei Zhang","title":"SummIt: Iterative Text Summarization via ChatGPT","comments":"work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Existing text summarization systems have made significant progress in recent\nyears but typically generates summaries in a single step. The one-shot\nsummarization setting is sometimes inadequate, however, as the generated\nsummary may contain hallucinations or overlook important details related to the\nreader's interests. In this paper, we address this limitation by proposing\nSummIt, an iterative text summarization framework based on large language\nmodels like ChatGPT. Our framework enables the model to refine the generated\nsummary iteratively through self-evaluation and feedback, closely resembling\nthe iterative process humans undertake when drafting and revising summaries. We\nalso explore using in-context learning to guide the rationale generation and\nsummary refinement. Furthermore, we explore the potential benefits of\nintegrating knowledge and topic extractors into the framework to enhance\nsummary faithfulness and controllability. We evaluate the performance of our\nframework on three benchmark summarization datasets through empirical and\nqualitative analyses. We also conduct a human evaluation to validate the\neffectiveness of the model's refinements and find a potential issue of\nover-correction. Our code is available at\n\\url{https://github.com/hpzhang94/summ_it}.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:40:06 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14836","submitter":"Tianwen Qian","authors":"Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, Yu-Gang Jiang","title":"NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for\n  Autonomous Driving Scenario","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We introduce a novel visual question answering (VQA) task in the context of\nautonomous driving, aiming to answer natural language questions based on\nstreet-view clues. Compared to traditional VQA tasks, VQA in autonomous driving\nscenario presents more challenges. Firstly, the raw visual data are\nmulti-modal, including images and point clouds captured by camera and LiDAR,\nrespectively. Secondly, the data are multi-frame due to the continuous,\nreal-time acquisition. Thirdly, the outdoor scenes exhibit both moving\nforeground and static background. Existing VQA benchmarks fail to adequately\naddress these complexities. To bridge this gap, we propose NuScenes-QA, the\nfirst benchmark for VQA in the autonomous driving scenario, encompassing 34K\nvisual scenes and 460K question-answer pairs. Specifically, we leverage\nexisting 3D detection annotations to generate scene graphs and design question\ntemplates manually. Subsequently, the question-answer pairs are generated\nprogrammatically based on these templates. Comprehensive statistics prove that\nour NuScenes-QA is a balanced large-scale benchmark with diverse question\nformats. Built upon it, we develop a series of baselines that employ advanced\n3D detection and VQA techniques. Our extensive experiments highlight the\nchallenges posed by this new task. Codes and dataset are available at\nhttps://github.com/qiantianwen/NuScenes-QA.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:40:50 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14837","submitter":"Stefano Zacchiroli","authors":"Yiming Sun (UVIC), Daniel M. German (UVIC), Stefano Zacchiroli (IP\n  Paris, LTCI)","title":"Using the Uniqueness of Global Identifiers to Determine the Provenance\n  of Python Software Source Code","comments":null,"journal-ref":"Empirical Software Engineering, In press","doi":"10.1007/s10664-023-10317-8","report-no":null,"categories":"cs.SE","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We consider the problem of identifying the provenance of free/open source\nsoftware (FOSS) and specifically the need of identifying where reused source\ncode has been copied from. We propose a lightweight approach to solve the\nproblem based on software identifiers-such as the names of variables, classes,\nand functions chosen by programmers. The proposed approach is able to\nefficiently narrow down to a small set of candidate origin products, to be\nfurther analyzed with more expensive techniques to make a final provenance\ndetermination.By analyzing the PyPI (Python Packaging Index) open source\necosystem we find that globally defined identifiers are very distinct. Across\nPyPI's 244 K packages we found 11.2 M different global identifiers (classes and\nmethod/function names-with only 0.6% of identifiers shared among the two types\nof entities); 76% of identifiers were used only in one package, and 93% in at\nmost 3. Randomly selecting 3 non-frequent global identifiers from an input\nproduct is enough to narrow down its origins to a maximum of 3 products within\n89% of the cases.We validate the proposed approach by mapping Debian source\npackages implemented in Python to the corresponding PyPI packages; this\napproach uses at most five trials, where each trial uses three randomly chosen\nglobal identifiers from a randomly chosen python file of the subject software\npackage, then ranks results using a popularity index and requires to inspect\nonly the top result. In our experiments, this method is effective at finding\nthe true origin of a project with a recall of 0.9 and precision of 0.77.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:42:11 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14838","submitter":"Chenyang Le","authors":"Chenyang Le, Yao Qian, Long Zhou, Shujie Liu, Michael Zeng, Xuedong\n  Huang","title":"ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text\n  Translation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.SD eess.AS","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Joint speech-language training is challenging due to the large demand for\ntraining data and GPU consumption, as well as the modality gap between speech\nand language. We present ComSL, a speech-language model built atop a composite\narchitecture of public pretrained speech-only and language-only models and\noptimized data-efficiently for spoken language tasks. Particularly, we propose\nto incorporate cross-modality learning into transfer learning and conduct them\nsimultaneously for downstream tasks in a multi-task learning manner. Our\napproach has demonstrated effectiveness in end-to-end speech-to-text\ntranslation tasks, achieving a new state-of-the-art average BLEU score of 31.5\non the multilingual speech to English text translation task for 21 languages,\nas measured on the public CoVoST2 evaluation set.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:42:15 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14839","submitter":"Yunshui Li","authors":"Yunshui Li, Binyuan Hui, ZhiChao Yin, Min Yang, Fei Huang and Yongbin\n  Li","title":"PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and\n  Compositional Experts","comments":"ACL 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Perceiving multi-modal information and fulfilling dialogues with humans is a\nlong-term goal of artificial intelligence. Pre-training is commonly regarded as\nan effective approach for multi-modal dialogue. However, due to the limited\navailability of multi-modal dialogue data, there is still scarce research on\nmulti-modal dialogue pre-training. Yet another intriguing challenge emerges\nfrom the encompassing nature of multi-modal dialogue, which involves various\nmodalities and tasks. Moreover, new forms of tasks may arise at unpredictable\npoints in the future. Hence, it is essential for designed multi-modal dialogue\nmodels to possess sufficient flexibility to adapt to such scenarios. This paper\nproposes \\textbf{PaCE}, a unified, structured, compositional multi-modal\ndialogue pre-training framework. It utilizes a combination of several\nfundamental experts to accommodate multiple dialogue-related tasks and can be\npre-trained using limited dialogue and extensive non-dialogue multi-modal data.\nFurthermore, we propose a progressive training method where old experts from\nthe past can assist new experts, facilitating the expansion of their\ncapabilities. Experimental results demonstrate that PaCE achieves\nstate-of-the-art results on eight multi-modal dialog benchmarks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:43:29 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14840","submitter":"Hong Wang","authors":"Hong Wang, Su Yang, Xiaoke Huang, Weishan Zhang","title":"Predicting Token Impact Towards Efficient Vision Transformer","comments":"10 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Token filtering to reduce irrelevant tokens prior to self-attention is a\nstraightforward way to enable efficient vision Transformer. This is the first\nwork to view token filtering from a feature selection perspective, where we\nweigh the importance of a token according to how much it can change the loss\nonce masked. If the loss changes greatly after masking a token of interest, it\nmeans that such a token has a significant impact on the final decision and is\nthus relevant. Otherwise, the token is less important for the final decision,\nso it can be filtered out. After applying the token filtering module\ngeneralized from the whole training data, the token number fed to the\nself-attention module can be obviously reduced in the inference phase, leading\nto much fewer computations in all the subsequent self-attention layers. The\ntoken filter can be realized using a very simple network, where we utilize\nmulti-layer perceptron. Except for the uniqueness of performing token filtering\nonly once from the very beginning prior to self-attention, the other core\nfeature making our method different from the other token filters lies in the\npredictability of token impact from a feature selection point of view. The\nexperiments show that the proposed method provides an efficient way to approach\na light weighted model after optimized with a backbone by means of fine tune,\nwhich is easy to be deployed in comparison with the existing methods based on\ntraining from scratch.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:44:16 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14841","submitter":"Abouzar Ghavami","authors":"Nima Hassanpour and Abouzar Ghavami","title":"Deep Learning-based Bio-Medical Image Segmentation using UNet\n  Architecture and Transfer Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV cs.CV cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Image segmentation is a branch of computer vision that is widely used in real\nworld applications including biomedical image processing. With recent\nadvancement of deep learning, image segmentation has achieved at a very high\nlevel performance. Recently, UNet architecture is found as the core of novel\ndeep learning segmentation methods. In this paper we implement UNet\narchitecture from scratch with using basic blocks in Pytorch and evaluate its\nperformance on multiple biomedical image datasets. We also use transfer\nlearning to apply novel modified UNet segmentation packages on the biomedical\nimage datasets. We fine tune the pre-trained transferred model with each\nspecific dataset. We compare its performance with our fundamental UNet\nimplementation. We show that transferred learning model has better performance\nin image segmentation than UNet model that is implemented from scratch.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:45:54 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14842","submitter":"Karthick Prasad Gunasekaran","authors":"Karthick Prasad Gunasekaran","title":"Exploring Sentiment Analysis Techniques in Natural Language Processing:\n  A Comprehensive Review","comments":null,"journal-ref":"International Journal of Advanced Research in Computer And\n  Communication Engineering Vol. 8, Issue 1, January 2019","doi":"10.17148/IJARCCE.2019.8126","report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Sentiment analysis (SA) is the automated process of detecting and\nunderstanding the emotions conveyed through written text. Over the past decade,\nSA has gained significant popularity in the field of Natural Language\nProcessing (NLP). With the widespread use of social media and online platforms,\nSA has become crucial for companies to gather customer feedback and shape their\nmarketing strategies. Additionally, researchers rely on SA to analyze public\nsentiment on various topics. In this particular research study, a comprehensive\nsurvey was conducted to explore the latest trends and techniques in SA. The\nsurvey encompassed a wide range of methods, including lexicon-based,\ngraph-based, network-based, machine learning, deep learning, ensemble-based,\nrule-based, and hybrid techniques. The paper also addresses the challenges and\nopportunities in SA, such as dealing with sarcasm and irony, analyzing\nmulti-lingual data, and addressing ethical concerns. To provide a practical\ncase study, Twitter was chosen as one of the largest online social media\nplatforms. Furthermore, the researchers shed light on the diverse application\nareas of SA, including social media, healthcare, marketing, finance, and\npolitics. The paper also presents a comparative and comprehensive analysis of\nexisting trends and techniques, datasets, and evaluation metrics. The ultimate\ngoal is to offer researchers and practitioners a systematic review of SA\ntechniques, identify existing gaps, and suggest possible improvements. This\nstudy aims to enhance the efficiency and accuracy of SA processes, leading to\nsmoother and error-free outcomes.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:48:41 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14843","submitter":"Hanxu Hu","authors":"Hanxu Hu, Frank Keller","title":"Meta-Learning For Vision-and-Language Cross-lingual Transfer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Current pre-trained vison-language models (PVLMs) achieve excellent\nperformance on a range of multi-modal datasets. Recent work has aimed at\nbuilding multilingual models, and a range of novel multilingual multi-modal\ndatasets have been proposed. Current PVLMs typically perform poorly on these\ndatasets when used for multi-modal zero-shot or few-shot cross-lingual\ntransfer, especially for low-resource languages. To alleviate this problem, we\npropose a novel meta-learning fine-tuning framework. Our framework makes\ncurrent PVLMs rapidly adaptive to new languages in vision-language scenarios by\ndesigning MAML in a cross-lingual multi-modal manner. Experiments show that our\nmethod boosts the performance of current state-of-the-art PVLMs in both\nzero-shot and few-shot cross-lingual transfer on a range of vision-language\nunderstanding tasks and datasets (XVNLI, xGQA, MaRVL, xFlicker&Co\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:51:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14844","submitter":"Bruno Ebner","authors":"Bruno Ebner and Norbert Henze and Simos Meintanis","title":"A unified approach to goodness-of-fit testing for spherical and\n  hyperspherical data","comments":"29 pages, 2 figures, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"math.ST stat.TH","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We propose a general and relatively simple method for the construction of\ngoodness-of-fit tests on the sphere and the hypersphere. The method is based on\nthe characterization of probability distributions via their characteristic\nfunction, and it leads to test criteria that are convenient regarding\napplications and consistent against arbitrary deviations from the model under\ntest. We emphasize goodness-of-fit tests for spherical distributions due to\ntheir importance in applications and the relative scarcity of available\nmethods.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:53:52 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14845","submitter":"Tiberiu Harko","authors":"Tiberiu Harko","title":"Dissipative quintessence, and its cosmological implications","comments":"23 pages, 8 figures, accepted for publication in PRD","journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc astro-ph.CO hep-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We consider a generalization of the quintessence type scalar field\ncosmological models, by adding a multiplicative dissipative term in the scalar\nfield Lagrangian, which is represented in an exponential form. The generalized\ndissipative Klein-Gordon equation is obtained from the variational principle in\na covariant form. The energy-momentum tensor of the dissipative scalar field is\nalso obtained from the dissipative Lagrangian. The generalized Friedmann\nequations in the presence of the dissipative scalar field are obtained for a\nspecific form of dissipation, with the dissipation exponent represented as the\ntime integral of the product of the Hubble function, and of a function\ndescribing the dissipative properties of the scalar field. Several cosmological\nmodels, corresponding to different choices of the dissipation function, and of\nthe scalar field potential, are considered in detail. The evolutions of the\nbasic cosmological parameters (Hubble function, deceleration parameter etc.)\nare investigated by using both analytical and numerical techniques. A\ncomparison with the observational data for the Hubble function, and with the\npredictions of the standard $\\Lambda$CDM paradigm is also presented for each\ndissipative scalar field model. In the large time limit the model describes an\naccelerating Universe, with the effective negative pressure induced by the\ndissipative effects associated to the scalar field. Accelerated expansion in\nthe absence of the scalar field potential is also possible, with the kinetic\nterm dominating the expansionary evolution. The dissipative scalar field models\ndescribe well the observational data, with the free parameters of the model\nobtained by a trial and error method. The obtained results show that the\ndissipative scalar field model offers an effective dynamical possibility for\nexplaining the recent cosmological observational data.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:54:24 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14846","submitter":"Junyoung Byun","authors":"Junyoung Byun, Myung-Joon Kwon, Seungju Cho, Yoonji Kim, Changick Kim","title":"Introducing Competition to Boost the Transferability of Targeted\n  Adversarial Examples through Clean Feature Mixup","comments":"CVPR 2023 camera-ready","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Deep neural networks are widely known to be susceptible to adversarial\nexamples, which can cause incorrect predictions through subtle input\nmodifications. These adversarial examples tend to be transferable between\nmodels, but targeted attacks still have lower attack success rates due to\nsignificant variations in decision boundaries. To enhance the transferability\nof targeted adversarial examples, we propose introducing competition into the\noptimization process. Our idea is to craft adversarial perturbations in the\npresence of two new types of competitor noises: adversarial perturbations\ntowards different target classes and friendly perturbations towards the correct\nclass. With these competitors, even if an adversarial example deceives a\nnetwork to extract specific features leading to the target class, this\ndisturbance can be suppressed by other competitors. Therefore, within this\ncompetition, adversarial examples should take different attack strategies by\nleveraging more diverse features to overwhelm their interference, leading to\nimproving their transferability to different models. Considering the\ncomputational complexity, we efficiently simulate various interference from\nthese two types of competitors in feature space by randomly mixing up stored\nclean features in the model inference and named this method Clean Feature Mixup\n(CFM). Our extensive experimental results on the ImageNet-Compatible and\nCIFAR-10 datasets show that the proposed method outperforms the existing\nbaselines with a clear margin. Our code is available at\nhttps://github.com/dreamflake/CFM.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:54:44 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14847","submitter":"Anisha Gunjal","authors":"Anisha Gunjal, Greg Durrett","title":"Drafting Event Schemas using Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Past work has studied event prediction and event language modeling, sometimes\nmediated through structured representations of knowledge in the form of event\nschemas. Such schemas can lead to explainable predictions and forecasting of\nunseen events given incomplete information. In this work, we look at the\nprocess of creating such schemas to describe complex events. We use large\nlanguage models (LLMs) to draft schemas directly in natural language, which can\nbe further refined by human curators as necessary. Our focus is on whether we\ncan achieve sufficient diversity and recall of key events and whether we can\nproduce the schemas in a sufficiently descriptive style. We show that large\nlanguage models are able to achieve moderate recall against schemas taken from\ntwo different datasets, with even better results when multiple prompts and\nmultiple samples are combined. Moreover, we show that textual entailment\nmethods can be used for both matching schemas to instances of events as well as\nevaluating overlap between gold and predicted schemas. Our method paves the way\nfor easier distillation of event knowledge from large language model into\nschemas.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:57:04 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14848","submitter":"Moritz Schick","authors":"Mareike Dressler and Salma Kuhlmann and Moritz Schick","title":"Geometrical Study of the Cone of Sums of Squares plus Sums of\n  Nonnegative Circuits","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.AG math.CO math.OC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this article, we combine sums of squares (SOS) and sums of nonnegative\ncircuit (SONC) forms, two independent nonnegativity certificates for real\nhomogeneous polynomials. We consider the convex cone SOS+SONC of forms that\ndecompose into a sum of an SOS and a SONC form and study it from a geometric\npoint of view. We show that the SOS+SONC cone is proper and neither closed\nunder multiplications nor under linear transformation of variables. Moreover,\nwe present an alternative proof of an analog of Hilbert's 1888 Theorem for the\nSOS+SONC cone and prove that in the non-Hilbert cases it provides a proper\nsuperset of both the SOS and the SONC cone. This follows by exploiting a new\nnecessary condition for membership in the SONC cone.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:59:18 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14849","submitter":"Taesun Yeom","authors":"Taesun Yeom, Minhyeok Lee","title":"DuDGAN: Improving Class-Conditional GANs via Dual-Diffusion","comments":"8 page, 3 figures, supplementary material included","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV eess.IV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Class-conditional image generation using generative adversarial networks\n(GANs) has been investigated through various techniques; however, it continues\nto face challenges such as mode collapse, training instability, and low-quality\noutput in cases of datasets with high intra-class variation. Furthermore, most\nGANs often converge in larger iterations, resulting in poor iteration efficacy\nin training procedures. While Diffusion-GAN has shown potential in generating\nrealistic samples, it has a critical limitation in generating class-conditional\nsamples. To overcome these limitations, we propose a novel approach for\nclass-conditional image generation using GANs called DuDGAN, which incorporates\na dual diffusion-based noise injection process. Our method consists of three\nunique networks: a discriminator, a generator, and a classifier. During the\ntraining process, Gaussian-mixture noises are injected into the two noise-aware\nnetworks, the discriminator and the classifier, in distinct ways. This noisy\ndata helps to prevent overfitting by gradually introducing more challenging\ntasks, leading to improved model performance. As a result, our method\noutperforms state-of-the-art conditional GAN models for image generation in\nterms of performance. We evaluated our method using the AFHQ, Food-101, and\nCIFAR-10 datasets and observed superior results across metrics such as FID,\nKID, Precision, and Recall score compared with comparison models, highlighting\nthe effectiveness of our approach.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:59:44 GMT"},{"version":"v2","created":"Tue, 6 Jun 2023 06:35:25 GMT"}],"update_date":"2023-06-07"}
{"id":"2305.14850","submitter":"Yan Rybalko","authors":"Kenneth Karlsen and Yan Rybalko","title":"On the well-posedness of a nonlocal (two-place) FORQ equation via a\n  two-component peakon system","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We investigate the Cauchy problem for a nonlocal (two-place) FORQ equation.\nBy interpreting this equation as a special case of a two-component peakon\nsystem (exhibiting a cubic nonlinearity), we convert the Cauchy problem into a\nsystem of ordinary differential equations in a Banach space. Using this\napproach, we are able to demonstrate local well-posedness in the Sobolev space\n$H^{s}$ where $s > 5/2$. We also establish the continuity properties for the\ndata-to-solution map for a range of Sobolev spaces. Finally, we briefly explore\nthe relationship between the two-component system and the bi-Hamiltonian AKNS\nhierarchy.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:00:12 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14851","submitter":"Pengfei He","authors":"Pengfei He, Han Xu, Jie Ren, Yingqian Cui, Hui Liu, Charu C. Aggarwal,\n  Jiliang Tang","title":"Sharpness-Aware Data Poisoning Attack","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Recent research has highlighted the vulnerability of Deep Neural Networks\n(DNNs) against data poisoning attacks. These attacks aim to inject poisoning\nsamples into the models' training dataset such that the trained models have\ninference failures. While previous studies have executed different types of\nattacks, one major challenge that greatly limits their effectiveness is the\nuncertainty of the re-training process after the injection of poisoning\nsamples, including the re-training initialization or algorithms. To address\nthis challenge, we propose a novel attack method called ''Sharpness-Aware Data\nPoisoning Attack (SAPA)''. In particular, it leverages the concept of DNNs'\nloss landscape sharpness to optimize the poisoning effect on the worst\nre-trained model. It helps enhance the preservation of the poisoning effect,\nregardless of the specific retraining procedure employed. Extensive experiments\ndemonstrate that SAPA offers a general and principled strategy that\nsignificantly enhances various types of poisoning attacks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:00:21 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14852","submitter":"Moonseok Choi","authors":"Moonseok Choi, Hyungi Lee, Giung Nam, Juho Lee","title":"SWAMP: Sparse Weight Averaging with Multiple Particles for Iterative\n  Magnitude Pruning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Given the ever-increasing size of modern neural networks, the significance of\nsparse architectures has surged due to their accelerated inference speeds and\nminimal memory demands. When it comes to global pruning techniques, Iterative\nMagnitude Pruning (IMP) still stands as a state-of-the-art algorithm despite\nits simple nature, particularly in extremely sparse regimes. In light of the\nrecent finding that the two successive matching IMP solutions are linearly\nconnected without a loss barrier, we propose Sparse Weight Averaging with\nMultiple Particles (SWAMP), a straightforward modification of IMP that achieves\nperformance comparable to an ensemble of two IMP solutions. For every\niteration, we concurrently train multiple sparse models, referred to as\nparticles, using different batch orders yet the same matching ticket, and then\nweight average such models to produce a single mask. We demonstrate that our\nmethod consistently outperforms existing baselines across different sparsities\nthrough extensive experiments on various data and neural network structures.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:01:49 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14853","submitter":"Chunjing  Xie","authors":"Kaijian Sha, Yun Wang, Chunjing Xie","title":"Uniqueness and uniform structural stability of Poiseuille flows with\n  large fluxes in two-dimensional strips","comments":"arXiv admin note: substantial text overlap with arXiv:2011.07467","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, we prove the uniform nonlinear structural stability of\nPoiseuille flows with suitably large flux for the steady Navier-Stokes system\nin a two-dimensional strip with arbitrary period. Furthermore, the\nwell-posedness theory for the Navier-Stokes system is also proved even when the\n$L^2$-norm of the external force is large. In particular, if the vertical\nvelocity is suitably small where the smallness is independent of the flux, then\nPoiseuille flow is the unique solution of the steady Navier-Stokes system in\nthe periodic strip. The key point is to establish uniform a priori estimates\nfor the corresponding linearized problem via the boundary layer analysis, where\nwe explore the particular features of odd and even stream functions. The\nanalysis for the even stream function is new, which not only generalizes the\nprevious study for the symmetric flows in \\cite{Rabier1}, but also provides an\nexplicit relation between the flux and period.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:03:01 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14854","submitter":"Zhi-Peng Xing","authors":"Zhi-Peng Xing, Xiao-Gang He, Fei Huang, Chang Yang","title":"A global analysis for determined and undetermined hadronic two body weak\n  decays of anti-triplet charmed baryons","comments":"9pages, 2 figures, 3 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  A large amount of data on hadronic two body weak decays of anti-triplet\ncharmed baryons $T_{c\\bar 3}$ to an octet baryon $T_8$ and an octet or singlet\npseudoscalar meson $P$, $T_{c \\bar 3} \\to T_8 P$, have been measured. The SU(3)\nflavor symmetry has been applied to study these decays to obtain insights about\nweak interactions for charm physics. However not all such decays needed to\ndetermine the SU(3) irreducible amplitudes have been measured forbidding a\ncomplete global analysis. Previously, it has been shown that data from measured\ndecays can be used to do a global fit to determine all except one parity\nviolating and one parity conserving amplitudes of the relevant SU(3)\nirreducible amplitudes causing 8 hadronic two body weak decay channels\ninvolving $\\Xi^0_c$ to $\\eta$ or $\\eta'$ transitions undetermined. It is\nimportant to obtain information about these decays in order to guide\nexperimental searches. In this work using newly measured decay modes by BESIII\nand Belle in 2022, we carry out a global analysis and parameterize the unknown\namplitudes to provide the ranges for the branching ratios of the 8 undetermined\ndecays. Our results indicate that the SU(3) flavor symmetry can explain the\nmeasured data exceptionally well, with a remarkable minimal $\\chi^2/d.o.f.$ of\n1.21 and predict 80 observables in 45 decays for future experimental data to\ntest. We then vary the unknown SU(3) amplitudes to obtain the allowed range of\nbranching ratios for the 8 undetermined decays. We find that some of them are\nwithin reach of near future experimental capabilities. We urge our experimental\ncolleagues to carry out related searches.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:05:09 GMT"},{"version":"v2","created":"Wed, 31 May 2023 11:57:06 GMT"}],"update_date":"2023-06-01"}
{"id":"2305.14855","submitter":"Tasneem Saleem","authors":"Tasneem Saleem, Salleh Ahmad, Jean-Baptiste Cizel, Christophe De La\n  Taille, Maxime Morenas, Vanessa Nadig, Florent Perez, Volkmar Schulz, Stefan\n  Gundacker and Julien Fleury","title":"Study experimental time resolution limits of recent ASICs at Weeroc with\n  different SiPMs and scintillators","comments":"Prepared for submission to JINST","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.ins-det","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Medical applications, such as positron emission tomography (PET), and space\napplication, such as Light Detection and Ranging (LIDAR), are in need for\nhighly specialized ASICs. Weeroc, in collaboration with different partners, is\nhighly involved in developing a new generation of front-end ASICs. In the\ncontext of a joined LIDAR project among Weeroc, CNES, and Airbus, Weeroc is\nworking on the development of Liroc, an ASIC for space LIDAR application.\nWeeroc is also working on advancing ASICs for medical applications, hence,\nanother ASIC, Radioroc, is under development and intended to be used for PET\napplications.\n  This study experimentally evaluates the time resolution limits of these ASICs\nin different configurations, with some of the most recent silicon\nphotomultipliers (SiPMs) technologies available on the market coupled with\ndifferent scintillation crystals. The best single-photon time resolution (SPTR)\nwas achieved using FBK NUV-HD SiPMs with an FWHM of 79 ps with Liroc and 73 ps\nwith Radioroc. Furthermore, the best coincidence time resolution (CTR) of\nRadioroc was determined to 83 ps (FWHM) with Broadcom Near UltraViolet - Metal\nTrench (NUV-MT) SiPMs coupled to TAC LYSO:Ce,Ca (2x2x3 mm3).\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:05:22 GMT"},{"version":"v2","created":"Fri, 26 May 2023 13:36:28 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.14856","submitter":"\\v{Z}iga Babnik","authors":"\\v{Z}iga Babnik, Naser Damer, Vitomir \\v{S}truc","title":"Optimization-Based Improvement of Face Image Quality Assessment\n  Techniques","comments":"In proceedings of the International Workshop on Biometrics and\n  Forensics (IWBF) 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Contemporary face recognition (FR) models achieve near-ideal recognition\nperformance in constrained settings, yet do not fully translate the performance\nto unconstrained (realworld) scenarios. To help improve the performance and\nstability of FR systems in such unconstrained settings, face image quality\nassessment (FIQA) techniques try to infer sample-quality information from the\ninput face images that can aid with the recognition process. While existing\nFIQA techniques are able to efficiently capture the differences between high\nand low quality images, they typically cannot fully distinguish between images\nof similar quality, leading to lower performance in many scenarios. To address\nthis issue, we present in this paper a supervised quality-label optimization\napproach, aimed at improving the performance of existing FIQA techniques. The\ndeveloped optimization procedure infuses additional information (computed with\na selected FR model) into the initial quality scores generated with a given\nFIQA technique to produce better estimates of the \"actual\" image quality. We\nevaluate the proposed approach in comprehensive experiments with six\nstate-of-the-art FIQA approaches (CR-FIQA, FaceQAN, SER-FIQ, PCNet, MagFace,\nSDD-FIQA) on five commonly used benchmarks (LFW, CFPFP, CPLFW, CALFW, XQLFW)\nusing three targeted FR models (ArcFace, ElasticFace, CurricularFace) with\nhighly encouraging results.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:06:12 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14857","submitter":"Akari Asai","authors":"Akari Asai, Sneha Kudugunta, Xinyan Velocity Yu, Terra Blevins, Hila\n  Gonen, Machel Reid, Yulia Tsvetkov, Sebastian Ruder, Hannaneh Hajishirzi","title":"BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual\n  Transfer","comments":"The data and code is available at https://buffetfs.github.io/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Despite remarkable advancements in few-shot generalization in natural\nlanguage processing, most models are developed and evaluated primarily in\nEnglish. To facilitate research on few-shot cross-lingual transfer, we\nintroduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across\n54 languages in a sequence-to-sequence format and provides a fixed set of\nfew-shot examples and instructions. BUFFET is designed to establish a rigorous\nand equitable evaluation framework for few-shot cross-lingual transfer across a\nbroad range of tasks and languages. Using BUFFET, we perform thorough\nevaluations of state-of-the-art multilingual large language models with\ndifferent transfer methods, namely in-context learning and fine-tuning. Our\nfindings reveal significant room for improvement in few-shot in-context\ncross-lingual transfer. In particular, ChatGPT with in-context learning often\nperforms worse than much smaller mT5-base models fine-tuned on English task\ndata and few-shot in-language examples. Our analysis suggests various avenues\nfor future research in few-shot cross-lingual transfer, such as improved\npretraining, understanding, and future evaluations.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:06:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14858","submitter":"Zixuan Jiang","authors":"Zixuan Jiang, Jiaqi Gu, Hanqing Zhu, David Z. Pan","title":"Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient\n  Pre-LN Transformers","comments":"15 pages, 5 tables, code available at\n  https://github.com/ZixuanJiang/pre-rmsnorm-transformer","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.NE","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Transformers have achieved great success in machine learning applications.\nNormalization techniques, such as Layer Normalization (LayerNorm, LN) and Root\nMean Square Normalization (RMSNorm), play a critical role in accelerating and\nstabilizing the training of Transformers. While LayerNorm recenters and\nrescales input vectors, RMSNorm only rescales the vectors by their RMS value.\nDespite being more computationally efficient, RMSNorm may compromise the\nrepresentation ability of Transformers. There is currently no consensus\nregarding the preferred normalization technique, as some models employ\nLayerNorm while others utilize RMSNorm, especially in recent large language\nmodels. It is challenging to convert Transformers with one normalization to the\nother type. While there is an ongoing disagreement between the two\nnormalization types, we propose a solution to unify two mainstream Transformer\narchitectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent\nredundant mean information in the main branch of Pre-LN Transformers, we can\nreduce LayerNorm to RMSNorm, achieving higher efficiency. We further propose\nthe Compressed RMSNorm (CRMSNorm) and Pre-CRMSNorm Transformer based on a\nlossless compression of the zero-mean vectors. We formally establish the\nequivalence of Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm Transformer variants in\nboth training and inference. It implies that Pre-LN Transformers can be\nsubstituted with Pre-(C)RMSNorm counterparts at almost no cost, offering the\nsame arithmetic functionality along with free efficiency improvement.\nExperiments demonstrate that we can reduce the training and inference time of\nPre-LN Transformers by up to 10%.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:08:26 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14859","submitter":"Bojun Huang","authors":"Huang Bojun, Fei Yuan","title":"Utility-Probability Duality of Neural Networks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL cs.NE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  It is typically understood that the training of modern neural networks is a\nprocess of fitting the probability distribution of desired output. However,\nrecent paradoxical observations in a number of language generation tasks let\none wonder if this canonical probability-based explanation can really account\nfor the empirical success of deep learning. To resolve this issue, we propose\nan alternative utility-based explanation to the standard supervised learning\nprocedure in deep learning. The basic idea is to interpret the learned neural\nnetwork not as a probability model but as an ordinal utility function that\nencodes the preference revealed in training data. In this perspective, training\nof the neural network corresponds to a utility learning process. Specifically,\nwe show that for all neural networks with softmax outputs, the SGD learning\ndynamic of maximum likelihood estimation (MLE) can be seen as an iteration\nprocess that optimizes the neural network toward an optimal utility function.\nThis utility-based interpretation can explain several otherwise-paradoxical\nobservations about the neural networks thus trained. Moreover, our\nutility-based theory also entails an equation that can transform the learned\nutility values back to a new kind of probability estimation with which\nprobability-compatible decision rules enjoy dramatic (double-digits)\nperformance improvements. These evidences collectively reveal a phenomenon of\nutility-probability duality in terms of what modern neural networks are (truly)\nmodeling: We thought they are one thing (probabilities), until the\nunexplainable showed up; changing mindset and treating them as another thing\n(utility values) largely reconcile the theory, despite remaining subtleties\nregarding its original (probabilistic) identity.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:09:07 GMT"},{"version":"v2","created":"Thu, 25 May 2023 09:25:24 GMT"}],"update_date":"2023-05-31"}
{"id":"2305.14860","submitter":"Bogar D\\'iaz","authors":"J. R. Alvarado Garc\\'ia, D. Rosales Herrera, A. Fern\\'andez T\\'ellez,\n  Bogar D\\'iaz, and J. E. Ram\\'irez","title":"Structure of the medium formed in heavy ion collisions","comments":"15 pages, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph nucl-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We investigate the structure of the medium formed in heavy ion collisions\nusing three different models: the Color String Percolation Model (CSPM), the\nCore-Shell-Color String Percolation Model (CSCSPM), and the Color Glass\nCondensate (CGC) framework. We analyze the radial distribution function of the\ntransverse representation of color flux tubes in each model to determine the\nmedium's structure. Our results indicate that the CSPM behaves as an ideal gas,\nwhile the CSCSPM exhibits a structural phase transition from a gas-like to a\nliquid-like structure. Additionally, our analysis of the CGC framework suggests\nthat it produces systems that behave like interacting gases for AuAu central\ncollisions at RHIC energies and liquid-like structures for PbPb central\ncollisions at LHC energies.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:13:46 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14861","submitter":"Fengjun Zhuo","authors":"Fengjun Zhuo, Jian Kang, Aur\\'elien Manchon, Zhenxiang Cheng","title":"Topological Phases in Magnonics: A Review","comments":"17 pages, 12 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Magnonics or magnon spintronics is an emerging field focusing on generating,\ndetecting, and manipulating magnons. As charge-neutral quasi-particles, magnons\nare promising information carriers because of their low energy dissipation and\nlong coherence length. In the past decade, topological phases in magnonics have\nattracted intensive attention due to their fundamental importance in\ncondensed-matter physics and potential applications of spintronic devices. In\nthis review, we mainly focus on recent progress in topological magnonics, such\nas the Hall effect of magnons, magnon Chern insulators, topological magnon\nsemimetals, etc. In addition, the evidence supporting topological phases in\nmagnonics and candidate materials are also discussed and summarized. The aim of\nthis review is to provide readers with a comprehensive and systematic\nunderstanding of the recent developments in topological magnonics.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:13:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14862","submitter":"Miren Mu\\~noz-Echeverr\\'ia","authors":"M. Mu\\~noz-Echeverr\\'ia, J. F. Mac\\'ias-P\\'erez, E. Artis, W. Cui, D.\n  de Andres, F. De Luca, M. De Petris, A. Ferragamo, C. Giocoli, C. Hanser, F.\n  Mayet, M. Meneghetti, A. Moyer, A. Paliwal, L. Perotto, E. Rasia, and G.\n  Yepes","title":"Galaxy cluster mass bias from projected mass maps: The Three\n  Hundred-NIKA2 LPSZ twin samples","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.CO","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The determination of the mass of galaxy clusters from observations is subject\nto systematic uncertainties. Beyond the errors due to instrumental and\nobservational systematic effects, in this work we investigate the bias\nintroduced by modelling assumptions. In particular, we consider the\nreconstruction of the mass of galaxy clusters from convergence maps employing\nspherical mass density models. We make use of The Three Hundred simulations,\nselecting clusters in the same redshift and mass range as the NIKA2\nSunyaev-Zel'dovich Large Program sample: $3 \\leq M_{500}/ 10^{14}\n\\mathrm{M}_{\\odot} \\leq 10$ and $0.5 \\leq z \\leq 0.9$. We study different\nmodelling and intrinsic uncertainties that should be accounted for when using\nthe single cluster mass estimates for scaling relations. We confirm that the\norientation of clusters and the radial ranges considered for the fit have an\nimportant impact on the mass bias. The effect of the projection adds\nuncertainties to the order of $10\\%$ to $14\\%$ to the mass estimates. We also\nfind that the scatter from cluster to cluster in the mass bias when using\nspherical mass models is less than $20\\%$ of the true mass of the clusters.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:14:08 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14863","submitter":"Marcin Glowacki","authors":"M. Glowacki, K. Lee-Waddell, A. T. Deller, N. Deg, A. C. Gordon, J. A.\n  Grundy, L. Marnoch, A. X. Shen, S. D. Ryder, R. M. Shannon, O. I. Wong, H.\n  D\\'enes, B. S. Koribalski, C. Murugeshan, J. Rhee, T. Westmeier, S. Bhandari,\n  A. Bosma, B. W. Holwerda, J. X. Prochaska","title":"WALLABY Pilot Survey: HI in the host galaxy of a Fast Radio Burst","comments":"13 pages, 5 figures. Published in ApJ","journal-ref":null,"doi":"10.3847/1538-4357/acc1e3","report-no":null,"categories":"astro-ph.GA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We report on the commensal ASKAP detection of a fast radio burst (FRB),\nFRB20211127I, and the detection of neutral hydrogen (HI) emission in the FRB\nhost galaxy, WALLABYJ131913-185018 (hereafter W13-18). This collaboration\nbetween the CRAFT and WALLABY survey teams marks the fifth, and most distant,\nFRB host galaxy detected in HI, not including the Milky Way. We find that\nW13-18 has a HI mass of $M_{\\rm HI}$ = 6.5 $\\times$ 10$^{9}$ M$_{\\odot}$, a\nHI-to-stellar mass ratio of 2.17, and coincides with a continuum radio source\nof flux density at 1.4 GHz of 1.3 mJy. The HI global spectrum of W13-18 appears\nto be asymmetric, albeit the HI observation has a low S/N, and the galaxy\nitself appears modestly undisturbed. These properties are compared to the early\nliterature of HI emission detected in other FRB hosts to date, where either the\nHI global spectra were strongly asymmetric, or there were clearly disrupted HI\nintensity map distributions. W13-18 lacks sufficient S/N to determine whether\nit is significantly less asymmetric in its HI distribution than previous\nexamples of FRB host galaxies. However, there are no strong signs of a major\ninteraction in the HI or optical image of the host galaxy that would stimulate\na burst of star formation and hence the production of putative FRB progenitors\nrelated to massive stars and their compact remnants.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:14:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14864","submitter":"Ananya Harsh Jha","authors":"Ananya Harsh Jha, Dirk Groeneveld, Emma Strubell, Iz Beltagy","title":"Large Language Model Distillation Doesn't Need a Teacher","comments":"10 pages, 3 figures, 5 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Knowledge distillation trains a smaller student model to match the output\ndistribution of a larger teacher to maximize the end-task performance under\ncomputational constraints. However, existing literature on language model\ndistillation primarily focuses on compressing encoder-only models that are then\nspecialized by task-specific supervised finetuning. We need to rethink this\nsetup for more recent large language models with tens to hundreds of billions\nof parameters. Task-specific finetuning is impractical at this scale, and model\nperformance is often measured using zero/few-shot prompting. Thus, in this\nwork, we advocate for task-agnostic zero-shot evaluated distillation for large\nlanguage models without access to end-task finetuning data. We propose a\nteacher-free task-agnostic distillation method, which uses a truncated version\nof the larger model for initialization, and continues pretraining this model\nusing a language modeling objective. Our teacher-free method shines in a\ndistillation regime where it is infeasible to fit both the student and teacher\ninto the GPU memory. Despite its simplicity, our method can effectively reduce\nthe model size by 50\\%, matching or outperforming the vanilla distillation\nmethod on perplexity and accuracy on 13 zero-shot end-tasks while being 1.5x\ncomputationally efficient.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:18:35 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14865","submitter":"Na Zhang","authors":"Na Zhang, Kun Yue, Chao Fang","title":"A Game-Theoretic Framework for AI Governance","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GT cs.CY","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  As a transformative general-purpose technology, AI has empowered various\nindustries and will continue to shape our lives through ubiquitous\napplications. Despite the enormous benefits from wide-spread AI deployment, it\nis crucial to address associated downside risks and therefore ensure AI\nadvances are safe, fair, responsible, and aligned with human values. To do so,\nwe need to establish effective AI governance. In this work, we show that the\nstrategic interaction between the regulatory agencies and AI firms has an\nintrinsic structure reminiscent of a Stackelberg game, which motivates us to\npropose a game-theoretic modeling framework for AI governance. In particular,\nwe formulate such interaction as a Stackelberg game composed of a leader and a\nfollower, which captures the underlying game structure compared to its\nsimultaneous play counterparts. Furthermore, the choice of the leader naturally\ngives rise to two settings. And we demonstrate that our proposed model can\nserves as a unified AI governance framework from two aspects: firstly we can\nmap one setting to the AI governance of civil domains and the other to the\nsafety-critical and military domains, secondly, the two settings of governance\ncould be chosen contingent on the capability of the intelligent systems. To the\nbest of our knowledge, this work is the first to use game theory for analyzing\nand structuring AI governance. We also discuss promising directions and hope\nthis can help stimulate research interest in this interdisciplinary area. On a\nhigh, we hope this work would contribute to develop a new paradigm for\ntechnology policy: the quantitative and AI-driven methods for the technology\npolicy field, which holds significant promise for overcoming many shortcomings\nof existing qualitative approaches.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:18:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14866","submitter":"Douadi  Drihem","authors":"Douadi Drihem","title":"Powers functions in Besov spaces of power weights. Necessary conditions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.FA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The aim of this paper is to present some necessary conditions for the\nboundedness of the mapping $f\\mapsto |f|^{\\mu },\\mu >0$ on Besov spaces\nequipped with power weights.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:19:13 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14867","submitter":"Rodrigo Diaz","authors":"Rodrigo Diaz, Charalampos Saitis, Mark Sandler","title":"Interactive Neural Resonators","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.HC eess.AS","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this work, we propose a method for the controllable synthesis of real-time\ncontact sounds using neural resonators. Previous works have used physically\ninspired statistical methods and physical modelling for object materials and\nexcitation signals. Our method incorporates differentiable second-order\nresonators and estimates their coefficients using a neural network that is\nconditioned on physical parameters. This allows for interactive dynamic control\nand the generation of novel sounds in an intuitive manner. We demonstrate the\npractical implementation of our method and explore its potential creative\napplications.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:19:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14868","submitter":"Ganghwi Kim","authors":"Ganghwi Kim, Dae-Han Jung, Hee-Sung Han and Ki-Suk Lee","title":"Resonance of Domain Wall in a Ferromagnetic Nanostrip: Relation Between\n  Distortion and Velocity","comments":"15 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci physics.comp-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The resonance of the magnetic domain wall under the applied field amplifies\nits velocity compared to the one-dimensional model. To quantify the\namplification, we define the distortion variation rate of the domain wall that\ncan represent how fast and severely the wall shape is variated. Introducing\nthat rate gives a way to bring the resonance into the one-dimensional domain\nwall dynamics model. We obtain the dissipated energy and domain wall velocity\namplification by calculating the distortion variation rate. The relationship\nbetween velocity and distortion variation rate agrees well with micromagnetic\nsimulation.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:21:26 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14869","submitter":"Weiqi Wang Mr.","authors":"Weiqi Wang, Tianqing Fang, Wenxuan Ding, Baixuan Xu, Xin Liu, Yangqiu\n  Song, Antoine Bosselut","title":"CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense\n  Question Answering","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The task of zero-shot commonsense question answering evaluates models on\ntheir capacity to reason about general scenarios beyond those presented in\nspecific datasets. Existing approaches for tackling this task leverage external\nknowledge from CommonSense Knowledge Bases (CSKBs) by pretraining the model on\nsynthetic QA pairs constructed from CSKBs. In these approaches, negative\nexamples (distractors) are formulated by randomly sampling from CSKBs using\nfairly primitive keyword constraints. However, two bottlenecks limit these\napproaches: the inherent incompleteness of CSKBs limits the semantic coverage\nof synthetic QA pairs, and the lack of human annotations makes the sampled\nnegative examples potentially uninformative and contradictory. To tackle these\nlimitations above, we propose Conceptualization-Augmented Reasoner (CAR), a\nzero-shot commonsense question-answering framework that fully leverages the\npower of conceptualization. Specifically, CAR abstracts a commonsense knowledge\ntriple to many higher-level instances, which increases the coverage of CSKB and\nexpands the ground-truth answer space, reducing the likelihood of selecting\nfalse-negative distractors. Extensive experiments demonstrate that CAR more\nrobustly generalizes to answering questions about zero-shot commonsense\nscenarios than existing methods, including large language models, such as\nGPT3.5 and ChatGPT. Our codes, data, and model checkpoints are available at\nhttps://github.com/HKUST-KnowComp/CAR.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:21:31 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14870","submitter":"Indrani Banerjee","authors":"Siddharth Kumar Sahoo, Neeraj Yadav and Indrani Banerjee","title":"Imprints of Einstein-Maxwell dilaton-axion gravity in the observed\n  shadows of Sgr A* and M87*","comments":"28 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc astro-ph.HE hep-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Einstein-Maxwell dilaton-axion (EMDA) gravity provides a simple framework to\ninvestigate the signatures of string theory. The axion and the dilaton fields\narising in EMDA gravity have important implications in inflationary cosmology\nand in addressing the late time acceleration of the universe. It is therefore\ninstructive to explore the implications of such a model in explaining the\nastrophysical observations. In this work we explore the role of EMDA gravity in\nexplaining the observed shadows of black holes (M87* and Sgr A*) released by\nthe Event Horizon Telescope (EHT) collaboration. The Kerr-Sen metric represents\nthe exact, stationary and axisymmetric black hole solution of EMDA gravity.\nSuch a black hole is characterized by the angular momentum $a$ acquired from\nthe axionic field and the dilatonic charge $r_2$ arising from string\ncompactifications. We study the role of spin and the dilaton charge in\nmodifying the shape and size of the black hole shadow. We note that black holes\nwith larger dilaton charge cast a smaller shadow. We investigate the\nconsequences of such a result in addressing the EHT observations of M87* and\nSgr A*. Our analysis reveals that the shadow of M87* exhibits a preference\ntowards the Kerr scenario. However, when 10% offset in the shadow diameter is\nconsidered, $0.1\\lesssim r_2\\lesssim 0.3$ is observationally favored within\n1-$\\sigma$. The shadow of Sgr A* on the other hand shows a preference towards\nthe Kerr-Sen scenario since the central value of its shadow can be better\nexplained by a non-zero dilaton charge $0.1 \\lesssim r_2 \\lesssim 0.4$.\nHowever, when the 1-$\\sigma$ interval is considered the Kerr scenario is\nincluded. We discuss the implications of our results.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:22:49 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14871","submitter":"Yuwei Zhang","authors":"Yuwei Zhang, Zihan Wang, Jingbo Shang","title":"ClusterLLM: Large Language Models as a Guide for Text Clustering","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We introduce ClusterLLM, a novel text clustering framework that leverages\nfeedback from an instruction-tuned large language model, such as ChatGPT.\nCompared with traditional unsupervised methods that builds upon \"small\"\nembedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the\nemergent capability of LLM even if its embeddings are inaccessible; and (2) it\nunderstands the user's preference on clustering through textual instruction\nand/or a few annotated data. First, we prompt ChatGPT for insights on\nclustering perspective by constructing hard triplet questions <does A better\ncorrespond to B than C>, where A, B and C are similar data points that belong\nto different clusters according to small embedder. We empirically show that\nthis strategy is both effective for fine-tuning small embedder and\ncost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on\nclustering granularity by carefully designed pairwise questions <do A and B\nbelong to the same category>, and tune the granularity from cluster hierarchies\nthat is the most consistent with the ChatGPT answers. Extensive experiments on\n14 datasets show that ClusterLLM consistently improves clustering quality, at\nan average cost of ~$0.6 per dataset.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:24:25 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14872","submitter":"Janek Gro{\\ss}","authors":"Janek Gro{\\ss}, Michael Kl\\\"as, Lisa J\\\"ockel, Pascal Gerber","title":"Timeseries-aware Uncertainty Wrappers for Uncertainty Quantification of\n  Information-Fusion-Enhanced AI Models based on Machine Learning","comments":"8 pages, 7 figures, VERDI workshop collocated with the DSN conference\n  2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.SE","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  As the use of Artificial Intelligence (AI) components in cyber-physical\nsystems is becoming more common, the need for reliable system architectures\narises. While data-driven models excel at perception tasks, model outcomes are\nusually not dependable enough for safety-critical applications. In this work,we\npresent a timeseries-aware uncertainty wrapper for dependable uncertainty\nestimates on timeseries data. The uncertainty wrapper is applied in combination\nwith information fusion over successive model predictions in time. The\napplication of the uncertainty wrapper is demonstrated with a traffic sign\nrecognition use case. We show that it is possible to increase model accuracy\nthrough information fusion and additionally increase the quality of uncertainty\nestimates through timeseries-aware input quality features.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:24:54 GMT"},{"version":"v2","created":"Wed, 31 May 2023 07:58:04 GMT"}],"update_date":"2023-06-01"}
{"id":"2305.14873","submitter":"Ming Ji","authors":"Ming Ji and Holger F. Hofmann","title":"Quantitative Relations Between Different Measurement Contexts","comments":"10 pages and 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In quantum theory, a measurement context is defined by an orthogonal basis in\na Hilbert space, where each basis vector represents a specific measurement\noutcome. The precise quantitative relation between two different measurement\ncontexts can thus be characterized by the inner products of nonorthogonal\nstates in that Hilbert space. Here, we use measurement outcomes that are shared\nby different contexts to derive specific quantitative relations between the\ninner products of the Hilbert space vectors that represent the different\ncontexts. It is shown that the probabilities that describe the paradoxes of\nquantum contextuality can be derived from a very small number of inner\nproducts, demonstrating that quantum contextuality is a necessary consequence\nof the quantitative relations between Hilbert space vectors representing\ndifferent measurement contexts. The application of our analysis to a product\nspace of two systems reveals that the non-locality of quantum entanglement can\nbe traced back to a local inner product representing the relation between\nmeasurement contexts in only one system. Our results thus indicate that the\nessential non-classical features of quantum mechanics can all be derived\nsystematically from the quantitative relations between different measurement\ncontexts described by the Hilbert space formalism.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:26:09 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14874","submitter":"Peter Jansen","authors":"Peter Jansen","title":"From Words to Wires: Generating Functioning Electronic Devices from\n  Natural Language Descriptions","comments":"13 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  In this work, we show that contemporary language models have a previously\nunknown skill -- the capacity for electronic circuit design from high-level\ntextual descriptions, akin to code generation. We introduce two benchmarks:\nPins100, assessing model knowledge of electrical components, and Micro25,\nevaluating a model's capability to design common microcontroller circuits and\ncode in the Arduino ecosystem that involve input, output, sensors, motors,\nprotocols, and logic -- with models such as GPT-4 and Claude-V1 achieving\nbetween 60% to 96% Pass@1 on generating full devices. We include six case\nstudies of using language models as a design assistant for moderately complex\ndevices, such as a radiation-powered random number generator, an emoji\nkeyboard, a visible spectrometer, and several assistive devices, while offering\na qualitative analysis performance, outlining evaluation challenges, and\nsuggesting areas of development to improve complex circuit design and practical\nutility. With this work, we aim to spur research at the juncture of natural\nlanguage processing and electronic design.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:28:59 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14875","submitter":"Alarith Uhde","authors":"Andreas F\\\"orster and Alarith Uhde and Mathias Komesker and Christina\n  Komesker and Irina Schmidt","title":"LoopBoxes -- Evaluation of a Collaborative Accessible Digital Musical\n  Instrument","comments":"10 pages, 9 figures, to be published in the Proceedings of the\n  International Conference on New Interfaces for Musical Expression (NIME'23)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.HC cs.CY cs.MM cs.SD eess.AS","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  LoopBoxes is an accessible digital musical instrument designed to create an\nintuitive access to loop based music making for children with special\neducational needs (SEN). This paper describes the evaluation of the instrument\nin the form of a pilot study during a music festival in Berlin, Germany, as\nwell as a case study with children and music teachers in a SEN school setting.\nWe created a modular system composed of three modules that afford single user\nas well as collaborative music making. The pilot study was evaluated using\ninformal observation and questionnaires (n = 39), and indicated that the\ninstrument affords music making for people with and without prior musical\nknowledge across all age groups and fosters collaborative musical processes.\nThe case study was based on observation and a qualitative interview. It\nconfirmed that the instrument meets the needs of the school settings and\nindicated how future versions could expand access to all students, especially\nthose experiencing complex disabilities. In addition, out-of-the-box\nfunctionality seems to be crucial for the long-term implementation of the\ninstrument in a school setting.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:29:14 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14876","submitter":"Yige Li","authors":"Yige Li, Xixiang Lyu, Xingjun Ma, Nodens Koren, Lingjuan Lyu, Bo Li,\n  Yu-Gang Jiang","title":"Reconstructive Neuron Pruning for Backdoor Defense","comments":"Accepted by ICML23","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Deep neural networks (DNNs) have been found to be vulnerable to backdoor\nattacks, raising security concerns about their deployment in mission-critical\napplications. While existing defense methods have demonstrated promising\nresults, it is still not clear how to effectively remove backdoor-associated\nneurons in backdoored DNNs. In this paper, we propose a novel defense called\n\\emph{Reconstructive Neuron Pruning} (RNP) to expose and prune backdoor neurons\nvia an unlearning and then recovering process. Specifically, RNP first unlearns\nthe neurons by maximizing the model's error on a small subset of clean samples\nand then recovers the neurons by minimizing the model's error on the same data.\nIn RNP, unlearning is operated at the neuron level while recovering is operated\nat the filter level, forming an asymmetric reconstructive learning procedure.\nWe show that such an asymmetric process on only a few clean samples can\neffectively expose and prune the backdoor neurons implanted by a wide range of\nattacks, achieving a new state-of-the-art defense performance. Moreover, the\nunlearned model at the intermediate step of our RNP can be directly used to\nimprove other backdoor defense tasks including backdoor removal, trigger\nrecovery, backdoor label detection, and backdoor sample detection. Code is\navailable at \\url{https://github.com/bboylyg/RNP}.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:29:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14877","submitter":"Sohee Yang","authors":"Sohee Yang, Jonghyeon Kim, Joel Jang, Seonghyeon Ye, Hyunji Lee,\n  Minjoon Seo","title":"Improving Probability-based Prompt Selection Through Unified Evaluation\n  and Analysis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Large Language Models (LLMs) have demonstrated great capabilities in solving\na wide range of tasks in a resource-efficient manner through prompting, which\ndoes not require task-specific training, but suffers from performance\nfluctuation when there are multiple prompt candidates. Previous works have\nintroduced gradient-free probability-based prompt selection methods that aim to\nchoose the optimal prompt among the candidates for a given task but fail to\nprovide a comprehensive and fair comparison between each other. In this paper,\nwe propose a unified framework to interpret and evaluate the existing\nprobability-based prompt selection methods by performing extensive experiments\non 13 common NLP tasks. We find that all existing methods can be unified into\nsome variant of the method that maximizes the mutual information between the\ninput and the corresponding model output (denoted as MI). Using the finding, we\ndevelop several variants of MI and increases the effectiveness of the best\nprompt selection method from 87.79% to 94.98%, measured as the ratio of the\nperformance of the selected prompt to that of the optimal oracle prompt.\nFurthermore, we propose a novel calibration method called Calibration by\nMarginalization (CBM) that is orthogonal to existing methods and helps increase\nthe prompt selection effectiveness of the best method by 99.44%. The code and\ndatasets used in our work will be released at\nhttps://github.com/soheeyang/unified-prompt-selection.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:29:50 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14878","submitter":"Vikas Raunak","authors":"Vikas Raunak, Amr Sharaf, Hany Hassan Awadallah, Arul Menezes","title":"Leveraging GPT-4 for Automatic Translation Post-Editing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  While Neural Machine Translation (NMT) represents the leading approach to\nMachine Translation (MT), the outputs of NMT models still require translation\npost-editing to rectify errors and enhance quality, particularly under critical\nsettings. In this work, we formalize the task of translation post-editing with\nLarge Language Models (LLMs) and explore the use of GPT-4 to automatically\npost-edit NMT outputs across several language pairs. Our results demonstrate\nthat GPT-4 is adept at translation post-editing and produces meaningful edits\neven when the target language is not English. Notably, we achieve\nstate-of-the-art performance on WMT-22 English-Chinese, English-German,\nChinese-English and German-English language pairs using GPT-4 based\npost-editing, as evaluated by state-of-the-art MT quality metrics.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:30:05 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14879","submitter":"Peter Jansen","authors":"Ruoyao Wang, Graham Todd, Eric Yuan, Ziang Xiao, Marc-Alexandre\n  C\\^ot\\'e, Peter Jansen","title":"ByteSized32: A Corpus and Challenge Task for Generating Task-Specific\n  World Models Expressed as Text Games","comments":"10 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  In this work we examine the ability of language models to generate explicit\nworld models of scientific and common-sense reasoning tasks by framing this as\na problem of generating text-based games. To support this, we introduce\nByteSized32, a corpus of 32 highly-templated text games written in Python\ntotaling 24k lines of code, each centered around a particular task, and paired\nwith a set of 16 unseen text game specifications for evaluation. We propose a\nsuite of automatic and manual metrics for assessing simulation validity,\ncompliance with task specifications, playability, winnability, and alignment\nwith the physical world. In a single-shot evaluation of GPT-4 on this\nsimulation-as-code-generation task, we find it capable of producing runnable\ngames in 27% of cases, highlighting the difficulty of this challenge task. We\ndiscuss areas of future improvement, including GPT-4's apparent capacity to\nperform well at simulating near canonical task solutions, with performance\ndropping off as simulations include distractors or deviate from canonical\nsolutions in the action space.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:31:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14880","submitter":"Shuting Yan","authors":"Shuting Yan, Pingping Chen, Honghui Chen, Huan Mao, Feng Chen and\n  Zhijian Lin","title":"Multiresolution Feature Guidance Based Transformer for Anomaly Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Anomaly detection is represented as an unsupervised learning to identify\ndeviated images from normal images. In general, there are two main challenges\nof anomaly detection tasks, i.e., the class imbalance and the unexpectedness of\nanomalies. In this paper, we propose a multiresolution feature guidance method\nbased on Transformer named GTrans for unsupervised anomaly detection and\nlocalization. In GTrans, an Anomaly Guided Network (AGN) pre-trained on\nImageNet is developed to provide surrogate labels for features and tokens.\nUnder the tacit knowledge guidance of the AGN, the anomaly detection network\nnamed Trans utilizes Transformer to effectively establish a relationship\nbetween features with multiresolution, enhancing the ability of the Trans in\nfitting the normal data manifold. Due to the strong generalization ability of\nAGN, GTrans locates anomalies by comparing the differences in spatial distance\nand direction of multi-scale features extracted from the AGN and the Trans. Our\nexperiments demonstrate that the proposed GTrans achieves state-of-the-art\nperformance in both detection and localization on the MVTec AD dataset. GTrans\nachieves image-level and pixel-level anomaly detection AUROC scores of 99.0%\nand 97.9% on the MVTec AD dataset, respectively.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:31:38 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14881","submitter":"Santiago Oviedo-Casado","authors":"Nicolas Staudenmaier, Anjusha Vijayakumar-Sreeja, Genko Genov, Daniel\n  Cohen, Christoph Findler, Johannes Lang, Alex Retzker, Fedor Jelezko,\n  Santiago Oviedo-Casado","title":"An optimal sensing protocol for statistically polarized nano-NMR with NV\n  centers","comments":"17 pages and 7 figures. Comments very welcome","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Diffusion noise represents a major constraint to successful liquid state\nnano-NMR spectroscopy. Using the Fisher information as a faithful measure, we\ncalculate theoretically and show experimentally that phase sensitive protocols\nare superior in most experimental scenarios, as they maximize information\nextraction from correlations in the sample. We derive the optimal experimental\nparameters for quantum heterodyne detection and present the most accurate\nstatistically polarized nano-NMR Qdyne experiments to date, leading the way to\nresolve chemical shifts and $J$-couplings at the nano-scale.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:32:28 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14882","submitter":"Xingyu Fu","authors":"Xingyu Fu, Ben Zhou, Sihao Chen, Mark Yatskar, Dan Roth","title":"Interpretable by Design Visual Question Answering","comments":"Multimodal, Vision and Language","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CV","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Model interpretability has long been a hard problem for the AI community\nespecially in the multimodal setting, where vision and language need to be\naligned and reasoned at the same time. In this paper, we specifically focus on\nthe problem of Visual Question Answering (VQA). While previous researches try\nto probe into the network structures of black-box multimodal models, we propose\nto tackle the problem from a different angle -- to treat interpretability as an\nexplicit additional goal.\n  Given an image and question, we argue that an interpretable VQA model should\nbe able to tell what conclusions it can get from which part of the image, and\nshow how each statement help to arrive at an answer. We introduce InterVQA:\nInterpretable-by-design VQA, where we design an explicit intermediate dynamic\nreasoning structure for VQA problems and enforce symbolic reasoning that only\nuse the structure for final answer prediction to take place. InterVQA produces\nhigh-quality explicit intermediate reasoning steps, while maintaining similar\nto the state-of-the-art (sota) end-task performance.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:33:15 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14883","submitter":"Ricard Vilar","authors":"Ricard Vilar and Simeon Ball","title":"Quantum cyclic redundancy check codes","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cs.IT math.IT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We extend the idea of classical cyclic redundancy check codes to quantum\ncyclic redundancy check codes. This allows us to construct codes quantum\nstabiliser codes which can correct burst errors where the burst length attains\nthe quantum Reiger bound. We then consider a certain family of quantum cyclic\nredundancy check codes for which we present a fast linear time decoding\nalgorithm.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:33:22 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14884","submitter":"Campbell Wheeler","authors":"Stavros Garoufalidis, Matthias Storzer and Campbell Wheeler","title":"Topological invariance of complex Chern-Simons and Teichm\\\"uller TQFT\n  perturbation theory","comments":"48 pages, 2 figures","journal-ref":null,"doi":null,"report-no":"MPIM-Bonn-2023","categories":"math.GT hep-th","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We prove the topological invariance of a formal power series defined using an\nideal triangulation of a 3-manifold with torus boundary components (together\nwith some further choices). This formal power series is conjectured to agree to\nall orders in perturbation theory with two important topological invariants of\nhyperbolic knots, namely the Kashaev invariant and the Andersen--Kashaev\ninvariant (also known as the state-integral) of Teichm\\\"uller TQFT.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:33:29 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.14885","submitter":"Chuhao Liu","authors":"Chuhao Liu and Shaojie Shen","title":"Towards View-invariant and Accurate Loop Detection Based on Scene Graph","comments":"Accepted by ICRA2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.RO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Loop detection plays a key role in visual Simultaneous Localization and\nMapping (SLAM) by correcting the accumulated pose drift. In indoor scenarios,\nthe richly distributed semantic landmarks are view-point invariant and hold\nstrong descriptive power in loop detection. The current semantic-aided loop\ndetection embeds the topology between semantic instances to search a loop.\nHowever, current semantic-aided loop detection methods face challenges in\ndealing with ambiguous semantic instances and drastic viewpoint differences,\nwhich are not fully addressed in the literature. This paper introduces a novel\nloop detection method based on an incrementally created scene graph, targeting\nthe visual SLAM at indoor scenes. It jointly considers the macro-view topology,\nmicro-view topology, and occupancy of semantic instances to find correct\ncorrespondences. Experiments using handheld RGB-D sequence show our method is\nable to accurately detect loops in drastically changed viewpoints. It maintains\na high precision in observing objects with similar topology and appearance. Our\nmethod also demonstrates that it is robust in changed indoor scenes.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:34:43 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14886","submitter":"Jiajia Chen","authors":"Jiajia Chen, Jiancan Wu, Jiawei Chen, Xin Xin, Yong Li, Xiangnan He","title":"How Graph Convolutions Amplify Popularity Bias for Recommendation?","comments":"Accepted by Frontiers of Computer Science","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Graph convolutional networks (GCNs) have become prevalent in recommender\nsystem (RS) due to their superiority in modeling collaborative patterns.\nAlthough improving the overall accuracy, GCNs unfortunately amplify popularity\nbias -- tail items are less likely to be recommended. This effect prevents the\nGCN-based RS from making precise and fair recommendations, decreasing the\neffectiveness of recommender systems in the long run.\n  In this paper, we investigate how graph convolutions amplify the popularity\nbias in RS. Through theoretical analyses, we identify two fundamental factors:\n(1) with graph convolution (\\textit{i.e.,} neighborhood aggregation), popular\nitems exert larger influence than tail items on neighbor users, making the\nusers move towards popular items in the representation space; (2) after\nmultiple times of graph convolution, popular items would affect more high-order\nneighbors and become more influential. The two points make popular items get\ncloser to almost users and thus being recommended more frequently. To rectify\nthis, we propose to estimate the amplified effect of popular nodes on each\nnode's representation, and intervene the effect after each graph convolution.\nSpecifically, we adopt clustering to discover highly-influential nodes and\nestimate the amplification effect of each node, then remove the effect from the\nnode embeddings at each graph convolution layer. Our method is simple and\ngeneric -- it can be used in the inference stage to correct existing models\nrather than training a new model from scratch, and can be applied to various\nGCN models. We demonstrate our method on two representative GCN backbones\nLightGCN and UltraGCN, verifying its ability in improving the recommendations\nof tail items without sacrificing the performance of popular items. Codes are\nopen-sourced \\footnote{https://github.com/MEICRS/DAP}.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:35:43 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14887","submitter":"Khalil Zakeri Lori","authors":"Khalil Zakeri and Christophe Berthod","title":"Theory of spin-polarized high-resolution electron energy loss\n  spectroscopy from nonmagnetic surfaces with a large spin-orbit coupling","comments":"8 pages, 1 figure","journal-ref":"Phys. Rev. B 106, 235117 (2022)","doi":"10.1103/PhysRevB.106.235117","report-no":null,"categories":"cond-mat.str-el cond-mat.supr-con","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The scattering theory of low-energy (slow) electrons has been developed by\nEvans and Mills [Phys. Rev. B 5, 4126 (1972)]. The formalism is merely based on\nthe electrostatic Coulomb interaction of the scattering electrons with the\ncharge-density fluctuations above the surface and can describe most of the\ninteresting features observed in the high-resolution electron energy-loss\nspectroscopy experiments. Here we extend this theory by including the\nspin-orbit coupling in the scattering process. We discuss the impact of this\ninteraction on the scattering cross section. In particular, we discuss cases in\nwhich a spin-polarized electron beam is scattered from nonmagnetic surfaces\nwith a strong spin-orbit coupling. We show that under some assumptions one can\nderive an expression for the scattering cross section, which can be used for\nnumerical calculations of the spin-polarized spectra recorded by spin-polarized\nhigh-resolution electron energy-loss spectroscopy experiments.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:37:25 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14888","submitter":"Yangsibo Huang","authors":"Yangsibo Huang, Samyak Gupta, Zexuan Zhong, Kai Li, Danqi Chen","title":"Privacy Implications of Retrieval-Based Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CR cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Retrieval-based language models (LMs) have demonstrated improved\ninterpretability, factuality, and adaptability compared to their parametric\ncounterparts, by incorporating retrieved text from external datastores. While\nit is well known that parametric models are prone to leaking private data, it\nremains unclear how the addition of a retrieval datastore impacts model\nprivacy. In this work, we present the first study of privacy risks in\nretrieval-based LMs, particularly $k$NN-LMs. Our goal is to explore the optimal\ndesign and training procedure in domains where privacy is of concern, aiming to\nstrike a balance between utility and privacy. Crucially, we find that $k$NN-LMs\nare more susceptible to leaking private information from their private\ndatastore than parametric models. We further explore mitigations of privacy\nrisks. When privacy information is targeted and readily detected in the text,\nwe find that a simple sanitization step would completely eliminate the risks,\nwhile decoupling query and key encoders achieves an even better utility-privacy\ntrade-off. Otherwise, we consider strategies of mixing public and private data\nin both datastore and encoder training. While these methods offer modest\nimprovements, they leave considerable room for future work. Together, our\nfindings provide insights for practitioners to better understand and mitigate\nprivacy risks in retrieval-based LMs. Our code is available at:\nhttps://github.com/Princeton-SysML/kNNLM_privacy .\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:37:27 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14889","submitter":"Ziang Xiao","authors":"Ziang Xiao, Susu Zhang, Vivian Lai, Q. Vera Liao","title":"Evaluating NLG Evaluation Metrics: A Measurement Theory Perspective","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We address the fundamental challenge in Natural Language Generation (NLG)\nmodel evaluation, the design and validation of evaluation metrics. Recognizing\nthe limitations of existing metrics and issues with human judgment, we propose\nusing measurement theory, the foundation of test design, as a framework for\nconceptualizing and evaluating the validity and reliability of NLG evaluation\nmetrics. This approach offers a systematic method for defining \"good\" metrics,\ndeveloping robust metrics, and assessing metric performance. In this paper, we\nintroduce core concepts in measurement theory in the context of NLG evaluation\nand key methods to evaluate the performance of NLG metrics. Through this\nframework, we aim to promote the design, evaluation, and interpretation of\nvalid and reliable metrics, ultimately contributing to the advancement of\nrobust and effective NLG models in real-world settings.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:38:23 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14890","submitter":"Arne Nix","authors":"Arne F. Nix, Max F. Burg, Fabian H. Sinz","title":"HARD: Hard Augmentations for Robust Distillation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Knowledge distillation (KD) is a simple and successful method to transfer\nknowledge from a teacher to a student model solely based on functional\nactivity. However, current KD has a few shortcomings: it has recently been\nshown that this method is unsuitable to transfer simple inductive biases like\nshift equivariance, struggles to transfer out of domain generalization, and\noptimization time is magnitudes longer compared to default non-KD model\ntraining. To improve these aspects of KD, we propose Hard Augmentations for\nRobust Distillation (HARD), a generally applicable data augmentation framework,\nthat generates synthetic data points for which the teacher and the student\ndisagree. We show in a simple toy example that our augmentation framework\nsolves the problem of transferring simple equivariances with KD. We then apply\nour framework in real-world tasks for a variety of augmentation models, ranging\nfrom simple spatial transformations to unconstrained image manipulations with a\npretrained variational autoencoder. We find that our learned augmentations\nsignificantly improve KD performance on in-domain and out-of-domain evaluation.\nMoreover, our method outperforms even state-of-the-art data augmentations and\nsince the augmented training inputs can be visualized, they offer a qualitative\ninsight into the properties that are transferred from the teacher to the\nstudent. Thus HARD represents a generally applicable, dynamically optimized\ndata augmentation technique tailored to improve the generalization and\nconvergence speed of models trained with KD.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:38:44 GMT"},{"version":"v2","created":"Thu, 25 May 2023 10:57:46 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.14891","submitter":"Luka Pavlovi\\'c","authors":"Luka Pavlovi\\'c","title":"Extracting Psychological Indicators Using Question Answering","comments":"4 pages, 0 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CY","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this work, we propose a method for extracting text spans that may indicate\none of the BIG5 psychological traits using a question-answering task with\nexamples that have no answer for the asked question. We utilized the RoBERTa\nmodel fine-tuned on SQuAD 2.0 dataset. The model was further fine-tuned\nutilizing comments from Reddit. We examined the effect of the percentage of\nexamples with no answer in the training dataset on the overall performance. The\nresults obtained in this study are in line with the SQuAD 2.0 benchmark and\npresent a good baseline for further research.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:41:23 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14892","submitter":"Mohammad Rowshan","authors":"Mohammad Rowshan and Jinhong Yuan","title":"Segmented GRAND: Combining Sub-patterns in Near-ML Order","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT eess.SP math.IT","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The recently introduced maximum-likelihood (ML) decoding scheme called\nguessing random additive noise decoding (GRAND) has demonstrated a remarkably\nlow time complexity in high signal-to-noise ratio (SNR) regimes. However, the\ncomplexity is not as low at low SNR regimes and low code rates. To mitigate\nthis concern, we propose a scheme for a near-ML variant of GRAND called ordered\nreliability bits GRAND (or ORBGRAND), which divides codewords into segments\nbased on the properties of the underlying code, generates sub-patterns for each\nsegment consistent with the syndrome (thus reducing the number of inconsistent\nerror patterns generated), and combines them in a near-ML order using two-level\ninteger partitions of logistic weight. The numerical evaluation demonstrates\nthat the proposed scheme, called segmented ORBGRAND, significantly reduces the\naverage number of queries at any SNR regime. Moreover, the segmented ORBGRAND\nwith abandonment also improves the error correction performance.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:43:07 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14893","submitter":"Najmeh Sadat Mirian","authors":"Najmeh Sadat Mirian, Elham Salehi, Frank Zimmermann","title":"Using the LHeC ERL to generate high-energy photons","comments":"4","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.acc-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The Large Hadron electron Collider (LHeC) is a proposed future particle\nphysics project colliding 60 GeV electrons from a six-pass recirculating\nenergy-recovery linac (ERL) with 7 TeV protons stored in the LHC. The ERL\ntechnology allows for much higher beam current and, therefore, higher\nluminosity than a traditional linac. The high-current, high-energy electron\nbeam can also be used to drive a free electron laser (FEL). In this\ncontribution, we examine how the LHeC ERL can serve as a source of high-energy\nphotons for studies in nuclear physics, high-energy physics, Axion detection,\ndark energy, and protein crystallography. In the first section, we discuss the\nperformance of the LHeC-based FEL, operated in the SASE mode for generating\nphoton pulses at wavelengths ranging from 200 keV to 600 keV. In the second\nsection, we investigate photon production via Laser Compton scattering (LCS).\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:44:41 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14894","submitter":"Bosiljka Tadic","authors":"Samir Sahoo, Bosiljka Tadic, Malayaja Chutani and Neelima Gupte","title":"Effect of hidden geometry and higher-order interactions on the\n  synchronization and hysteresis behaviour of phase oscillators on 5-cliques\n  simplicial assemblies","comments":"9 pages, 7 figures; regular article, submitted","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.stat-mech nlin.AO","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The hidden geometry of simplicial complexes can influence the collective\ndynamics of nodes in different ways depending on the simplex-based interactions\nof various orders and competition between local and global structural features.\nWe study a system of phase oscillators attached to nodes of 4-dimensional\nsimplicial complexes and interacting via positive/negative edges-based pairwise\n$K_1$ and triangle-based triple $K_2\\geq 0$ couplings. Three prototypal\nsimplicial complexes are grown by aggregation of 5-cliques, controlled by the\nchemical affinity parameter $\\nu$, resulting in sparse, mixed, and compact\narchitecture, all of which have 1-hyperbolic graphs but different spectral\ndimensions. By changing the interaction strength $K_1\\in[-4,2]$ along the\nforward and backward sweeps, we numerically determine individual phases of each\noscillator and a global order parameter to measure the level of\nsynchronisation. Our results reveal how different architectures of simplicial\ncomplexes, in conjunction with the interactions and internal-frequency\ndistributions, impact the shape of the hysteresis loop and lead to patterns of\nlocally synchronised groups that hinder global network synchronisation.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:45:19 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14895","submitter":"Zhixing Ling Dr","authors":"Z.X. Ling, X.J. Sun, C. Zhang, S.L. Sun, G. Jin, S.N. Zhang, X.F.\n  Zhang, J.B. Chang, F.S. Chen, Y.F. Chen, Z.W. Cheng, W. Fu, Y.X. Han, H. Li,\n  J.F. Li, Y. Li, Z.D. Li, P.R. Liu, Y.H. Lv, X.H. Ma, Y.J. Tang, C.B. Wang,\n  R.J. Xie, Y.L. Xue, A.L. Yan, Q. Zhang, C.Y. Bao, H.B. Cai, H.Q. Cheng, C.Z.\n  Cui, Y.F. Dai, D.W. Fan, H.B. Hu, J.W. Hu, M.H. Huang, Z.Q. Jia, C.C. Jin,\n  D.Y. Li, J.Q. Li, H.Y. Liu, M.J. Liu, Y. Liu, H.W. Pan, Y.L. Qiu, M.\n  Sugizaki, H. Sun, W.X. Wang, Y.L. Wang, Q.Y. Wu, X.P. Xu, Y.F. Xu, H.N. Yang,\n  X. Yang, B. Zhang, M. Zhang, W.D. Zhang, Z. Zhang, D.H. Zhao, X.Q. Cong, B.W.\n  Jiang, L.H. Li, X.B. Qiu, J.N. Sun, D.T. Su, J. Wang, C. Wu, Z. Xu, X.M.\n  Yang, S.K. Zhang, Z. Zhang, N. Zhang, Y.F. Zhu, H.Y. Ban, X.Z. Bi, Z.M. Cai,\n  W. Chen, X. Chen, Y.H. Chen, Y. Cui, X.L. Duan, Z.G Feng, Y. Gao, J.W. He, T.\n  He, J.J. Huang, F. Li, J.S. Li, T.J. Li, T.T. Li, H.Q. Liu, L. Liu, R. Liu,\n  S. Liu, N. Meng, Q. Shi, A.T. Sun, Y.M. Wang, Y.B. Wang, H.C. Wu, D.X Xu, Y.Q\n  Yang, Y. Yang, X.S. Yu, K.X. Zhang, Y.L. Zhang, Y.H. Zhang, Y.T. Zhang, H.\n  Zhou, X.C. Zhu, J.S. Cheng, L. Qin, L. Wang, Q.L. Wang, M. Bai, R.L. Gao, Z.\n  Ji, Y.R. Liu, F.L. Ma, Y.J. Shi, J. Su, Y.Y. Tan, J.Z. Tong, H.T. Xu, C.B.\n  Xue, G.F. Xue, W. Yuan","title":"The Lobster Eye Imager for Astronomy Onboard the SATech-01 Satellite","comments":"Accepted by RAA","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.IM hep-ex physics.ins-det","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  The Lobster Eye Imager for Astronomy (LEIA), a pathfinder of the Wide-field\nX-ray Telescope of the Einstein Probe (EP) mission, was successfully launched\nonboard the SATech-01 satellite of the Chinese Academy of Sciences on 27 July\n2022. In this paper, we introduce the design and on-ground test results of the\nLEIA instrument. Using state-of-the-art Micro-Pore Optics (MPO), a wide\nfield-of-view (FoV) of 346 square degrees (18.6 degrees * 18.6 degrees) of the\nX-ray imager is realized. An optical assembly composed of 36 MPO chips is used\nto focus incident X-ray photons, and four large-format complementary\nmetal-oxide semiconductor (CMOS) sensors, each of 6 cm * 6 cm, are used as the\nfocal plane detectors. The instrument has an angular resolution of 4 - 8 arcmin\n(in FWHM) for the central focal spot of the point spread function, and an\neffective area of 2 - 3 cm2 at 1 keV in essentially all the directions within\nthe field of view. The detection passband is 0.5 - 4 keV in the soft X-rays and\nthe sensitivity is 2 - 3 * 10-11 erg s-1 cm-2 (about 1 mini-Crab) at 1,000\nsecond observation. The total weight of LEIA is 56 kg and the power is 85 W.\nThe satellite, with a design lifetime of 2 years, operates in a Sun-synchronous\norbit of 500 km with an orbital period of 95 minutes. LEIA is paving the way\nfor future missions by verifying in flight the technologies of both novel\nfocusing imaging optics and CMOS sensors for X-ray observation, and by\noptimizing the working setups of the instrumental parameters. In addition, LEIA\nis able to carry out scientific observations to find new transients and to\nmonitor known sources in the soft X-ray band, albeit limited useful observing\ntime available.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:47:05 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14896","submitter":"Krzysztof Oleszkiewicz","authors":"Krzysztof Oleszkiewicz","title":"Boolean functions with small second order influences on the discrete\n  cube","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Motivated by a recent paper of Kevin Tanguy, in which the concept of second\norder influences on the discrete cube and Gauss space has been investigated in\ndetail, the present note studies it in a more specific context of Boolean\nfunctions on the discrete cube. Some bounds which Tanguy obtained as\napplications of his more general approach are extended and complemented.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:48:19 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14897","submitter":"Amita Kamath","authors":"Amita Kamath, Jack Hessel, Kai-Wei Chang","title":"Text encoders are performance bottlenecks in contrastive vision-language\n  models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CV cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Performant vision-language (VL) models like CLIP represent captions using a\nsingle vector. How much information about language is lost in this bottleneck?\nWe first curate CompPrompts, a set of increasingly compositional image captions\nthat VL models should be able to capture (e.g., single object, to\nobject+property, to multiple interacting objects). Then, we train text-only\nrecovery probes that aim to reconstruct captions from single-vector text\nrepresentations produced by several VL models. This approach doesn't require\nimages, allowing us to test on a broader range of scenes compared to prior\nwork. We find that: 1) CLIP's text encoder falls short on object relationships,\nattribute-object association, counting, and negations; 2) some text encoders\nwork significantly better than others; and 3) text-only recovery performance\npredicts multi-modal matching performance on ControlledImCaps: a new evaluation\nbenchmark we collect+release consisting of fine-grained compositional\nimages+captions. Specifically -- our results suggest text-only recoverability\nis a necessary (but not sufficient) condition for modeling compositional\nfactors in contrastive vision+language models. We release data+code.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:48:44 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14898","submitter":"Keming Lu","authors":"Keming Lu, Xiaoman Pan, Kaiqiang Song, Hongming Zhang, Dong Yu,\n  Jianshu Chen","title":"PIVOINE: Instruction Tuning for Open-world Information Extraction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We consider the problem of Open-world Information Extraction (Open-world IE),\nwhich extracts comprehensive entity profiles from unstructured texts. Different\nfrom the conventional closed-world setting of Information Extraction (IE),\nOpen-world IE considers a more general situation where entities and relations\ncould be beyond a predefined ontology. More importantly, we seek to develop a\nlarge language model (LLM) that is able to perform Open-world IE to extract\ndesirable entity profiles characterized by (possibly fine-grained) natural\nlanguage instructions. We achieve this by finetuning LLMs using instruction\ntuning. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction\ntuning dataset for Open-world IE enriched with a comprehensive corpus,\nextensive annotations, and diverse instructions. We finetune the pretrained\nBLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world IE\nwith strong instruction-following capabilities. Our experiments demonstrate\nthat PIVOINE significantly outperforms traditional closed-world methods and\nother LLM baselines, displaying impressive generalization capabilities on both\nunseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as\na promising solution to tackle the open-world challenge in IE effectively.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:52:08 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14899","submitter":"Emilio Martinez-Nunez","authors":"Luis Guerrero-Mendez, Anxo Lema-Saavedra, Elena Jimenez, Antonio\n  Fernandez-Ramos, Emilio Martinez-Nunez","title":"Gas-phase formation of glycolonitrile in the interstellar medium","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.GA physics.chem-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Our automated reaction discovery program, AutoMeKin, has been utilized to\ninvestigate the formation of glycolonitrile (HOCH$_{2}$CN) in the gas phase\nunder the low temperatures of the interstellar medium (ISM). The feasibility of\na proposed pathway depends on the absence of barriers above the energy of\nreactants and the availability of the suggested precursors in the ISM. Based on\nthese criteria, several radical-radical reactions and a radical-molecule\nreaction have been identified as viable formation routes in the ISM. Among the\nradical-radical reactions, OH+CH$_{2}$CN appears to be the most relevant,\nconsidering the energy of the radicals and its ability to produce\nglycolonitrile in a single step. However, our analysis reveals that this\nreaction produces hydrogen isocyanide (HNC) and formaldehyde (CH$_{2}$O), with\nrate coefficients ranging from (7.3-11.5)$\\times$10$^{-10}$ cm$^3$\nmolecule$^{-1}$ s$^{-1}$ across the temperature range of 10-150 K. This finding\nis particularly interesing given the persistently unexplained overabundance of\nhydrogen isocyanide in the ISM. Among the radical-molecule reactions\ninvestigated, the most promising one is OH+CH$_{2}$CNH, which forms\nglycolonitrile and atomic hydrogen with rate coefficients in the range\n(0.3-6.6)$\\times$10$^{-10}$ cm$^3$ molecule$^{-1}$ s$^{-1}$ within the 10-150 K\ntemperature range. Our calculations indicate that the formation of both\nhydrogen isocyanide and glycolonitrile is efficient under the harsh conditions\nof the ISM.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:54:10 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14900","submitter":"Jasper Ischebeck","authors":"Jasper Ischebeck","title":"Central limit theorems for fringe trees in patricia tries","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We give theorems about asymptotic normality of general additive functionals\non patricia tries in an i.i.d. setting, derived from results on tries by Janson\n(2022). These theorems are applied to show asymptotic normality of the\ndistribution of random fringe trees in patricia tries. Formulas for asymptotic\nmean and variance are given. The proportion of fringe trees with $k$ keys is\nasymptotically, ignoring oscillations, given by $(1-\\rho(k))/(H+J)k(k-1)$ with\nthe source entropy $H$, an entropy-like constant $J$, that is $H$ in the binary\ncase, and an exponentially decreasing function $\\rho(k)$. Another application\ngives asymptotic normality of the independence number.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:54:46 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14901","submitter":"Wang Zhu","authors":"Wang Zhu, Jesse Thomason, Robin Jia","title":"Chain-of-Questions Training with Latent Answers for Robust Multistep\n  Question Answering","comments":"12 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We train a language model (LM) to robustly answer multistep questions by\ngenerating and answering sub-questions. We propose Chain-of-Questions, a\nframework that trains a model to generate sub-questions and sub-answers one at\na time by leveraging human annotated question decomposition meaning\nrepresentation (QDMR). The key technical challenge is that QDMR only contains\nsub-questions but not answers to those sub-questions, so we treat sub-answers\nas latent variables and optimize them using a novel dynamic mixture of Hard-EM\nand MAPO. Chain-of-Questions greatly outperforms strong neuro-symbolic methods\nby 9.0 F1 on DROP contrast set, and outperforms GPT-3.5 by 24.3 F1 on HOTPOTQA\nadversarial set, thus demonstrating the effectiveness and robustness of our\nframework.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:55:08 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14902","submitter":"Yuxia Wang","authors":"Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem\n  Shelmanov, Akim Tsvigun, Chenxi Whitehouse, Osama Mohammed Afzal, Tarek\n  Mahmoud, Alham Fikri Aji, Preslav Nakov","title":"M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box\n  Machine-Generated Text Detection","comments":"11 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large language models (LLMs) have demonstrated remarkable capability to\ngenerate fluent responses to a wide variety of user queries, but this has also\nresulted in concerns regarding the potential misuse of such texts in\njournalism, educational, and academic context. In this work, we aim to develop\nautomatic systems to identify machine-generated text and to detect potential\nmisuse. We first introduce a large-scale benchmark M4, which is\nmulti-generator, multi-domain, and multi-lingual corpus for machine-generated\ntext detection. Using the dataset, we experiment with a number of methods and\nwe show that it is challenging for detectors to generalize well on unseen\nexamples if they are either from different domains or are generated by\ndifferent large language models. In such cases, detectors tend to misclassify\nmachine-generated text as human-written. These results show that the problem is\nfar from solved and there is a lot of room for improvement. We believe that our\ndataset M4, which covers different generators, domains and languages, will\nenable future research towards more robust approaches for this pressing\nsocietal problem. The M4 dataset is available at\nhttps://github.com/mbzuai-nlp/M4.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:55:11 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14903","submitter":"Francesco Marin","authors":"P. Vezio, M. Bonaldi, A. Borrielli, F. Marino, B. Morana, P.M. Sarro,\n  E. Serra, and F. Marin","title":"Optical self-cooling of a membrane oscillator in a cavity optomechanical\n  experiment at room temperature","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Thermal noise is a major obstacle to observing quantum behavior in\nmacroscopic systems. To mitigate its effect, quantum optomechanical experiments\nare typically performed in a cryogenic environment. However, this condition\nrepresents a considerable complication in the transition from fundamental\nresearch to quantum technology applications. It is therefore interesting to\nexplore the possibility of achieving the quantum regime in room temperature\nexperiments. In this work we test the limits of sideband cooling vibration\nmodes of a SiN membrane in a cavity optomechanical experiment. We obtain an\neffective temperature of a few mK, corresponding to a phononic occupation\nnumber of around 100. We show that further cooling is prevented by the excess\nclassical noise of our laser source, and we outline the road toward the\nachievement of ground state cooling\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:56:23 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14904","submitter":"Alexander Spangher","authors":"Alexander Spangher, Nanyun Peng, Jonathan May, Emilio Ferrara","title":"Identifying Informational Sources in News Articles","comments":"13 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CY","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  News articles are driven by the informational sources journalists use in\nreporting. Modeling when, how and why sources get used together in stories can\nhelp us better understand the information we consume and even help journalists\nwith the task of producing it. In this work, we take steps toward this goal by\nconstructing the largest and widest-ranging annotated dataset, to date, of\ninformational sources used in news writing. We show that our dataset can be\nused to train high-performing models for information detection and source\nattribution. We further introduce a novel task, source prediction, to study the\ncompositionality of sources in news articles. We show good performance on this\ntask, which we argue is an important proof for narrative science exploring the\ninternal structure of news articles and aiding in planning-based language\ngeneration, and an important step towards a source-recommendation system to aid\njournalists.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:56:35 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14905","submitter":"Tinatin Baratashvili","authors":"Tinatin Baratashvili, Christine Verbeke, Rony Keppens, Stefaan Poedts","title":"Exploring the effects of numerical methods and slope limiters in\n  heliospheric modeling","comments":"17 pages, 9 figures, 4 tables, to appear in Sun and Geosphere","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.SR physics.comp-ph physics.plasm-ph physics.space-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Coronal mass ejections (CMEs) are large eruptions close to the solar surface,\nwhere plasma is ejected outwards into space at large speeds. When directed\ntowards Earth, they interfere with Earth's magnetic fields and cause strong\ngeo-effective storms. In order to mitigate the potential damage, forecasting\ntools are implemented. Recently, a novel heliospheric modelling tool, Icarus,\nhas been implemented, which exploits the open-source framework MPI-AMRVAC as\nits core MHD solver. This new model efficiently performs 3D MHD simulations of\nthe solar wind and the evolution of interplanetary CMEs with the help of\nadvanced techniques, such as adaptive mesh refinement and gradual radial grid\nstretching. The numerical methods applied in the simulations can have\nsignificant effects on the simulation results and on the efficiency of the\nmodel. In this study, the effect of different combinations of numerical schemes\nand slope limiters, for reconstructing edge-based variabes used in fluxes, is\nconsidered. We explore frequently exploited combinations from the available\nnumerical schemes in MPI-AMRVAC: TVDLF, HLL and HLLC along with the slope\nlimiters 'woodward', 'minmod', 'vanleer', and 'koren'. For analysis purposes,\nwe selected one particular solar wind configuration and studied the influence\non variables at 1 AU in the equatorial plane. The goal is to find the optimal\ncombination to produce accurate results fast and in a robust way so that the\nmodel can be reliable for day-to-day use by space weather scientists. As a\nconclusion, the best result assessed with these two criteria is the combination\nof the TVDLF scheme with the 'woodward' limiter.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:56:35 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14906","submitter":"Alba Garc\\'ia-Ruiz","authors":"Alba Garc\\'ia-Ruiz","title":"A relation between two different formulations of the Berry's conjecture","comments":"19 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.SP math.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The Random Wave Conjecture of M. V. Berry is the heuristic that\neigenfunctions of a classically chaotic system should behave like Gaussian\nrandom fields, in the large eigenvalue limit. In this work we collect some\ndefinitions and properties of Gaussian random fields, and show that the\nformulation of the Berry's conjecture proposed using local weak limits is\nequivalent to the one that is based on the Benjamini-Schramm convergence.\nFinally, we see that both these formulations of the Berry's property imply\nanother property known as inverse localization that relates high energy\neigenfunctions and solutions to the Euclidean Helmholtz equation.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:56:38 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14907","submitter":"Shivanshu Gupta","authors":"Shivanshu Gupta, Matt Gardner, Sameer Singh","title":"Coverage-based Example Selection for In-Context Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In-context learning (ICL), the ability of large language models to perform\nnovel tasks by conditioning on a prompt with a few task examples, requires\ndemonstrations that are informative about the test instance. The standard\napproach of independently selecting the most similar examples selects redundant\ndemonstrations while overlooking important information. This work proposes a\nframework for assessing the informativeness of demonstrations based on their\ncoverage of salient aspects (e.g., reasoning patterns) of the test input. Using\nthis framework, we show that contextual token embeddings effectively capture\nthese salient aspects, and their recall measured using BERTScore-Recall (BSR)\nyields a reliable measure of informativeness. Further, we extend recall metrics\nlike BSR to propose their set versions to find maximally informative sets of\ndemonstrations. On 6 complex compositional generation tasks and 7 diverse LLMs,\nwe show that Set-BSR outperforms the standard similarity-based approach by up\nto 16% on average and, despite being learning-free, often surpasses methods\nthat leverage task or LLM-specific training.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:58:28 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14908","submitter":"Anthony Chen","authors":"Anthony Chen, Panupong Pasupat, Sameer Singh, Hongrae Lee and Kelvin\n  Guu","title":"PURR: Efficiently Editing Language Model Hallucinations by Denoising\n  Language Model Corruptions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The remarkable capabilities of large language models have been accompanied by\na persistent drawback: the generation of false and unsubstantiated claims\ncommonly known as \"hallucinations\". To combat this issue, recent research has\nintroduced approaches that involve editing and attributing the outputs of\nlanguage models, particularly through prompt-based editing. However, the\ninference cost and speed of using large language models for editing currently\nbottleneck prompt-based methods. These bottlenecks motivate the training of\ncompact editors, which is challenging due to the scarcity of training data for\nthis purpose. To overcome these challenges, we exploit the power of large\nlanguage models to introduce corruptions (i.e., noise) into text and\nsubsequently fine-tune compact editors to denoise the corruptions by\nincorporating relevant evidence. Our methodology is entirely unsupervised and\nprovides us with faux hallucinations for training in any domain. Our Petite\nUnsupervised Research and Revision model, PURR, not only improves attribution\nover existing editing methods based on fine-tuning and prompting, but also\nachieves faster execution times by orders of magnitude.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:59:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14909","submitter":"Lin Guan","authors":"Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati","title":"Leveraging Pre-trained Large Language Models to Construct and Utilize\n  World Models for Model-based Task Planning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  There is a growing interest in applying pre-trained large language models\n(LLMs) to planning problems. However, methods that use LLMs directly as\nplanners are currently impractical due to several factors, including limited\ncorrectness of plans, strong reliance on feedback from interactions with\nsimulators or even the actual environment, and the inefficiency in utilizing\nhuman feedback. In this work, we introduce a novel alternative paradigm that\nconstructs an explicit world (domain) model in planning domain definition\nlanguage (PDDL) and then uses it to plan with sound domain-independent\nplanners. To address the fact that LLMs may not generate a fully functional\nPDDL model initially, we employ LLMs as an interface between PDDL and sources\nof corrective feedback, such as PDDL validators and humans. For users who lack\na background in PDDL, we show that LLMs can translate PDDL into natural\nlanguage and effectively encode corrective feedback back to the underlying\ndomain model. Our framework not only enjoys the correctness guarantee offered\nby the external planners but also reduces human involvement by allowing users\nto correct domain models at the beginning, rather than inspecting and\ncorrecting (through interactive prompting) every generated plan as in previous\nwork. On two IPC domains and a Household domain that is more complicated than\ncommonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be\nleveraged to produce high-quality PDDL models for over 40 actions, and the\ncorrected PDDL models are then used to successfully solve 48 challenging\nplanning tasks. Resources including the source code will be released at:\nhttps://guansuns.github.io/pages/llm-dm.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:59:15 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14910","submitter":"Muhao Chen","authors":"Qin Liu, Fei Wang, Chaowei Xiao, Muhao Chen","title":"From Shortcuts to Triggers: Backdoor Defense with Denoised PoE","comments":"Work in Progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CR cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Language models are often at risk of diverse backdoor attacks, especially\ndata poisoning. Thus, it is important to investigate defense solutions for\naddressing them. Existing backdoor defense methods mainly focus on backdoor\nattacks with explicit triggers, leaving a universal defense against various\nbackdoor attacks with diverse triggers largely unexplored. In this paper, we\npropose an end-to-end ensemble-based backdoor defense framework, DPoE (Denoised\nProduct-of-Experts), which is inspired by the shortcut nature of backdoor\nattacks, to defend various backdoor attacks. DPoE consists of two models: a\nshallow model that captures the backdoor shortcuts and a main model that is\nprevented from learning the backdoor shortcuts. To address the label flip\ncaused by backdoor attackers, DPoE incorporates a denoising design. Experiments\non SST-2 dataset show that DPoE significantly improves the defense performance\nagainst various types of backdoor triggers including word-level,\nsentence-level, and syntactic triggers. Furthermore, DPoE is also effective\nunder a more challenging but practical setting that mixes multiple types of\ntrigger.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:59:25 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14911","submitter":"Qian Zhang","authors":"Jianqing Chen and Qian Zhang","title":"Ground states solution of Nehari-Poho\\v{z}aev type for periodic\n  quasilinear Schr\\\"{o}dinger system","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper is concerned with a quasilinear Schr\\\"{o}dinger system in $\\mathbb\nR^{N}$ $$\\left\\{\\aligned &-\\Delta\nu+A(x)u-\\frac{1}{2}\\triangle(u^{2})u=\\frac{2\\alpha}{\\alpha+\\beta}|u|^{\\alpha-2}u|v|^{\\beta},\\\\\n&-\\Delta\nv+B(x)v-\\frac{1}{2}\\triangle(v^{2})v=\\frac{2\\beta}{\\alpha+\\beta}|u|^{\\alpha}|v|^{\\beta-2}v,\\\\\n& u(x)\\to 0\\ \\hbox{and}\\quad v(x)\\to 0\\ \\hbox{as}\\ |x|\\to\n\\infty,\\endaligned\\right. $$ where $\\alpha,\\beta>1$ and\n$2<\\alpha+\\beta<\\frac{4N}{N-2}$ ($N \\geq 3$). $A(x)$ and $B(x)$ are two\nperiodic functions. By minimization under a convenient constraint and\nconcentration-compactness lemma, we prove the existence of ground states\nsolution. Our result covers the case of $\\alpha+\\beta\\in(2,4)$ which seems to\nbe the first result for coupled quasilinear Schr\\\"{o}dinger system in the\nperiodic situation.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:01:23 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14912","submitter":"Yu-Bang Zheng","authors":"Yu-Bang Zheng, Xi-Le Zhao, Junhua Zeng, Chao Li, Qibin Zhao, Heng-Chao\n  Li, Ting-Zhu Huang","title":"SVDinsTN: An Integrated Method for Tensor Network Representation with\n  Efficient Structure Search","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Tensor network (TN) representation is a powerful technique for data analysis\nand machine learning. It practically involves a challenging TN structure search\n(TN-SS) problem, which aims to search for the optimal structure to achieve a\ncompact representation. Existing TN-SS methods mainly adopt a bi-level\noptimization method that leads to excessive computational costs due to repeated\nstructure evaluations. To address this issue, we propose an efficient\nintegrated (single-level) method named SVD-inspired TN decomposition\n(SVDinsTN), eliminating the need for repeated tedious structure evaluation. By\ninserting a diagonal factor for each edge of the fully-connected TN, we\ncalculate TN cores and diagonal factors simultaneously, with factor sparsity\nrevealing the most compact TN structure. Experimental results on real-world\ndata demonstrate that SVDinsTN achieves approximately $10^2\\sim{}10^3$ times\nacceleration in runtime compared to the existing TN-SS methods while\nmaintaining a comparable level of representation ability.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:02:01 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14913","submitter":"Qianhui Wu","authors":"Tingting Ma, Qianhui Wu, Huiqiang Jiang, B\\\"orje F. Karlsson, Tiejun\n  Zhao, Chin-Yew Lin","title":"CoLaDa: A Collaborative Label Denoising Framework for Cross-lingual\n  Named Entity Recognition","comments":"ACL 2023. Our code is available at\n  https://github.com/microsoft/vert-papers/tree/master/papers/CoLaDa","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Cross-lingual named entity recognition (NER) aims to train an NER system that\ngeneralizes well to a target language by leveraging labeled data in a given\nsource language. Previous work alleviates the data scarcity problem by\ntranslating source-language labeled data or performing knowledge distillation\non target-language unlabeled data. However, these methods may suffer from label\nnoise due to the automatic labeling process. In this paper, we propose CoLaDa,\na Collaborative Label Denoising Framework, to address this problem.\nSpecifically, we first explore a model-collaboration-based denoising scheme\nthat enables models trained on different data sources to collaboratively\ndenoise pseudo labels used by each other. We then present an\ninstance-collaboration-based strategy that considers the label consistency of\neach token's neighborhood in the representation space for denoising.\nExperiments on different benchmark datasets show that the proposed CoLaDa\nachieves superior results compared to previous methods, especially when\ngeneralizing to distant languages.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:03:01 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14914","submitter":"Zhitong Xiong","authors":"Zhitong Xiong, Sining Chen, Yi Wang, Lichao Mou, Xiao Xiang Zhu","title":"GAMUS: A Geometry-aware Multi-modal Semantic Segmentation Benchmark for\n  Remote Sensing Data","comments":"13 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Geometric information in the normalized digital surface models (nDSM) is\nhighly correlated with the semantic class of the land cover. Exploiting two\nmodalities (RGB and nDSM (height)) jointly has great potential to improve the\nsegmentation performance. However, it is still an under-explored field in\nremote sensing due to the following challenges. First, the scales of existing\ndatasets are relatively small and the diversity of existing datasets is\nlimited, which restricts the ability of validation. Second, there is a lack of\nunified benchmarks for performance assessment, which leads to difficulties in\ncomparing the effectiveness of different models. Last, sophisticated\nmulti-modal semantic segmentation methods have not been deeply explored for\nremote sensing data. To cope with these challenges, in this paper, we introduce\na new remote-sensing benchmark dataset for multi-modal semantic segmentation\nbased on RGB-Height (RGB-H) data. Towards a fair and comprehensive analysis of\nexisting methods, the proposed benchmark consists of 1) a large-scale dataset\nincluding co-registered RGB and nDSM pairs and pixel-wise semantic labels; 2) a\ncomprehensive evaluation and analysis of existing multi-modal fusion strategies\nfor both convolutional and Transformer-based networks on remote sensing data.\nFurthermore, we propose a novel and effective Transformer-based intermediary\nmulti-modal fusion (TIMF) module to improve the semantic segmentation\nperformance through adaptive token-level multi-modal fusion.The designed\nbenchmark can foster future research on developing new methods for multi-modal\nlearning on remote sensing data. Extensive analyses of those methods are\nconducted and valuable insights are provided through the experimental results.\nCode for the benchmark and baselines can be accessed at\n\\url{https://github.com/EarthNets/RSI-MMSegmentation}.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:03:18 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14915","submitter":"Dennis Trautwein","authors":"Harald Garcke and Dennis Trautwein","title":"Approximation and existence of a viscoelastic phase-field model for\n  tumour growth in two and three dimensions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  In this work, we present a phase-field model for tumour growth, where a\ndiffuse interface separates a tumour from the surrounding host tissue. In our\nmodel, we consider transport processes by an internal, non-solenoidal velocity\nfield. We include viscoelastic effects with the help of a general Oldroyd-B\ntype description with relaxation and possible stress generation by growth. The\nelastic energy density is coupled to the phase-field variable which allows to\nmodel invasive growth towards areas with less mechanical resistance. The main\nanalytical result is the existence of weak solutions in two and three space\ndimensions in the case of additional stress diffusion. The idea behind the\nproof is to use a numerical approximation with a fully-practical, stable and\n(subsequence) converging finite element scheme. The physical properties of the\nmodel are preserved with the help of a regularization technique, uniform\nestimates and a limit passage on the fully-discrete level. Finally, we\nillustrate the practicability of the discrete scheme with the help of numerical\nsimulations in two and three dimensions.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:03:28 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14916","submitter":"Louis Sharrock","authors":"Louis Sharrock, Daniel Dodd, Christopher Nemeth","title":"CoinEM: Tuning-Free Particle-Based Variational Inference for Latent\n  Variable Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG stat.ME","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We introduce two new particle-based algorithms for learning latent variable\nmodels via marginal maximum likelihood estimation, including one which is\nentirely tuning-free. Our methods are based on the perspective of marginal\nmaximum likelihood estimation as an optimization problem: namely, as the\nminimization of a free energy functional. One way to solve this problem is to\nconsider the discretization of a gradient flow associated with the free energy.\nWe study one such approach, which resembles an extension of the popular Stein\nvariational gradient descent algorithm. In particular, we establish a descent\nlemma for this algorithm, which guarantees that the free energy decreases at\neach iteration. This method, and any other obtained as the discretization of\nthe gradient flow, will necessarily depend on a learning rate which must be\ncarefully tuned by the practitioner in order to ensure convergence at a\nsuitable rate. With this in mind, we also propose another algorithm for\noptimizing the free energy which is entirely learning rate free, based on coin\nbetting techniques from convex optimization. We validate the performance of our\nalgorithms across a broad range of numerical experiments, including several\nhigh-dimensional settings. Our results are competitive with existing\nparticle-based methods, without the need for any hyperparameter tuning.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:03:55 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14917","submitter":"Gijs Wijnholds","authors":"Gijs Wijnholds and Michael Moortgat","title":"Structural Ambiguity and its Disambiguation in Language Model Based\n  Parsers: the Case of Dutch Clause Relativization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  This paper addresses structural ambiguity in Dutch relative clauses. By\ninvestigating the task of disambiguation by grounding, we study how the\npresence of a prior sentence can resolve relative clause ambiguities. We apply\nthis method to two parsing architectures in an attempt to demystify the parsing\nand language model components of two present-day neural parsers. Results show\nthat a neurosymbolic parser, based on proof nets, is more open to data bias\ncorrection than an approach based on universal dependencies, although both\nsetups suffer from a comparable initial data bias.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:04:18 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14918","submitter":"Xingxing Zuo","authors":"Xingxing Zuo, Nan Yang, Nathaniel Merrill, Binbin Xu, Stefan\n  Leutenegger","title":"Incremental Dense Reconstruction from Monocular Video with Guided Sparse\n  Feature Volume Fusion","comments":"8 pages, 5 figures, RA-L 2023","journal-ref":null,"doi":"10.1109/LRA.2023.3273509","report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Incrementally recovering 3D dense structures from monocular videos is of\nparamount importance since it enables various robotics and AR applications.\nFeature volumes have recently been shown to enable efficient and accurate\nincremental dense reconstruction without the need to first estimate depth, but\nthey are not able to achieve as high of a resolution as depth-based methods due\nto the large memory consumption of high-resolution feature volumes. This letter\nproposes a real-time feature volume-based dense reconstruction method that\npredicts TSDF (Truncated Signed Distance Function) values from a novel\nsparsified deep feature volume, which is able to achieve higher resolutions\nthan previous feature volume-based methods, and is favorable in large-scale\noutdoor scenarios where the majority of voxels are empty. An uncertainty-aware\nmulti-view stereo (MVS) network is leveraged to infer initial voxel locations\nof the physical surface in a sparse feature volume. Then for refining the\nrecovered 3D geometry, deep features are attentively aggregated from multiview\nimages at potential surface locations, and temporally fused. Besides achieving\nhigher resolutions than before, our method is shown to produce more complete\nreconstructions with finer detail in many cases. Extensive evaluations on both\npublic and self-collected datasets demonstrate a very competitive real-time\nreconstruction result for our method compared to state-of-the-art\nreconstruction methods in both indoor and outdoor settings.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:06:01 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14919","submitter":"Pawan Goyal","authors":"Bishal Santra, Sakya Basak, Abhinandan De, Manish Gupta, Pawan Goyal","title":"Frugal Prompting for Dialog Models","comments":"First two authors have equal contribution","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The use of large language models (LLMs) in natural language processing (NLP)\ntasks is rapidly increasing, leading to changes in how researchers approach\nproblems in the field. To fully utilize these models' abilities, a better\nunderstanding of their behavior for different input protocols is required. With\nLLMs, users can directly interact with the models through a text-based\ninterface to define and solve various tasks. Hence, understanding the\nconversational abilities of these LLMs, which may not have been specifically\ntrained for dialog modeling, is also important. This study examines different\napproaches for building dialog systems using LLMs by considering various\naspects of the prompt. As part of prompt tuning, we experiment with various\nways of providing instructions, exemplars, current query and additional\ncontext. The research also analyzes the representations of dialog history that\nhave the optimal usable-information density. Based on the findings, the paper\nsuggests more compact ways of providing dialog history information while\nensuring good performance and reducing model's inference-API costs. The\nresearch contributes to a better understanding of how LLMs can be effectively\nused for building interactive systems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:06:49 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14920","submitter":"Andreas D\\\"opp","authors":"Andreas D\\\"opp, Igor Andriyash, and Kim Ta Phuoc","title":"All-optical Compton scattering at shallow interaction angles","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.acc-ph physics.plasm-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  All-optical Compton sources combine laser wakefield accelerators and intense\nscattering pulses to generate ultrashort bursts of backscattered radiation. The\nscattering pulse plays the role of a short-period undulator in which\nrelativistic electrons oscillate and emit x-ray radiation. To date, most of the\nworking laser-plasma accelerators operate preferably at energies of a few\nhundreds of MeV and the Compton sources developed so far produce radiation in\nthe range from hundreds of keV to a few MeV. However, for such applications as\nmedical imaging and tomography the relevant energy range is 10-100 keV. In this\narticle, we discuss different scattering geometries for the generation of\nX-rays in this range. Through numerical simulations, we study the influence of\nelectron beam parameters on the backscattered photons. We find that the\nspectral bandwidth remains constant for beams of the same emittance regardless\nof the scattering geometry. A shallow interaction angle of 30 degrees or less\nseems particularly promising for imaging applications given parameters of\nexisting laser-plasma accelerators. Finally, we discuss the influence of the\nradiation properties for potential applications in medical imaging and\nnon-destructive testing.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:07:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14921","submitter":"Juanjuan Xu","authors":"Juanjuan Xu and Huanshui Zhang","title":"Decentralized Control of Linear Systems with Private Input and\n  Measurement Information","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, we study the linear quadratic (LQ) optimal control problem of\nlinear systems with private input and measurement information. The main\nchallenging lies in the unavailability of other regulators' historical input\ninformation. To overcome this difficulty, we introduce a kind of novel\nobservers by using the private input and measurement information and\naccordingly design a kind of new decentralized controllers. In particular, it\nis verified that the corresponding cost function under the proposed\ndecentralized controllers are asymptotically optimal as comparison with the\noptimal cost under optimal state-feedback controller. The presented results in\nthis paper are new to the best of our knowledge, which represent the\nfundamental contribution to classical decentralized control.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:07:38 GMT"},{"version":"v2","created":"Fri, 26 May 2023 03:29:04 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.14922","submitter":"Antonio Di Francesco","authors":"Matteo Colangeli, Antonio Di Francesco, Lamberto Rondoni","title":"Finite reservoirs and irreversibility corrections to Hamiltonian systems\n  statistics","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.stat-mech math-ph math.MP","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We consider several Hamiltonian systems perturbed by external agents, that\npreserve their Hamiltonian structure. We investigate the corrections to the\ncanonical statistics resulting from coupling such systems with possibly large\nbut finite reservoirs, and from the onset of processes breaking the time\nreversal symmetry. We analyze exactly solvable oscillators systems, and perform\nsimulations of relatively more complex ones. This indicates that the standard\nstatistical mechanical formalism needs to be adjusted, in the ever more\ninvestigated nano-scale science and technology. In particular, the hypothesis\nthat heat reservoirs be considered infinite and be described by the classical\nensembles is found to be critical when exponential quantities are considered,\nsince the large size limit may not coincide with the infinite size canonical\nresult. Furthermore, process-dependent emergent irreversibility affects\nensemble averages, effectively frustrating, on a statistical level, the time\nreversal invariance of Hamiltonian dynamics, that is used to obtain numerous\nresults.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:09:13 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14923","submitter":"Alberto Mayorgas","authors":"M. Calixto, A. Mayorgas, N. A. Cordero, E. Romera, O. Casta\\~nos","title":"Faraday rotation and transmittance as markers of topological phase\n  transitions in 2D materials","comments":"15 pages, 11 figures, including supplemental material with 5 pages\n  and 4 figures, 3 animated gifs","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall cond-mat.mtrl-sci","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We analyze the magneto-optical conductivity (and related magnitudes like\ntransmittance and Faraday rotation of the irradiated polarized light) of some\nelemental two-dimensional Dirac materials of group IV (graphene analogues,\nbuckled honeycomb lattices, like silicene, germanene, stannane, etc.), group V\n(phosphorene), and zincblende heterostructures (like HgTe/CdTe quantum wells)\nnear the Dirac and gamma points, under out-of-plane magnetic and electric\nfields, to characterize topological-band insulator phase transitions and their\ncritical points. We provide plots of the Faraday angle and transmittance as a\nfunction of the polarized light frequency, for different external electric and\nmagnetic fields, chemical potential, HgTe layer thickness and temperature, to\ntune the material magneto-optical properties. We have shown that\nabsortance/transmittance acquires extremal values at the critical point, where\nthe Faraday angle changes sign, thus providing fine markers of the topological\nphase transition.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:09:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14924","submitter":"Zhenyu Zhang","authors":"Zhenyu Zhang, Yehui Hou, Minyong Guo, Bin Chen","title":"Light rings and shadows of rotating black holes in the semiclassical\n  gravity with trace anomaly","comments":"14 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc astro-ph.HE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In a recent work by Fernandes [arXiv:2305.10382], an exact stationary and\naxisymmetric solution was discovered in semiclassical gravity with type-A trace\nanomaly, identified as a quantum-corrected version of the Kerr black hole. This\ndiscovery presents exciting research opportunities for observing non-circular\nspacetimes. In this study, we explore the light rings and shadow of this black\nhole solution. Our investigation reveals that there exist prograde and\nretrograde normal light rings, whose radii increase monotonically with the\ncoupling parameter $\\alpha$. We also observe that when $\\alpha$ is negative,\nthe shadow area for the quantum-corrected black hole is smaller than that of\nthe Kerr black hole, whereas when $\\alpha$ is positive, the area is larger.\nFurthermore, the NHEKline for nearly extreme black hole disappears when\n$\\alpha$ is greater than zero, while it appears for negative $\\alpha$, even if\nthe spin is not too high. Such line sinks in the middle part when $|\\alpha|$ is\nrelatively large if $\\alpha$ is less than zero.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:09:38 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14925","submitter":"Dnyaneshwar Tadas","authors":"S. D. Katore, S. P. Hatkar, D. P. Tadas","title":"Accelerating Kaluza-Klein Universe in Modified Theory of Gravitation","comments":"20 Pages, 08 Figures","journal-ref":"Astrophysics 66 98-113 (2023)","doi":"10.1007/s10511-023-09773-3","report-no":null,"categories":"gr-qc","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The purpose of this paper is to study the Kaluza-Klein universe in the\ncontext of the $f(R,T)$ gravity theory using magnetized strange quark matter\n(MSQM). To obtain exact solutions of field equations, we assume two types of\nvolumetric expansion: power law and exponential law volumetric expansions. The\nviolation of energy conditions has been studied. The physical and geometrical\nproperties of the examined model have also been investigated thoroughly.\nKeywords: Kaluza-Klein metric, Magnetized Strange Quark Matter, Power and\nExponential law, $f(R,T)$ gravity.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:09:44 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14926","submitter":"Xingchen Wan","authors":"Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Hanjun Dai, Julian Martin\n  Eisenschlos, Sercan O. Arik, Tomas Pfister","title":"Universal Self-adaptive Prompting","comments":"10 pages, 3 figures, 4 tables (19 pages, 5 figures and 9 tables\n  including references and appendices)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  A hallmark of modern large language models (LLMs) is their impressive general\nzero-shot and few-shot abilities, often elicited through prompt-based and/or\nin-context learning. However, while highly coveted and being the most general,\nzero-shot performances in LLMs are still typically weaker due to the lack of\nguidance and the difficulty of applying existing automatic prompt design\nmethods in general tasks when ground-truth labels are unavailable. In this\nstudy, we address this by presenting Universal Self-adaptive Prompting (USP),\nan automatic prompt design approach specifically tailored for zero-shot\nlearning (while compatible with few-shot). Requiring only a small amount of\nunlabeled data & an inference-only LLM, USP is highly versatile: to achieve\nuniversal prompting, USP categorizes a possible NLP task into one of the three\npossible task types, and then uses a corresponding selector to select the most\nsuitable queries & zero-shot model-generated responses as\npseudo-demonstrations, thereby generalizing ICL to the zero-shot setup in a\nfully automated way. We evaluate zero-shot USP with two PaLM models, and\ndemonstrate performances that are considerably stronger than standard zero-shot\nbaselines and are comparable to or even superior than few-shot baselines across\nmore than 20 natural language understanding (NLU) and natural language\ngeneration (NLG) tasks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:09:48 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14927","submitter":"Cheng Wang","authors":"Rui-Qian Li and Yi-Wei Shen and Bao-De Lin and Jingyi Yu and Xuming He\n  and Cheng Wang","title":"Scalable wavelength-multiplexing photonic reservoir computing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.optics eess.SP","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Photonic reservoir computing (PRC) is a special hardware recurrent neural\nnetwork, which is featured with fast training speed and low training cost. This\nwork shows a wavelength-multiplexing PRC architecture, taking advantage of the\nnumerous longitudinal modes in a Fabry-Perot semiconductor laser. These modes\nconstruct connected physical neurons in parallel, while an optical feedback\nloop provides interactive virtual neurons in series. We experimentally\ndemonstrate a four-channel wavelength-multiplexing PRC, which runs four times\nfaster than the single-channel case. It is proved that the multiplexing PRC\nexhibits superior performance on the task of signal equalization in an optical\nfiber communication link. Particularly, this scheme is highly scalable owing to\nthe rich mode resources in Fabry-Perot lasers.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:10:06 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14928","submitter":"Kellin Pelrine","authors":"Kellin Pelrine, Meilina Reksoprodjo, Caleb Gupta, Joel Christoph,\n  Reihaneh Rabbany","title":"Towards Reliable Misinformation Mitigation: Generalization, Uncertainty,\n  and GPT-4","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Misinformation poses a critical societal challenge, and current approaches\nhave yet to produce an effective solution. We propose focusing on\ngeneralization, soft classification, and leveraging recent large language\nmodels to create more practical tools in contexts where perfect predictions\nremain unattainable. We begin by demonstrating that GPT-4 and other language\nmodels can outperform existing methods in the literature. Next, we explore\ntheir generalization, revealing that GPT-4 and RoBERTa-large exhibit critical\ndifferences in failure modes, which offer potential for significant performance\nimprovements. Finally, we show that these models can be employed in soft\nclassification frameworks to better quantify uncertainty. We find that models\nwith inferior hard classification results can achieve superior soft\nclassification performance. Overall, this research lays groundwork for future\ntools that can drive real-world progress on misinformation.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:10:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14929","submitter":"Bodhisattwa Prasad Majumder","authors":"EunJeong Hwang, Bodhisattwa Prasad Majumder, Niket Tandon","title":"Aligning Language Models to User Opinions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  An important aspect of developing LLMs that interact with humans is to align\nmodels' behavior to their users. It is possible to prompt an LLM into behaving\nas a certain persona, especially a user group or ideological persona the model\ncaptured during its pertaining stage. But, how to best align an LLM with a\nspecific user and not a demographic or ideological group remains an open\nquestion. Mining public opinion surveys (by Pew Research), we find that the\nopinions of a user and their demographics and ideologies are not mutual\npredictors. We use this insight to align LLMs by modeling both user opinions as\nwell as user demographics and ideology, achieving up to 7 points accuracy gains\nin predicting public opinions from survey questions across a broad set of\ntopics. In addition to the typical approach of prompting LLMs with demographics\nand ideology, we discover that utilizing the most relevant past opinions from\nindividual users enables the model to predict user opinions more accurately.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:11:11 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14930","submitter":"Leonard Salewski","authors":"Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz,\n  Zeynep Akata","title":"In-Context Impersonation Reveals Large Language Models' Strengths and\n  Biases","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In everyday conversations, humans can take on different roles and adapt their\nvocabulary to their chosen roles. We explore whether LLMs can take on, that is\nimpersonate, different roles when they generate text in-context. We ask LLMs to\nassume different personas before solving vision and language tasks. We do this\nby prefixing the prompt with a persona that is associated either with a social\nidentity or domain expertise. In a multi-armed bandit task, we find that LLMs\npretending to be children of different ages recover human-like developmental\nstages of exploration. In a language-based reasoning task, we find that LLMs\nimpersonating domain experts perform better than LLMs impersonating non-domain\nexperts. Finally, we test whether LLMs' impersonations are complementary to\nvisual information when describing different categories. We find that\nimpersonation can improve performance: an LLM prompted to be a bird expert\ndescribes birds better than one prompted to be a car expert. However,\nimpersonation can also uncover LLMs' biases: an LLM prompted to be a man\ndescribes cars better than one prompted to be a woman. These findings\ndemonstrate that LLMs are capable of taking on diverse roles and that this\nin-context impersonation can be used to uncover their hidden strengths and\nbiases.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:13:15 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14931","submitter":"The ATLAS Collaboration","authors":"ATLAS Collaboration","title":"Search for Majorana neutrinos in same-sign $WW$ scattering events from\n  $pp$ collisions at $\\sqrt{s}=13$ TeV","comments":"36 pages in total, author list starting page 19, 3 figures, 1 table,\n  submitted to EPJC. All figures including auxiliary figures are available at\n  https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/EXOT-2020-06/","journal-ref":null,"doi":null,"report-no":"CERN-EP-2023-099","categories":"hep-ex","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  A search for Majorana neutrinos in same-sign $WW$ scattering events is\npresented. The analysis uses $\\sqrt{s}= 13$ TeV proton-proton collision data\nwith an integrated luminosity of 140 fb$^{-1}$ recorded during 2015-2018 by the\nATLAS detector at the Large Hadron Collider. The analysis targets final states\nincluding exactly two same-sign muons and at least two hadronic jets well\nseparated in rapidity. The modelling of the main backgrounds, from Standard\nModel same-sign $WW$ scattering and $WZ$ production, is constrained with data\nin dedicated signal-depleted control regions. The distribution of the\ntransverse momentum of the second-hardest muon is used to search for signals\noriginating from a heavy Majorana neutrino with a mass between 50 GeV and 20\nTeV. No significant excess is observed over the background expectation. The\nresults are interpreted in a benchmark scenario of the Phenomenological Type-I\nSeesaw model. In addition, the sensitivity to the Weinberg operator is\ninvestigated. Upper limits at the 95% confidence level are placed on the\nsquared muon-neutrino-heavy-neutrino mass-mixing matrix element $\\vert V_{\\mu\nN} \\vert^{2}$ as a function of the heavy Majorana neutrino's mass $m_N$, and on\nthe effective $\\mu\\mu$ Majorana neutrino mass $|m_{\\mu\\mu}|$.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:13:37 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14932","submitter":"Fani Derveni Dr.","authors":"Fani Derveni, Arefeh Abbasi, Pedro M. Reis","title":"Defect-Defect Interactions in the Buckling of Imperfect Spherical Shells","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.soft","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  We perform finite element simulations to study the impact of defect-defect\ninteractions on the pressure-induced buckling of thin, elastic, spherical\nshells containing two dimpled imperfections. Throughout, we quantify the\ncritical buckling pressure of these shells using their knockdown factor. We\nexamine cases featuring either identical or different geometric defects and\nsystematically explore the parameter space, including the angular separation\nbetween the defects, their widths and amplitudes, and the radius-to-thickness\nratio of the shell. As the angular separation between the defects is increased,\nthe buckling strength initially decreases, then increases before reaching a\nplateau. Our primary finding is that the onset of defect-defect interactions,\nas quantified by a characteristic length scale associated with the onset of the\nplateau, is set by the critical buckling wavelength reported in the classic\nshell-buckling literature. Beyond this threshold, within the plateau regime,\nthe buckling behavior of the shell is dictated by the largest defect.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:14:47 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14933","submitter":"Rui-Chen Zheng","authors":"Rui-Chen Zheng, Yang Ai, Zhen-Hua Ling","title":"Incorporating Ultrasound Tongue Images for Audio-Visual Speech\n  Enhancement through Knowledge Distillation","comments":"To be published in InterSpeech 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.AS cs.SD","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Audio-visual speech enhancement (AV-SE) aims to enhance degraded speech along\nwith extra visual information such as lip videos, and has been shown to be more\neffective than audio-only speech enhancement. This paper proposes further\nincorporating ultrasound tongue images to improve lip-based AV-SE systems'\nperformance. Knowledge distillation is employed at the training stage to\naddress the challenge of acquiring ultrasound tongue images during inference,\nenabling an audio-lip speech enhancement student model to learn from a\npre-trained audio-lip-tongue speech enhancement teacher model. Experimental\nresults demonstrate significant improvements in the quality and intelligibility\nof the speech enhanced by the proposed method compared to the traditional\naudio-lip speech enhancement baselines. Further analysis using phone error\nrates (PER) of automatic speech recognition (ASR) shows that palatal and velar\nconsonants benefit most from the introduction of ultrasound tongue images.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:15:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14934","submitter":"Muhammad Khalifa","authors":"Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Lu\n  Wang","title":"Discriminator-Guided Multi-step Reasoning with Language Models","comments":"19 pages, 7 figures, and 8 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In the context of multi-step reasoning, language models (LMs) probabilities\nare often miscalibrated -- solutions with high probabilities are not always\ncorrect. Therefore, greedy decoding, which is the standard decoding method for\nreasoning tasks, often yields incorrect solutions. In addition, methods such as\nself-consistency and verifiers rely on sampling from the LM distribution and do\nnot tackle the underlying issue. To address this, we introduce Guiding\nMulti-step ReAsoning with a CorrectnEss Discriminator (GRACE), a stepwise\ndecoding approach that nudges the model towards producing correct reasoning\nsteps. GRACE employs a discriminator model, which is trained to differentiate\ncorrect steps from invalid ones, to adjust decoding preferences based on the\ncorrectness of each reasoning step. Importantly, GRACE does not require\nfine-tuning or re-training the LMs. When compared with conventional decoding\nstrategies over four popular math reasoning benchmarks, GRACE exhibits\nsignificant improvements in both final answer accuracy and step correctness,\noutperforming both greedy decoding and self-consistency.\\footnote{Our code can\nbe found at \\url{https://github.com/mukhal/grace.}}\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:16:51 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14935","submitter":"Timon Ziegenbein","authors":"Timon Ziegenbein, Shahbaz Syed, Felix Lange, Martin Potthast and\n  Henning Wachsmuth","title":"Modeling Appropriate Language in Argumentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Online discussion moderators must make ad-hoc decisions about whether the\ncontributions of discussion participants are appropriate or should be removed\nto maintain civility. Existing research on offensive language and the resulting\ntools cover only one aspect among many involved in such decisions. The question\nof what is considered appropriate in a controversial discussion has not yet\nbeen systematically addressed. In this paper, we operationalize appropriate\nlanguage in argumentation for the first time. In particular, we model\nappropriateness through the absence of flaws, grounded in research on argument\nquality assessment, especially in aspects from rhetoric. From these, we derive\na new taxonomy of 14 dimensions that determine inappropriate language in online\ndiscussions. Building on three argument quality corpora, we then create a\ncorpus of 2191 arguments annotated for the 14 dimensions. Empirical analyses\nsupport that the taxonomy covers the concept of appropriateness\ncomprehensively, showing several plausible correlations with argument quality\ndimensions. Moreover, results of baseline approaches to assessing\nappropriateness suggest that all dimensions can be modeled computationally on\nthe corpus.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:17:05 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14936","submitter":"Ivan Habernal","authors":"Cleo Matzken, Steffen Eger, Ivan Habernal","title":"Trade-Offs Between Fairness and Privacy in Language Modeling","comments":"Findings of ACL 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Protecting privacy in contemporary NLP models is gaining in importance. So\ndoes the need to mitigate social biases of such models. But can we have both at\nthe same time? Existing research suggests that privacy preservation comes at\nthe price of worsening biases in classification tasks. In this paper, we\nexplore the extent to which this tradeoff really holds when we incorporate both\nprivacy preservation and de-biasing techniques into training text generation\nmodels. How does improving the model along one dimension affect the other\ndimension as well as the utility of the model? We conduct an extensive set of\nexperiments that include bias detection, privacy attacks, language modeling,\nand performance on downstream tasks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:18:28 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14937","submitter":"Natalie Prange","authors":"Hannah Bast and Matthias Hertel and Natalie Prange","title":"A Fair and In-Depth Evaluation of Existing End-to-End Entity Linking\n  Systems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Existing evaluations of entity linking systems often say little about how the\nsystem is going to perform for a particular application. There are four\nfundamental reasons for this: many benchmarks focus on named entities; it is\nhard to define which other entities to include; there are ambiguities in entity\nrecognition and entity linking; many benchmarks have errors or artifacts that\ninvite overfitting or lead to evaluation results of limited meaningfulness.\n  We provide a more meaningful and fair in-depth evaluation of a variety of\nexisting end-to-end entity linkers. We characterize the strengths and\nweaknesses of these linkers and how well the results from the respective\npublications can be reproduced. Our evaluation is based on several widely used\nbenchmarks, which exhibit the problems mentioned above to various degrees, as\nwell as on two new benchmarks, which address these problems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:20:15 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14938","submitter":"Minje Choi","authors":"Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu and David Jurgens","title":"Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large\n  Language Models with SocKET Benchmark","comments":"24 pages, 7 tables, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large language models (LLMs) have been shown to perform well at a variety of\nsyntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed\nin many forms including conversational agents that interact with humans, we\nlack a grounded benchmark to measure how well LLMs understand \\textit{social}\nlanguage. Here, we introduce a new theory-driven benchmark, SocKET, that\ncontains 58 NLP tasks testing social knowledge which we group into five\ncategories: humor & sarcasm, offensiveness, sentiment & emotion, and\ntrustworthiness. In tests on the benchmark, we demonstrate that current models\nattain only moderate performance but reveal significant potential for task\ntransfer among different types and categories of tasks, which were predicted\nfrom theory. Through zero-shot evaluations, we show that pretrained models\nalready possess some innate but limited capabilities of social language\nunderstanding and training on one category of tasks can improve zero-shot\ntesting on others. Our benchmark provides a systematic way to analyze model\nperformance on an important dimension of language and points to clear room for\nimprovement to build more socially-aware LLMs. The associated resources are\nreleased at https://github.com/minjechoi/SOCKET.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:21:06 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14939","submitter":"Ke Wei","authors":"Jianzhou Luo, Dingchuan Yang, and Ke Wei","title":"Improved Complexity Analysis of the Sinkhorn and Greenkhorn Algorithms\n  for Optimal Transport","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The Sinkhorn algorithm is a widely used method for solving the optimal\ntransport problem, and the Greenkhorn algorithm is one of its variants. While\nthere are modified versions of these two algorithms whose computational\ncomplexities are $O({n^2\\|C\\|_\\infty^2\\log n}/{\\varepsilon^2})$ to achieve an\n$\\varepsilon$-accuracy, the best known complexities for the vanilla versions\nare $O({n^2\\|C\\|_\\infty^3\\log n}/{\\varepsilon^3})$. In this paper we fill this\ngap and show that the complexities of the vanilla Sinkhorn and Greenkhorn\nalgorithms are indeed $O({n^2\\|C\\|_\\infty^2\\log n}/{\\varepsilon^2})$. The\nanalysis relies on the equicontinuity of the dual variables of the entropic\nregularized optimal transport problem, which is of independent interest.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:23:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14940","submitter":"Siddhartha Ganguly","authors":"Siddhartha Ganguly and Souvik Das and Debasish Chatterjee and Ravi\n  Banavar","title":"A discrete-time Pontryagin maximum principle under rate constraints","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC cs.SY eess.SY","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Limited bandwidth and limited saturation in actuators are practical concerns\nin control systems. Mathematically, these limitations manifest as constraints\nbeing imposed on the control actions, their rates of change, and more\ngenerally, the global behavior of their paths. While the problem of actuator\nsaturation has been studied extensively, little attention has been devoted to\nthe problem of actuators having limited bandwidth. While attempts have been\nmade in the direction of incorporating frequency constraints on state-action\ntrajectories before, rate constraints on the control at the design stage have\nnot been studied extensively in the discrete-time regime. This article\ncontributes toward filling this lacuna. In particular, we establish a new\ndiscrete-time Pontryagin maximum principle with rate constraints being imposed\non the control trajectories, and derive first-order necessary conditions for\noptimality. A brief discussion on the existence of optimal control is included,\nand numerical examples are provided to illustrate the results.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:23:48 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14941","submitter":"Michael Seifert","authors":"Michael Seifert (1), Tom\\'a\\v{s} Rauch (1), Miguel A. L. Marques (2)\n  and Silvana Botti (1 and 3) ((1) Institut f\\\"ur Festk\\\"orpertheorie und\n  -optik, Friedrich-Schiller-Universit\\\"at Jena and European Theoretical\n  Spectroscopy Facility, (2) Research Center Future Energy Materials and\n  Systems of the University Alliance Ruhr, Faculty of Mechanical Engineering,\n  Ruhr University Bochum, (3) Research Center Future Energy Materials and\n  Systems, Faculty of Physics and Astronomy, Ruhr Universit\\\"at Bochum)","title":"Structure prediction and characterization of CuI-based ternary $p$-type\n  transparent conductors","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Zincblende copper iodide has attracted significant interest as a potential\nmaterial for transparent electronics, thanks to its exceptional light\ntransmission capabilities in the visible range and remarkable hole\nconductivity. However, remaining challenges hinder the utilization of copper\niodide's unique properties in real-world applications. To address this,\nchalcogen doping has emerged as a viable approach to enhance the hole\nconcentration in copper iodide. In search of further strategies to improve and\ntune the electronic properties of this transparent semiconductor, we\ninvestigate the ternary phase diagram of copper and iodine with sulphur or\nselenium by performing structure prediction calculations using the minima\nhopping method. As a result, we find 11 structures located on or near the\nconvex hull, 9 of which are unreported. Based on our band structure\ncalculations, it appears that sulphur and selenium are promising candidates for\nachieving ternary semiconductors suitable as $p$-type transparent conducting\nmaterials. Additionally, our study reveals the presence of unreported phases\nthat exhibit intriguing topological properties. These findings broaden the\nscope of potential applications for these ternary systems, highlighting the\npossibility of harnessing their unique electronic characteristics in diverse\nelectronic devices and systems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:27:11 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14942","submitter":"Tomi K Baikie","authors":"Tomi K. Baikie, Philip Calado, Krzysztof Galkowski, Zahra\n  Andaji-Garmaroudi, Yi-Chun Chin, Joel Luke, Charlie Henderson, Tom Dunlop,\n  James McGettrick, Ji-Seon Kim, Akshay Rao, Jenny Nelson, Samuel D. Stranks,\n  Piers R. B. Barnes","title":"Generalised Framework for Controlling and Understanding Ion Dynamics\n  with Passivated Lead Halide Perovskites","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Metal halide perovskite solar cells have gained widespread attention due to\ntheir high efficiency and high defect tolerance. The absorbing perovskite layer\nis as a mixed electron-ion conductor that supports high rates of ion and charge\ntransport at room temperature, but the migration of mobile defects can lead to\ndegradation pathways. We combine experimental observations and drift-diffusion\nmodelling to demonstrate a new framework to interpret surface photovoltage\n(SPV) measurements in perovskite systems and mixed electronic ionic conductors\nmore generally. We conclude that the SPV in mixed electronic ionic conductors\ncan be understood in terms of the change in electric potential at the surface\nassociated with changes in the net charge within the semiconductor system. We\nshow that by modifying the interfaces of perovskite bilayers, we may control\ndefect migration behaviour throughout the perovskite bulk. Our new framework\nfor SPV has broad implications for developing strategies to improve the\nstability of perovskite devices by controlling defect accumulation at\ninterfaces. More generally, in mixed electronic conductors our framework\nprovides new insights into the behaviour of mobile defects and their\ninteraction with photoinduced charges, which are foundational to physical\nmechanisms in memristivity, logic, impedance, sensors and energy storage.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:28:07 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14943","submitter":"Louis Sharrock","authors":"Louis Sharrock, Lester Mackey, Christopher Nemeth","title":"Learning Rate Free Bayesian Inference in Constrained Domains","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG stat.ME","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We introduce a suite of new particle-based algorithms for sampling on\nconstrained domains which are entirely learning rate free. Our approach\nleverages coin betting ideas from convex optimisation, and the viewpoint of\nconstrained sampling as a mirrored optimisation problem on the space of\nprobability measures. Based on this viewpoint, we also introduce a unifying\nframework for several existing constrained sampling algorithms, including\nmirrored Langevin dynamics and mirrored Stein variational gradient descent. We\ndemonstrate the performance of our algorithms on a range of numerical examples,\nincluding sampling from targets on the simplex, sampling with fairness\nconstraints, and constrained sampling problems in post-selection inference. Our\nresults indicate that our algorithms achieve competitive performance with\nexisting constrained sampling methods, without the need to tune any\nhyperparameters.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:31:18 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14944","submitter":"Lucas Slot","authors":"Sander Gribling, Sven Polak, Lucas Slot","title":"A note on the computational complexity of the moment-SOS hierarchy for\n  polynomial optimization","comments":"10 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC cs.CC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The moment-sum-of-squares (moment-SOS) hierarchy is one of the most\ncelebrated and widely applied methods for approximating the minimum of an\nn-variate polynomial over a feasible region defined by polynomial\n(in)equalities. A key feature of the hierarchy is that, at a fixed level, it\ncan be formulated as a semidefinite program of size polynomial in the number of\nvariables n. Although this suggests that it may therefore be computed in\npolynomial time, this is not necessarily the case. Indeed, as O'Donnell (2017)\nand later Raghavendra & Weitz (2017) show, there exist examples where the\nsos-representations used in the hierarchy have exponential bit-complexity. We\nstudy the computational complexity of the moment-SOS hierarchy, complementing\nand expanding upon earlier work of Raghavendra & Weitz (2017). In particular,\nwe establish algebraic and geometric conditions under which polynomial-time\ncomputation is guaranteed to be possible.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:32:02 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14945","submitter":"Svend-Age Biehs","authors":"S.-A. Biehs and G.S. Agarwal","title":"Enhancement of synthetic magnetic field induced nonreciprocity via bound\n  states in continuum in dissipatively coupled systems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall physics.optics","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The nonreciprocal propagation of light typically requires use of materials\nlike ferrites or magneto-optical media with a strong magnetic bias or methods\nbased on material nonlinearities which require use of strong electromagnetic\nfields. A simpler possibility to produce nonreciprocity is to use\nspatio-temporal modulations to produce magnetic fields in synthetic dimensions.\nIn this paper we show that dissipatively coupled systems can lead to\nconsiderable enhancement of nonreciprocity in synthetic fields. The enhancement\ncomes about from the existence of nearly nondecaying mode -bound state in\ncontinuum (BIC) in dissipatively coupled systems. The dissipative coupling\noccurs in a wide class of systems coupled via transmission lines, waveguides,\nor nano fibers. The systems could be optical resonators or microscopic qubits.\nRemarkably we find that for specific choice of the modulation amplitudes, the\ntransmission say in forward direction is completely extinguished whereas in the\nbackward direction it becomes maximum. The synthetic fields produce\ntransmission resonances which show significant line narrowing which owe their\norigin to existence of BIC's in dissipative systems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:32:25 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14946","submitter":"Seyed Mohammadhossein Tabatabaee","authors":"Seyed Mohammadhossein Tabatabaee, Anne Bouillard, Jean-Yves Le Boudec","title":"Quasi-Deterministic Burstiness Bound for Aggregate of Independent,\n  Periodic Flows","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.NI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Time-sensitive networks require timely and accurate monitoring of the status\nof the network. To achieve this, many devices send packets periodically, which\nare then aggregated and forwarded to the controller. Bounding the aggregate\nburstiness of the traffic is then crucial for effective resource management. In\nthis paper, we are interested in bounding this aggregate burstiness for\nindependent and periodic flows. A deterministic bound is tight only when flows\nare perfectly synchronized, which is highly unlikely in practice and would be\noverly pessimistic. We compute the probability that the aggregate burstiness\nexceeds some value. When all flows have the same period and packet size, we\nobtain a closed-form bound using the Dvoretzky-Kiefer-Wolfowitz inequality. In\nthe heterogeneous case, we group flows and combine the bounds obtained for each\ngroup using the convolution bound. Our bounds are numerically close to\nsimulations and thus fairly tight. The resulting aggregate burstiness estimated\nfor a non-zero violation probability is considerably smaller than the\ndeterministic one: it grows in $\\sqrt{n\\log{n}}$, instead of $n$, where $n$ is\nthe number of flows.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:34:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14947","submitter":"Qinyuan Ye","authors":"Qinyuan Ye, Harvey Yiyun Fu, Xiang Ren, Robin Jia","title":"How Predictable Are Large Language Model Capabilities? A Case Study on\n  BIG-bench","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  We investigate the predictability of large language model (LLM) capabilities:\ngiven records of past experiments using different model families, numbers of\nparameters, tasks, and numbers of in-context examples, can we accurately\npredict LLM performance on new experiment configurations? Answering this\nquestion has practical implications for LLM users (e.g., deciding which models\nto try), developers (e.g., prioritizing evaluation on representative tasks),\nand the research community (e.g., identifying hard-to-predict capabilities that\nwarrant further investigation).\n  We study the performance prediction problem on experiment records from\nBIG-bench. On a random train-test split, an MLP-based predictor achieves RMSE\nbelow 5%, demonstrating the presence of learnable patterns within the\nexperiment records. Further, we formulate the problem of searching for\n\"small-bench,\" an informative subset of BIG-bench tasks from which the\nperformance of the full set can be maximally recovered, and find a subset as\ninformative for evaluating new model families as BIG-bench Hard, while being 3x\nsmaller.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:35:34 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14948","submitter":"Christopher Johann Clarke","authors":"Christopher Johann Clarke","title":"Music Representing Corpus Virtual: An Open Sourced Library for\n  Explorative Music Generation, Sound Design, and Instrument Creation with\n  Artificial Intelligence and Machine Learning","comments":"16 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.AS cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Music Representing Corpus Virtual (MRCV) is an open source software suite\ndesigned to explore the capabilities of Artificial Intelligence (AI) and\nMachine Learning (ML) in Music Generation, Sound Design, and Virtual Instrument\nCreation (MGSDIC). The software is accessible to users of varying levels of\nexperience, with an emphasis on providing an explorative approach to MGSDIC.\nThe main aim of MRCV is to facilitate creativity, allowing users to customize\ninput datasets for training the neural networks, and offering a range of\noptions for each neural network (thoroughly documented in the Github Wiki). The\nsoftware suite is designed to be accessible to musicians, audio professionals,\nsound designers, and composers, regardless of their prior experience in AI or\nML. The documentation is prepared in such a way as to abstract technical\ndetails, thereby making it easy to understand. The software is open source,\nmeaning users can contribute to its development, and the community can\ncollectively benefit from the insights and experience of other users.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:36:04 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14949","submitter":"Qi Gou","authors":"Qi Gou, Zehua Xia, Wenzhe Du","title":"Cross-lingual Data Augmentation for Document-grounded Dialog Systems in\n  Low Resource Languages","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper proposes a framework to address the issue of data scarcity in\nDocument-Grounded Dialogue Systems(DGDS). Our model leverages high-resource\nlanguages to enhance the capability of dialogue generation in low-resource\nlanguages. Specifically, We present a novel pipeline CLEM (Cross-Lingual\nEnhanced Model) including adversarial training retrieval (Retriever and\nRe-ranker), and Fid (fusion-in-decoder) generator. To further leverage\nhigh-resource language, we also propose an innovative architecture to conduct\nalignment across different languages with translated training. Extensive\nexperiment results demonstrate the effectiveness of our model and we achieved\n4th place in the DialDoc 2023 Competition. Therefore, CLEM can serve as a\nsolution to resource scarcity in DGDS and provide useful guidance for\nmulti-lingual alignment tasks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:40:52 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14950","submitter":"Muhao Chen","authors":"Jiongxiao Wang, Zichen Liu, Keun Hee Park, Muhao Chen, Chaowei Xiao","title":"Adversarial Demonstration Attacks on Large Language Models","comments":"Work in Progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CR cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  With the emergence of more powerful large language models (LLMs), such as\nChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence\nin leveraging these models for specific tasks by utilizing data-label pairs as\nprecondition prompts. While incorporating demonstrations can greatly enhance\nthe performance of LLMs across various tasks, it may introduce a new security\nconcern: attackers can manipulate only the demonstrations without changing the\ninput to perform an attack. In this paper, we investigate the security concern\nof ICL from an adversarial perspective, focusing on the impact of\ndemonstrations. We propose an ICL attack based on TextAttack, which aims to\nonly manipulate the demonstration without changing the input to mislead the\nmodels. Our results demonstrate that as the number of demonstrations increases,\nthe robustness of in-context learning would decreases. Furthermore, we also\nobserve that adversarially attacked demonstrations exhibit transferability to\ndiverse input examples. These findings emphasize the critical security risks\nassociated with ICL and underscore the necessity for extensive research on the\nrobustness of ICL, particularly given its increasing significance in the\nadvancement of LLMs.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:40:56 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14951","submitter":"Jue Liu","authors":"Jue Liu, Feipeng Da","title":"Dual-Side Feature Fusion 3D Pose Transfer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  3D pose transfer solves the problem of additional input and correspondence of\ntraditional deformation transfer, only the source and target meshes need to be\ninput, and the pose of the source mesh can be transferred to the target mesh.\nSome lightweight methods proposed in recent years consume less memory but cause\nspikes and distortions for some unseen poses, while others are costly in\ntraining due to the inclusion of large matrix multiplication and adversarial\nnetworks. In addition, the meshes with different numbers of vertices also\nincrease the difficulty of pose transfer. In this work, we propose a Dual-Side\nFeature Fusion Pose Transfer Network to improve the pose transfer accuracy of\nthe lightweight method. Our method takes the pose features as one of the side\ninputs to the decoding network and fuses them into the target mesh layer by\nlayer at multiple scales. Our proposed Feature Fusion Adaptive Instance\nNormalization has the characteristic of having two side input channels that\nfuse pose features and identity features as denormalization parameters, thus\nenhancing the pose transfer capability of the network. Extensive experimental\nresults show that our proposed method has stronger pose transfer capability\nthan state-of-the-art methods while maintaining a lightweight network\nstructure, and can converge faster.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:42:08 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14952","submitter":"Itamar Zimerman","authors":"Shahar Lutati, Itamar Zimerman, Lior Wolf","title":"Focus Your Attention (with Adaptive IIR Filters)","comments":"11 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG eess.SP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We present a new layer in which dynamic (i.e.,input-dependent) Infinite\nImpulse Response (IIR) filters of order two are used to process the input\nsequence prior to applying conventional attention. The input is split into\nchunks, and the coefficients of these filters are determined based on previous\nchunks to maintain causality. Despite their relatively low order, the causal\nadaptive filters are shown to focus attention on the relevant sequence\nelements. The layer performs on-par with state of the art networks, with a\nfraction of the parameters and with time complexity that is sub-quadratic with\ninput size. The obtained layer is favorable to layers such as Heyna, GPT2, and\nMega, both with respect to the number of parameters and the obtained level of\nperformance on multiple long-range sequence problems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:42:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14953","submitter":"Waleed El Hanafy","authors":"Waleed El Hanafy and Adel Awad","title":"Implications of the Conformal Sound Speed Constraint on the Radius of\n  PSR J0952-0607 within Rastall Gravity","comments":"PdfLaTeX, 12 pages, 3 figures, 1 table, 1 appendix. To appear in ApJ","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.HE gr-qc hep-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  It has been shown that the nonminimal coupling between geometry and matter\ncan provide models for massive compact Stars \\citep{ElHanafy:2022kjl}, which\nare consistent with the conformal bound on the sound speed, $0\\leq c_s^2 \\leq\nc^2/3$, where the core density approaches a few times the nuclear saturation\ndensity. We impose the conformal sound speed upper bound constraint on\nRastall's field equations of gravity, with Krori-Barua potentials in presence\nof an anisotropic fluid as a matter source, to estimate the radius of the most\nmassive pulsar PSR J0952\\textendash{0607} ever observed. For its measured mass\n$M = 2.35\\pm 0.17\\, M_\\odot$, we obtain a radius $R=14.087 \\pm 1.0186$~km as\ninferred by the model. We investigate possible connection between Rastall\ngarvity and MIT bag model with an EoS, $p_r(\\rho) \\approx c_s^2\\left(\\rho -\n\\rho_\\text{s}\\right)$, in the radial direction, with $c_s=c/\\sqrt{3}$ and a\nsurface density $\\rho_\\text{s}$ slightly above the nuclear saturation density\n$\\rho_\\text{nuc}=2.7\\times 10^{14}$~g/cm$^3$. The corresponding\nmass\\textendash{radius} diagram is in agreement with our estimated value of the\nradius and with astrophysical observations of other pulsars at 68\\% C.L.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:42:35 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14954","submitter":"Valeria Giunta","authors":"Valeria Giunta, Thomas Hillen, Mark A. Lewis, Jonathan R. Potts","title":"Weakly nonlinear analysis of a two-species non-local advection-diffusion\n  system","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Nonlocal interactions are ubiquitous in nature and play a central role in\nmany biological systems. In this paper, we perform a bifurcation analysis of a\nwidely-applicable advection-diffusion model with nonlocal advection terms\ndescribing the species movements generated by inter-species interactions. We\nuse linear analysis to assess the stability of the constant steady state, then\nweakly nonlinear analysis to recover the shape and stability of non-homogeneous\nsolutions. Since the system arises from a conservation law, the resulting\namplitude equations consist of a Ginzburg-Landau equation coupled with an\nequation for the zero mode. In particular, this means that supercritical\nbranches from the Ginzburg-Landau equation need not be stable. Indeed, we find\nthat, depending on the parameters, bifurcations can be subcritical (always\nunstable), stable supercritical, or unstable supercritical. We show numerically\nthat, when small amplitude patterns are unstable, the system exhibits large\namplitude patterns and hysteresis, even in supercritical regimes. Finally, we\nconstruct bifurcation diagrams by combining our analysis with a previous study\nof the minimisers of the associated energy functional. Through this approach we\nreveal parameter regions in which stable small amplitude patterns coexist with\nstrongly modulated solutions.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:43:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14955","submitter":"Jiayi Zhu","authors":"Jiayi Zhu, Xuebin Qin, Abdulmotaleb Elsaddik","title":"DC-Net: Divide-and-Conquer for Salient Object Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, we introduce Divide-and-Conquer into the salient object\ndetection (SOD) task to enable the model to learn prior knowledge that is for\npredicting the saliency map. We design a novel network, Divide-and-Conquer\nNetwork (DC-Net) which uses two encoders to solve different subtasks that are\nconducive to predicting the final saliency map, here is to predict the edge\nmaps with width 4 and location maps of salient objects and then aggregate the\nfeature maps with different semantic information into the decoder to predict\nthe final saliency map. The decoder of DC-Net consists of our newly designed\ntwo-level Residual nested-ASPP (ResASPP$^{2}$) modules, which have the ability\nto capture a large number of different scale features with a small number of\nconvolution operations and have the advantages of maintaining high resolution\nall the time and being able to obtain a large and compact effective receptive\nfield (ERF). Based on the advantage of Divide-and-Conquer's parallel computing,\nwe use Parallel Acceleration to speed up DC-Net, allowing it to achieve\ncompetitive performance on six LR-SOD and five HR-SOD datasets under high\nefficiency (60 FPS and 55 FPS). Codes and results are available:\nhttps://github.com/PiggyJerry/DC-Net.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:49:25 GMT"},{"version":"v2","created":"Mon, 5 Jun 2023 04:15:26 GMT"}],"update_date":"2023-06-06"}
{"id":"2305.14956","submitter":"Debanjan Mondal","authors":"Anshita Gupta, Debanjan Mondal, Akshay Krishna Sheshadri, Wenlong\n  Zhao, Xiang Lorraine Li, Sarah Wiegreffe, Niket Tandon","title":"Editing Commonsense Knowledge in GPT","comments":"Code and data is available at https://github.com/anshitag/memit_csk","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Memory editing methods for updating encyclopedic knowledge in transformers\nhave received increasing attention for their efficacy, specificity, and\ngeneralization advantages. However, it remains unclear if such methods can be\nadapted for the more nuanced domain of commonsense knowledge. We propose\n$MEMIT_{CSK}$, an adaptation of MEMIT to edit commonsense mistakes in GPT-2\nLarge and XL. We extend editing to various token locations and employ a robust\nlayer selection strategy. Models edited by $MEMIT_{CSK}$ outperforms the\nfine-tuning baselines by 10.97% and 10.73% F1 scores on subsets of PEP3k and\n20Q. We further propose a novel evaluation dataset, MEMIT-CSK-PROBE, that\ncontains unaffected neighborhood, affected neighborhood, affected paraphrase,\nand affected reasoning challenges. $MEMIT_{CSK}$ demonstrates favorable\nsemantic generalization, outperforming fine-tuning baselines by 13.72% and\n5.57% overall scores on MEMIT-CSK-PROBE. These results suggest a compelling\nfuture direction of incorporating context-specific user feedback concerning\ncommonsense in GPT by direct model editing, rectifying and customizing model\nbehaviors via human-in-the-loop systems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:50:54 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14957","submitter":"Alessandro Ursi","authors":"Alessandro Ursi, Nicol\\`o Parmiggiani, Mauro Messerotti, Alberto\n  Pellizzoni, Carlotta Pittori, Francesco Longo, Francesco Verrecchia, Andrea\n  Argan, Andrea Bulgarelli, Marco Tavani, Patrizio Tempesta, Fabio D'Amico","title":"The First AGILE Solar Flare Catalog","comments":"22 pages, 10 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.SR","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  We report the Astrorivelatore Gamma ad Immagini LEggero (AGILE) observations\nof solar flares, detected by the on board anticoincidence system in the 80-200\nkeV energy range, from 2007 May 1st to 2022 August 31st. In more than 15 yr,\nAGILE detected 5003 X-ray, minute-lasting transients, compatible with a solar\norigin. A cross-correlation of these transients with the Geostationary\nOperational Environmental Satellites (GOES) official solar flare database\nallowed to associate an intensity class (i.e., B, C, M, or X) to 3572 of them,\nfor which we investigated the main temporal and intensity parameters. The AGILE\ndata clearly revealed the solar activity covering the last stages of the 23rd\ncycle, the whole 24th cycle, and the beginning of the current 25th cycle. In\norder to compare our results with other space missions operating in the\nhigh-energy range, we also analyzed the public lists of solar flares reported\nby RHESSI and Fermi Gamma-ray Burst Monitor. This catalog reports 1424 events\nnot contained in the GOES official dataset, which, after statistical\ncomparisons, are compatible with low-intensity, short-duration solar flares.\n  Besides providing a further dataset of solar flares detected in the hard\nX-ray range, this study allowed to point out two main features: a longer\npersistence of the decay phase in the high-energy regime, with respect to the\nsoft X-rays, and a tendency of the flare maximum to be reached earlier in the\nsoft X-rays with respect to the hard X-rays. Both these aspects support a\ntwo-phase acceleration mechanism of electrons in the solar atmosphere.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:50:58 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14958","submitter":"Fran\\c{c}ois-Xavier Coudert","authors":"Fran\\c{c}ois-Xavier Coudert","title":"Failure to Reproduce the Results of \"A new transferable interatomic\n  potential for molecular dynamics simulations of borosilicate glasses''","comments":null,"journal-ref":"J. Non-Cryst. Solids, 2023, 615, 122423","doi":"10.1016/j.jnoncrysol.2023.122423","report-no":null,"categories":"physics.comp-ph cond-mat.mtrl-sci","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We reproduced the simulations described in Wang et al [J. Non-Cryst. Sol.,\n498 (2018) 294-304] and found we could not obtain the results reported. The\nroot cause was identified to be incorrect atom masses in the original\nsimulation files. As a consequence, the potential does not reproduce the\nexperimental glass density -- and presumably, other structural properties --\nand should be considered with great caution.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:50:59 GMT"}],"update_date":"2023-06-09"}
{"id":"2305.14959","submitter":"Omid Esrafilian","authors":"Omid Esrafilian, Rajeev Gangula, and David Gesbert","title":"UAV Trajectory Optimization and Tracking for User Localization in\n  Wireless Networks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT math.IT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, we investigate the problem of UAV-aided user localization in\nwireless networks. Unlike the existing works, we do not assume perfect\nknowledge of the UAV location, hence we not only need to localize the users but\nalso to track the UAV location. To do so, we utilize the time-of-arrival along\nwith received signal strength radio measurements collected from users using a\nUAV. A simultaneous localization and mapping (SLAM) framework building on the\nExpectation-Maximization-based least-squares method is proposed to classify\nmeasurements into line-of-sight or non-line-of-sight categories and learn the\nradio channel, and at the same, localize the users and track the UAV. This\nframework also allows us to exploit other types of measurements such as the\nrough estimate of the UAV location available from GPS, and the UAV velocity\nmeasured by an inertial measurement unit (IMU) on-board, to achieve better\nlocalization accuracy. Moreover, the trajectory of the UAV is optimized which\nbrings considerable improvement to the localization performance. The\nsimulations show the out-performance of the developed algorithm when compared\nto other approaches.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:52:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14960","submitter":"Jingwen Ma","authors":"Jingwen Ma, Ding Jia, Li Zhang, Yi-jun Guan, Yong Ge, Hong-xiang Sun,\n  Shou-qi Yuan, Hongsheng Chen, Yihao Yang, Xiang Zhang","title":"Observation of vortex-string chiral modes in metamaterials","comments":"3 Figures","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.app-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  As a hypothetical topological defect in the geometry of spacetime, vortex\nstrings play a crucial role in shaping the clusters of galaxies that exist\ntoday, and their distinct features can provide observable clues about the early\nuniverse's evolution. A key feature of vortex strings is that they can interact\nwith Weyl fermionic modes and support topological chiral-anomaly states with\nmassless dispersions at the core of strings. To date, despite many attempts to\ndetect vortex strings in astrophysics or to emulate them in artificially\ncreated systems, observation of these topological vortex-string chiral modes\nremains experimentally elusive. Here we report the experimental observation of\nsuch vortex-string chiral modes using a metamaterial system. This is\nimplemented by inhomogeneous perturbation of a Yang-monopole phononic\nmetamaterial. The measured linear dispersion and modal profiles confirm the\nexistence of topological modes bound to and propagating along the vortex string\nwith the chiral anomaly. Our work not only provides a platform for studying\ndiverse cosmic topological defects in astrophysics but also offers intriguing\ndevice applications as topological fibres in signal processing and\ncommunication techniques.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:55:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14961","submitter":"Simon Wiegrebe","authors":"Simon Wiegrebe, Philipp Kopper, Raphael Sonabend, and Andreas Bender","title":"Deep Learning for Survival Analysis: A Review","comments":"24 pages, 6 figures, 2 tables, 1 interactive table","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The influx of deep learning (DL) techniques into the field of survival\nanalysis in recent years, coupled with the increasing availability of\nhigh-dimensional omics data and unstructured data like images or text, has led\nto substantial methodological progress; for instance, learning from such\nhigh-dimensional or unstructured data. Numerous modern DL-based survival\nmethods have been developed since the mid-2010s; however, they often address\nonly a small subset of scenarios in the time-to-event data setting - e.g.,\nsingle-risk right-censored survival tasks - and neglect to incorporate more\ncomplex (and common) settings. Partially, this is due to a lack of exchange\nbetween experts in the respective fields.\n  In this work, we provide a comprehensive systematic review of DL-based\nmethods for time-to-event analysis, characterizing them according to both\nsurvival- and DL-related attributes. In doing so, we hope to provide a helpful\noverview to practitioners who are interested in DL techniques applicable to\ntheir specific use case as well as to enable researchers from both fields to\nidentify directions for future investigation. We provide a detailed\ncharacterization of the methods included in this review as an open-source,\ninteractive table: https://survival-org.github.io/DL4Survival. As this research\narea is advancing rapidly, we encourage the research community to contribute to\nkeeping the information up to date.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:56:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14962","submitter":"Maksym Lysak","authors":"Christoph Auer, Ahmed Nassar, Maksym Lysak, Michele Dolfi, Nikolaos\n  Livathinos, Peter Staar","title":"ICDAR 2023 Competition on Robust Layout Segmentation in Corporate\n  Documents","comments":"ICDAR 2023, 10 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Transforming documents into machine-processable representations is a\nchallenging task due to their complex structures and variability in formats.\nRecovering the layout structure and content from PDF files or scanned material\nhas remained a key problem for decades. ICDAR has a long tradition in hosting\ncompetitions to benchmark the state-of-the-art and encourage the development of\nnovel solutions to document layout understanding. In this report, we present\nthe results of our \\textit{ICDAR 2023 Competition on Robust Layout Segmentation\nin Corporate Documents}, which posed the challenge to accurately segment the\npage layout in a broad range of document styles and domains, including\ncorporate reports, technical literature and patents. To raise the bar over\nprevious competitions, we engineered a hard competition dataset and proposed\nthe recent DocLayNet dataset for training. We recorded 45 team registrations\nand received official submissions from 21 teams. In the presented solutions, we\nrecognize interesting combinations of recent computer vision models, data\naugmentation strategies and ensemble methods to achieve remarkable accuracy in\nthe task we posed. A clear trend towards adoption of vision-transformer based\nmethods is evident. The results demonstrate substantial progress towards\nachieving robust and highly generalizing methods for document layout\nunderstanding.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:56:47 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14963","submitter":"Yau-Shian Wang","authors":"Yau-Shian Wang and Ta-Chung Chi and Ruohong Zhang and Yiming Yang","title":"PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text\n  Classification","comments":"accepted by ACL 2023","journal-ref":"ACL 2023","doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We present PESCO, a novel contrastive learning framework that substantially\nimproves the performance of zero-shot text classification. We formulate text\nclassification as a neural text matching problem where each document is treated\nas a query, and the system learns the mapping from each query to the relevant\nclass labels by (1) adding prompts to enhance label matching, and (2) using\nretrieved labels to enrich the training set in a self-training loop of\ncontrastive learning. PESCO achieves state-of-the-art performance on four\nbenchmark text classification datasets. On DBpedia, we achieve 98.5\\% accuracy\nwithout any labeled data, which is close to the fully-supervised result.\nExtensive experiments and analyses show all the components of PESCO are\nnecessary for improving the performance of zero-shot text classification.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:57:06 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14964","submitter":"Einat Minkov","authors":"Sagi Penzel, Nir Lotan, Alon Zoizner, Einat Minkov","title":"Detecting and Characterizing Political Incivility on Social Media","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Researchers of political communication study the impact and perceptions of\npolitical incivility on social media. Yet, so far, relatively few works\nattempted to automatically detect and characterize political incivility. In our\nwork, we study political incivility in Twitter, presenting several research\ncontributions. First, we present state-of-the-art incivility detection results\nusing a large dataset, which we collected and labeled via crowd sourcing.\nImportantly, we distinguish between uncivil political speech that is impolite\nand intolerant anti-democratic discourse. Applying political incivility\ndetection at large-scale, we derive insights regarding the prevalence of this\nphenomenon across users, and explore the network characteristics of users who\nare susceptible to disseminating uncivil political content online. Finally, we\npropose an approach for modeling social context information about the tweet\nauthor alongside the tweet content, showing that this leads to significantly\nimproved performance on the task of political incivility detection. This result\nholds promise for related tasks, such as hate speech and stance detection.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:57:12 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14965","submitter":"Abhinav Rao","authors":"Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, Monojit\n  Choudhury","title":"Tricking LLMs into Disobedience: Understanding, Analyzing, and\n  Preventing Jailbreaks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Recent explorations with commercial Large Language Models (LLMs) have shown\nthat non-expert users can jailbreak LLMs by simply manipulating the prompts;\nresulting in degenerate output behavior, privacy and security breaches,\noffensive outputs, and violations of content regulator policies. Limited formal\nstudies have been carried out to formalize and analyze these attacks and their\nmitigations. We bridge this gap by proposing a formalism and a taxonomy of\nknown (and possible) jailbreaks. We perform a survey of existing jailbreak\nmethods and their effectiveness on open-source and commercial LLMs (such as GPT\n3.5, OPT, BLOOM, and FLAN-T5-xxl). We further propose a limited set of prompt\nguards and discuss their effectiveness against known attack types.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:57:37 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14966","submitter":"Sanoopkumar P. S.","authors":"Sanoopkumar P. S., Stephen McWade, Arman Farhang","title":"Truncated Turbo Equalizer with SIC for OTFS","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Orthogonal time frequency space (OTFS) is a promising candidate waveform for\nthe next generation wireless communication systems. OTFS places data in the\ndelay-Doppler (DD) domain, which simplifies channel estimation in highmobility\nscenarios. However, due to the 2-D convolution effect of the time-varying\nchannel in the DD domain, equalization is still a challenge for OTFS. Existing\nequalizers for OTFS are either highly complex or they do not consider\nintercarrier interference present in high-mobility scenarios. Hence, in this\npaper, we propose a novel two-stage detection technique for coded OTFS systems.\nOur proposed detector brings orders of magnitude computational complexity\nreduction compared to existing methods. At the first stage, it truncates the\nchannel by considering only the significant coefficients along the Doppler\ndimension and performs turbo equalization. To reduce the computational load of\nthe turbo equalizer, our proposed method deploys the modified LSQR (mLSQR)\nalgorithm. At the second stage, with only two successive interference\ncancellation (SIC) iterations, our proposed detector removes the residual\ninterference caused by channel truncation. To evaluate the performance of our\nproposed truncated turbo equalizer with SIC (TTE-SIC), we set the minimum mean\nsquared error (MMSE) equalizer without channel truncation as a benchmark. Our\nsimulation results show that the proposed TTE-SIC technique achieves about the\nsame bit error rate (BER) performance as the benchmark.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:59:08 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14967","submitter":"M.A. Moreno-Fr\\'ias","authors":"M.A. Moreno-Fr\\'ias and J.C. Rosales","title":"The covariety of perfect numerical semigroups with fixed Frobenius\n  number","comments":"arXiv admin note: text overlap with arXiv:2302.09121,\n  arXiv:2303.12470, arXiv:2305.02070, arXiv:2305.13881","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Let $S$ be a numerical semigroup. We will say that $h\\in {\\mathbb{N}}\n\\backslash S$ is an {\\it isolated gap }of $S$ if $\\{h-1,h+1\\}\\subseteq S.$ A\nnumerical semigroup without isolated gaps is called perfect numerical\nsemigroup. Denote by ${\\mathrm m}(S)$ the multiplicity of a numerical semigroup\n$S$. A covariety is a nonempty family ${\\mathscr{C}}$ of numerical semigroups\nthat fulfills the following conditions: there is the minimum of\n${\\mathscr{C}},$ the intersection of two elements of ${\\mathscr{C}}$ is again\nan element of ${\\mathscr{C}}$ and $S\\backslash \\{{\\mathrm m}(S)\\}\\in\n{\\mathscr{C}}$ for all $S\\in {\\mathscr{C}}$ such that $S\\neq\n\\min({\\mathscr{C}}).$ In this work we prove that the set\n${\\mathscr{P}}(F)=\\{S\\mid S \\mbox{ is a perfect numerical}\\ \\mbox{semigroup\nwith Frobenius number }F\\}$ is a covariety. Also, we describe three algorithms\nwhich compute: the set ${\\mathscr{P}}(F),$ the maximal elements of\n${\\mathscr{P}}(F)$ and the elements of ${\\mathscr{P}}(F)$ with a given genus. A\n${\\mathrm{Parf}}$-semigroup (respectively, ${\\mathrm{Psat}}$-semigroup) is a\nperfect numerical semigroup that in addition is an Arf numerical semigroup\n(respectively, saturated numerical semigroup). We will prove that the sets:\n${\\mathrm{Parf}}(F)=\\{S\\mid S \\mbox{ is a ${\\mathrm{Parf}}$-numerical semigroup\nwith Frobenius number} F\\}$ and ${\\mathrm{Psat}}(F)=\\{S\\mid S \\mbox{ is a\n${\\mathrm{Psat}}$-numerical semigroup with Frobenius number } F\\}$ are\ncovarieties. As a consequence we present some algorithms to compute\n${\\mathrm{Parf}}(F)$ and ${\\mathrm{Psat}}(F)$.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:00:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14968","submitter":"Kris Nikov","authors":"Simon Wegener, Kris K. Nikov, Jose Nunez-Yanez, Kerstin Eder","title":"EnergyAnalyzer: Using Static WCET Analysis Techniques to Estimate the\n  Energy Consumption of Embedded Applications","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SE","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  This paper presents EnergyAnalyzer, a code-level static analysis tool for\nestimating the energy consumption of embedded software based on statically\npredictable hardware events. The tool utilises techniques usually used for\nworst-case execution time (WCET) analysis together with bespoke energy models\ndeveloped for two predictable architectures - the ARM Cortex-M0 and the Gaisler\nLEON3 - to perform energy usage analysis. EnergyAnalyzer has been applied in\nvarious use cases, such as selecting candidates for an optimised convolutional\nneural network, analysing the energy consumption of a camera pill prototype,\nand analysing the energy consumption of satellite communications software. The\ntool was developed as part of a larger project called TeamPlay, which aimed to\nprovide a toolchain for developing embedded applications where energy\nproperties are first-class citizens, allowing the developer to reflect directly\non these properties at the source code level. The analysis capabilities of\nEnergyAnalyzer are validated across a large number of benchmarks for the two\ntarget architectures and the results show that the statically estimated energy\nconsumption has, with a few exceptions, less than 1% difference compared to the\nunderlying empirical energy models which have been validated on real hardware.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:01:32 GMT"},{"version":"v2","created":"Thu, 25 May 2023 10:30:19 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.14969","submitter":"Yichen Yan","authors":"Yichen Yan, Xingjian He, Wenxuan Wan, Jing Liu","title":"MMNet: Multi-Mask Network for Referring Image Segmentation","comments":"10 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Referring image segmentation aims to segment an object referred to by natural\nlanguage expression from an image. However, this task is challenging due to the\ndistinct data properties between text and image, and the randomness introduced\nby diverse objects and unrestricted language expression. Most of previous work\nfocus on improving cross-modal feature fusion while not fully addressing the\ninherent uncertainty caused by diverse objects and unrestricted language. To\ntackle these problems, we propose an end-to-end Multi-Mask Network for\nreferring image segmentation(MMNet). we first combine picture and language and\nthen employ an attention mechanism to generate multiple queries that represent\ndifferent aspects of the language expression. We then utilize these queries to\nproduce a series of corresponding segmentation masks, assigning a score to each\nmask that reflects its importance. The final result is obtained through the\nweighted sum of all masks, which greatly reduces the randomness of the language\nexpression. Our proposed framework demonstrates superior performance compared\nto state-of-the-art approaches on the two most commonly used datasets, RefCOCO,\nRefCOCO+ and G-Ref, without the need for any post-processing. This further\nvalidates the efficacy of our proposed framework.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:02:27 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14970","submitter":"Tianqing Fang","authors":"Tianqing Fang, Zhaowei Wang, Wenxuan Zhou, Hongming Zhang, Yangqiu\n  Song, Muhao Chen","title":"Getting Sick After Seeing a Doctor? Diagnosing and Mitigating Knowledge\n  Conflicts in Event Temporal Reasoning","comments":"13 pages, 1 figure","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Event temporal reasoning aims at identifying the temporal relations between\ntwo or more events. However, knowledge conflicts arise when there is a mismatch\nbetween the actual temporal relations of events in the context and the prior\nknowledge or biases learned by the model. We first systematically define\ndistinct kinds of bias in event temporal reasoning, which include event\nrelation prior bias, tense bias, narrative bias, and dependency bias, as\nindicators to study knowledge conflicts. To mitigate such event-related\nknowledge conflict, we introduce a Counterfactual Data Augmentation based\nmethod that can be applied to both Pre-trained Language Models (PLMs) and Large\nLanguage Models (LLMs) either as additional training data or demonstrations for\nIn-Context Learning. Experiments suggest the importance of mitigating knowledge\nconflicts in event temporal reasoning tasks for reducing hallucination and\nhighlight the potential of counterfactual data augmentation for improving model\nperformance.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:04:06 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14971","submitter":"Marilena Caramazza","authors":"M. Caramazza, B. Stelzer, E. Magaudda, St. Raetz, M. G\\\"udel, S.\n  Orlando, K. Poppenh\\\"ager","title":"Complete X-ray census of Mdwarfs in the solar Neighborhood I. GJ 745 AB:\n  Coronal-hole Stars in the 10 pc Sample","comments":"accepted for publication in Astronomy & Astrophysics (A&A)","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.SR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We have embarked in a systematic study of the X-ray emission in a\nvolume-limited sample of M dwarf stars, in order to explore the full range of\nactivity levels present in their coronae and, thus, to understand the\nconditions in their outer atmospheres and their possible impact on the\ncircumstellar environment. We identify in a recent catalog of the Gaia objects\nwithin 10 pc from the Sun all the stars with spectral type between M0 and M4,\nand search systematically for X-ray measurements of this sample. To this end,\nwe use both archival data (from ROSAT, XMM-Newton, and from the ROentgen Survey\nwith an Imaging Telescope Array (eROSITA) onboard the Russian\nSpektrum-Roentgen-Gamma mission) and our own dedicated XMM-Newton observations.\nTo make inferences on the properties of the M dwarf corona we compare the range\nof their observed X-ray emission levels to the flux radiated by the Sun from\ndifferent types of magnetic structures: coronal holes, background corona,\nactive regions and cores of active regions. At the current state of our\nproject, with more than 90\\% of the 10pc M dwarf sample observed in X-rays,\nonly GJ 745 A has no detection. With an upper limit luminosity of log Lx\n[erg/s] < 25.4 and an X-ray surface flux of log FX,SURF [erg/cm^2/s] < 3.6 GJ\n745 A defines the lower boundary of the X-ray emission level of M dwarfs.\nTogether with its companion GJ 745 B, GJ 745 A it is the only star in this\nvolume-complete sample located in the range of FX,SURF that corresponds to the\nfaintest solar coronal structures, the coronal holes. The ultra-low X-ray\nemission level of GJ 745 B (log Lx [erg/s] = 25.6 and log FX,SURF [erg/cm^2/s]\n= 3.8) is entirely attributed to flaring activity, indicating that, while its\ncorona is dominated by coronal holes, at least one magnetically active\nstructure is present and determines the total X-ray brightness and the coronal\ntemperature of the star.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:04:07 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14972","submitter":"Vadim Sokolov","authors":"Nicholas G. Polson and Vadim Sokolov","title":"Generative AI for Bayesian Computation","comments":"arXiv admin note: text overlap with arXiv:2209.02163","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.CO","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  We develop Generative AI (Gen-AI) methods for Bayesian Computation. Gen-AI\nnaturally applies to Bayesian models which are easily simulated. We generate a\nlarge training dataset and together with deep neural networks we uncover the\ninverse Bayes map for inference and prediction. To do this, we require high\ndimensional regression methods and dimensionality reduction (a.k.a feature\nselection). The main advantage of Generative AI is its ability to be model-free\nand the fact that it doesn't rely on densities. Bayesian computation is\nreplaced by pattern recognition of an input-output map. This map is learned\nfrom empirical model simulation. We show that Deep Quantile NNs provide a\ngeneral framework for inference decision making. To illustrate our methodology,\nwe provide three examples: a stylized synthetic example, a traffic flow\nprediction problem and we analyze the well-known Ebola data-set. Finally, we\nconclude with directions for future research.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:05:47 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14973","submitter":"Jiazheng Li","authors":"Jiazheng Li, Runcong Zhao, Yulan He, Lin Gui","title":"OverPrompt: Enhancing ChatGPT Capabilities through an Efficient\n  In-Context Learning Approach","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The exceptional performance of pre-trained large language models has\nrevolutionised various applications, but their adoption in production\nenvironments is hindered by prohibitive costs and inefficiencies, particularly\nwhen utilising long prompts. This paper proposes OverPrompt, an in-context\nlearning method aimed at improving LLM efficiency and performance by processing\nmultiple inputs in parallel. Evaluated across diverse datasets, OverPrompt\nenhances task efficiency and integrates a diverse range of examples for\nimproved performance. Particularly, it amplifies fact-checking and sentiment\nanalysis tasks when supplemented with contextual information. Synthetic data\ngrouping further enhances performance, suggesting a viable approach for data\naugmentation.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:08:04 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14974","submitter":"David Kappel","authors":"David Kappel, Khaleelulla Khan Nazeer, Cabrel Teguemne Fokam,\n  Christian Mayr, Anand Subramoney","title":"Block-local learning with probabilistic latent representations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The ubiquitous backpropagation algorithm requires sequential updates across\nblocks of a network, introducing a locking problem. Moreover, backpropagation\nrelies on the transpose of weight matrices to calculate updates, introducing a\nweight transport problem across blocks. Both these issues prevent efficient\nparallelisation and horizontal scaling of models across devices. We propose a\nnew method that introduces a twin network that propagates information backwards\nfrom the targets to the input to provide auxiliary local losses. Forward and\nbackward propagation can work in parallel and with different sets of weights,\naddressing the problems of weight transport and locking. Our approach derives\nfrom a statistical interpretation of end-to-end training which treats\nactivations of network layers as parameters of probability distributions. The\nresulting learning framework uses these parameters locally to assess the\nmatching between forward and backward information. Error backpropagation is\nthen performed locally within each block, leading to `block-local' learning.\nSeveral previously proposed alternatives to error backpropagation emerge as\nspecial cases of our model. We present results on various tasks and\narchitectures, including transformers, demonstrating state-of-the-art\nperformance using block-local learning. These results provide a new principled\nframework to train very large networks in a distributed setting and can also be\napplied in neuromorphic systems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:11:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14975","submitter":"Eric A Mitchell","authors":"Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael\n  Rafailov, Huaxiu Yao, Chelsea Finn, Christopher D. Manning","title":"Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence\n  Scores from Language Models Fine-Tuned with Human Feedback","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  A trustworthy real-world prediction system should be well-calibrated; that\nis, its confidence in an answer is indicative of the likelihood that the answer\nis correct, enabling deferral to a more expensive expert in cases of\nlow-confidence predictions. While recent studies have shown that unsupervised\npre-training produces large language models (LMs) that are remarkably\nwell-calibrated, the most widely-used LMs in practice are fine-tuned with\nreinforcement learning with human feedback (RLHF-LMs) after the initial\nunsupervised pre-training stage, and results are mixed as to whether these\nmodels preserve the well-calibratedness of their ancestors. In this paper, we\nconduct a broad evaluation of computationally feasible methods for extracting\nconfidence scores from LLMs fine-tuned with RLHF. We find that with the right\nprompting strategy, RLHF-LMs verbalize probabilities that are much better\ncalibrated than the model's conditional probabilities, enabling fairly\nwell-calibrated predictions. Through a combination of prompting strategy and\ntemperature scaling, we find that we can reduce the expected calibration error\nof RLHF-LMs by over 50%.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:12:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14976","submitter":"Md Tawkat Islam Khondaker","authors":"Md Tawkat Islam Khondaker, Abdul Waheed, El Moatez Billah Nagoudi,\n  Muhammad Abdul-Mageed","title":"GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The recent emergence of ChatGPT has brought a revolutionary change in the\nlandscape of NLP. Although ChatGPT has consistently shown impressive\nperformance on English benchmarks, its exact capabilities on most other\nlanguages remain largely unknown. To better understand ChatGPT's capabilities\non Arabic, we present a large-scale evaluation of the model on a broad range of\nArabic NLP tasks. Namely, we evaluate ChatGPT on 32 diverse natural language\nunderstanding and generation tasks on over 60 different datasets. To the best\nof our knowledge, our work offers the first performance analysis of ChatGPT on\nArabic NLP at such a massive scale. Our results show that, despite its success\non English benchmarks, ChatGPT trained in-context (few-shot) is consistently\noutperformed by much smaller dedicated models finetuned on Arabic. These\nresults suggest that there is significant place for improvement for\ninstruction-tuned LLMs such as ChatGPT.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:12:39 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14977","submitter":"Florian Heidecker","authors":"Florian Heidecker, Ahmad El-Khateeb, Bernhard Sick","title":"Sampling-based Uncertainty Estimation for an Instance Segmentation\n  Network","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The examination of uncertainty in the predictions of machine learning (ML)\nmodels is receiving increasing attention. One uncertainty modeling technique\nused for this purpose is Monte-Carlo (MC)-Dropout, where repeated predictions\nare generated for a single input. Therefore, clustering is required to describe\nthe resulting uncertainty, but only through efficient clustering is it possible\nto describe the uncertainty from the model attached to each object. This\narticle uses Bayesian Gaussian Mixture (BGM) to solve this problem. In\naddition, we investigate different values for the dropout rate and other\ntechniques, such as focal loss and calibration, which we integrate into the\nMask-RCNN model to obtain the most accurate uncertainty approximation of each\ninstance and showcase it graphically.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:12:50 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14978","submitter":"Nathanael Bosch","authors":"Nathanael Bosch, Philipp Hennig, Filip Tronarp","title":"Probabilistic Exponential Integrators","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.LG cs.NA stat.ML","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Probabilistic solvers provide a flexible and efficient framework for\nsimulation, uncertainty quantification, and inference in dynamical systems.\nHowever, like standard solvers, they suffer performance penalties for certain\nstiff systems, where small steps are required not for reasons of numerical\naccuracy but for the sake of stability. This issue is greatly alleviated in\nsemi-linear problems by the probabilistic exponential integrators developed in\nthis paper. By including the fast, linear dynamics in the prior, we arrive at a\nclass of probabilistic integrators with favorable properties. Namely, they are\nproven to be L-stable, and in a certain case reduce to a classic exponential\nintegrator -- with the added benefit of providing a probabilistic account of\nthe numerical error. The method is also generalized to arbitrary non-linear\nsystems by imposing piece-wise semi-linearity on the prior via Jacobians of the\nvector field at the previous estimates, resulting in probabilistic exponential\nRosenbrock methods. We evaluate the proposed methods on multiple stiff\ndifferential equations and demonstrate their improved stability and efficiency\nover established probabilistic solvers. The present contribution thus expands\nthe range of problems that can be effectively tackled within probabilistic\nnumerics.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:13:13 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14979","submitter":"Gabriel Kasmi","authors":"Gabriel Kasmi and Laurent Dubus and Yves-Marie Saint Drenan and\n  Philippe Blanc","title":"Scale Matters: Attribution Meets the Wavelet Domain to Explain Model\n  Sensitivity to Image Corruptions","comments":"main: 9 pages, appendix 19 pages, 32 figures, 5 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI stat.ML","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Neural networks have shown remarkable performance in computer vision, but\ntheir deployment in real-world scenarios is challenging due to their\nsensitivity to image corruptions. Existing attribution methods are\nuninformative for explaining the sensitivity to image corruptions, while the\nliterature on robustness only provides model-based explanations. However, the\nability to scrutinize models' behavior under image corruptions is crucial to\nincrease the user's trust. Towards this end, we introduce the Wavelet sCale\nAttribution Method (WCAM), a generalization of attribution from the pixel\ndomain to the space-scale domain. Attribution in the space-scale domain reveals\nwhere and on what scales the model focuses. We show that the WCAM explains\nmodels' failures under image corruptions, identifies sufficient information for\nprediction, and explains how zoom-in increases accuracy.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:13:32 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14980","submitter":"Yuehong Chen","authors":"Yuehong Chen, Yu Dai and Mingde Ding","title":"An Atypical Plateau-like Extreme-ultraviolet Late-phase Solar Flare\n  Driven by the Non-radial Eruption of a Magnetic Flux Rope","comments":"Accepted by A&A","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.SR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Recent observations in extreme-ultraviolet (EUV) wavelengths reveal an EUV\nlate phase in some solar flares, which is characterized by a second peak in the\nwarm coronal emissions (about 3 MK) occurring several tens of minutes to a few\nhours after the corresponding main flare peak. We aim to clarify the physical\norigin of an atypical plateau-like EUV late phase in an X1.8-class solar flare\noccurring on 2011 September 7 from active region (AR) 11283. We first\ncharacterize the plateau-like late phase using EUV Variability Experiment (EVE)\nfull-disk integrated irradiance observations and Atmospheric Imaging Assembly\n(AIA) spatially-resolved imaging observations on board the Solar Dynamics\nObservatory (SDO). Then we perform a nonlinear force-free-field (NLFFF)\nextrapolation, from which a filament-hosting magnetic flux rope (MFR) is\nrevealed. The eruption of the MFR is tracked both in the plane of the sky (POS)\nand along the line of sight (LOS) through visual inspection and spectral\nfitting, respectively. Finally, we carry out differential emission measure\n(DEM) analysis to explore the thermodynamics of the late-phase loops. The MFR\nshows a non-radial eruption from a fan-spine magnetic structure. The eruption\nof the MFR and its interaction with overlying arcades invoke multiple magnetic\nreconnections, which are responsible for the production of different groups of\nlate-phase loops. Afterwards, the late-phase loops enter a long-lasting cooling\nstage, appearing sequentially in AIA passbands of decreasing response\ntemperatures. Due to their different lengths, the different groups of\nlate-phase loops cool down at different cooling rates, which makes their warm\ncoronal emission peaks temporally separated from each other. Combing the\nemissions from all late-phase loops together, an elongated plateau-like late\nphase is formed.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:13:57 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14981","submitter":"Muhao Chen","authors":"Tanay Dixit, Fei Wang, Muhao Chen","title":"Improving Factuality of Abstractive Summarization without Sacrificing\n  Summary Quality","comments":"ACL 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Improving factual consistency of abstractive summarization has been a widely\nstudied topic. However, most of the prior works on training factuality-aware\nmodels have ignored the negative effect it has on summary quality. We propose\nEFACTSUM (i.e., Effective Factual Summarization), a candidate summary\ngeneration and ranking technique to improve summary factuality without\nsacrificing summary quality. We show that using a contrastive learning\nframework with our refined candidate summaries leads to significant gains on\nboth factuality and similarity-based metrics. Specifically, we propose a\nranking strategy in which we effectively combine two metrics, thereby\npreventing any conflict during training. Models trained using our approach show\nup to 6 points of absolute improvement over the base model with respect to\nFactCC on XSUM and 11 points on CNN/DM, without negatively affecting either\nsimilarity-based metrics or absractiveness.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:15:17 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14982","submitter":"Firoj Alam","authors":"Ahmed Abdelali, Hamdy Mubarak, Shammur Absar Chowdhury, Maram\n  Hasanain, Basel Mousi, Sabri Boughorbel, Yassine El Kheir, Daniel Izham,\n  Fahim Dalvi, Majd Hawasly, Nizi Nazar, Yousseif Elshahawy, Ahmed Ali, Nadir\n  Durrani, Natasa Milic-Frayling, Firoj Alam","title":"Benchmarking Arabic AI with Large Language Models","comments":"Foundation Models, Large Language Models, Arabic NLP, Arabic Speech,\n  Arabic AI, , CHatGPT Evaluation, USM Evaluation, Whisper Evaluation","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  With large Foundation Models (FMs), language technologies (AI in general) are\nentering a new paradigm: eliminating the need for developing large-scale\ntask-specific datasets and supporting a variety of tasks through set-ups\nranging from zero-shot to few-shot learning. However, understanding FMs\ncapabilities requires a systematic benchmarking effort by comparing FMs\nperformance with the state-of-the-art (SOTA) task-specific models. With that\ngoal, past work focused on the English language and included a few efforts with\nmultiple languages. Our study contributes to ongoing research by evaluating FMs\nperformance for standard Arabic NLP and Speech processing, including a range of\ntasks from sequence tagging to content classification across diverse domains.\nWe start with zero-shot learning using GPT-3.5-turbo, Whisper, and USM,\naddressing 33 unique tasks using 59 publicly available datasets resulting in 96\ntest setups. For a few tasks, FMs performs on par or exceeds the performance of\nthe SOTA models but for the majority it under-performs. Given the importance of\nprompt for the FMs performance, we discuss our prompt strategies in detail and\nelaborate on our findings. Our future work on Arabic AI will explore few-shot\nprompting, expand the range of tasks, and investigate additional open-source\nmodels.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:16:16 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14983","submitter":"Benjamin Lowe","authors":"Benjamin Lowe, Bernard Field, Jack Hellerstedt, Julian Ceddia, Henry\n  L. Nourse, Ben J. Powell, Nikhil V. Medhekar, and Agustin Schiffrin","title":"Gate control of Mott metal-insulator transition in a 2D metal-organic\n  framework","comments":"19 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.str-el","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Strong electron-electron Coulomb interactions in materials can lead to a vast\nrange of exotic many-body quantum phenomena, including Mott metal-insulator\ntransitions, magnetic order, quantum spin liquids, and unconventional\nsuperconductivity. These many-body phases are strongly dependent on band\noccupation and can hence be controlled via the chemical potential. Flat\nelectronic bands in two-dimensional (2D) and layered materials such as the\nkagome lattice, enhance strong electronic correlations. Although theoretically\npredicted, correlated-electron phases in monolayer 2D metal-organic frameworks\n(MOFs) - which benefit from efficient synthesis protocols and tunable\nproperties - with a kagome structure have not yet been realised experimentally.\nHere, we synthesise a 2D kagome MOF comprised of 9,10-dicyanoanthracene\nmolecules and copper atoms on an atomically thin insulator, monolayer hexagonal\nboron nitride (hBN) on Cu(111). Scanning tunnelling microscopy (STM) and\nspectroscopy reveal an electronic energy gap of ~200 meV in this MOF,\nconsistent with dynamical mean-field theory predictions of a Mott insulating\nphase. By tuning the electron population of kagome bands, via either\ntemplate-induced (via local work function variations of the hBN/Cu(111)\nsubstrate) or tip-induced (via the STM probe) gating, we are able to induce\nMott metal-insulator transitions in the MOF. These findings pave the way for\ndevices and technologies based on 2D MOFs and on electrostatic control of\nmany-body quantum phases therein.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:17:10 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14984","submitter":"Manuel Gl\\\"ockler","authors":"Manuel Gl\\\"ockler, Michael Deistler, Jakob H. Macke","title":"Adversarial robustness of amortized Bayesian inference","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Bayesian inference usually requires running potentially costly inference\nprocedures separately for every new observation. In contrast, the idea of\namortized Bayesian inference is to initially invest computational cost in\ntraining an inference network on simulated data, which can subsequently be used\nto rapidly perform inference (i.e., to return estimates of posterior\ndistributions) for new observations. This approach has been applied to many\nreal-world models in the sciences and engineering, but it is unclear how robust\nthe approach is to adversarial perturbations in the observed data. Here, we\nstudy the adversarial robustness of amortized Bayesian inference, focusing on\nsimulation-based estimation of multi-dimensional posterior distributions. We\nshow that almost unrecognizable, targeted perturbations of the observations can\nlead to drastic changes in the predicted posterior and highly unrealistic\nposterior predictive samples, across several benchmark tasks and a real-world\nexample from neuroscience. We propose a computationally efficient\nregularization scheme based on penalizing the Fisher information of the\nconditional density estimator, and show how it improves the adversarial\nrobustness of amortized Bayesian inference.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:18:45 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14985","submitter":"Haoxuan You","authors":"Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu Wang, Hammad A.\n  Ayyubi, Kai-Wei Chang, Shih-Fu Chang","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via\n  Large Language Models","comments":"13 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The field of vision-and-language (VL) understanding has made unprecedented\nprogress with end-to-end large pre-trained VL models (VLMs). However, they\nstill fall short in zero-shot reasoning tasks that require multi-step\ninferencing. To achieve this goal, previous works resort to a\ndivide-and-conquer pipeline. In this paper, we argue that previous efforts have\nseveral inherent shortcomings: 1) They rely on domain-specific sub-question\ndecomposing models. 2) They force models to predict the final answer even if\nthe sub-questions or sub-answers provide insufficient information. We address\nthese limitations via IdealGPT, a framework that iteratively decomposes VL\nreasoning using large language models (LLMs). Specifically, IdealGPT utilizes\nan LLM to generate sub-questions, a VLM to provide corresponding sub-answers,\nand another LLM to reason to achieve the final answer. These three modules\nperform the divide-and-conquer procedure iteratively until the model is\nconfident about the final answer to the main question. We evaluate IdealGPT on\nmultiple challenging VL reasoning tasks under a zero-shot setting. In\nparticular, our IdealGPT outperforms the best existing GPT-4-like models by an\nabsolute 10% on VCR and 15% on SNLI-VE. Code is available at\nhttps://github.com/Hxyou/IdealGPT\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:19:57 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14986","submitter":"Gorana Goji\\'c","authors":"Gorana Goji\\'c, Vladimir Vincan, Ognjen Kunda\\v{c}ina, Dragi\\v{s}a\n  Mi\\v{s}kovi\\'c and Dinu Dragan","title":"Non-adversarial Robustness of Deep Learning Methods for Computer Vision","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Non-adversarial robustness, also known as natural robustness, is a property\nof deep learning models that enables them to maintain performance even when\nfaced with distribution shifts caused by natural variations in data. However,\nachieving this property is challenging because it is difficult to predict in\nadvance the types of distribution shifts that may occur. To address this\nchallenge, researchers have proposed various approaches, some of which\nanticipate potential distribution shifts, while others utilize knowledge about\nthe shifts that have already occurred to enhance model generalizability. In\nthis paper, we present a brief overview of the most recent techniques for\nimproving the robustness of computer vision methods, as well as a summary of\ncommonly used robustness benchmark datasets for evaluating the model's\nperformance under data distribution shifts. Finally, we examine the strengths\nand limitations of the approaches reviewed and identify general trends in deep\nlearning robustness improvement for computer vision.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:21:31 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14987","submitter":"Yilun Zhao","authors":"Yilun Zhao, Haowei Zhang, Shengyun Si, Linyong Nan, Xiangru Tang,\n  Arman Cohan","title":"Large Language Models are Effective Table-to-Text Generators,\n  Evaluators, and Feedback Providers","comments":"work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large language models (LLMs) have shown remarkable ability on controllable\ntext generation. However, the potential of LLMs in generating text from\nstructured tables remains largely under-explored. In this paper, we study the\ncapabilities of LLMs for table-to-text generation tasks, particularly aiming to\ninvestigate their performance in generating natural language statements that\ncan be logically entailed by a provided table. First, we investigate how LLMs\ncompare to state-of-the-art table-to-text fine-tuned models, and demonstrate\nthat LLMs can generate statements with higher faithfulness compared with\nprevious state-of-the-art fine-tuned models. Given this finding, we next\nexplore whether LLMs can serve as faithfulness-level automated evaluation\nmetrics. Through human evaluation, we show that evaluation metrics adopted from\nLLMs correlates better with human judgments compared with existing\nfaithfulness-level metrics. Finally, we demonstrate that LLMs using\nchain-of-thought prompting can generate high-fidelity natural language feedback\nfor other table-to-text models' generations, provide insights for future work\nregarding the distillation of text generation capabilities from LLMs to smaller\nmodels.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:22:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14988","submitter":"Shraddha Rajkhowa","authors":"Shraddha Rajkhowa and Nipen Saikia","title":"Some Identities of Ramanujan's q-Continued Fractions of Order Fourteen\n  and Twenty-Eight, and Vanishing Coefficients","comments":"8 Pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.NT","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We deduce $q$-continued fractions $S_{1}(q)$, $S_{2}(q)$ and $S_{3}(q)$ of\norder fourteen, and continued fractions $V_{1}(q)$, $V_{2}(q)$ and $V_{3}(q)$\nof order twenty-eight from a general continued fraction identity of Ramanujan.\nWe establish some theta-function identities for the continued fractions and\nderive some colour partition identities as applications. Some vanishing\ncoefficients results arising from the continued fractions are also offered.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:22:39 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14989","submitter":"Abdelrahim Elmadany","authors":"El Moatez Billah Nagoudi, Ahmed El-Shangiti, AbdelRahim Elmadany,\n  Muhammad Abdul-Mageed","title":"Dolphin: A Challenging and Diverse Benchmark for Arabic NLG","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We present Dolphin, a novel benchmark that addresses the need for an\nevaluation framework for the wide collection of Arabic languages and varieties.\nThe proposed benchmark encompasses a broad range of 13 different NLG tasks,\nincluding text summarization, machine translation, question answering, and\ndialogue generation, among others. Dolphin comprises a substantial corpus of 40\ndiverse and representative public datasets across 50 test splits, carefully\ncurated to reflect real-world scenarios and the linguistic richness of Arabic.\nIt sets a new standard for evaluating the performance and generalization\ncapabilities of Arabic and multilingual models, promising to enable researchers\nto push the boundaries of current methodologies. We provide an extensive\nanalysis of Dolphin, highlighting its diversity and identifying gaps in current\nArabic NLG research. We also evaluate several Arabic and multilingual models on\nour benchmark, allowing us to set strong baselines against which researchers\ncan compare.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:24:10 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14990","submitter":"Ratna Kumar Annabattula","authors":"Akhil Reddy Peeketi, Edwin Joseph, Narasimhan Swaminathan, Ratna Kumar\n  Annabattula","title":"Photo-activated dynamic isomerization induced large density changes in\n  liquid crystal polymers: A molecular dynamics study","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.soft","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Recent experimental results [Liu and Broer, Nat. Commun. 6 8334 (2015)]\nreveal that light-responsive azo-doped liquid crystal polymers under\ndual-wavelength illumination exhibit a significant reduction in density. This\nreduction in density was attributed to dynamic trans-cis-trans isomerization\ncycles. The light-induced isomerization kinetics suggest that the fraction of\nisomers undergoing dynamic isomerization increases with the light sources'\nintensity. However, experiments have shown that such an increase in intensity\ndoes not result in a monotonic decrease in density. Further, it was observed\nthat there exists an optimal combination of the intensities of the\ndual-wavelength illumination that results in a maximum density reduction. The\nexact reason for the existence of such an optimal combination remains elusive.\nIn this work, we have performed atomistic simulations to confirm the hypothesis\nthat the density reduction is caused by the dynamic trans-cis-trans\nisomerization cycles. Subsequently, the atomistic simulations are used to\ndecipher the underlying physics responsible for the counter-intuitive relation\nbetween density reduction and intensities. Intensity variations are simulated\nby varying the forward and backward isomerization probabilities. The\nsimulations show that an optimal combination of these two probabilities will\nexhibit a maximum density reduction corroborating experimental observations.\nConsequently, we discovered that a specific frequency of the dynamic\ntrans-cis-trans isomerization cycles would induce maximum distortion in the\npolymer network resulting in significant density reduction.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:25:50 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14991","submitter":"Leshem Choshen","authors":"Taelin Karidi, Leshem Choshen, Gal Patel, Omri Abend","title":"MuLER: Detailed and Scalable Reference-based Evaluation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We propose a novel methodology (namely, MuLER) that transforms any\nreference-based evaluation metric for text generation, such as machine\ntranslation (MT) into a fine-grained analysis tool.\n  Given a system and a metric, MuLER quantifies how much the chosen metric\npenalizes specific error types (e.g., errors in translating names of\nlocations). MuLER thus enables a detailed error analysis which can lead to\ntargeted improvement efforts for specific phenomena.\n  We perform experiments in both synthetic and naturalistic settings to support\nMuLER's validity and showcase its usability in MT evaluation, and other tasks,\nsuch as summarization. Analyzing all submissions to WMT in 2014-2020, we find\nconsistent trends. For example, nouns and verbs are among the most frequent POS\ntags. However, they are among the hardest to translate. Performance on most POS\ntags improves with overall system performance, but a few are not thus\ncorrelated (their identity changes from language to language). Preliminary\nexperiments with summarization reveal similar trends.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:26:13 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14992","submitter":"Shibo Hao","authors":"Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe\n  Wang, Zhiting Hu","title":"Reasoning with Language Model is Planning with World Model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Large language models (LLMs) have shown remarkable reasoning capabilities,\nespecially when prompted to generate intermediate reasoning steps (e.g.,\nChain-of-Thought, CoT). However, LLMs can still struggle with problems that are\neasy for humans, such as generating action plans for executing tasks in a given\nenvironment, or performing complex math, logical, and commonsense reasoning.\nThe deficiency stems from the key fact that LLMs lack an internal\n$\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment\nstatus, intermediate variable values) and simulate long-term outcomes of\nactions. This prevents LLMs from performing deliberate planning akin to human\nbrains, which involves exploring alternative reasoning paths, anticipating\nfuture states and rewards, and iteratively refining existing reasoning steps.\nTo overcome the limitations, we propose a new LLM reasoning framework,\n$\\underline{R}\\textit{easoning vi}\\underline{a} \\underline{P}\\textit{lanning}$\n$\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning\nagent, and incorporates a principled planning algorithm (based on Monto Carlo\nTree Search) for strategic exploration in the vast reasoning space. During\nreasoning, the LLM (as agent) incrementally builds a reasoning tree under the\nguidance of the LLM (as world model) and task-specific rewards, and obtains a\nhigh-reward reasoning path efficiently with a proper balance between\nexploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of\nchallenging reasoning problems including plan generation, math reasoning, and\nlogical inference. Empirical results on these tasks demonstrate the superiority\nof RAP over various strong baselines, including CoT and least-to-most prompting\nwith self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33%\nrelative improvement in a plan generation setting.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:28:28 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14993","submitter":"Sweta Agrawal","authors":"Sweta Agrawal and Marine Carpuat","title":"How To Control Text Simplification? An Empirical Study of Control Tokens\n  for Meaning Preserving Controlled Simplification","comments":"work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Text simplification rewrites text to be more readable for a specific\naudience, while preserving its meaning. However, determining what makes a text\neasy to read depends on who are the intended readers. Recent work has\nintroduced a wealth of techniques to control output simplicity, ranging from\nspecifying the desired reading grade level to providing control tokens that\ndirectly encode low-level simplification edit operations. However, it remains\nunclear how to set the input parameters that control simplification in\npractice. Existing approaches set them at the corpus level, disregarding the\ncomplexity of individual source text, and do not directly evaluate them at the\ninstance level. In this work, we conduct an empirical study to understand how\ndifferent control mechanisms impact the adequacy and simplicity of model\noutputs. Based on these insights, we introduce a simple method for predicting\ncontrol tokens at the sentence level to enhance the quality of the simplified\ntext. Predicting control token values using features extracted from the\noriginal complex text and a user-specified degree of complexity improves the\nquality of the simplified outputs over corpus-level search-based heuristics.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:29:45 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14994","submitter":"Dongjie Yang","authors":"Dongjie Yang, Ruifeng Yuan, YuanTao Fan, YiFei Yang, Zili Wang, Shusen\n  Wang, Hai Zhao","title":"RefGPT: Reference -> Truthful & Customized Dialogues Generation by GPTs\n  and for GPTs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  General chat models, like ChatGPT, have attained impressive capability to\nresolve a wide range of NLP tasks by tuning Large Language Models (LLMs) with\nhigh-quality instruction data. However, collecting human-written high-quality\ndata, especially multi-turn dialogues, is expensive and unattainable for most\npeople. Though previous studies have used powerful LLMs to generate the\ndialogues automatically, but they all suffer from generating untruthful\ndialogues because of the LLMs hallucination. Therefore, we propose a method\ncalled RefGPT to generate enormous truthful and customized dialogues without\nworrying about factual errors caused by the model hallucination. RefGPT solves\nthe model hallucination in dialogue generation by restricting the LLMs to\nleverage the given reference instead of reciting their own knowledge to\ngenerate dialogues. Additionally, RefGPT adds detailed controls on every\nutterances to enable highly customization capability, which previous studies\nhave ignored. On the basis of RefGPT, we also propose two high-quality dialogue\ndatasets generated by GPT-4, namely RefGPT-Fact and RefGPT-Code. RefGPT-Fact is\n100k multi-turn dialogue datasets based on factual knowledge and RefGPT-Code is\n76k multi-turn dialogue dataset covering a wide range of coding scenarios. Our\ncode and datasets are released in https://github.com/ziliwangnlp/RefGPT\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:30:42 GMT"},{"version":"v2","created":"Thu, 25 May 2023 02:20:51 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.14995","submitter":"Benjamin Arras","authors":"Benjamin Arras","title":"Some Notes on Quantitative Generalized CLTs with Self-Decomposable\n  Limiting Laws by Spectral Methods","comments":"102 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR math.FA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In these notes, we obtain new stability estimates for centered\nself-decomposable probability measures on $\\mathbb{R}^d$ with finite second\nmoment and for non-degenerate symmetric $\\alpha$-stable probability measures on\n$\\mathbb{R}^d$ with $\\alpha \\in (1,2)$. These new results are refinements of\nthe corresponding ones available in the literature. The proofs are based on\nStein's method for self-decomposable laws, recently developed in a series of\npapers, and on closed forms techniques together with a new ingredient: weighted\nPoincar\\'e-type inequalities. As applications, rates of convergence in\nWasserstein-type distances are computed for several instances of the\ngeneralized central limit theorems (CLTs). In particular, a\n$n^{1-2/\\alpha}$-rate is obtained in $1$-Wasserstein distance when the target\nlaw is a non-degenerate symmetric $\\alpha$-stable one with $\\alpha \\in (1,2)$.\nFinally, the non-degenerate symmetric Cauchy case is studied at length from a\nspectral point of view. At last, in this Cauchy situation, a $n^{-1}$-rate of\nconvergence is obtained when the initial law is a certain instance of layered\nstable distributions.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:32:47 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14996","submitter":"Yanxia Qin","authors":"Shaurya Rohatgi, Yanxia Qin, Benjamin Aw, Niranjana Unnithan, Min-Yen\n  Kan","title":"The ACL OCL Corpus: advancing Open science in Computational Linguistics","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.DL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We present a scholarly corpus from the ACL Anthology to assist Open\nscientific research in the Computational Linguistics domain, named as ACL OCL.\nCompared with previous ARC and AAN versions, ACL OCL includes structured\nfull-texts with logical sections, references to figures, and links to a large\nknowledge resource (semantic scholar). ACL OCL contains 74k scientific papers,\ntogether with 210k figures extracted up to September 2022. To observe the\ndevelopment in the computational linguistics domain, we detect the topics of\nall OCL papers with a supervised neural model. We observe ''Syntax: Tagging,\nChunking and Parsing'' topic is significantly shrinking and ''Natural Language\nGeneration'' is resurging. Our dataset is open and available to download from\nHuggingFace in https://huggingface.co/datasets/ACL-OCL/ACL-OCL-Corpus.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:35:56 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14997","submitter":"Jianhua Zhang","authors":"Zhaowei Chang and Jianhua Zhang and Pan Tang and Lei Tian and Yadong\n  Yang and Jiaxin Lin and and Guangyi Liu","title":"3GPP-Like THz Channel Modeling for Indoor Office and Urban Microcellular\n  Scenarios","comments":"13 pages, 12 figures, 3 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Terahertz (THz) communication is envisioned as the possible technology for\nthe sixth-generation (6G) communication system. THz channel propagation\ncharacteristics are the basis of designing and evaluating for THz communication\nsystem. In this paper, THz channel measurements at 100 GHz and 132 GHz are\nconducted in an indoor office scenario and an urban microcellular (UMi)\nscenario, respectively. Based on the measurement, the 3GPP-like channel\nparameters are extracted and analyzed. Moreover, the parameters models are\navailable for the simulation of the channel impulse response by the\ngeometry-based stochastic model (GBSM). Then, the comparisons between\nmeasurement-based parameter models and 3rd Generation Partnership Project\n(3GPP) channel models are investigated. It is observed that the case with path\nloss approaching free space exists in the NLoS scenario. Besides, the cluster\nnumber are 4 at LoS and 5 at NLoS in the indoor office and 4 at LoS and 3 at\nNLoS in the UMi, which are much less than 3GPP. The multipath component (MPC)\nin the THz channel distributes more simpler and more sparsely than the 3GPP\nmillimeter wave (mm-wave) channel models. Furthermore, the ergodic capacity of\nmm-wave and THz are evaluated by the proposed THz GBSM implementation\nframework. The THz measurement model predicts the smallest capacity, indicating\nthat high carrier frequency is limited to the single transmission mechanism of\nreflection and results in the reduction of cluster numbers and ergodic\ncapacity. Generally, these results are helpful to understand and model the THz\nchannel and apply the THz communication technique for 6G.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:36:04 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.14998","submitter":"Saba Ahmadi","authors":"Saba Ahmadi, Aishwarya Agrawal","title":"An Examination of the Robustness of Reference-Free Image Captioning\n  Evaluation Metrics","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CV cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Recently, reference-free metrics such as CLIPScore (Hessel et al., 2021) and\nUMIC (Lee et al., 2021) have been proposed for automatic evaluation of image\ncaptions, demonstrating a high correlation with human judgment. In this work,\nour focus lies in evaluating the robustness of these metrics in scenarios that\nrequire distinguishing between two captions with high lexical overlap but very\ndifferent meanings. Our findings reveal that despite their high correlation\nwith human judgment, both CLIPScore and UMIC struggle to identify fine-grained\nerrors in captions. However, when comparing different types of fine-grained\nerrors, both metrics exhibit limited sensitivity to implausibility of captions\nand strong sensitivity to lack of sufficient visual grounding. Probing further\ninto the visual grounding aspect, we found that both CLIPScore and UMIC are\nimpacted by the size of image-relevant objects mentioned in the caption, and\nthat CLIPScore is also sensitive to the number of mentions of image-relevant\nobjects in the caption. In terms of linguistic aspects of a caption, we found\nthat both metrics lack the ability to comprehend negation, UMIC is sensitive to\ncaption lengths, and CLIPScore is insensitive to the structure of the sentence.\nWe hope our findings will serve as a valuable guide towards improving\nreference-free evaluation in image captioning.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:36:12 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.14999","submitter":"Zhiyang Xu","authors":"Jingyuan Qi, Zhiyang Xu, Ying Shen, Minqian Liu, Di Jin, Qifan Wang,\n  Lifu Huang","title":"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with\n  Recursive Thinking and Self-Questioning","comments":"15 pages, 12 figure, 2 algorithms","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Chain-of-Thought prompting (CoT) enables large-scale language models to solve\ncomplex reasoning problems by decomposing the problem and tackling it\nstep-by-step. However, Chain-of-Thought is a greedy thinking process that\nrequires the language model to come up with a starting point and generate the\nnext step solely based on previous steps. This thinking process is different\nfrom how humans approach a complex problem e.g., we proactively raise\nsub-problems related to the original problem and recursively answer them. In\nthis work, we propose Socratic Questioning, a divide-and-conquer fashion\nalgorithm that simulates the self-questioning and recursive thinking process.\nSocratic Questioning is driven by a Self-Questioning module that employs a\nlarge-scale language model to propose sub-problems related to the original\nproblem as intermediate steps and Socratic Questioning recursively backtracks\nand answers the sub-problems until reaches the original problem. We apply our\nproposed algorithm to the visual question-answering task as a case study and by\nevaluating it on three public benchmark datasets, we observe a significant\nperformance improvement over all baselines on (almost) all datasets. In\naddition, the qualitative analysis clearly demonstrates the intermediate\nthinking steps elicited by Socratic Questioning are similar to the human's\nrecursively thinking process of a complex reasoning problem.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:36:14 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15000","submitter":"Christian Berger","authors":"Christian Berger, L\\'ivio Rodrigues, Hans P. Reiser, Vinicius Cogo,\n  Alysson Bessani","title":"Chasing the Speed of Light: Low-Latency Planetary-Scale Adaptive\n  Byzantine Consensus","comments":"18 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Blockchain technology has sparked renewed interest in planetary-scale\nByzantine fault-tolerant (BFT) state machine replication (SMR). While recent\nworks have mainly focused on improving the scalability and throughput of these\nprotocols, few have addressed latency. We present FlashConsensus, a novel\ntransformation for optimizing the latency of quorum-based BFT consensus\nprotocols. FLASHCONSENSUS uses an adaptive resilience threshold that enables\nfaster transaction ordering when the system contains few faulty replicas. Our\nconstruction exploits adaptive weighted replication to automatically assign\nhigh voting power to the fastest replicas, forming small quorums that\nsignificantly speed up consensus. Even when using such quorums with a smaller\nresilience threshold, FlashConsensus still satisfies the standard SMR safety\nand liveness guarantees with optimal resilience, thanks to the judicious\nintegration of abortable SMR and BFT forensics techniques. Our experiments with\ntens of replicas spread in all continents show that FLASHCONSENSUS can order\ntransactions with finality in less than 0.4s, half the time of a PBFT-like\nprotocol (with optimal consensus latency) in the same network, and matching the\nlatency of this protocol running on the theoretically best possible internet\nlinks (transmitting at 67% of the speed of light).\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:37:17 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15001","submitter":"Anand Gopalakrishnan","authors":"Aleksandar Stani\\'c, Anand Gopalakrishnan, Kazuki Irie, J\\\"urgen\n  Schmidhuber","title":"Contrastive Training of Complex-Valued Autoencoders for Object Discovery","comments":"26 pages, 14 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Current state-of-the-art object-centric models use slots and attention-based\nrouting for binding. However, this class of models has several conceptual\nlimitations: the number of slots is hardwired; all slots have equal capacity;\ntraining has high computational cost; there are no object-level relational\nfactors within slots. Synchrony-based models in principle can address these\nlimitations by using complex-valued activations which store binding information\nin their phase components. However, working examples of such synchrony-based\nmodels have been developed only very recently, and are still limited to toy\ngrayscale datasets and simultaneous storage of less than three objects in\npractice. Here we introduce architectural modifications and a novel contrastive\nlearning method that greatly improve the state-of-the-art synchrony-based\nmodel. For the first time, we obtain a class of synchrony-based models capable\nof discovering objects in an unsupervised manner in multi-object color datasets\nand simultaneously representing more than three objects\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:37:43 GMT"},{"version":"v2","created":"Thu, 25 May 2023 08:57:57 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.15002","submitter":"Asahi Ushio","authors":"Asahi Ushio and Jose Camacho Collados and Steven Schockaert","title":"A RelEntLess Benchmark for Modelling Graded Relations between Named\n  Entities","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Relations such as \"is influenced by\", \"is known for\" or \"is a competitor of\"\nare inherently graded: we can rank entity pairs based on how well they satisfy\nthese relations, but it is hard to draw a line between those pairs that satisfy\nthem and those that do not. Such graded relations play a central role in many\napplications, yet they are typically not covered by existing Knowledge Graphs.\nIn this paper, we consider the possibility of using Large Language Models\n(LLMs) to fill this gap. To this end, we introduce a new benchmark, in which\nentity pairs have to be ranked according to how much they satisfy a given\ngraded relation. The task is formulated as a few-shot ranking problem, where\nmodels only have access to a description of the relation and five prototypical\ninstances. We use the proposed benchmark to evaluate state-of-the-art relation\nembedding strategies as well as several recent LLMs, covering both publicly\navailable LLMs and closed models such as GPT-4. Overall, we find a strong\ncorrelation between model size and performance, with smaller Language Models\nstruggling to outperform a naive baseline. The results of the largest Flan-T5\nand OPT models are remarkably strong, although a clear gap with human\nperformance remains.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:41:24 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15003","submitter":"Ashwin George","authors":"Ashwin George, Luciano Cavalcante Siebert, David Abbink and Arkady\n  Zgonnikov","title":"Measuring Causal Responsibility in Multi-Agent Spatial Interactions with\n  Feasible Action-Space Reduction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.MA","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  Modelling causal responsibility in multi-agent spatial interactions is\ncrucial for safety and efficiency of interactions of humans with autonomous\nagents. However, current formal metrics and models of responsibility either\nlack grounding in ethical and philosophical concepts of responsibility, or\ncannot be applied to spatial interactions. In this work we propose a metric of\ncausal responsibility which is tailored to multi-agent spatial interactions,\nfor instance interactions in traffic. In such interactions, a given agent can,\nby reducing another agent's feasible action space, influence the latter.\nTherefore, we propose feasible action space reduction (FeAR) as a metric for\ncausal responsibility among agents. Specifically, we look at ex-post causal\nresponsibility for simultaneous actions. We propose the use of Moves de Rigueur\n- a consistent set of prescribed actions for agents - to model the effect of\nnorms on responsibility allocation. We apply the metric in a grid world\nsimulation for spatial interactions and show how the actions, contexts, and\nnorms affect the causal responsibility ascribed to agents. Finally, we\ndemonstrate the application of this metric in complex multi-agent interactions.\nWe argue that the FeAR metric is a step towards an interdisciplinary framework\nfor quantifying responsibility that is needed to ensure safety and meaningful\nhuman control in human-AI systems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:44:57 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15004","submitter":"Kangxi Wu","authors":"Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng and Tat-Seng Chua","title":"LLMDet: A Large Language Models Detection Tool","comments":"7 pages, 1 figure","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  With the advancement of generative language models, the generated text has\ncome remarkably close to high-quality human-authored text in terms of fluency\nand diversity. This calls for a highly practical detection tool that can\nidentify the source of text, preferably pinpointing the language model it\noriginates from. However, existing detection tools typically require access to\nlanguage models and can only differentiate between machine-generated and\nhuman-authored text, failing to meet the requirements of rapid detection and\ntext tracing. Therefore, in this paper, we propose an efficient, secure, and\nscalable detection tool called LLMDet, which calculates the proxy perplexity of\ntext by utilizing the prior information of the model's next-token\nprobabilities, obtained through pre-training. Subsequently, we use the\nself-watermarking information of the model, as measured by proxy perplexity, to\ndetect the source of the text. We found that our method demonstrates impressive\ndetection performance while ensuring speed and security, particularly achieving\na recognition accuracy of 97.97\\% for human-authored text. Furthermore, our\ndetection tool also shows promising results in identifying the large language\nmodel (e.g., GPT-2, OPT, LLaMA, Vicuna...) responsible for the text. We release\nthe code and processed data at \\url{https://github.com/TrustedLLM/LLMDet}.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:45:16 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15005","submitter":"Wenxuan Zhang","authors":"Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, Lidong Bing","title":"Sentiment Analysis in the Era of Large Language Models: A Reality Check","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Sentiment analysis (SA) has been a long-standing research area in natural\nlanguage processing. It can offer rich insights into human sentiments and\nopinions and has thus seen considerable interest from both academia and\nindustry. With the advent of large language models (LLMs) such as ChatGPT,\nthere is a great potential for their employment on SA problems. However, the\nextent to which existing LLMs can be leveraged for different sentiment analysis\ntasks remains unclear. This paper aims to provide a comprehensive investigation\ninto the capabilities of LLMs in performing various sentiment analysis tasks,\nfrom conventional sentiment classification to aspect-based sentiment analysis\nand multifaceted analysis of subjective texts. We evaluate performance across\n13 tasks on 26 datasets and compare the results against small language models\n(SLMs) trained on domain-specific datasets. Our study reveals that while LLMs\ndemonstrate satisfactory performance in simpler tasks, they lag behind in more\ncomplex tasks requiring deeper understanding or structured sentiment\ninformation. However, LLMs significantly outperform SLMs in few-shot learning\nsettings, suggesting their potential when annotation resources are limited. We\nalso highlight the limitations of current evaluation practices in assessing\nLLMs' SA abilities and propose a novel benchmark, \\textsc{SentiEval}, for a\nmore comprehensive and realistic evaluation. Data and code during our\ninvestigations are available at\n\\url{https://github.com/DAMO-NLP-SG/LLM-Sentiment}.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:45:25 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15006","submitter":"Nicola Leschke","authors":"Michael Gebauer, Faraz Maschhur, Nicola Leschke, Elias Gr\\\"unewald,\n  Frank Pallas","title":"A Human-in-the-Loop Approach for Information Extraction from Privacy\n  Policies under Data Scarcity","comments":"Accepted for 2023 IEEE European Symposium on Security and Privacy\n  Workshops (EuroS&P)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Machine-readable representations of privacy policies are door openers for a\nbroad variety of novel privacy-enhancing and, in particular,\ntransparency-enhancing technologies (TETs). In order to generate such\nrepresentations, transparency information needs to be extracted from written\nprivacy policies. However, respective manual annotation and extraction\nprocesses are laborious and require expert knowledge. Approaches for fully\nautomated annotation, in turn, have so far not succeeded due to overly high\nerror rates in the specific domain of privacy policies. In the end, a lack of\nproperly annotated privacy policies and respective machine-readable\nrepresentations persists and enduringly hinders the development and\nestablishment of novel technical approaches fostering policy perception and\ndata subject informedness.\n  In this work, we present a prototype system for a `Human-in-the-Loop'\napproach to privacy policy annotation that integrates ML-generated suggestions\nand ultimately human annotation decisions. We propose an ML-based suggestion\nsystem specifically tailored to the constraint of data scarcity prevalent in\nthe domain of privacy policy annotation. On this basis, we provide meaningful\npredictions to users thereby streamlining the annotation process. Additionally,\nwe also evaluate our approach through a prototypical implementation to show\nthat our ML-based extraction approach provides superior performance over other\nrecently used extraction models for legal documents.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:45:26 GMT"},{"version":"v2","created":"Wed, 31 May 2023 09:58:15 GMT"}],"update_date":"2023-06-01"}
{"id":"2305.15007","submitter":"Jacopo Giordano","authors":"Jacopo Giordano, Angelo Cenedese","title":"Quaternion-based non-singular terminal sliding mode control for a\n  satellite-mounted space manipulator","comments":"New figures and some text clarifications","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, a robust control solution for a satellite equipped with a\nrobotic manipulator is presented. First, the dynamic model of the system is\nderived based on quaternions to describe the evolution of the attitude of the\nbase satellite. Then, a non-singular terminal sliding mode controller that\nemploys quaternions for attitude control, is proposed for concurrently handling\nall the degrees of freedom of the space manipulator. Moreover, an additional\nadaptive term is embedded in the controller to estimate the upper bounds of\ndisturbances and uncertainties. The result is a resilient solution able to\nwithstand unmodelled dynamics and interactions. Lyapunov theory is used to\nprove the stability of the controller and numerical simulations allow assessing\nperformance and fuel efficiency.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:46:42 GMT"},{"version":"v2","created":"Thu, 25 May 2023 13:27:53 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.15008","submitter":"Aman Priyanshu","authors":"Aman Priyanshu, Supriti Vijay, Ayush Kumar, Rakshit Naidu and\n  Fatemehsadat Mireshghallah","title":"Are Chatbots Ready for Privacy-Sensitive Applications? An Investigation\n  into Input Regurgitation and Prompt-Induced Sanitization","comments":"12 pages, 9 figures, and 4 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CY","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  LLM-powered chatbots are becoming widely adopted in applications such as\nhealthcare, personal assistants, industry hiring decisions, etc. In many of\nthese cases, chatbots are fed sensitive, personal information in their prompts,\nas samples for in-context learning, retrieved records from a database, or as\npart of the conversation. The information provided in the prompt could directly\nappear in the output, which might have privacy ramifications if there is\nsensitive information there. As such, in this paper, we aim to understand the\ninput copying and regurgitation capabilities of these models during inference\nand how they can be directly instructed to limit this copying by complying with\nregulations such as HIPAA and GDPR, based on their internal knowledge of them.\nMore specifically, we find that when ChatGPT is prompted to summarize cover\nletters of a 100 candidates, it would retain personally identifiable\ninformation (PII) verbatim in 57.4% of cases, and we find this retention to be\nnon-uniform between different subgroups of people, based on attributes such as\ngender identity. We then probe ChatGPT's perception of privacy-related policies\nand privatization mechanisms by directly instructing it to provide compliant\noutputs and observe a significant omission of PII from output.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:48:05 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15009","submitter":"Elena Losero","authors":"Elena Losero, Valentin Goblot, Yuchun Zhu, Hossein Babashah, Victor\n  Boureau, Florian Burkart, and Christophe Galland","title":"Creation of NV centers in diamond under 155 MeV electron irradiation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph physics.app-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Single-crystal diamond substrates presenting a high concentration of\nnegatively charged nitrogen-vacancy centers (NV-) are on high demand for the\ndevelopment of optically pumped solid-state sensors such as magnetometers,\nthermometers or electrometers. While nitrogen impurities can be easily\nincorporated during crystal growth, the creation of vacancies requires further\ntreatment. Electron irradiation and annealing is often chosen in this context,\noffering advantages with respect to irradiation by heavier particles that\nnegatively affect the crystal lattice structure and consequently the NV-\noptical and spin properties. A thorough investigation of electron irradiation\npossibilities is needed to optimize the process and improve the sensitivity of\nNV-based sensors. In this work we examine the effect of electron irradiation in\na previously unexplored regime: extremely high energy electrons, at 155 MeV. We\ndevelop a simulation model to estimate the concentration of created vacancies\nand experimentally demonstrate an increase of NV- concentration by more than 3\norders of magnitude following irradiation of a nitrogen-rich HPHT diamond over\na very large sample volume, which translates into an important gain in\nsensitivity. Moreover, we discuss the impact of electron irradiation in this\npeculiar regime on other figures of merits relevant for NV sensing, i.e. charge\nstate conversion efficiency and spin relaxation time. Finally, the effect of\nextremely high energy irradiation is compared with the more conventional low\nenergy irradiation process, employing 200 keV electrons from a transmission\nelectron microscope, for different substrates and irradiation fluences,\nevidencing sixty-fold higher yield of vacancy creation per electron at 155 MeV.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:48:47 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15010","submitter":"Benyou Wang","authors":"Hongbo Zhang and Xiang Wan and Benyou Wang","title":"Injecting Knowledge into Biomedical Pre-trained Models via Polymorphism\n  and Synonymous Substitution","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Pre-trained language models (PLMs) were considered to be able to store\nrelational knowledge present in the training data. However, some relational\nknowledge seems to be discarded unsafely in PLMs due to \\textbf{report bias}:\nlow-frequency relational knowledge might be underexpressed compared to\nhigh-frequency one in PLMs. This gives us a hint that relational knowledge\nmight not be redundant to the stored knowledge of PLMs, but rather be\ncomplementary. To additionally inject relational knowledge into PLMs, we\npropose a simple-yet-effective approach to inject relational knowledge into\nPLMs, which is inspired by three observations (namely, polymorphism, synonymous\nsubstitution, and association). In particular, we switch entities in the\ntraining corpus to related entities (either hypernyms/hyponyms/synonyms, or\narbitrarily-related concepts). Experimental results show that the proposed\napproach could not only better capture relational knowledge, but also improve\nthe performance in various biomedical downstream tasks. Our model is available\nin \\url{https://github.com/StevenZHB/BioPLM_InjectingKnowledge}.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:48:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15011","submitter":"Fajri Koto","authors":"Haonan Li and Fajri Koto and Minghao Wu and Alham Fikri Aji and\n  Timothy Baldwin","title":"Bactrian-X : A Multilingual Replicable Instruction-Following Model with\n  Low-Rank Adaptation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Instruction tuning has shown great promise in the field of natural language\nprocessing. However, the research on multilingual instruction tuning has been\nlimited due to the scarcity of high-quality instruction-response datasets. To\naddress this gap, we present Bactrian-X, a comprehensive multilingual parallel\ndataset of 3.4 million instruction-response pairs across 52 languages.\nLeveraging this dataset, we train a set of adapters using low-rank adaptation\n(LoRA), which are lightweight components seamlessly integrated with\nfoundational models. These adapters have a significantly smaller parameter\ncount than the base model, making them easily replaceable and usable as\nplug-ins for different languages or language groups. Through extensive\nexperiments on 52 languages, we demonstrate the superior performance of our\nmodels in various multilingual evaluation settings. Our proposed models\noutperform both the vanilla models and the existing instruction-tuned models.\nThe code and models are publicly available at\nhttps://github.com/mbzuai-nlp/bactrian-x.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:50:31 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15012","submitter":"Mir Alimuddin","authors":"Jitendra Joshi, Mir Alimuddin, T S Mahesh, Manik Banik","title":"Experimental Verification of Many-Body Entanglement Using Thermodynamic\n  Quantities","comments":"4.5 pages (two-column) + 9.5 pages (one-column) + 5 figures; Comments\n  are welcome","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.other","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The phenomenon of quantum entanglement underlies several important protocols\nthat enable emerging quantum technologies. Being an extremely delicate resource\nentangled states easily get perturbed by their external environment, and thus\nmakes the question of entanglement certification immensely crucial for\nsuccessful implementation of the protocols involving entanglement. In this\nwork, we propose a set of entanglement criteria for multi-qubit systems that\ncan be easily verified by measuring certain thermodynamic quantities. In\nparticular, the criteria depend on the difference in optimal works extractable\nfrom an isolated quantum system under global and local interactions,\nrespectively. As a proof of principle, we demonstrate the proposed\nthermodynamic criteria on nuclear spin registers of up to 10 qubits using\nNuclear Magnetic Resonance architecture. We prepare noisy\nGreenberger-Horne-Zeilinger class of states in star-topology systems and\ncertify their entanglement through our proposed criteria. We also provide\nelegant means of entanglement certification in many-body systems when only\npartial or even no knowledge about the state is available.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:52:35 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15013","submitter":"Linxuan Pan","authors":"Linxuan Pan, Shenghui Song","title":"Local SGD Accelerates Convergence by Exploiting Second Order Information\n  of the Loss Function","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  With multiple iterations of updates, local statistical gradient descent\n(L-SGD) has been proven to be very effective in distributed machine learning\nschemes such as federated learning. In fact, many innovative works have shown\nthat L-SGD with independent and identically distributed (IID) data can even\noutperform SGD. As a result, extensive efforts have been made to unveil the\npower of L-SGD. However, existing analysis failed to explain why the multiple\nlocal updates with small mini-batches of data (L-SGD) can not be replaced by\nthe update with one big batch of data and a larger learning rate (SGD). In this\npaper, we offer a new perspective to understand the strength of L-SGD. We\ntheoretically prove that, with IID data, L-SGD can effectively explore the\nsecond order information of the loss function. In particular, compared with\nSGD, the updates of L-SGD have much larger projection on the eigenvectors of\nthe Hessian matrix with small eigenvalues, which leads to faster convergence.\nUnder certain conditions, L-SGD can even approach the Newton method. Experiment\nresults over two popular datasets validate the theoretical results.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:54:45 GMT"},{"version":"v2","created":"Fri, 26 May 2023 05:18:28 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.15014","submitter":"Xingxuan Li","authors":"Xingxuan Li, Liying Cheng, Qingyu Tan, Hwee Tou Ng, Shafiq Joty,\n  Lidong Bing","title":"Unlocking Temporal Question Answering for Large Language Models Using\n  Code Execution","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Large language models (LLMs) have made significant progress in natural\nlanguage processing (NLP), and are utilized extensively in various\napplications. Recent works, such as chain-of-thought (CoT), have shown that\nintermediate reasoning steps can improve the performance of LLMs for complex\nreasoning tasks, such as math problems and symbolic question-answering tasks.\nHowever, we notice the challenge that LLMs face when it comes to temporal\nreasoning. Our preliminary experiments show that generating intermediate\nreasoning steps does not always boost the performance of complex temporal\nquestion-answering tasks. Therefore, we propose a novel framework that combines\nthe extraction capability of LLMs and the logical reasoning capability of a\nPython solver to tackle this issue. Extensive experiments and analysis\ndemonstrate the effectiveness of our framework in handling intricate time-bound\nreasoning tasks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:57:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15015","submitter":"Daniel Reich","authors":"Daniel Reich, Felix Putze, Tanja Schultz","title":"Measuring Faithful and Plausible Visual Grounding in VQA","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Metrics for Visual Grounding (VG) in Visual Question Answering (VQA) systems\nprimarily aim to measure a system's reliance on relevant parts of the image\nwhen inferring an answer to the given question. Lack of VG has been a common\nproblem among state-of-the-art VQA systems and can manifest in over-reliance on\nirrelevant image parts or a disregard for the visual modality entirely.\nAlthough inference capabilities of VQA models are often illustrated by a few\nqualitative illustrations, most systems are not quantitatively assessed for\ntheir VG properties. We believe, an easily calculated criterion for\nmeaningfully measuring a system's VG can help remedy this shortcoming, as well\nas add another valuable dimension to model evaluations and analysis. To this\nend, we propose a new VG metric that captures if a model a) identifies\nquestion-relevant objects in the scene, and b) actually relies on the\ninformation contained in the relevant objects when producing its answer, i.e.,\nif its visual grounding is both \"faithful\" and \"plausible\". Our metric, called\n\"Faithful and Plausible Visual Grounding\" (FPVG), is straightforward to\ndetermine for most VQA model designs.\n  We give a detailed description of FPVG and evaluate several reference systems\nspanning various VQA architectures. Code to support the metric calculations on\nthe GQA data set is available on GitHub.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:58:02 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15016","submitter":"Kostis Gourgoulias","authors":"Najah Ghalyan, Kostis Gourgoulias, Yash Satsangi, Sean Moran, Maxime\n  Labonne, Joseph Sabelja","title":"An Unsupervised Method for Estimating Class Separability of Datasets\n  with Application to LLMs Fine-Tuning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  This paper proposes an unsupervised method that leverages topological\ncharacteristics of data manifolds to estimate class separability of the data\nwithout requiring labels. Experiments conducted in this paper on several\ndatasets demonstrate a clear correlation and consistency between the class\nseparability estimated by the proposed method with supervised metrics like\nFisher Discriminant Ratio~(FDR) and cross-validation of a classifier, which\nboth require labels. This can enable implementing learning paradigms aimed at\nlearning from both labeled and unlabeled data, like semi-supervised and\ntransductive learning. This would be particularly useful when we have limited\nlabeled data and a relatively large unlabeled dataset that can be used to\nenhance the learning process. The proposed method is implemented for language\nmodel fine-tuning with automated stopping criterion by monitoring class\nseparability of the embedding-space manifold in an unsupervised setting. The\nproposed methodology has been first validated on synthetic data, where the\nresults show a clear consistency between class separability estimated by the\nproposed method and class separability computed by FDR. The method has been\nalso implemented on both public and internal data. The results show that the\nproposed method can effectively aid -- without the need for labels -- a\ndecision on when to stop or continue the fine-tuning of a language model and\nwhich fine-tuning iteration is expected to achieve a maximum classification\nperformance through quantification of the class separability of the embedding\nmanifold.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:58:09 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15017","submitter":"Marek Kadl\\v{c}\\'ik","authors":"Marek Kadl\\v{c}\\'ik, Michal \\v{S}tef\\'anik","title":"Calc-X: Enriching Arithmetical Chain-of-Thoughts Datasets by Interaction\n  with Symbolic Systems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  This report overviews our ongoing work in enriching chain-of-thoughts\ndatasets requiring arithmetical reasoning with the integration of\nnon-parametric components, such as a calculator. We conduct an analysis of\nprominent relevant datasets such as GSM8K, Ape210K, AQuA-RAT, and MathQA and\npropose a machine-processable HTML-like format specifically tailored for\nworking with semi-structured chains. By converting the datasets into this\nunified format, we enable the effective integration of large language models\nand symbolic systems, empowering them to tackle arithmetical reasoning tasks\nmore efficiently.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:58:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15018","submitter":"Xiang-Bin Wang","authors":"Jian Leng, Fan Yang, Xiang-Bin Wang","title":"Modifying $n$-qubit controlled-$ZX$ gate to be $n$-qubit Toffoli gate","comments":"6 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The decomposition for controlled-$ZX$ gate in [Phys. Rev. A, 87, 062318\n(2013)] has a shallow circuit depth $8n-20$ with no ancilla. Here we modify\nthis decomposition to decompose $n$-qubit Toffoli gate with only $2n-3$\nadditional single-qubit gates. The circuit depth is unchanged and no ancilla is\nneeded. We explicitly show that the circuit after decomposition can be easily\nconstructed in present physical systems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:58:54 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15019","submitter":"Anurag Dey","authors":"Anurag Dey and Probal Chaudhuri","title":"A comparison of estimators of mean and its functions in finite\n  populations","comments":null,"journal-ref":null,"doi":"10.5705/ss.202022.0181","report-no":null,"categories":"math.ST stat.ME stat.TH","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  Several well known estimators of finite population mean and its functions are\ninvestigated under some standard sampling designs. Such functions of mean\ninclude the variance, the correlation coefficient and the regression\ncoefficient in the population as special cases. We compare the performance of\nthese estimators under different sampling designs based on their asymptotic\ndistributions. Equivalence classes of estimators under different sampling\ndesigns are constructed so that estimators in the same class have equivalent\nperformance in terms of asymptotic mean squared errors (MSEs). Estimators in\ndifferent equivalence classes are then compared under some superpopulations\nsatisfying linear models. It is shown that the pseudo empirical likelihood\n(PEML) estimator of the population mean under simple random sampling without\nreplacement (SRSWOR) has the lowest asymptotic MSE among all the estimators\nunder different sampling designs considered in this paper. It is also shown\nthat for the variance, the correlation coefficient and the regression\ncoefficient of the population, the plug-in estimators based on the PEML\nestimator have the lowest asymptotic MSEs among all the estimators considered\nin this paper under SRSWOR. On the other hand, for any high entropy $\\pi$PS\n(HE$\\pi$PS) sampling design, which uses the auxiliary information, the plug-in\nestimators of those parameters based on the H\\'ajek estimator have the lowest\nasymptotic MSEs among all the estimators considered in this paper.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:00:07 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15020","submitter":"Asahi Ushio","authors":"Asahi Ushio and Yi Zhou and Jose Camacho-Collados","title":"An Efficient Multilingual Language Model Compression through Vocabulary\n  Trimming","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Multilingual language model (LM) have become a powerful tool in NLP\nespecially for non-English languages. Nevertheless, model parameters of\nmultilingual LMs remain large due to the larger embedding matrix of the\nvocabulary covering tokens in different languages. On the contrary, monolingual\nLMs can be trained in a target language with the language-specific vocabulary\nonly, but this requires a large budget and availability of reliable corpora to\nachieve a high-quality LM from scratch. In this paper, we propose\nvocabulary-trimming (VT), a method to reduce a multilingual LM vocabulary to a\ntarget language by deleting irrelevant tokens from its vocabulary. In theory,\nVT can compress any existing multilingual LM to build monolingual LMs in any\nlanguage covered by the multilingual LM. In our experiments, we show that VT\ncan retain the original performance of the multilingual LM, while being smaller\nin size (in general around 50% of the original vocabulary size is enough) than\nthe original multilingual LM. The evaluation is performed over four NLP tasks\n(two generative and two classification tasks) among four widely used\nmultilingual LMs in seven languages. Finally, we show that this methodology can\nkeep the best of both monolingual and multilingual worlds by keeping a small\nsize as monolingual models without the need for specifically retraining them,\nand even limiting potentially harmful social biases.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:00:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15021","submitter":"Yao Mu Mark","authors":"Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun\n  Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo","title":"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI cs.CV cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Embodied AI is a crucial frontier in robotics, capable of planning and\nexecuting action sequences for robots to accomplish long-horizon tasks in\nphysical environments. In this work, we introduce EmbodiedGPT, an end-to-end\nmulti-modal foundation model for embodied AI, empowering embodied agents with\nmulti-modal understanding and execution capabilities. To achieve this, we have\nmade the following efforts: (i) We craft a large-scale embodied planning\ndataset, termed EgoCOT. The dataset consists of carefully selected videos from\nthe Ego4D dataset, along with corresponding high-quality language instructions.\nSpecifically, we generate a sequence of sub-goals with the \"Chain of Thoughts\"\nmode for effective embodied planning. (ii) We introduce an efficient training\napproach to EmbodiedGPT for high-quality plan generation, by adapting a 7B\nlarge language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We\nintroduce a paradigm for extracting task-related features from LLM-generated\nplanning queries to form a closed loop between high-level planning and\nlow-level control. Extensive experiments show the effectiveness of EmbodiedGPT\non embodied tasks, including embodied planning, embodied control, visual\ncaptioning, and visual question answering. Notably, EmbodiedGPT significantly\nenhances the success rate of the embodied control task by extracting more\neffective features. It has achieved a remarkable 1.6 times increase in success\nrate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World\nbenchmark, compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:04:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15022","submitter":"Nick Whiteley Prof.","authors":"Annie Gray, Alexander Modell, Patrick Rubin-Delanchy, Nick Whiteley","title":"Hierarchical clustering with dot products recovers hidden tree structure","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper we offer a new perspective on the well established\nagglomerative clustering algorithm, focusing on recovery of hierarchical\nstructure. We recommend a simple variant of the standard algorithm, in which\nclusters are merged by maximum average dot product and not, for example, by\nminimum distance or within-cluster variance. We demonstrate that the tree\noutput by this algorithm provides a bona fide estimate of generative\nhierarchical structure in data, under a generic probabilistic graphical model.\nThe key technical innovations are to understand how hierarchical information in\nthis model translates into tree geometry which can be recovered from data, and\nto characterise the benefits of simultaneously growing sample size and data\ndimension. We demonstrate superior tree recovery performance with real data\nover existing approaches such as UPGMA, Ward's method, and HDBSCAN.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:05:12 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15023","submitter":"Gen Luo","authors":"Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, Rongrong\n  Ji","title":"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large\n  Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Recently, growing interest has been aroused in extending the multimodal\ncapability of large language models (LLMs), e.g., vision-language (VL)\nlearning, which is regarded as the next milestone of artificial general\nintelligence. However, existing solutions are prohibitively expensive, which\nnot only need to optimize excessive parameters, but also require another\nlarge-scale pre-training before VL instruction tuning. In this paper, we\npropose a novel and affordable solution for the effective VL adaption of LLMs,\ncalled Mixture-of-Modality Adaptation (MMA). Instead of using large neural\nnetworks to connect the image encoder and LLM, MMA adopts lightweight modules,\ni.e., adapters, to bridge the gap between LLMs and VL tasks, which also enables\nthe joint optimization of the image and language models. Meanwhile, MMA is also\nequipped with a routing algorithm to help LLMs achieve an automatic shift\nbetween single- and multi-modal instructions without compromising their ability\nof natural language understanding. To validate MMA, we apply it to a recent LLM\ncalled LLaMA and term this formed large vision-language instructed model as\nLaVIN. To validate MMA and LaVIN, we conduct extensive experiments under two\nsetups, namely multimodal science question answering and multimodal dialogue.\nThe experimental results not only demonstrate the competitive performance and\nthe superior training efficiency of LaVIN than existing multimodal LLMs, but\nalso confirm its great potential as a general-purpose chatbot. More\nimportantly, the actual expenditure of LaVIN is extremely cheap, e.g., only 1.4\ntraining hours with 3.8M trainable parameters, greatly confirming the\neffectiveness of MMA. Our project is released at\nhttps://luogen1996.github.io/lavin.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:06:15 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15024","submitter":"Weiqiang Jin","authors":"Biao Zhao, Weiqiang Jin, Javier Del Ser, Guang Yang","title":"ChatAgri: Exploring Potentials of ChatGPT on Cross-linguistic\n  Agricultural Text Classification","comments":"24 pages,10+figures,46references.Both the first two authors, Biao\n  Zhao and Weiqiang Jin, made equal contributions to this work. Corresponding\n  author: Guang Yang","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  In the era of sustainable smart agriculture, a massive amount of agricultural\nnews text is being posted on the Internet, in which massive agricultural\nknowledge has been accumulated. In this context, it is urgent to explore\neffective text classification techniques for users to access the required\nagricultural knowledge with high efficiency. Mainstream deep learning\napproaches employing fine-tuning strategies on pre-trained language models\n(PLMs), have demonstrated remarkable performance gains over the past few years.\nNonetheless, these methods still face many drawbacks that are complex to solve,\nincluding: 1. Limited agricultural training data due to the expensive-cost and\nlabour-intensive annotation; 2. Poor domain transferability, especially of\ncross-linguistic ability; 3. Complex and expensive large models\ndeployment.Inspired by the extraordinary success brought by the recent ChatGPT\n(e.g. GPT-3.5, GPT-4), in this work, we systematically investigate and explore\nthe capability and utilization of ChatGPT applying to the agricultural\ninformatization field. ....(shown in article).... Code has been released on\nGithub\nhttps://github.com/albert-jin/agricultural_textual_classification_ChatGPT.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:06:23 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15025","submitter":"Tianyu Yang","authors":"Tianyu Yang and Thy Thy Tran and Iryna Gurevych","title":"Dior-CVAE: Diffusion Priors in Variational Dialog Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Conditional variational autoencoders (CVAEs) have been used recently for\ndiverse response generation, by introducing latent variables to represent the\nrelationship between a dialog context and its potential responses. However, the\ndiversity of the generated responses brought by a CVAE model is limited due to\nthe oversimplified assumption of the isotropic Gaussian prior. We propose,\nDior-CVAE, a hierarchical CVAE model with an informative prior produced by a\ndiffusion model. Dior-CVAE derives a series of layer-wise latent variables\nusing attention mechanism and infusing them into decoder layers accordingly. We\npropose memory dropout in the latent infusion to alleviate posterior collapse.\nThe prior distribution of the latent variables is parameterized by a diffusion\nmodel to introduce a multimodal distribution. Overall, experiments on two\npopular open-domain dialog datasets indicate the advantages of our approach\nover previous Transformer-based variational dialog models in dialog response\ngeneration. We publicly release the code for reproducing Dior-CVAE and all\nbaselines at\nhttps://github.com/SkyFishMoon/Latent-Diffusion-Response-Generation.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:06:52 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15026","submitter":"Rodrigo Valerio","authors":"Rodrigo Valerio, Joao Bordalo, Michal Yarom, Yonatan Bitton, Idan\n  Szpektor, Joao Magalhaes","title":"Transferring Visual Attributes from Natural Language to Verified Image\n  Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Text to image generation methods (T2I) are widely popular in generating art\nand other creative artifacts. While visual hallucinations can be a positive\nfactor in scenarios where creativity is appreciated, such artifacts are poorly\nsuited for cases where the generated image needs to be grounded in complex\nnatural language without explicit visual elements. In this paper, we propose to\nstrengthen the consistency property of T2I methods in the presence of natural\ncomplex language, which often breaks the limits of T2I methods by including\nnon-visual information, and textual elements that require knowledge for\naccurate generation. To address these phenomena, we propose a Natural Language\nto Verified Image generation approach (NL2VI) that converts a natural prompt\ninto a visual prompt, which is more suitable for image generation. A T2I model\nthen generates an image for the visual prompt, which is then verified with VQA\nalgorithms. Experimentally, aligning natural prompts with image generation can\nimprove the consistency of the generated images by up to 11% over the state of\nthe art. Moreover, improvements can generalize to challenging domains like\ncooking and DIY tasks, where the correctness of the generated image is crucial\nto illustrate actions.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:08:26 GMT"},{"version":"v2","created":"Mon, 29 May 2023 09:34:31 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.15027","submitter":"Sahra Ghalebikesabi","authors":"Veit David Wild, Sahra Ghalebikesabi, Dino Sejdinovic, Jeremias\n  Knoblauch","title":"A Rigorous Link between Deep Ensembles and (Variational) Bayesian\n  Methods","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.AI cs.LG math.ST stat.ME stat.TH","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We establish the first mathematically rigorous link between Bayesian,\nvariational Bayesian, and ensemble methods. A key step towards this it to\nreformulate the non-convex optimisation problem typically encountered in deep\nlearning as a convex optimisation in the space of probability measures. On a\ntechnical level, our contribution amounts to studying generalised variational\ninference through the lense of Wasserstein gradient flows. The result is a\nunified theory of various seemingly disconnected approaches that are commonly\nused for uncertainty quantification in deep learning -- including deep\nensembles and (variational) Bayesian methods. This offers a fresh perspective\non the reasons behind the success of deep ensembles over procedures based on\nparameterised variational inference, and allows the derivation of new\nensembling schemes with convergence guarantees. We showcase this by proposing a\nfamily of interacting deep ensembles with direct parallels to the interactions\nof particle systems in thermodynamics, and use our theory to prove the\nconvergence of these algorithms to a well-defined global minimiser on the space\nof probability measures.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:13:59 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15028","submitter":"Qingxiu Dong","authors":"Heming Xia, Qingxiu Dong, Lei Li, Jingjing Xu, Ziwei Qin, Zhifang Sui","title":"ImageNetVC: Zero-Shot Visual Commonsense Evaluation on 1000 ImageNet\n  Categories","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Recently, Pretrained Language Models (PLMs) have been serving as\ngeneral-purpose interfaces, posing a significant demand for comprehensive\nvisual knowledge. However, it remains unclear how well current PLMs and their\nvisually augmented counterparts (VaLMs) can master visual commonsense\nknowledge. To investigate this, we propose ImageNetVC, a fine-grained,\nhuman-annotated dataset specifically designed for zero-shot visual commonsense\nevaluation across 1,000 ImageNet categories. Utilizing ImageNetVC, we delve\ninto the fundamental visual commonsense knowledge of both unimodal PLMs and\nVaLMs, uncovering the scaling law and the influence of the backbone model on\nVaLMs. Furthermore, we investigate the factors affecting the visual commonsense\nknowledge of large-scale models, providing insights into the development of\nlanguage models enriched with visual commonsense knowledge. Our code and\ndataset are available at https://github.com/hemingkx/ImageNetVC.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:14:31 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15029","submitter":"Eros Mariani","authors":"Jamie Le Signe, Thomas McDermott and Eros Mariani","title":"Tunable mechanically-induced hysteresis in suspended Josephson junctions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.supr-con cond-mat.mes-hall","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  The coupling of superconducting systems to mechanical resonators is an\nemerging field, with wide reaching implications including high precision\nsensing and metrology. Experimental signatures of this coupling have so far\nbeen small, seldom and often reliant on high frequency AC electronics. To\novercome this limitation, in this work we consider a mechanical resonator\nsuspended between two superconducting contacts to form a suspended Josephson\njunction in which the electronic normal- and super-currents can be coupled to\nmechanical motion via the Lorentz force due to an external magnetic field. We\nshow both analytically and numerically that this electro-mechanical coupling\nproduces unprecedented mechanically-induced hysteresis loops in the junction's\nDC I-V characteristic. Firstly, we unveil how this new hysteresis may be\nexploited to access a huge mechanically-induced Shapiro-like voltage plateau,\nextending over a current range comparable with the junction's critical current.\nWe then investigate a sudden mechanically-induced retrapping that occurs at\nstrong coupling. Our analytical treatment provides a clear explanation for the\neffects above and allows us to derive simple relationships between the features\nin the DC I-V characteristic and the resonance frequency and quality factor of\nthe mechanical resonator. We stress that our setup requires only DC current\nbias and voltage measurements, allowing the activation and detection of\nhigh-frequency mechanical oscillations in state of the art devices and without\nthe need of any AC equipment.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:14:32 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15030","submitter":"Shilv Cai","authors":"Shilv Cai, Xu Zou, Liqun Chen, Luxin Yan, Sheng Zhong","title":"Jointly Optimizing Image Compression with Low-light Image Enhancement","comments":"arXiv admin note: text overlap with arXiv:2303.06705 by other authors","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV eess.IV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Learning-based image compression methods have made great progress. Most of\nthem are designed for generic natural images. In fact, low-light images\nfrequently occur due to unavoidable environmental influences or technical\nlimitations, such as insufficient lighting or limited exposure time. %When\ngeneral-purpose image compression algorithms compress low-light images, useful\ndetail information is lost, resulting in a dramatic decrease in image\nenhancement. Once low-light images are compressed by existing general image\ncompression approaches, useful information(e.g., texture details) would be lost\nresulting in a dramatic performance decrease in low-light image enhancement. To\nsimultaneously achieve a higher compression rate and better enhancement\nperformance for low-light images, we propose a novel image compression\nframework with joint optimization of low-light image enhancement. We design an\nend-to-end trainable two-branch architecture with lower computational cost,\nwhich includes the main enhancement branch and the signal-to-noise ratio~(SNR)\naware branch. Experimental results show that our proposed joint optimization\nframework achieves a significant improvement over existing ``Compress before\nEnhance\" or ``Enhance before Compress\" sequential solutions for low-light\nimages. Source codes are included in the supplementary material.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:14:40 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15031","submitter":"Fanny Kassel","authors":"Jonas Beyrer, Fanny Kassel","title":"$\\mathbb{H}^{p,q}$-convex cocompactness and higher higher Teichm\\\"uller\n  spaces","comments":"59 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.GT math.GR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  For any integers $p\\geq 2$ and $q\\geq 1$, let $\\mathbb{H}^{p,q}$ be the\npseudo-Riemannian hyperbolic space of signature $(p,q)$. We prove that if\n$\\Gamma$ is the fundamental group of a closed aspherical $p$-manifold, then the\nset of representations of $\\Gamma$ to $\\mathrm{PO}(p,q+1)$ which are convex\ncocompact in $\\mathbb{H}^{p,q}$ is a union of connected components of\n$\\mathrm{Hom}(\\Gamma,\\mathrm{PO}(p,q+1))$. This gives new examples of\nhigher-dimensional higher-rank Teichm\\\"uller spaces.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:15:37 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15032","submitter":"Xinpeng Wang","authors":"Xinpeng Wang, Leonie Weissweiler, Hinrich Sch\\\"utze, Barbara Plank","title":"How to Distill your BERT: An Empirical Study on the Impact of Weight\n  Initialisation and Distillation Objectives","comments":"ACL 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Recently, various intermediate layer distillation (ILD) objectives have been\nshown to improve compression of BERT models via Knowledge Distillation (KD).\nHowever, a comprehensive evaluation of the objectives in both task-specific and\ntask-agnostic settings is lacking. To the best of our knowledge, this is the\nfirst work comprehensively evaluating distillation objectives in both settings.\nWe show that attention transfer gives the best performance overall. We also\nstudy the impact of layer choice when initializing the student from the teacher\nlayers, finding a significant impact on the performance in task-specific\ndistillation. For vanilla KD and hidden states transfer, initialisation with\nlower layers of the teacher gives a considerable improvement over higher\nlayers, especially on the task of QNLI (up to an absolute percentage change of\n17.8 in accuracy). Attention transfer behaves consistently under different\ninitialisation settings. We release our code as an efficient transformer-based\nmodel distillation framework for further studies.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:16:09 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15033","submitter":"Zekun Wang","authors":"Zekun Wang, Jingchang Chen, Wangchunshu Zhou, Ming Liu, Bing Qin","title":"SmartTrim: Adaptive Tokens and Parameters Pruning for Efficient\n  Vision-Language Models","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Despite achieving remarkable performance on various vision-language tasks,\nTransformer-based pretrained vision-language models (VLMs) still suffer from\nefficiency issues arising from long inputs and numerous parameters, limiting\ntheir real-world applications. However, the huge computation is redundant for\nmost samples and the degree of redundancy and the respective components vary\nsignificantly depending on tasks and input instances. In this work, we propose\nan adaptive acceleration method SmartTrim for VLMs, which adjusts the inference\noverhead based on the complexity of instances. Specifically, SmartTrim\nincorporates lightweight trimming modules into the backbone to perform\ntask-specific pruning on redundant inputs and parameters, without the need for\nadditional pre-training or data augmentation. Since visual and textual\nrepresentations complement each other in VLMs, we propose to leverage\ncross-modal interaction information to provide more critical semantic guidance\nfor identifying redundant parts. Meanwhile, we introduce a self-distillation\nstrategy that encourages the trimmed model to be consistent with the\nfull-capacity model, which yields further performance gains. Experimental\nresults demonstrate that SmartTrim significantly reduces the computation\noverhead (2-3 times) of various VLMs with comparable performance (only a 1-2%\ndegradation) on various vision-language tasks. Compared to previous\nacceleration methods, SmartTrim attains a better efficiency-performance\ntrade-off, demonstrating great potential for application in\nresource-constrained scenarios.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:18:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15034","submitter":"Adam Kubica","authors":"Adam Kubica, Katarzyna Ryszewska, Rico Zacher","title":"Holder continuity of weak solutions to evolution equations with\n  distributed order fractional time derivative","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We study the regularity of weak solutions to evolution equations with\ndistributed order fractional time derivative. We prove a weak Harnack\ninequality for nonnegative weak supersolutions and H\\\"older continuity of weak\nsolutions to this problem. Our results substantially generalise analogous known\nresults for the problem with single order fractional time derivative.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:21:11 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15035","submitter":"Wei-Lin Chen","authors":"Wei-Lin Chen, Cheng-Kuang Wu, Hsin-Hsi Chen","title":"Self-ICL: Zero-Shot In-Context Learning with Self-Generated\n  Demonstrations","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Large language models (LMs) have exhibited superior in-context learning (ICL)\nability to adopt to target tasks by prompting with a few input-output\ndemonstrations. Towards better ICL, different methods are proposed to select\nrepresentative demonstrations from existing training corpora. However, such a\nsetting is not aligned with real-world practices, as end-users usually query\nLMs without accesses to demonstration pools. Inspired by evidence suggesting\nLMs' zero-shot capabilities are underrated, and the role of demonstrations are\nprimarily for exposing models' intrinsic functionalities, we introduce\nSelf-ICL, a simple framework for zero-shot ICL. Given a test input, Self-ICL\nfirst prompts the model to generate pseudo-inputs. Next, the model predicts\npseudo-labels for the pseudo-inputs via zero-shot prompting. Finally, we\nconstruct pseudo-demonstrations from pseudo-input-label pairs, and perform ICL\nfor the test input. Evaluation on BIG-Bench Hard shows Self-ICL steadily\nsurpasses zero-shot and zero-shot chain-of-thought baselines on head-to-head\nand all-task average performance. Our findings suggest the possibility to\nbootstrap LMs' intrinsic capabilities towards better zero-shot performance.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:22:34 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15036","submitter":"Junchen Fu","authors":"Junchen Fu, Fajie Yuan, Yu Song, Zheng Yuan, Mingyue Cheng, Shenghui\n  Cheng, Jiaqi Zhang, Jie Wang, Yunzhu Pan","title":"Exploring Adapter-based Transfer Learning for Recommender Systems:\n  Empirical Studies and Practical Insights","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Adapters, a plug-in neural network module with some tunable parameters, have\nemerged as a parameter-efficient transfer learning technique for adapting\npre-trained models to downstream tasks, especially for natural language\nprocessing (NLP) and computer vision (CV) fields. Meanwhile, learning\nrecommendation models directly from raw item modality features -- e.g., texts\nof NLP and images of CV -- can enable effective and transferable recommender\nsystems (called TransRec). In view of this, a natural question arises: can\nadapter-based learning techniques achieve parameter-efficient TransRec with\ngood performance?\n  To this end, we perform empirical studies to address several key\nsub-questions. First, we ask whether the adapter-based TransRec performs\ncomparably to TransRec based on standard full-parameter fine-tuning? does it\nhold for recommendation with different item modalities, e.g., textual RS and\nvisual RS. If yes, we benchmark these existing adapters, which have been shown\nto be effective in NLP and CV tasks, in the item recommendation settings.\nThird, we carefully study several key factors for the adapter-based TransRec in\nterms of where and how to insert these adapters? Finally, we look at the\neffects of adapter-based TransRec by either scaling up its source training data\nor scaling down its target training data. Our paper provides key insights and\npractical guidance on unified & transferable recommendation -- a less studied\nrecommendation scenario. We promise to release all code & datasets for future\nresearch.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:23:31 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15037","submitter":"Ulf-G. Mei{\\ss}ner","authors":"Zhengxue Ren, Serdar Elhatisari, Timo A. L\\\"ahde, Dean Lee, Ulf-G.\n  Mei{\\ss}ner","title":"Ab initio study of nuclear clustering in hot dilute nuclear matter","comments":"6+8 pages, 4+8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"nucl-th hep-ph nucl-ex","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We present a systematic ab initio study of clustering in hot dilute nuclear\nmatter using nuclear lattice effective field theory with an SU(4)-symmetric\ninteraction. We introduce a method called light-cluster distillation to\ndetermine the abundances of dimers, trimers, and alpha clusters as a function\nof density and temperature. Our lattice results are compared with an ideal gas\nmodel composed of free nucleons and clusters. Excellent agreement is found at\nvery low density, while deviations from ideal gas abundances appear at\nincreasing density due to cluster-nucleon and cluster-cluster interactions. In\naddition to determining the composition of hot dilute nuclear matter as a\nfunction of density and temperature, the lattice calculations also serve as\nbenchmarks for virial expansion calculations, statistical models, and transport\nmodels of fragmentation and clustering in nucleus-nucleus collisions.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:26:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15038","submitter":"Liying Cheng","authors":"Liying Cheng, Xingxuan Li, Lidong Bing","title":"Is GPT-4 a Good Data Analyst?","comments":"11 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  As large language models (LLMs) have demonstrated their powerful capabilities\nin plenty of domains and tasks, including context understanding, code\ngeneration, language generation, data storytelling, etc., many data analysts\nmay raise concerns if their jobs will be replaced by AI. This controversial\ntopic has drawn a lot of attention in public. However, we are still at a stage\nof divergent opinions without any definitive conclusion. Motivated by this, we\nraise the research question of \"is GPT-4 a good data analyst?\" in this work and\naim to answer it by conducting head-to-head comparative studies. In detail, we\nregard GPT-4 as a data analyst to perform end-to-end data analysis with\ndatabases from a wide range of domains. We propose a framework to tackle the\nproblems by carefully designing the prompts for GPT-4 to conduct experiments.\nWe also design several task-specific evaluation metrics to systematically\ncompare the performance between several professional human data analysts and\nGPT-4. Experimental results show that GPT-4 can achieve comparable performance\nto humans. We also provide in-depth discussions about our results to shed light\non further studies before we reach the conclusion that GPT-4 can replace data\nanalysts.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:26:59 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15039","submitter":"Irina Molodtsova","authors":"E.B. Balbutsev, I.V. Molodtsova","title":"Electric $1^+$ state below nuclear scissors","comments":"20 pages, 6 figures, 2 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"nucl-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The solution of time dependent Hartree-Fock-Bogoliubov equations by the\nWigner function moments method predicts four low-lying $1^+$ states. Three of\nthem are known as various scissors modes. Fourth state is disposed below all\nscissors modes and has the electrical nature. It is found that it represents\none of three branches of $2^+$ state which can exist in spherical nuclei and\nwhich is split %due to a deformation. in deformed nuclei. It is discovered,\nthat the antiferromagnetic properties of nuclei lead to the splitting of $2^+$\nstates already at the zero deformation.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:27:18 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15040","submitter":"Yotam Perlitz","authors":"Yotam Perlitz and Ariel Gera, Michal Shmueli-Scheuer, Dafna Sheinwald,\n  Noam Slonim, Liat Ein-Dor","title":"Active Learning for Natural Language Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  The field of text generation suffers from a severe shortage of labeled data\ndue to the extremely expensive and time consuming process involved in manual\nannotation. A natural approach for coping with this problem is active learning\n(AL), a well-known machine learning technique for improving annotation\nefficiency by selectively choosing the most informative examples to label.\nHowever, while AL has been well-researched in the context of text\nclassification, its application to text generation remained largely unexplored.\nIn this paper, we present a first systematic study of active learning for text\ngeneration, considering a diverse set of tasks and multiple leading AL\nstrategies. Our results indicate that existing AL strategies, despite their\nsuccess in classification, are largely ineffective for the text generation\nscenario, and fail to consistently surpass the baseline of random example\nselection. We highlight some notable differences between the classification and\ngeneration scenarios, and analyze the selection behaviors of existing AL\nstrategies. Our findings motivate exploring novel approaches for applying AL to\nNLG tasks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:27:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15041","submitter":"Veniamin Veselovsky","authors":"Veniamin Veselovsky, Manoel Horta Ribeiro, Akhil Arora, Martin\n  Josifoski, Ashton Anderson, Robert West","title":"Generating Faithful Synthetic Data with Large Language Models: A Case\n  Study in Computational Social Science","comments":"8 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large Language Models (LLMs) have democratized synthetic data generation,\nwhich in turn has the potential to simplify and broaden a wide gamut of NLP\ntasks. Here, we tackle a pervasive problem in synthetic data generation: its\ngenerative distribution often differs from the distribution of real-world data\nresearchers care about (in other words, it is unfaithful). In a case study on\nsarcasm detection, we study three strategies to increase the faithfulness of\nsynthetic data: grounding, filtering, and taxonomy-based generation. We\nevaluate these strategies using the performance of classifiers trained with\ngenerated synthetic data on real-world data. While all three strategies improve\nthe performance of classifiers, we find that grounding works best for the task\nat hand. As synthetic data generation plays an ever-increasing role in NLP\nresearch, we expect this work to be a stepping stone in improving its utility.\nWe conclude this paper with some recommendations on how to generate\nhigh(er)-fidelity synthetic data for specific tasks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:27:59 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15042","submitter":"Zaccharie Ramzi","authors":"Zaccharie Ramzi, Pierre Ablin, Gabriel Peyr\\'e, Thomas Moreau","title":"Test like you Train in Implicit Deep Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Implicit deep learning has recently gained popularity with applications\nranging from meta-learning to Deep Equilibrium Networks (DEQs). In its general\nformulation, it relies on expressing some components of deep learning pipelines\nimplicitly, typically via a root equation called the inner problem. In\npractice, the solution of the inner problem is approximated during training\nwith an iterative procedure, usually with a fixed number of inner iterations.\nDuring inference, the inner problem needs to be solved with new data. A popular\nbelief is that increasing the number of inner iterations compared to the one\nused during training yields better performance. In this paper, we question such\nan assumption and provide a detailed theoretical analysis in a simple setting.\nWe demonstrate that overparametrization plays a key role: increasing the number\nof iterations at test time cannot improve performance for overparametrized\nnetworks. We validate our theory on an array of implicit deep-learning\nproblems. DEQs, which are typically overparametrized, do not benefit from\nincreasing the number of iterations at inference while meta-learning, which is\ntypically not overparametrized, benefits from it.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:30:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15043","submitter":"Jacob Holford","authors":"Jacob Holford, Myoungkyu Lee, Yongyun Hwang","title":"A data-driven quasi-linear approximation for turbulent channel flow","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.flu-dyn","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  A data-driven implementation of a quasi-linear approximation is presented,\nextending a minimal quasi-linear approximation (MQLA) (Hwang & Ekchardt, J.\nFluid Mech., 2020, 894:A23) to incorporate non-zero streamwise Fourier modes. A\ndata-based approach is proposed, matching the two-dimensional wavenumber\nspectra for a fixed spanwise wavenumber between a direct numerical simulation\n(DNS) (Lee & Moser, J. Fluid Mech., 2015, 774:395-415) and that generated by\nthe eddy viscosity-enhanced linearised Navier-Stokes equations at $Re{\\tau}\n\\simeq 5200$. Leveraging the self-similar nature of the energy-containing part\nin the DNS velocity spectra, a universal self-similar streamwise wavenumber\nweight is determined for the linearised fluctuation equations at $Re_{\\tau}\n\\simeq 5200$. This data-driven quasi-linear approximation (DQLA) offers\nqualitatively similar findings to the MQLA, with quantitative improvements in\nthe turbulence intensities and additional insights from the streamwise\nwavenumber spectra. By comparing the one-dimensional streamwise wavenumber\nspectra and two-dimensional spectra to DNS results, the limitations of the\npresented framework are discussed, mainly pertaining to the lack of the streak\ninstability (or transient growth) mechanism and energy cascade from the\nlinearised model. The DQLA is subsequently employed over a range of Reynolds\nnumbers up to $Re_{\\tau} = 10^5$. Overall, the turbulence statistics and\nspectra produced by the DQLA scale consistently with the available DNS and\nexperimental data, with the Townsend-Perry constants displaying a mild Reynolds\ndependence (Hwang, Hutchins & Marusic, J. Fluid Mech., 2022, 933:A8). The\nscaling behaviour of the turbulence intensity profiles deviates away from the\nclassic $\\ln(Re_{\\tau})$ scaling, following the inverse centreline velocity\nscaling for the higher Reynolds numbers.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:31:56 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15044","submitter":"Xiao Pu","authors":"Xiao Pu, Mingqi Gao, Xiaojun Wan","title":"Is Summary Useful or Not? An Extrinsic Human Evaluation of Text\n  Summaries on Downstream Tasks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Research on automated text summarization relies heavily on human and\nautomatic evaluation. While recent work on human evaluation mainly adopted\nintrinsic evaluation methods, judging the generic quality of text summaries,\ne.g. informativeness and coherence, our work focuses on evaluating the\nusefulness of text summaries with extrinsic methods. We carefully design three\ndifferent downstream tasks for extrinsic human evaluation of summaries, i.e.,\nquestion answering, text classification and text similarity assessment. We\ncarry out experiments using system rankings and user behavior data to evaluate\nthe performance of different summarization models. We find summaries are\nparticularly useful in tasks that rely on an overall judgment of the text,\nwhile being less effective for question answering tasks. The results show that\nsummaries generated by fine-tuned models lead to higher consistency in\nusefulness across all three tasks, as rankings of fine-tuned summarization\nsystems are close across downstream tasks according to the proposed extrinsic\nmetrics. Summaries generated by models in the zero-shot setting, however, are\nfound to be biased towards the text classification and similarity assessment\ntasks, due to its general and less detailed summary style. We further evaluate\nthe correlation of 14 intrinsic automatic metrics with human criteria and show\nthat intrinsic automatic metrics perform well in evaluating the usefulness of\nsummaries in the question-answering task, but are less effective in the other\ntwo tasks. This highlights the limitations of relying solely on intrinsic\nautomatic metrics in evaluating the performance and usefulness of summaries.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:34:39 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15045","submitter":"Xiyan Fu","authors":"Xiyan Fu, Anette Frank","title":"SETI: Systematicity Evaluation of Textual Inference","comments":"Accepted to Findings of ACL2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We propose SETI (Systematicity Evaluation of Textual Inference), a novel and\ncomprehensive benchmark designed for evaluating pre-trained language models\n(PLMs) for their systematicity capabilities in the domain of textual inference.\nSpecifically, SETI offers three different NLI tasks and corresponding datasets\nto evaluate various types of systematicity in reasoning processes. In order to\nsolve these tasks, models are required to perform compositional inference based\non known primitive constituents. We conduct experiments of SETI on six widely\nused PLMs. Results show that various PLMs are able to solve unseen\ncompositional inferences when having encountered the knowledge of how to\ncombine primitives, with good performance. However, they are considerably\nlimited when this knowledge is unknown to the model (40-100% points decrease).\nFurthermore, we find that PLMs can improve drastically once exposed to crucial\ncompositional knowledge in minimalistic shots. These findings position SETI as\nthe first benchmark for measuring the future progress of PLMs in achieving\nsystematicity generalization in the textual inference.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:35:31 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15046","submitter":"Yanbo Hu","authors":"Geng Chen, Yanbo Hu, Qingtian Zhang","title":"Initial-boundary value problems for Poiseuille flow of nematic liquid\n  crystal via full Ericksen-Leslie model","comments":"36 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper, we study the initial-boundary value problem for the Poiseuille\nflow of hyperbolic-parabolic Ericksen-Leslie model of nematic liquid crystals\nin one space dimension. Due to the quasilinearity, the solution of this model\nin general forms cusp singularity. We prove the global existence of H\\\"older\ncontinuous solution, which may include cusp singularity, for initial-boundary\nvalue problems with different types of boundary conditions.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:35:56 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15047","submitter":"Nicholas Tomlin","authors":"Vivek Verma, Eve Fleisig, Nicholas Tomlin, Dan Klein","title":"Ghostbuster: Detecting Text Ghostwritten by Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We introduce Ghostbuster, a state-of-the-art system for detecting\nAI-generated text. Our method works by passing documents through a series of\nweaker language models and running a structured search over possible\ncombinations of their features, then training a classifier on the selected\nfeatures to determine if the target document was AI-generated. Crucially,\nGhostbuster does not require access to token probabilities from the target\nmodel, making it useful for detecting text generated by black-box models or\nunknown model versions. In conjunction with our model, we release three new\ndatasets of human and AI-generated text as detection benchmarks that cover\nmultiple domains (student essays, creative fiction, and news) and task setups:\ndocument-level detection, author identification, and a challenge task of\nparagraph-level detection. Ghostbuster averages 99.1 F1 across all three\ndatasets on document-level detection, outperforming previous approaches such as\nGPTZero and DetectGPT by up to 32.7 F1.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:37:10 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15048","submitter":"Mete Sertkan","authors":"Mete Sertkan, Sophia Althammer and Sebastian Hofst\\\"atter","title":"Ranger: A Toolkit for Effect-Size Based Multi-Task Evaluation","comments":"Accepted at ACL 2023 (System Demonstrations)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.IR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, we introduce Ranger - a toolkit to facilitate the easy use of\neffect-size-based meta-analysis for multi-task evaluation in NLP and IR. We\nobserved that our communities often face the challenge of aggregating results\nover incomparable metrics and scenarios, which makes conclusions and take-away\nmessages less reliable. With Ranger, we aim to address this issue by providing\na task-agnostic toolkit that combines the effect of a treatment on multiple\ntasks into one statistical evaluation, allowing for comparison of metrics and\ncomputation of an overall summary effect. Our toolkit produces\npublication-ready forest plots that enable clear communication of evaluation\nresults over multiple tasks. Our goal with the ready-to-use Ranger toolkit is\nto promote robust, effect-size-based evaluation and improve evaluation\nstandards in the community. We provide two case studies for common IR and NLP\nsettings to highlight Ranger's benefits.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:38:39 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15049","submitter":"Bobby Eka Gunara","authors":"Mulyanto, Fiki Taufik Akbar, and Bobby Eka Gunara","title":"Decay Estimate of Maxwell-Higgs System on Schwarzschild Black Holes","comments":"36 pages, no figure, comments are welcome","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP gr-qc hep-th math-ph math.MP","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper, we prove the decay estimate of Maxwell-Higgs system on four\ndimensional Schwarzschild spacetimes. We show that if the field equations\nsupport a Morawetz type estimate supported around the trapped surface, the\nuniform decay properties in the entire exterior of the Schwarzschild black\nholes can be obtained by using Sobolev inequalities and energy estimates. Our\nresults also consider various forms of physical potential such as the mass\nterms, $\\phi^4$-theory, sine Gordon potential, and Toda potential.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:39:02 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15050","submitter":"Sunay Ibryamov","authors":"Sunay Ibryamov, Gabriela Zidarova, Evgeni Semkov, Stoyanka Peneva","title":"Study of the long-term $BVR_{c}I_{c}$ photometric variability of eight\n  PMS stars in the young open cluster Trumpler 37","comments":"13 pages, 11 figures, 4 tables, accepted for publication in Research\n  in Astronomy and Astrophysics (RAA)","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.SR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper reports results from our long-term $BV(RI)_{c}$ photometric CCD\nobservations of eight pre-main-sequence stars collected from June 2008 to\nOctober 2022. These stars are located in the young open cluster Trumpler 37, in\nthe field of GM Cephei. The observational data indicate that all stars from our\nstudy exhibit variability in all-optical passbands, typical for young stars. In\nthis paper, we describe and discuss the photometric behavior of the stars and\nthe possible reasons for their variability. For two of the objects, we\nidentified periodicity in their light variation.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:39:36 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15051","submitter":"Erica Cai","authors":"Erica Cai, Brendan O'Connor","title":"A Monte Carlo Language Model Pipeline for Zero-Shot Sociopolitical Event\n  Extraction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We consider dyadic zero-shot event extraction (EE) to identify actions\nbetween pairs of actors. The \\emph{zero-shot} setting allows social scientists\nor other non-computational researchers to extract any customized,\nuser-specified set of events without training, resulting in a \\emph{dyadic}\nevent database, allowing insight into sociopolitical relational dynamics among\nactors and the higher level organizations or countries they represent.\nUnfortunately, we find that current zero-shot EE methods perform poorly for the\ntask, with issues including word sense ambiguity, modality mismatch, and\nefficiency. Straightforward application of large language model prompting\ntypically performs even worse. We address these challenges with a new\nfine-grained, multi-stage generative question-answer method, using a Monte\nCarlo approach to exploit and overcome the randomness of generative outputs. It\nperforms 90\\% fewer queries than a previous approach, with strong performance\non the widely-used Automatic Content Extraction dataset. Finally, we extend our\nmethod to extract affiliations of actor arguments and demonstrate our method\nand findings on a dyadic international relations case study.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:41:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15052","submitter":"Davi Rohe Rodrigues","authors":"Rayan Moukhader, Davi Rodrigues, Eleonora Raimondo, Vito Puliafito,\n  Bruno Azzerboni, Mario Carpentieri, Abbass Hamadeh, Giovanni Finocchio,\n  Riccardo Tomasello","title":"Manipulation of magnetic solitons under the influence of DMI gradients","comments":"19 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Magnetic solitons are promising for applications due to their intrinsic\nproperties such as small size, topological stability, ultralow power\nmanipulation and potentially ultrafast operations. To date, research has\nfocused on the manipulation of skyrmions, domain walls, and vortices by applied\ncurrents. The discovery of new methods to control magnetic parameters, such as\nthe interfacial Dzyaloshinskii-Moriya interaction (DMI) by strain, geometry\ndesign, temperature gradients, and applied voltages promises new avenues for\nenergetically efficient manipulation of magnetic structures. The latter has\nshown significant progress in 2d material-based technology. In this work, we\npresent a comprehensive study using numerical and analytical methods of the\nstability and motion of different magnetic textures under the influence of DMI\ngradients. Our results show that under the influence of linear DMI gradients,\nN\\'eel and Bloch-type skyrmions and radial vortex exhibit motion with finite\nskyrmion Hall angle, while the circular vortex undergoes expulsion dynamics.\nThis work provides a deeper and crucial understanding of the stability and\ngradient-driven dynamics of magnetic solitons, and paves the way for the design\nof alternative low-power sources of magnetization manipulation in the emerging\nfield of 2d materials.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:42:18 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15053","submitter":"Kyle Lo","authors":"Kevin Lin and Kyle Lo and Joseph E. Gonzalez and Dan Klein","title":"Decomposing Complex Queries for Tip-of-the-tongue Retrieval","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.IR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  When re-finding items, users who forget or are uncertain about identifying\ndetails often rely on creative strategies for expressing their information\nneeds -- complex queries that describe content elements (e.g., book characters\nor events), information beyond the document text (e.g., descriptions of book\ncovers), or personal context (e.g., when they read a book). This retrieval\nsetting, called tip of the tongue (TOT), is especially challenging for models\nheavily reliant on lexical and semantic overlap between query and document\ntext. In this work, we introduce a simple yet effective framework for handling\nsuch complex queries by decomposing the query into individual clues, routing\nthose as sub-queries to specialized retrievers, and ensembling the results.\nThis approach allows us to take advantage of off-the-shelf retrievers (e.g.,\nCLIP for retrieving images of book covers) or incorporate retriever-specific\nlogic (e.g., date constraints). We show that our framework incorportating query\ndecompositions into retrievers can improve gold book recall up to 7% relative\nagain for Recall@5 on a new collection of 14,441 real-world query-book pairs\nfrom an online community for resolving TOT inquiries.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:43:40 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15054","submitter":"Alessandro Stolfo","authors":"Alessandro Stolfo, Yonatan Belinkov, Mrinmaya Sachan","title":"Understanding Arithmetic Reasoning in Language Models using Causal\n  Mediation Analysis","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Mathematical reasoning in large language models (LLMs) has garnered attention\nin recent research, but there is limited understanding of how these models\nprocess and store information related to arithmetic tasks. In this paper, we\npresent a mechanistic interpretation of LLMs for arithmetic-based questions\nusing a causal mediation analysis framework. By intervening on the activations\nof specific model components and measuring the resulting changes in predicted\nprobabilities, we identify the subset of parameters responsible for specific\npredictions. We analyze two pre-trained language models with different sizes\n(2.8B and 6B parameters). Experimental results reveal that a small set of\nmid-late layers significantly affect predictions for arithmetic-based\nquestions, with distinct activation patterns for correct and wrong predictions.\nWe also investigate the role of the attention mechanism and compare the model's\nactivation patterns for arithmetic queries with the prediction of factual\nknowledge. Our findings provide insights into the mechanistic interpretation of\nLLMs for arithmetic tasks and highlight the specific components involved in\narithmetic reasoning.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:43:47 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15055","submitter":"Mayank Singh","authors":"Mayank Kumar Singh, Naoya Takahashi, Onoe Naoyuki","title":"Iteratively Improving Speech Recognition and Voice Conversion","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.AI eess.AS","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Many existing works on voice conversion (VC) tasks use automatic speech\nrecognition (ASR) models for ensuring linguistic consistency between source and\nconverted samples. However, for the low-data resource domains, training a\nhigh-quality ASR remains to be a challenging task. In this work, we propose a\nnovel iterative way of improving both the ASR and VC models. We first train an\nASR model which is used to ensure content preservation while training a VC\nmodel. In the next iteration, the VC model is used as a data augmentation\nmethod to further fine-tune the ASR model and generalize it to diverse\nspeakers. By iteratively leveraging the improved ASR model to train VC model\nand vice-versa, we experimentally show improvement in both the models. Our\nproposed framework outperforms the ASR and one-shot VC baseline models on\nEnglish singing and Hindi speech domains in subjective and objective\nevaluations in low-data resource settings.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:45:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15056","submitter":"Jiajie Zhang","authors":"Jiajie Zhang, Shulin Cao, Tingjia Zhang, Xin Lv, Jiaxin Shi, Qi Tian,\n  Juanzi Li, Lei Hou","title":"Reasoning over Hierarchical Question Decomposition Tree for Explainable\n  Question Answering","comments":"has been accepted by ACL2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Explainable question answering (XQA) aims to answer a given question and\nprovide an explanation why the answer is selected. Existing XQA methods focus\non reasoning on a single knowledge source, e.g., structured knowledge bases,\nunstructured corpora, etc. However, integrating information from heterogeneous\nknowledge sources is essential to answer complex questions. In this paper, we\npropose to leverage question decomposing for heterogeneous knowledge\nintegration, by breaking down a complex question into simpler ones, and\nselecting the appropriate knowledge source for each sub-question. To facilitate\nreasoning, we propose a novel two-stage XQA framework, Reasoning over\nHierarchical Question Decomposition Tree (RoHT). First, we build the\nHierarchical Question Decomposition Tree (HQDT) to understand the semantics of\na complex question; then, we conduct probabilistic reasoning over HQDT from\nroot to leaves recursively, to aggregate heterogeneous knowledge at different\ntree levels and search for a best solution considering the decomposing and\nanswering probabilities. The experiments on complex QA datasets KQA Pro and\nMusique show that our framework outperforms SOTA methods significantly,\ndemonstrating the effectiveness of leveraging question decomposing for\nknowledge integration and our RoHT framework.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:45:59 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15057","submitter":"Tianyu Liu","authors":"Tianyu Liu, Afra Amini, Mrinmaya Sachan, Ryan Cotterell","title":"Learning the String Partial Order","comments":"12 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We show that most structured prediction problems can be solved in linear time\nand space by considering them as partial orderings of the tokens in the input\nstring. Our method computes real numbers for each token in an input string and\nsorts the tokens accordingly, resulting in as few as 2 total orders of the\ntokens in the string. Each total order possesses a set of edges oriented from\nsmaller to greater tokens. The intersection of total orders results in a\npartial order over the set of input tokens, which is then decoded into a\ndirected graph representing the desired structure. Experiments show that our\nmethod achieves 95.4 LAS and 96.9 UAS by using an intersection of 2 total\norders, 95.7 LAS and 97.1 UAS with 4 on the English Penn Treebank dependency\nparsing benchmark. Our method is also the first linear-complexity coreference\nresolution model and achieves 79.2 F1 on the English OntoNotes benchmark, which\nis comparable with state of the art.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:47:35 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15058","submitter":"Lucas Giroto de Oliveira","authors":"Lucas Giroto de Oliveira, David Brunner, Axel Diewald, Charlotte Muth,\n  Laurent Schmalen, Thomas Zwick and Benjamin Nuss","title":"Bistatic OFDM-based Joint Radar-Communication: Synchronization, Data\n  Communication and Sensing","comments":"Accepted for presentation at the focused session \"Joint Communication\n  and Radar Sensing - a step towards 6G'' of the EuMW 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  This article introduces a bistatic joint radar-communication (RadCom) system\nbased on orthogonal frequency-division multiplexing (OFDM). In this context,\nthe adopted OFDM frame structure is described and system model encompassing\ntime, frequency, and sampling synchronization mismatches between the\ntransmitter and receiver of the bistatic system is outlined. Next, the signal\nprocessing approaches for synchronization and communication are discussed, and\nradar sensing processing approaches using either only pilots or a reconstructed\nOFDM frame based on the estimated receive communication data are presented.\nFinally, proof-of-concept measurement results are presented to validate the\ninvestigated system and a trade-off between frame size and the performance of\nthe aforementioned processing steps is observed.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:48:19 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15059","submitter":"Baptiste Vergain","authors":"Bernard Boigelot (1), Pascal Fontaine (1), Baptiste Vergain (1) ((1)\n  Montefiore Institute, Universit\\'e de Li\\`ege, Belgium)","title":"Decidability of Difference Logic over the Reals with Uninterpreted Unary\n  Predicates","comments":"This is the preprint for the submission published in CADE-29. It also\n  includes an additional detailed proof in the appendix. The Version of Record\n  of this contribution will be published in CADE-29","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LO","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  First-order logic fragments mixing quantifiers, arithmetic, and uninterpreted\npredicates are often undecidable, as is, for instance, Presburger arithmetic\nextended with a single uninterpreted unary predicate. In the SMT world,\ndifference logic is a quite popular fragment of linear arithmetic which is less\nexpressive than Presburger arithmetic. Difference logic on integers with\nuninterpreted unary predicates is known to be decidable, even in the presence\nof quantifiers. We here show that (quantified) difference logic on real numbers\nwith a single uninterpreted unary predicate is undecidable, quite surprisingly.\nMoreover, we prove that difference logic on integers, together with order on\nreals, combined with uninterpreted unary predicates, remains decidable.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:48:51 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15060","submitter":"Jamin Shin","authors":"Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo\n  Yun, Jamin Shin, Gunhee Kim","title":"Who Wrote this Code? Watermarking for Code Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large language models for code have recently shown remarkable performance in\ngenerating executable code. However, this rapid advancement has been\naccompanied by many legal and ethical concerns, such as code licensing issues,\ncode plagiarism, and malware generation, making watermarking machine-generated\ncode a very timely problem. Despite such imminent needs, we discover that\nexisting watermarking and machine-generated text detection methods for LLMs\nfail to function with code generation tasks properly. Hence, in this work, we\npropose a new watermarking method, SWEET, that significantly improves upon\nprevious approaches when watermarking machine-generated code. Our proposed\nmethod selectively applies watermarking to the tokens with high enough entropy,\nsurpassing a defined threshold. The experiments on code generation benchmarks\nshow that our watermarked code has superior quality compared to code produced\nby the previous state-of-the-art LLM watermarking method. Furthermore, our\nwatermark method also outperforms DetectGPT for the task of machine-generated\ncode detection.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:49:52 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15061","submitter":"Javier Ure\\~na-Carrion","authors":"Javier Ure\\~na-Carrion, Fariba Karimi, Gerardo I\\~niguez, Mikko\n  Kivel\\\"a","title":"Assortative and preferential attachment lead to core-periphery networks","comments":"5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.soc-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Core-periphery is a key feature of large-scale networks underlying a wide\nrange of social, biological, and transportation phenomena. Despite its\nprevalence in empirical data, it is unclear whether this property is a\nconsequence of more fundamental network evolution processes. While preferential\nattachment can create degree heterogeneity indistinguishable from\ncore-periphery, it doesn't explain why specific groups of nodes gain dominance\nand become cores. We show that even small amounts of assortative attachment,\ne.g. homophily in social networks, can break this symmetry, and that the\ninterplay of the two mechanisms leads to one of the groups emerging as a\nprominent core. A systematic analysis of the phase space of the proposed model\nreveals the levels of assortative and preferential attachment necessary for a\ngroup to become either core or periphery, depending on initial conditions. We\nfind that relative group size is significant, with minority groups typically\nhaving a disadvantage on becoming the core for similar assortative attachment\nlevels among groups. We also find that growing networks are less prone to\ndevelop core-periphery than dynamically evolving networks, and that these two\nnetwork evolution mechanisms lead to different types of core-periphery\nstructures. Analyzing five empirical networks, our findings suggest that core\nnodes are highly assortative, illustrating the potential of our model as a tool\nfor designing and analyzing interventions on evolving networks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:51:54 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15062","submitter":"Quzhe Huang","authors":"Quzhe Huang, Mingxu Tao, Zhenwei An, Chen Zhang, Cong Jiang, Zhibin\n  Chen, Zirui Wu, Yansong Feng","title":"Lawyer LLaMA Technical Report","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Large Language Models (LLMs), like LLaMA, have exhibited remarkable\nperformances across various tasks. Nevertheless, when deployed to specific\ndomains such as law or medicine, the models still confront the challenge of a\ndeficiency in domain-specific knowledge and an inadequate capability to\nleverage that knowledge to resolve domain-related problems. In this paper, we\nfocus on the legal domain and explore how to inject domain knowledge during the\ncontinual training stage and how to design proper supervised finetune tasks to\nhelp the model tackle practical issues. Moreover, to alleviate the\nhallucination problem during model's generation, we add a retrieval module and\nextract relevant articles before the model answers any queries. Augmenting with\nthe extracted evidence, our model could generate more reliable responses. We\nrelease our data and model at https://github.com/AndrewZhe/lawyer-llama.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:52:07 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15063","submitter":"Riccardo Walter Maffucci","authors":"Riccardo W. Maffucci","title":"Rao's Theorem for forcibly planar sequences revisited","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.CO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We consider the graph degree sequences such that every realisation is a\npolyhedron. It turns out that there are exactly eight of them. All of these are\nunigraphic, in the sense that each is realised by exactly one polyhedron. This\nis a revisitation of a Theorem of Rao about sequences that are realised by only\nplanar graphs.\n  Our proof yields additional geometrical insight on this problem. Moreover,\nour proof is constructive: for each graph degree sequence that is not forcibly\npolyhedral, we construct a non-polyhedral realisation.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:52:18 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15064","submitter":"Siqi Ouyang","authors":"Siqi Ouyang and Lei Li","title":"Prompt Optimization of Large Language Model for Interactive Tasks\n  without Gradient and Demonstrations","comments":"Draft. Work in Progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  Large language models (LLMs) have demonstrated remarkable language\nproficiency, but they face challenges when solving interactive tasks\nindependently. Existing methods either rely on gradient access, which is often\ninaccessible in state-of-the-art LLMs like GPT-4, or necessitate diverse and\nhigh-quality in-context demonstrations. In this study, we propose LLM-PO, a\nnovel approach that enables LLMs to address these tasks without gradient access\nor extensive demonstrations. The key idea is to maintain a text-based plan and\nask LLMs to reflect on pros and cons of the current plan based on experience\ncollected with it, to update the plan, and to collect more experiences with the\nnew plan. Experiments on HotpotQA demonstrate that LLM-PO achieves higher or on\npar success rates compared to in-context learning (ICL) baselines while\nrequiring less inference cost.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:52:23 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15065","submitter":"Ximing Lu","authors":"Ximing Lu, Faeze Brahman, Peter West, Jaehun Jang, Khyathi Chandu,\n  Abhilasha Ravichander, Lianhui Qin, Prithviraj Ammanabrolu, Liwei Jiang,\n  Sahana Ramnath, Nouha Dziri, Jillian Fisher, Bill Yuchen Lin, Skyler\n  Hallinan, Xiang Ren, Sean Welleck, Yejin Choi","title":"Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs\n  without Fine-tuning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large language models excel at a variety of language tasks when prompted with\nexamples or instructions. Yet controlling these models through prompting alone\nis limited. Tailoring language models through fine-tuning (e.g., via\nreinforcement learning) can be effective, but it is expensive and requires\nmodel access.\n  We propose Inference-time Policy Adapters (IPA), which efficiently tailors a\nlanguage model such as GPT-3 without fine-tuning it. IPA guides a large base\nmodel during decoding time through a lightweight policy adaptor trained to\noptimize an arbitrary user objective with reinforcement learning.\n  On five challenging text generation tasks, such as toxicity reduction and\nopen-domain generation, IPA consistently brings significant improvements over\noff-the-shelf language models. It outperforms competitive baseline methods,\nsometimes even including expensive fine-tuning. In particular, tailoring GPT-2\nwith IPA can outperform GPT-3, while tailoring GPT- 3 with IPA brings a major\nperformance boost over GPT-3 (and sometimes even over GPT-4). Our promising\nresults highlight the potential of IPA as a lightweight alternative to\ntailoring extreme-scale language models.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:52:55 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15066","submitter":"Jiayan Guo","authors":"Jiayan Guo and Lun Du and Hengyu Liu","title":"GPT4Graph: Can Large Language Models Understand Graph Structured Data ?\n  An Empirical Evaluation and Benchmarking","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Large language models~(LLM) like ChatGPT have become indispensable to\nartificial general intelligence~(AGI), demonstrating excellent performance in\nvarious natural language processing tasks. In the real world, graph data is\nubiquitous and an essential part of AGI and prevails in domains like social\nnetwork analysis, bioinformatics and recommender systems. The training corpus\nof large language models often includes some algorithmic components, which\nallows them to achieve certain effects on some graph data-related problems.\nHowever, there is still little research on their performance on a broader range\nof graph-structured data. In this study, we conduct an extensive investigation\nto assess the proficiency of LLMs in comprehending graph data, employing a\ndiverse range of structural and semantic-related tasks. Our analysis\nencompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph\nunderstanding. Through our study, we not only uncover the current limitations\nof language models in comprehending graph structures and performing associated\nreasoning tasks but also emphasize the necessity for further advancements and\nnovel approaches to enhance their graph processing capabilities. Our findings\ncontribute valuable insights towards bridging the gap between language models\nand graph understanding, paving the way for more effective graph mining and\nknowledge extraction.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:53:19 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15067","submitter":"Tianyi Tang","authors":"Tianyi Tang, Hongyuan Lu, Yuchen Eleanor Jiang, Haoyang Huang,\n  Dongdong Zhang, Wayne Xin Zhao, Furu Wei","title":"Not All Metrics Are Guilty: Improving NLG Evaluation with LLM\n  Paraphrasing","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Most research about natural language generation (NLG) relies on evaluation\nbenchmarks with limited references for a sample, which may result in poor\ncorrelations with human judgements. The underlying reason is that one semantic\nmeaning can actually be expressed in different forms, and the evaluation with a\nsingle or few references may not accurately reflect the quality of the model's\nhypotheses. To address this issue, this paper presents a novel method, named\nPara-Ref, to enhance existing evaluation benchmarks by enriching the number of\nreferences. We leverage large language models (LLMs) to paraphrase a single\nreference into multiple high-quality ones in diverse expressions. Experimental\nresults on representative NLG tasks of machine translation, text summarization,\nand image caption demonstrate that our method can effectively improve the\ncorrelation with human evaluation for sixteen automatic evaluation metrics by\n+7.82% in ratio. We release the code and data at\nhttps://github.com/RUCAIBox/Para-Ref.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:53:29 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15068","submitter":"Lingyu Gao","authors":"Xiaomeng Ma, Lingyu Gao, Qihui Xu","title":"ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks\n  for Exploring Theory of Mind","comments":"work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Theory of Mind (ToM), the capacity to comprehend the mental states of\ndistinct individuals, is essential for numerous practical applications. With\nthe development of large language models, there is a heated debate about\nwhether they are able to perform ToM tasks. Previous studies have used\ndifferent tasks and prompts to test the ToM on large language models and the\nresults are inconsistent: some studies asserted these models are capable of\nexhibiting ToM, while others suggest the opposite. In this study, We present\nToMChallenges, a dataset for comprehensively evaluating Theory of Mind based on\nSally-Anne and Smarties tests. We created 30 variations of each test (e.g.,\nchanging the person's name, location, and items). For each variation, we test\nthe model's understanding of different aspects: reality, belief, 1st order\nbelief, and 2nd order belief. We adapt our data for various tasks by creating\nunique prompts tailored for each task category: Fill-in-the-Blank, Multiple\nChoice, True/False, Chain-of-Thought True/False, Question Answering, and Text\nCompletion. If the model has a robust ToM, it should be able to achieve good\nperformance for different prompts across different tests. We evaluated two\nGPT-3.5 models, text-davinci-003 and gpt-3.5-turbo-0301, with our datasets. Our\nresults indicate that consistent performance in ToM tasks remains a challenge.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:54:07 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15069","submitter":"Lucas Giroto de Oliveira","authors":"Lucas Giroto de Oliveira, Elizabeth Bekker, Axel Diewald, Benjamin\n  Nuss, Theresa Antes, Yueheng Li, Akanksha Bhutani, and Thomas Zwick","title":"Enabling Joint Radar-Communication Operation in Shift Register-Based\n  PMCW Radars","comments":"Accepted for presentation at the focused session \"Automotive PMCW\n  Radars'' of the EuMW 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  This article introduces adaptations to the conventional frame structure in\nbinary phase-modulated continuous wave (PMCW) radars with sequence generation\nvia linear-feedbck shift registers and additional processing steps to enable\njoint radar-communication (RadCom) operation. In this context, a preamble\nstructure based on pseudorandom binary sequences (PRBSs) that is compatible\nwith existing synchronization algorithms is outlined, and the allocation of\npilot PRBS blocks is discussed. Finally, results from proof-of-concept\nmeasurements are presented to illustrate the effects of the choice of system\nand signal parameters and validate the investigated PMCW-based RadCom system\nand synchronization strategy.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:54:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15070","submitter":"London Lowmanstone","authors":"London Lowmanstone, Ruyuan Wan, Risako Owan, Jaehyung Kim, Dongyeop\n  Kang","title":"Annotation Imputation to Individualize Predictions: Initial Studies on\n  Distribution Dynamics and Model Predictions","comments":"12 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Annotating data via crowdsourcing is time-consuming and expensive. Owing to\nthese costs, dataset creators often have each annotator label only a small\nsubset of the data. This leads to sparse datasets with examples that are marked\nby few annotators; if an annotator is not selected to label an example, their\nopinion regarding it is lost. This is especially concerning for subjective NLP\ndatasets where there is no correct label: people may have different valid\nopinions. Thus, we propose using imputation methods to restore the opinions of\nall annotators for all examples, creating a dataset that does not leave out any\nannotator's view. We then train and prompt models with data from the imputed\ndataset (rather than the original sparse dataset) to make predictions about\nmajority and individual annotations. Unfortunately, the imputed data provided\nby our baseline methods does not improve predictions. However, through our\nanalysis of it, we develop a strong understanding of how different imputation\nmethods impact the original data in order to inform future imputation\ntechniques. We make all of our code and data publicly available.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:54:46 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15071","submitter":"Hongjian Sun Prof.","authors":"Yue Cao and Sifan Li and Chenchen Lv and Di Wang and Hongjian Sun and\n  Jing Jiang and Fanlin Meng and Lexi Xu and Xinzhou Cheng","title":"Towards Cyber Security for Low-Carbon Transportation: Overview,\n  Challenges and Future Directions","comments":"34 pages, 6 figures, accepted by journal Renewable and Sustainable\n  Energy Reviews","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT cs.CR math.IT","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  In recent years, low-carbon transportation has become an indispensable part\nas sustainable development strategies of various countries, and plays a very\nimportant responsibility in promoting low-carbon cities. However, the security\nof low-carbon transportation has been threatened from various ways. For\nexample, denial of service attacks pose a great threat to the electric vehicles\nand vehicle-to-grid networks. To minimize these threats, several methods have\nbeen proposed to defense against them. Yet, these methods are only for certain\ntypes of scenarios or attacks. Therefore, this review addresses security aspect\nfrom holistic view, provides the overview, challenges and future directions of\ncyber security technologies in low-carbon transportation. Firstly, based on the\nconcept and importance of low-carbon transportation, this review positions the\nlow-carbon transportation services. Then, with the perspective of network\narchitecture and communication mode, this review classifies its typical attack\nrisks. The corresponding defense technologies and relevant security suggestions\nare further reviewed from perspective of data security, network management\nsecurity and network application security. Finally, in view of the long term\ndevelopment of low-carbon transportation, future research directions have been\nconcerned.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:54:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15072","submitter":"Yuxuan Sun","authors":"Yuxuan Sun, Chenglu Zhu, Sunyi Zheng, Kai Zhang, Zhongyi Shui,\n  Xiaoxuan Yu, Yizhi Zhao, Honglin Li, Yunlong Zhang, Ruojia Zhao, Xinheng Lyu,\n  Lin Yang","title":"PathAsst: Redefining Pathology through Generative Foundation AI\n  Assistant for Pathology","comments":"13 pages, 5 figures, conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.MM","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  As advances in large language models (LLMs) and multimodal techniques\ncontinue to mature, the development of general-purpose multimodal large\nlanguage models (MLLMs) has surged, with significant applications in natural\nimage interpretation. However, the field of pathology has largely remained\nuntapped in this regard, despite the growing need for accurate, timely, and\npersonalized diagnostics. To bridge the gap in pathology MLLMs, we present the\nPathAsst in this study, which is a generative foundation AI assistant to\nrevolutionize diagnostic and predictive analytics in pathology. To develop\nPathAsst, we collect over 142K high-quality pathology image-text pairs from a\nvariety of reliable sources, including PubMed, comprehensive pathology\ntextbooks, reputable pathology websites, and private data annotated by\npathologists. Leveraging the advanced capabilities of ChatGPT/GPT-4, we\ngenerate over 180K instruction-following samples. Furthermore, we devise\nadditional instruction-following data, specifically tailored for the invocation\nof the pathology-specific models, allowing the PathAsst to effectively interact\nwith these models based on the input image and user intent, consequently\nenhancing the model's diagnostic capabilities. Subsequently, our PathAsst is\ntrained based on Vicuna-13B language model in coordination with the CLIP vision\nencoder. The results of PathAsst show the potential of harnessing the\nAI-powered generative foundation model to improve pathology diagnosis and\ntreatment processes. We are committed to open-sourcing our meticulously curated\ndataset, as well as a comprehensive toolkit designed to aid researchers in the\nextensive collection and preprocessing of their own datasets. Resources can be\nobtained at\nhttps://github.com/superjamessyx/Generative-Foundation-AI-Assistant-for-Pathology.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:55:50 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15073","submitter":"Hristo Tonchev","authors":"Hristo Tonchev, Petar Danev","title":"Robustness of Quantum Random Walk Search Algorithm in Hypercube when\n  only first or both first and second neighbors are measured","comments":"32 pages, 15 figures, 3 tables, 2 Appendices","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this work we study the robustness of two modifications of quantum random\nwalk search algorithm on hypercube. In the first previously suggested\nmodification, on each even iteration only quantum walk is applied. And in the\nsecond, the closest neighbors of the solution are measured classically. In our\napproach the traversing coin is constructed by both generalized Householder\nreflection and an additional phase multiplier and we investigate the stability\nof the algorithm to deviations in those phases. We have shown that the\nunmodified algorithm becomes more robust when a certain relation between those\nphases is preserved. The first modification we study here does not lead to any\nchange in the robustness of quantum random walk search algorithm. However, when\na measurement of the first and second neighbors is included, there are some\ndifferences. The most important one, in view of our study of the robustness, is\nan increase in the stability of the algorithm, especially for large coin\ndimensions.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:55:52 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15074","submitter":"Daman Arora","authors":"Daman Arora, Himanshu Gaurav Singh, Mausam","title":"Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For\n  Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  The performance on Large Language Models (LLMs) on existing reasoning\nbenchmarks has shot up considerably over the past years. In response, we\npresent JEEBench, a considerably more challenging benchmark dataset for\nevaluating the problem solving abilities of LLMs. We curate 450 challenging\npre-engineering mathematics, physics and chemistry problems from the IIT\nJEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is\nessential for solving problems in this benchmark. Our evaluation on the GPT\nseries of models reveals that although performance improves with newer models,\nthe best being GPT-4, the highest performance, even after using techniques like\nSelf-Consistency and Chain-of-Thought prompting is less than 40 percent. Our\nanalysis demonstrates that errors in algebraic manipulation and failure in\nretrieving relevant domain specific concepts are primary contributors to GPT4's\nlow performance. Given the challenging nature of the benchmark, we hope that it\ncan guide future research in problem solving using LLMs. Our code and dataset\nis available here.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:55:59 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15075","submitter":"Benyou Wang","authors":"Hongbo Zhang and Junying Chen and Feng Jiang and Fei Yu and Zhihong\n  Chen and Jianquan Li and Guiming Chen and Xiangbo Wu and Zhiyi Zhang and\n  Qingying Xiao and Xiang Wan and Benyou Wang and Haizhou Li","title":"HuatuoGPT, towards Taming Language Model to Be a Doctor","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper, we present HuatuoGPT, a large language model (LLM) for medical\nconsultation. The core recipe of HuatuoGPT is to leverage both\n\\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors}\nin the supervised fine-tuned stage. The responses of ChatGPT are usually\ndetailed, well-presented and informative while it cannot perform like a doctor\nin many aspects, e.g. for integrative diagnosis. We argue that real-world data\nfrom doctors would be complementary to distilled data in the sense the former\ncould tame a distilled language model to perform like doctors. To better\nleverage the strengths of both data, we train a reward model to align the\nlanguage model with the merits that both data bring, following an RLAIF\n(reinforced learning from AI feedback) fashion. To evaluate and benchmark the\nmodels, we propose a comprehensive evaluation scheme (including automatic and\nmanual metrics). Experimental results demonstrate that HuatuoGPT achieves\nstate-of-the-art results in performing medical consultation among open-source\nLLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It\nis worth noting that by using additional real-world data and RLAIF, the\ndistilled language model (i.e., HuatuoGPT) outperforms its teacher model\nChatGPT in most cases. Our code, data, and models are publicly available at\n\\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is\navailable at \\url{https://www.HuatuoGPT.cn/}.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:56:01 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15076","submitter":"Eric A Mitchell","authors":"Nathan Hu, Eric Mitchell, Christopher D. Manning, Chelsea Finn","title":"Meta-Learning Online Adaptation of Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large language models encode surprisingly broad knowledge about the world\ninto their parameters. However, the knowledge in static language models can\nfall out of date, limiting the model's effective \"shelf life.\" While online\nfine-tuning can reduce this degradation, we find that fine-tuning on a stream\nof documents using standard optimizers such as Adam leads to a disappointingly\nlow level of information uptake. We hypothesize that online fine-tuning does\nnot sufficiently 'attend' to important information. That is, the gradient\nsignal from important tokens representing factual information is drowned out by\nthe gradient from inherently noisy tokens, suggesting a dynamic, context-aware\nlearning rate may be beneficial. To test this hypothesis, we meta-train a\nsmall, autoregressive model to reweight the language modeling loss for each\ntoken during online fine-tuning, with the objective of maximizing the\nout-of-date base language model's ability to answer questions about a document\nafter a single weighted gradient step. We call this approach Context-aware\nMeta-learned Loss Scaling (CaMeLS). Across three different distributions of\ndocuments, our experiments find that fine-tuning on streams of thousands of\ndocuments with CaMeLS substantially improves knowledge retention compared to\nstandard online fine-tuning. Finally, we find that the meta-learned weights are\ngeneral, and that a single reweighting model can be used to enhance the online\nadaptation of many LMs.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:56:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15077","submitter":"Junxian He","authors":"Junlei Zhang, Zhenzhong Lan, Junxian He","title":"Contrastive Learning of Sentence Embeddings from Scratch","comments":"Preprint","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Contrastive learning has been the dominant approach to train state-of-the-art\nsentence embeddings. Previous studies have typically learned sentence\nembeddings either through the use of human-annotated natural language inference\n(NLI) data or via large-scale unlabeled sentences in an unsupervised manner.\nHowever, even in the case of unlabeled data, their acquisition presents\nchallenges in certain domains due to various reasons. To address these issues,\nwe present SynCSE, a contrastive learning framework that trains sentence\nembeddings with synthesized data. Specifically, we explore utilizing large\nlanguage models to synthesize the required data samples for contrastive\nlearning, including (1) producing positive and negative annotations given\nunlabeled sentences (SynCSE-partial), and (2) generating sentences along with\ntheir corresponding annotations from scratch (SynCSE-scratch). Experimental\nresults on sentence similarity and reranking tasks indicate that both\nSynCSE-partial and SynCSE-scratch greatly outperform unsupervised baselines,\nand SynCSE-partial even achieves comparable performance to the supervised\nmodels in most settings.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:56:21 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15078","submitter":"Yunfan Lu","authors":"Yunfan Lu, Guoqiang Liang, Lin Wang","title":"Learning INR for Event-guided Rolling Shutter Frame Correction, Deblur,\n  and Interpolation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Images captured by rolling shutter (RS) cameras under fast camera motion\noften contain obvious image distortions and blur, which can be modeled as a\nrow-wise combination of a sequence of global shutter (GS) frames within the\nexposure time naturally, recovering high-frame-rate GS sharp frames from an RS\nblur image needs to simultaneously consider RS correction, deblur, and frame\ninterpolation Taking this task is nontrivial, and to our knowledge, no feasible\nsolutions exist by far. A naive way is to decompose the complete process into\nseparate tasks and simply cascade existing methods; however, this results in\ncumulative errors and noticeable artifacts. Event cameras enjoy many\nadvantages, e.g., high temporal resolution, making them potential for our\nproblem. To this end, we make the first attempt to recover high-frame-rate\nsharp GS frames from an RS blur image and paired event data. Our key idea is to\nlearn an implicit neural representation (INR) to directly map the position and\ntime coordinates to RGB values to address the interlocking degradations in the\nimage restoration process. Specifically, we introduce spatial-temporal implicit\nencoding (STE) to convert an RS blur image and events into a spatial-temporal\nrepresentation (STR). To query a specific sharp frame (GS or RS), we embed the\nexposure time into STR and decode the embedded features to recover a sharp\nframe. Moreover, we propose an RS blur image-guided integral loss to better\ntrain the network. Our method is relatively lightweight as it contains only\n0.379M parameters and demonstrates high efficiency as the STE is called only\nonce for any number of interpolation frames. Extensive experiments show that\nour method significantly outperforms prior methods addressing only one or two\nof the tasks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:57:03 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15079","submitter":"Zhuoxiao Cheng","authors":"Yinguo Yang, Yiling Ye, Zhuoxiao Cheng, Guangchun Ruan, Qiuyu Lu, Xuan\n  Wang, Haiwang Zhong","title":"Life cycle economic viability analysis of battery storage in electricity\n  market","comments":"17 pages, accepted by JPS","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  Battery storage is essential to enhance the flexibility and reliability of\nelectric power systems by providing auxiliary services and load shifting.\nStorage owners typically gains incentives from quick responses to auxiliary\nservice prices, but frequent charging and discharging also reduce its lifetime.\nTherefore, this paper embeds the battery degradation cost into the operation\nsimulation to avoid overestimated profits caused by an aggressive bidding\nstrategy. Based on an operation simulation model, this paper conducts the\neconomic viability analysis of whole life cycle using the internal rate of\nreturn(IRR). A clustering method and a typical day method are developed to\nreduce the huge computational burdens in the life-cycle simulation of battery\nstorage. Our models and algorithms are validated by the case study of two\nmainstream technology routes currently: lithium nickel cobalt manganese oxide\n(NCM) batteries and lithium iron phosphate (LFP) batteries. Then a sensitivity\nanalysis is presented to identify the critical factors that boost battery\nstorage in the future. We evaluate the IRR results of different types of\nbattery storage to provide guidance for investment portfolio.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:59:02 GMT"},{"version":"v2","created":"Sun, 28 May 2023 09:05:58 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.15080","submitter":"Geewook Kim","authors":"Geewook Kim, Hodong Lee, Daehee Kim, Haeji Jung, Sanghee Park, Yoonsik\n  Kim, Sangdoo Yun, Taeho Kil, Bado Lee, Seunghyun Park","title":"Cream: Visually-Situated Natural Language Understanding with Contrastive\n  Reading Model and Frozen Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Advances in Large Language Models (LLMs) have inspired a surge of research\nexploring their expansion into the visual domain. While recent models exhibit\npromise in generating abstract captions for images and conducting natural\nconversations, their performance on text-rich images leaves room for\nimprovement. In this paper, we propose the Contrastive Reading Model (Cream), a\nnovel neural architecture designed to enhance the language-image understanding\ncapability of LLMs by capturing intricate details typically overlooked by\nexisting methods. Cream integrates vision and auxiliary encoders, complemented\nby a contrastive feature alignment technique, resulting in a more effective\nunderstanding of textual information within document images. Our approach,\nthus, seeks to bridge the gap between vision and language understanding, paving\nthe way for more sophisticated Document Intelligence Assistants. Rigorous\nevaluations across diverse tasks, such as visual question answering on document\nimages, demonstrate the efficacy of Cream as a state-of-the-art model in the\nfield of visual document understanding. We provide our codebase and\nnewly-generated datasets at https://github.com/naver-ai/cream\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:59:13 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15081","submitter":"Maria Giovanna Dainotti","authors":"Petrosian Vah/'e and Maria Giovanna Dainotti","title":"Progenitors of Low Redshift Gamma-ray Bursts","comments":"6 pages, 4 figures of two panels","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.HE","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Bimodal distribution of the observed duration of gamma-ray bursts (GRBs) has\nled to two distinct progenitors; compact star mergers, either two neutron stars\n(NSs) or a NS and a black hole (BH), for short GRBs (SGRBs), and so-called\ncollapsars for long GRBs (LGRBs). It is therefore expected that formation rate\n(FR) of LGRBs should be similar to the cosmic star formation rate (SFR), while\nthat of SGRBs to be delayed relative to the SFR. The localization of some LGRBs\nin and around the star forming regions of host galaxies and some SGRBs away\nform such regions support this expectation. Another distinct feature of SGRBs\nis their association with gravitational wave (GW) sources and kilonovae.\nHowever, several independent investigations of the FRs of long and short\nbursts, using the Efron-Petrosian non-parametric method have shown a LGRB FR\nthat is significantly larger than SFR at low redhift, and similar to the FR of\nSGRBs. In addition, recent discovery of association of a low redshift long\nGRB211211A with a kilonova raises doubt about its collapsar origin. In this\nletter we review these results and show that low redshift LGRBs could also have\ncompact star mergers as progenitor increasing the expected rate of the GW\nsources and kilonovae significantly.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:59:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15082","submitter":"Rahul Rao","authors":"Rahul Rao, Ryan Selhorst, Jie Jiang, Benjamin S. Conner, Ryan\n  Siebenaller, Emmanuel Rowe, Andrea Giordano, Ruth Pachter, Michael A. Susner","title":"Investigating strain between phase-segregated domains in Cu-deficient\n  CuInP2S6","comments":"12 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  CuInP2S6 (CIPS) is an emerging layered ferroelectric material with a TC above\nroom temperature. When synthesized with Cu deficiencies (i.e.,\nCu1-xIn1+x/3P2S6), the material segregates into CIPS and In4/3P2S6 (IPS)\nself-assembled heterostructures within the same single crystal. This\nsegregation results in significant in-plane and out-of-plane strains between\nthe CIPS and IPS phases as the volume fraction of CIPS (IPS) domains shrink\n(grow) with decreasing Cu fraction. Here, we synthesized CIPS with varying\namounts of Cu (x = 0, 0.2, 0.3, 0.4, 0.5, 0.7, 0.8 and 1) and measured the\nstrains between the CIPS and IPS phases through the evolution of the respective\nRaman, infrared, and optical reflectance spectra. Density functional theory\ncalculations revealed vibrational modes unique to the CIPS and IPS phases,\nwhich can be used to distinguish between the two phases through two-dimensional\nRaman mapping. A comparison of the composition-dependent frequencies and\nintensities of the CIPS and IPS Raman peaks showed interesting trends with\ndecreasing CIPS phase fraction (i.e., Cu/In ratio). Our data reveal red- and\nblue-shifted Raman and infrared peak frequencies that we correlate to lattice\nstrains arising from the segregation of the material into CIPS and IPS chemical\ndomains. The strain is highest for a Cu/In ratio of 0.33 (Cu0.4In1.2P2S6),\nwhich we attribute to equal and opposite strains exerted by the CIPS and IPS\nphases on each other. In addition, bandgaps extracted from the optical\nreflectance spectra revealed a decrease in values, with the lowest value (~ 2.3\neV) for Cu0.4In1.2P2S6.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:00:23 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15083","submitter":"Jiahuan Li","authors":"Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Chen, Jiajun Chen","title":"Eliciting the Translation Ability of Large Language Models via\n  Multilingual Finetuning with Translation Instructions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large-scale Pretrained Language Models~(LLMs), such as ChatGPT and GPT4, have\nshown strong abilities in multilingual translations, without being explicitly\ntrained on parallel corpora. It is interesting how the LLMs obtain their\nability to carry out translation instructions for different languages. In this\npaper, we present a detailed analysis by finetuning a multilingual pretrained\nlanguage model, XGLM-7B, to perform multilingual translation following given\ninstructions. Firstly, we show that the multilingual LLMs have stronger\ntranslation abilities than previously demonstrated. For a certain language\npair, the performance depends on both the language families and the amount of\ndata used in the pretraining phase. Secondly, we find that LLMs' ability to\ncarry out translation instructions relies on the understanding of translation\ninstruction and the alignment among different languages. With proper\nenhancement, LLMs could perform the translation task well even for those\nlanguage pairs unseen during the instruction tuning phase.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:00:24 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15084","submitter":"Arian Bakhtiarnia","authors":"B{\\l}a\\.zej Leporowski, Arian Bakhtiarnia, Nicole Bonnici, Adrian\n  Muscat, Luca Zanella, Yiming Wang and Alexandros Iosifidis","title":"Audio-Visual Dataset and Method for Anomaly Detection in Traffic Videos","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We introduce the first audio-visual dataset for traffic anomaly detection\ntaken from real-world scenes, called MAVAD, with a diverse range of weather and\nillumination conditions. In addition, we propose a novel method named AVACA\nthat combines visual and audio features extracted from video sequences by means\nof cross-attention to detect anomalies. We demonstrate that the addition of\naudio improves the performance of AVACA by up to 5.2%. We also evaluate the\nimpact of image anonymization, showing only a minor decrease in performance\naveraging at 1.7%.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:02:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15085","submitter":"Yoshimichi Ueda","authors":"Yoshiki Aibara and Yoshimichi Ueda","title":"Lebesgue decomposition for positive operators revisited","comments":"13pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.FA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We explain how Pusz--Woronowicz's idea of their functional calculus fits the\ntheory of Lebesgue decomposition for positive operators on Hilbert spaces\ninitially developed by Ando. In this way, we reconstruct the essential and\nfundamental part of the theory.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:02:58 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15086","submitter":"Jong Chul Ye","authors":"Beomsu Kim, Gihyun Kwon, Kwanyoung Kim, Jong Chul Ye","title":"Unpaired Image-to-Image Translation via Neural Schr\\\"odinger Bridge","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG stat.ML","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Diffusion models are a powerful class of generative models which simulate\nstochastic differential equations (SDEs) to generate data from noise. Although\ndiffusion models have achieved remarkable progress in recent years, they have\nlimitations in the unpaired image-to-image translation tasks due to the\nGaussian prior assumption. Schr\\\"odinger Bridge (SB), which learns an SDE to\ntranslate between two arbitrary distributions, have risen as an attractive\nsolution to this problem. However, none of SB models so far have been\nsuccessful at unpaired translation between high-resolution images. In this\nwork, we propose the Unpaired Neural Schr\\\"odinger Bridge (UNSB), which\ncombines SB with adversarial training and regularization to learn a SB between\nunpaired data. We demonstrate that UNSB is scalable, and that it successfully\nsolves various unpaired image-to-image translation tasks. Code:\n\\url{https://github.com/cyclomon/UNSB}\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:05:24 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15087","submitter":"Philipp Sadler","authors":"Philipp Sadler and David Schlangen","title":"Pento-DIARef: A Diagnostic Dataset for Learning the Incremental\n  Algorithm for Referring Expression Generation from Examples","comments":"9 pages, Accepted to EACL 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  NLP tasks are typically defined extensionally through datasets containing\nexample instantiations (e.g., pairs of image i and text t), but motivated\nintensionally through capabilities invoked in verbal descriptions of the task\n(e.g., \"t is a description of i, for which the content of i needs to be\nrecognised and understood\"). We present Pento-DIARef, a diagnostic dataset in a\nvisual domain of puzzle pieces where referring expressions are generated by a\nwell-known symbolic algorithm (the \"Incremental Algorithm\"), which itself is\nmotivated by appeal to a hypothesised capability (eliminating distractors\nthrough application of Gricean maxims). Our question then is whether the\nextensional description (the dataset) is sufficient for a neural model to pick\nup the underlying regularity and exhibit this capability given the simple task\ndefinition of producing expressions from visual inputs. We find that a model\nsupported by a vision detection step and a targeted data generation scheme\nachieves an almost perfect BLEU@1 score and sentence accuracy, whereas simpler\nbaselines do not.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:05:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15088","submitter":"Anton Kutsenko A","authors":"Anton A. Kutsenko","title":"Complete left tail asymptotic for the density of branching processes in\n  the Schr\\\"oder case","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR math.FA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  For the density of Galton-Watson processes in the Schr\\\"oder case, we derive\na complete left tail asymptotic series consisting of power terms multiplied by\nperiodic factors.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:14:28 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15089","submitter":"Wiktoria Zajkowska","authors":"Wiktoria Zajkowska, Jakub Turczynski, Boguslawa Kurowska, Henryk\n  Teisseyre, Krzysztof Fronc, Jerzy Dabrowski and Slawomir Kret","title":"ZnO nanowires grown on Al2O3-ZnAl2O4 nanostructure using solid-vapor\n  mechanism","comments":"Conference: 13th Polish-Japanese Joint Seminar on Micro and Nano\n  Analysis","journal-ref":null,"doi":"10.24425/amm.2023.145491","report-no":null,"categories":"cond-mat.mtrl-sci physics.chem-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We present Al2O3-ZnAl2O4-ZnO nanostructure, which could be a prominent\ncandidate for optoelectronics, mechanical and sensing applications. While ZnO\nand ZnAl2O4 composites are mostly synthesized by sol-gel technique, we propose\na solid-vapor growth mechanism. To produce Al2O3-ZnAl2O4-ZnO nanostructure, we\nconduct ZnO:C powder heating resulting in ZnO nanowires (NWs) growth on\nsapphire substrate and ZnAl2O4 spinel layer at the interface. The nanostructure\nwas examined with Scanning Electron Microscopy (SEM) method. Focused Ion Beam\n(FIB) technique enabled us to prepare a lamella for Transmission Electron\nMicroscopy (TEM) imaging. TEM examination revealed high crystallographic\nquality of both spinel and NW structure. Epitaxial relationships of\nAl2O3-ZnAl2O4 and ZnAl2O4-ZnO are given.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:15:17 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15090","submitter":"Mingyu Derek Ma","authors":"Mingyu Derek Ma, Xiaoxuan Wang, Po-Nien Kung, P. Jeffrey Brantingham,\n  Nanyun Peng, Wei Wang","title":"STAR: Boosting Low-Resource Event Extraction by Structure-to-Text Data\n  Generation with Large Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Structure prediction tasks such as event extraction require an in-depth\nunderstanding of the output structure and sub-task dependencies, thus they\nstill heavily rely on task-specific training data to obtain reasonable\nperformance. Due to the high cost of human annotation, low-resource event\nextraction, which requires minimal human cost, is urgently needed in real-world\ninformation extraction applications. We propose to synthesize data instances\ngiven limited seed demonstrations to boost low-resource event extraction\nperformance. We propose STAR, a structure-to-text data generation method that\nfirst generates complicated event structures (Y) and then generates input\npassages (X), all with Large Language Models. We design fine-grained\nstep-by-step instructions and the error cases and quality issues identified\nthrough self-reflection can be self-refined. Our experiments indicate that data\ngenerated by STAR can significantly improve the low-resource event extraction\nperformance and they are even more effective than human-curated data points in\nsome cases.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:15:19 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15091","submitter":"Wadii Boulila Prof.","authors":"Zouhayra Ayadi, Wadii Boulila, Imed Riadh Farah","title":"Modeling Complex Object Changes in Satellite Image Time-Series: Approach\n  based on CSP and Spatiotemporal Graph","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  This paper proposes a method for automatically monitoring and analyzing the\nevolution of complex geographic objects. The objects are modeled as a\nspatiotemporal graph, which separates filiation relations, spatial relations,\nand spatiotemporal relations, and is analyzed by detecting frequent sub-graphs\nusing constraint satisfaction problems (CSP). The process is divided into four\nsteps: first, the identification of complex objects in each satellite image;\nsecond, the construction of a spatiotemporal graph to model the spatiotemporal\nchanges of the complex objects; third, the creation of sub-graphs to be\ndetected in the base spatiotemporal graph; and fourth, the analysis of the\nspatiotemporal graph by detecting the sub-graphs and solving a constraint\nnetwork to determine relevant sub-graphs. The final step is further broken down\ninto two sub-steps: (i) the modeling of the constraint network with defined\nvariables and constraints, and (ii) the solving of the constraint network to\nfind relevant sub-graphs in the spatiotemporal graph. Experiments were\nconducted using real-world satellite images representing several cities in\nSaudi Arabia, and the results demonstrate the effectiveness of the proposed\napproach.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:15:19 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15092","submitter":"Philipp Wiesner","authors":"Philipp Wiesner, Ramin Khalili, Dennis Grinwald, Pratik Agrawal,\n  Lauritz Thamsen, Odej Kao","title":"FedZero: Leveraging Renewable Excess Energy in Federated Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.DC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Federated Learning (FL) is an emerging machine learning technique that\nenables distributed model training across data silos or edge devices without\ndata sharing. Yet, FL inevitably introduces inefficiencies compared to\ncentralized model training, which will further increase the already high energy\nusage and associated carbon emissions of machine learning in the future.\nAlthough the scheduling of workloads based on the availability of low-carbon\nenergy has received considerable attention in recent years, it has not yet been\ninvestigated in the context of FL. However, FL is a highly promising use case\nfor carbon-aware computing, as training jobs constitute of energy-intensive\nbatch processes scheduled in geo-distributed environments.\n  We propose FedZero, a FL system that operates exclusively on renewable excess\nenergy and spare capacity of compute infrastructure to effectively reduce the\ntraining's operational carbon emissions to zero. Based on energy and load\nforecasts, FedZero leverages the spatio-temporal availability of excess energy\nby cherry-picking clients for fast convergence and fair participation. Our\nevaluation, based on real solar and load traces, shows that FedZero converges\nconsiderably faster under the mentioned constraints than state-of-the-art\napproaches, is highly scalable, and is robust against forecasting errors.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:17:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15093","submitter":"Ameet Deshpande","authors":"Ameet Deshpande, Carlos E. Jimenez, Howard Chen, Vishvak Murahari,\n  Victoria Graf, Tanmay Rajpurohit, Ashwin Kalyan, Danqi Chen, Karthik\n  Narasimhan","title":"CSTS: Conditional Semantic Textual Similarity","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Semantic textual similarity (STS) has been a cornerstone task in NLP that\nmeasures the degree of similarity between a pair of sentences, with\napplications in information retrieval, question answering, and embedding\nmethods. However, it is an inherently ambiguous task, with the sentence\nsimilarity depending on the specific aspect of interest. We resolve this\nambiguity by proposing a novel task called conditional STS (C-STS) which\nmeasures similarity conditioned on an aspect elucidated in natural language\n(hereon, condition). As an example, the similarity between the sentences \"The\nNBA player shoots a three-pointer.\" and \"A man throws a tennis ball into the\nair to serve.\" is higher for the condition \"The motion of the ball.\" (both\nupward) and lower for \"The size of the ball.\" (one large and one small).\nC-STS's advantages are two-fold: (1) it reduces the subjectivity and ambiguity\nof STS, and (2) enables fine-grained similarity evaluation using diverse\nconditions. C-STS contains almost 20,000 instances from diverse domains and we\nevaluate several state-of-the-art models to demonstrate that even the most\nperformant fine-tuning and in-context learning models (GPT-4, Flan, SimCSE)\nfind it challenging, with Spearman correlation scores of <50. We encourage the\ncommunity to evaluate their models on C-STS to provide a more holistic view of\nsemantic similarity and natural language understanding.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:18:50 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15094","submitter":"Dongqing Wang","authors":"Dongqing Wang, Tong Zhang, Alaa Abboud, Sabine S\\\"usstrunk","title":"InpaintNeRF360: Text-Guided 3D Inpainting on Unbounded Neural Radiance\n  Fields","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Neural Radiance Fields (NeRF) can generate highly realistic novel views.\nHowever, editing 3D scenes represented by NeRF across 360-degree views,\nparticularly removing objects while preserving geometric and photometric\nconsistency, remains a challenging problem due to NeRF's implicit scene\nrepresentation. In this paper, we propose InpaintNeRF360, a unified framework\nthat utilizes natural language instructions as guidance for inpainting\nNeRF-based 3D scenes.Our approach employs a promptable segmentation model by\ngenerating multi-modal prompts from the encoded text for multiview\nsegmentation. We apply depth-space warping to enforce viewing consistency in\nthe segmentations, and further refine the inpainted NeRF model using perceptual\npriors to ensure visual plausibility. InpaintNeRF360 is capable of\nsimultaneously removing multiple objects or modifying object appearance based\non text instructions while synthesizing 3D viewing-consistent and\nphoto-realistic inpainting. Through extensive experiments on both unbounded and\nfrontal-facing scenes trained through NeRF, we demonstrate the effectiveness of\nour approach and showcase its potential to enhance the editability of implicit\nradiance fields.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:22:23 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15095","submitter":"David Viennot","authors":"David Viennot","title":"Metrics and geodesics on fuzzy spaces","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math-ph gr-qc hep-th math.MP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We study the fuzzy spaces (as special examples of noncommutative manifolds)\nwith their quasicoherent states in order to find their pertinent metrics. We\nshow that they are naturally endowed with two natural \"quantum metrics\" which\nare associated with quantum fluctuations of \"paths\". The first one provides the\nlength the mean path whereas the second one provides the average length of the\nfluctuated paths. Onto the classical manifold associated with the quasicoherent\nstate (manifold of the mean values of the coordinate observables in the state\nminimising their quantum uncertainties) these two metrics provides two\nminimising geodesic equations. Moreover, fuzzy spaces being not torsion free,\nwe have also two different autoparallel geodesic equations associated with two\ndifferent adiabatic regimes in the move of a probe onto the fuzzy space. We\napply these mathematical results to quantum gravity in BFSS matrix models, and\nto the quantum information theory of a controlled qubit submitted to noises of\na large quantum environment.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:23:40 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15096","submitter":"Zachary Ankner","authors":"Zachary Ankner, Naomi Saphra, Davis Blalock, Jonathan Frankle, and\n  Matthew L. Leavitt","title":"Dynamic Masking Rate Schedules for MLM Pretraining","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Most works on transformers trained with the Masked Language Modeling (MLM)\nobjective use the original BERT model's fixed masking rate of 15%. Our work\ninstead dynamically schedules the masking ratio throughout training. We found\nthat linearly decreasing the masking rate from 30% to 15% over the course of\npretraining improves average GLUE accuracy by 0.46% in BERT-base, compared to a\nstandard 15% fixed rate. Further analyses demonstrate that the gains from\nscheduling come from being exposed to both high and low masking rate regimes.\nOur results demonstrate that masking rate scheduling is a simple way to improve\nthe quality of masked language models and achieve up to a 1.89x speedup in\npretraining.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:24:12 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15097","submitter":"Jiesheng Yang","authors":"Jiesheng Yang, Andreas Wilde, Karsten Menzel, Md Zubair Sheikh, Boris\n  Kuznetsov","title":"Computer Vision for Construction Progress Monitoring: A Real-Time Object\n  Detection Approach","comments":"15 Pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Construction progress monitoring (CPM) is essential for effective project\nmanagement, ensuring on-time and on-budget delivery. Traditional CPM methods\noften rely on manual inspection and reporting, which are time-consuming and\nprone to errors. This paper proposes a novel approach for automated CPM using\nstate-of-the-art object detection algorithms. The proposed method leverages\ne.g. YOLOv8's real-time capabilities and high accuracy to identify and track\nconstruction elements within site images and videos. A dataset was created,\nconsisting of various building elements and annotated with relevant objects for\ntraining and validation. The performance of the proposed approach was evaluated\nusing standard metrics, such as precision, recall, and F1-score, demonstrating\nsignificant improvement over existing methods. The integration of Computer\nVision into CPM provides stakeholders with reliable, efficient, and\ncost-effective means to monitor project progress, facilitating timely\ndecision-making and ultimately contributing to the successful completion of\nconstruction projects.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:27:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15098","submitter":"Michael Tang","authors":"Michael Tang, Shunyu Yao, John Yang, Karthik Narasimhan","title":"Referral Augmentation for Zero-Shot Information Retrieval","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We propose Referral-Augmented Retrieval (RAR), a simple technique that\nconcatenates document indices with referrals, i.e. text from other documents\nthat cite or link to the given document, to provide significant performance\ngains for zero-shot information retrieval. The key insight behind our method is\nthat referrals provide a more complete, multi-view representation of a\ndocument, much like incoming page links in algorithms like PageRank provide a\ncomprehensive idea of a webpage's importance. RAR works with both sparse and\ndense retrievers, and outperforms generative text expansion techniques such as\nDocT5Query and Query2Doc a 37% and 21% absolute improvement on ACL paper\nretrieval Recall@10 -- while also eliminating expensive model training and\ninference. We also analyze different methods for multi-referral aggregation and\nshow that RAR enables up-to-date information retrieval without re-training.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:28:35 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15099","submitter":"Ziwei He","authors":"Ziwei He, Meng Yang, Minwei Feng, Jingcheng Yin, Xinbing Wang, Jingwen\n  Leng, Zhouhan Lin","title":"Fourier Transformer: Fast Long Range Modeling by Removing Sequence\n  Redundancy with FFT Operator","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  The transformer model is known to be computationally demanding, and\nprohibitively costly for long sequences, as the self-attention module uses a\nquadratic time and space complexity with respect to sequence length. Many\nresearchers have focused on designing new forms of self-attention or\nintroducing new parameters to overcome this limitation, however a large portion\nof them prohibits the model to inherit weights from large pretrained models. In\nthis work, the transformer's inefficiency has been taken care of from another\nperspective. We propose Fourier Transformer, a simple yet effective approach by\nprogressively removing redundancies in hidden sequence using the ready-made\nFast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation\n(DCT). Fourier Transformer is able to significantly reduce computational costs\nwhile retain the ability to inherit from various large pretrained models.\nExperiments show that our model achieves state-of-the-art performances among\nall transformer-based models on the long-range modeling benchmark LRA with\nsignificant improvement in both speed and space. For generative seq-to-seq\ntasks including CNN/DailyMail and ELI5, by inheriting the BART weights our\nmodel outperforms the standard BART and other efficient models. \\footnote{Our\ncode is publicly available at\n\\url{https://github.com/LUMIA-Group/FourierTransformer}}\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:33:06 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15100","submitter":"Jing Liu","authors":"Jing Liu","title":"Distinguishing nanohertz gravitational wave sources through the\n  observations of ultracompact minihalos","comments":"7 pages, 1 figure, 1 table","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.CO gr-qc hep-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The common-spectrum process observed by pulsar-timing arrays is interpreted\nas stochastic gravitational wave backgrounds originating from various sources\nin the early Universe. Along with generating gravitational waves, we find\nenergy density perturbations also arise with the sources such as bubble\ncollisions and sound waves during first-order phase transitions, cosmic\nstrings, domain walls, condensate fragmentation, and primordial curvature\nperturbations from inflation. These perturbations can lead to the formation of\nabundant ultracompact minihalos. Currently, the observational precision is\ninadequate for discriminating between different models. Then, ongoing and\nfuture astrophysical observations of ultracompact minihalos can help to\ndistinguish and constrain the gravitational-wave sources in the nanohertz and\n$\\mu$Hz bands.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:37:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15101","submitter":"Felix Joos","authors":"Felix Joos and Jonathan Schrodt","title":"Counting oriented trees in digraphs with large minimum semidegree","comments":"24 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.CO","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Let $T$ be an oriented tree on $n$ vertices with maximum degree at most\n$e^{o(\\sqrt{\\log n})}$. If $G$ is a digraph on $n$ vertices with minimum\nsemidegree $\\delta^0(G)\\geq(\\frac12+o(1))n$, then $G$ contains $T$ as a\nspanning tree, as recently shown by Kathapurkar and Montgomery (in fact, they\nonly require maximum degree $o(n/\\log n)$). This generalizes the corresponding\nresult by Koml\\'os, S\\'ark\\\"ozy and Szemer\\'edi for graphs. We investigate the\nnatural question how many copies of $T$ the digraph $G$ contains. Our main\nresult states that every such $G$ contains at least\n$|Aut(T)|^{-1}(\\frac12-o(1))^nn!$ copies of $T$, which is optimal. This implies\nthe analogous result in the undirected case.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:37:03 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15102","submitter":"Diederick Vermetten","authors":"Diederick Vermetten and Manuel L\\'opez-Ib\\'a\\~nez and Olaf Mersmann\n  and Richard Allmendinger and Anna V. Kononova","title":"Analysis of modular CMA-ES on strict box-constrained problems in the\n  SBOX-COST benchmarking suite","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.NE","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Box-constraints limit the domain of decision variables and are common in\nreal-world optimization problems, for example, due to physical, natural or\nspatial limitations. Consequently, solutions violating a box-constraint may not\nbe evaluable. This assumption is often ignored in the literature, e.g.,\nexisting benchmark suites, such as COCO/BBOB, allow the optimizer to evaluate\ninfeasible solutions. This paper presents an initial study on the\nstrict-box-constrained benchmarking suite (SBOX-COST), which is a variant of\nthe well-known BBOB benchmark suite that enforces box-constraints by returning\nan invalid evaluation value for infeasible solutions. Specifically, we want to\nunderstand the performance difference between BBOB and SBOX-COST as a function\nof two initialization methods and six constraint-handling strategies all tested\nwith modular CMA-ES. We find that, contrary to what may be expected, handling\nbox-constraints by saturation is not always better than not handling them at\nall. However, across all BBOB functions, saturation is better than not\nhandling, and the difference increases with the number of dimensions. Strictly\nenforcing box-constraints also has a clear negative effect on the performance\nof classical CMA-ES (with uniform random initialization and no constraint\nhandling), especially as problem dimensionality increases.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:37:03 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15103","submitter":"Andrea Seppi","authors":"Andrea Seppi, Graham Smith, J\\'er\\'emy Toulisse","title":"On complete maximal submanifolds in pseudo-hyperbolic space","comments":"60 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.DG math.GT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We provide a full classification of complete maximal $p$-dimensional\nspacelike submanifolds in the pseudo-hyperbolic space $\\mathbf{H}^{p,q}$, and\nwe study its applications to Teichm\\\"uller theory and to the theory of Anosov\nrepresentations of hyperbolic groups in $\\mathsf{PO}(p,q+1)$.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:39:17 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15104","submitter":"Yican Sun","authors":"Yican Sun, Hongfei Fu, Krishnendu Chatterjee and Amir Kafshdar\n  Goharshady","title":"Automated Tail Bound Analysis for Probabilistic Recurrence Relations","comments":"46 pages, 15 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DS","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Probabilistic recurrence relations (PRRs) are a standard formalism for\ndescribing the runtime of a randomized algorithm. Given a PRR and a time limit\n$\\kappa$, we consider the classical concept of tail probability $\\Pr[T \\ge\n\\kappa]$, i.e., the probability that the randomized runtime $T$ of the PRR\nexceeds the time limit $\\kappa$. Our focus is the formal analysis of tail\nbounds that aims at finding a tight asymptotic upper bound $u \\geq\n\\Pr[T\\ge\\kappa]$ in the time limit $\\kappa$. To address this problem, the\nclassical and most well-known approach is the cookbook method by Karp (JACM\n1994), while other approaches are mostly limited to deriving tail bounds of\nspecific PRRs via involved custom analysis.\n  In this work, we propose a novel approach for deriving\nexponentially-decreasing tail bounds (a common type of tail bounds) for PRRs\nwhose preprocessing time and random passed sizes observe discrete or\n(piecewise) uniform distribution and whose recursive call is either a single\nprocedure call or a divide-and-conquer. We first establish a theoretical\napproach via Markov's inequality, and then instantiate the theoretical approach\nwith a template-based algorithmic approach via a refined treatment of\nexponentiation. Experimental evaluation shows that our algorithmic approach is\ncapable of deriving tail bounds that are (i) asymptotically tighter than Karp's\nmethod, (ii) match the best-known manually-derived asymptotic tail bound for\nQuickSelect, and (iii) is only slightly worse (with a $\\log\\log n$ factor) than\nthe manually-proven optimal asymptotic tail bound for QuickSort. Moreover, our\nalgorithmic approach handles all examples (including realistic PRRs such as\nQuickSort, QuickSelect, DiameterComputation, etc.) in less than 0.1 seconds,\nshowing that our approach is efficient in practice.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:47:50 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15105","submitter":"Giorgio Di Russo","authors":"Massimo Bianchi, Giorgio Di Russo, Alfredo Grillo, Jose Francisco\n  Morales, Giuseppe Sudano","title":"On the stability and deformability of top stars","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc hep-th","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Topological stars, or top stars for brevity, are smooth horizonless static\nsolutions of Einstein-Maxwell theory in 5-d that reduce to spherically\nsymmetric solutions of Einstein-Maxwell-Dilaton theory in 4-d. We study linear\nscalar perturbations of top stars and argue for their stability and\ndeformability. We tackle the problem with different techniques including WKB\napproximation, numerical analysis, Breit-Wigner resonance method and quantum\nSeiberg-Witten curves. We identify three classes of quasi-normal modes\ncorresponding to prompt-ring down modes, long-lived meta-stable modes and what\nwe dub `blind' modes. All mode frequencies we find have negative imaginary\nparts, thus suggesting linear stability of top stars. Moreover we determine the\ntidal Love and dissipation numbers encoding the response to tidal deformations\nand, similarly to black holes, we find zero value in the static limit but,\ncontrary to black holes, we find non-trivial dynamical Love numbers and\nvanishing dissipative effects at linear order. For the sake of illustration in\na simpler context, we also consider a toy model with a piece-wise constant\npotential and a centrifugal barrier that captures most of the above features in\na qualitative fashion.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:48:14 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15106","submitter":"Amir Subba Mr","authors":"Amir Subba, Ritesh K. Singh","title":"Study of anomalous $W^-W^+\\gamma/Z$ couplings using polarizations and\n  spin correlations in $e^-e^+\\to W^-W^+$ with polarized beams","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We study the anomalous $W^-W^+\\gamma/Z$ couplings in $e^-e^+\\to W^-W^+$\nfollowed by semileptonic decay using a complete set of polarization and spin\ncorrelation observables of $W$ boson with the longitudinally polarized beam. We\nconsider a complete set of dimension-six operators affecting $W^-W^+\\gamma/Z$\nvertex, which are $SU(2)\\times U(1)$ gauge invariant. Some of the polarization\nand spin correlation asymmetries average out if the daughter of $W^+$ is not\ntagged. We developed an artificial neural network and boosted decision trees to\ndistinguish down-type jets from up-type jets. We obtain bounds on the anomalous\ncouplings for center of mass energy $\\sqrt{s} = 250$ GeV with integrated\nluminosities of~$\\mathcal{L}\\in\\{100~\\text{fb}^{-1}, 250~\\text{fb}^{-1},\n1000~\\text{fb}^{-1}, 3000~\\text{fb}^{-1}\\}$. We find that using spin-related\nobservables and cross~section in the presence of initial beam polarization\nsignificantly improves the bounds on anomalous couplings compared to previous\nstudies.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:50:02 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15107","submitter":"David Meadon","authors":"Sven-Erik Ekstr\\\"om and David Meadon","title":"On the eigenvalues of Toeplitz matrices with two off-diagonals","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Consider the Toeplitz matrix $T_n(f)$ generated by the symbol\n$f(\\theta)=\\hat{f}_r e^{\\mathbf{i}r\\theta}+\\hat{f}_0+\\hat{f}_{-s}\ne^{-\\mathbf{i}s\\theta}$, where $\\hat{f}_r, \\hat{f}_0, \\hat{f}_{-s} \\in\n\\mathbb{C}$ and $0<r<n,~0<s<n$. For $r=s=1$ we have the classical tridiagonal\nToeplitz matrices, for which the eigenvalues and eigenvectors are known.\nSimilarly, the eigendecompositions are known for $1<r=s$, when the generated\nmatrices are ``symmetrically sparse tridiagonal''.\n  In the current paper we study the eigenvalues of $T_n(f)$ for $1\\leq r<s$,\nwhich are ``non-symmetrically sparse tridiagonal''. We propose an algorithm\nwhich constructs one or two ad hoc matrices smaller than $T_n(f)$, whose\neigenvalues are sufficient for determining the full spectrum of $T_n(f)$. The\nalgorithm is explained through use of a conjecture for which examples and\nnumerical experiments are reported for supporting it and for clarifying the\npresentation. Open problems are briefly discussed.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:53:28 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15108","submitter":"Debayan Banerjee","authors":"Debayan Banerjee, Pranav Ajit Nair, Ricardo Usbeck, Chris Biemann","title":"The Role of Output Vocabulary in T2T LMs for SPARQL Semantic Parsing","comments":"Accepted as a short paper to ACL 2023 findings","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this work, we analyse the role of output vocabulary for text-to-text (T2T)\nmodels on the task of SPARQL semantic parsing. We perform experiments within\nthe the context of knowledge graph question answering (KGQA), where the task is\nto convert questions in natural language to the SPARQL query language. We\nobserve that the query vocabulary is distinct from human vocabulary. Language\nModels (LMs) are pre-dominantly trained for human language tasks, and hence, if\nthe query vocabulary is replaced with a vocabulary more attuned to the LM\ntokenizer, the performance of models may improve. We carry out carefully\nselected vocabulary substitutions on the queries and find absolute gains in the\nrange of 17% on the GrailQA dataset.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:55:04 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15109","submitter":"Maximilian Prokop","authors":"Jan Kretinsky, Tobias Meggendorfer, Maximilian Prokop, Sabine Rieder","title":"Guessing Winning Policies in LTL Synthesis by Semantic Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.SY eess.SY","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We provide a learning-based technique for guessing a winning strategy in a\nparity game originating from an LTL synthesis problem. A cheaply obtained guess\ncan be useful in several applications. Not only can the guessed strategy be\napplied as best-effort in cases where the game's huge size prohibits rigorous\napproaches, but it can also increase the scalability of rigorous LTL synthesis\nin several ways. Firstly, checking whether a guessed strategy is winning is\neasier than constructing one. Secondly, even if the guess is wrong in some\nplaces, it can be fixed by strategy iteration faster than constructing one from\nscratch. Thirdly, the guess can be used in on-the-fly approaches to prioritize\nexploration in the most fruitful directions.\n  In contrast to previous works, we (i)~reflect the highly structured logical\ninformation in game's states, the so-called semantic labelling, coming from the\nrecent LTL-to-automata translations, and (ii)~learn to reflect it properly by\nlearning from previously solved games, bringing the solving process closer to\nhuman-like reasoning.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:57:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15111","submitter":"Elise \\\"Ozalp","authors":"Elise \\\"Ozalp and Georgios Margazoglou and Luca Magri","title":"Reconstruction, forecasting, and stability of chaotic dynamics from\n  partial data","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"nlin.AO cs.LG nlin.CD","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The forecasting and computation of the stability of chaotic systems from\npartial observations are tasks for which traditional equation-based methods may\nnot be suitable. In this computational paper, we propose data-driven methods to\n(i) infer the dynamics of unobserved (hidden) chaotic variables (full-state\nreconstruction); (ii) time forecast the evolution of the full state; and (iii)\ninfer the stability properties of the full state. The tasks are performed with\nlong short-term memory (LSTM) networks, which are trained with observations\n(data) limited to only part of the state: (i) the low-to-high resolution LSTM\n(LH-LSTM), which takes partial observations as training input, and requires\naccess to the full system state when computing the loss; and (ii) the\nphysics-informed LSTM (PI-LSTM), which is designed to combine partial\nobservations with the integral formulation of the dynamical system's evolution\nequations. First, we derive the Jacobian of the LSTMs. Second, we analyse a\nchaotic partial differential equation, the Kuramoto-Sivashinsky (KS), and the\nLorenz-96 system. We show that the proposed networks can forecast the hidden\nvariables, both time-accurately and statistically. The Lyapunov exponents and\ncovariant Lyapunov vectors, which characterize the stability of the chaotic\nattractors, are correctly inferred from partial observations. Third, the\nPI-LSTM outperforms the LH-LSTM by successfully reconstructing the hidden\nchaotic dynamics when the input dimension is smaller or similar to the\nKaplan-Yorke dimension of the attractor. This work opens new opportunities for\nreconstructing the full state, inferring hidden variables, and computing the\nstability of chaotic systems from partial data.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:01:51 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15112","submitter":"Shivam Bajpeyi Dr.","authors":"Shivam Bajpeyi, Dhiraj Patel and S. Sivananthan","title":"Random Sampling of Mellin Band-limited Signals","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.FA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper, we address the random sampling problem for the class of Mellin\nband-limited functions BT which is concentrated on a bounded cube. It is\nestablished that any function in BT can be approximated by an element in a\nfinite-dimensional subspace of BT. Utilizing the notion of covering number and\nBernstein's inequality to the sum of independent random variables, we prove\nthat the random sampling inequality holds with an overwhelming probability\nprovided the sampling size is large enough.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:03:44 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15113","submitter":"Martin Uray","authors":"Simon Schindler, Martin Uray, Stefan Huber","title":"A Mini Review on the utilization of Reinforcement Learning with OPC UA","comments":"submitted to INDIN'23","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Reinforcement Learning (RL) is a powerful machine learning paradigm that has\nbeen applied in various fields such as robotics, natural language processing\nand game playing achieving state-of-the-art results. Targeted to solve\nsequential decision making problems, it is by design able to learn from\nexperience and therefore adapt to changing dynamic environments. These\ncapabilities make it a prime candidate for controlling and optimizing complex\nprocesses in industry. The key to fully exploiting this potential is the\nseamless integration of RL into existing industrial systems. The industrial\ncommunication standard Open Platform Communications UnifiedArchitecture (OPC\nUA) could bridge this gap. However, since RL and OPC UA are from different\nfields,there is a need for researchers to bridge the gap between the two\ntechnologies. This work serves to bridge this gap by providing a brief\ntechnical overview of both technologies and carrying out a semi-exhaustive\nliterature review to gain insights on how RL and OPC UA are applied in\ncombination. With this survey, three main research topics have been identified,\nfollowing the intersection of RL with OPC UA. The results of the literature\nreview show that RL is a promising technology for the control and optimization\nof industrial processes, but does not yet have the necessary standardized\ninterfaces to be deployed in real-world scenarios with reasonably low effort.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:03:48 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15114","submitter":"Fenghe Tang","authors":"Lingtao Wang, Jianrui Ding, Fenghe Tang, Chunping Ning","title":"Thinking Twice: Clinical-Inspired Thyroid Ultrasound Lesion Detection\n  Based on Feature Feedback","comments":"20 pages, 11 figures, released code for\n  https://github.com/HIT-wanglingtao/Thinking-Twice","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Accurate detection of thyroid lesions is a critical aspect of computer-aided\ndiagnosis. However, most existing detection methods perform only one feature\nextraction process and then fuse multi-scale features, which can be affected by\nnoise and blurred features in ultrasound images. In this study, we propose a\nnovel detection network based on a feature feedback mechanism inspired by\nclinical diagnosis. The mechanism involves first roughly observing the overall\npicture and then focusing on the details of interest. It comprises two parts: a\nfeedback feature selection module and a feature feedback pyramid. The feedback\nfeature selection module efficiently selects the features extracted in the\nfirst phase in both space and channel dimensions to generate high semantic\nprior knowledge, which is similar to coarse observation. The feature feedback\npyramid then uses this high semantic prior knowledge to enhance feature\nextraction in the second phase and adaptively fuses the two features, similar\nto fine observation. Additionally, since radiologists often focus on the shape\nand size of lesions for diagnosis, we propose an adaptive detection head\nstrategy to aggregate multi-scale features. Our proposed method achieves an AP\nof 70.3% and AP50 of 99.0% on the thyroid ultrasound dataset and meets the\nreal-time requirement. The code is available at\nhttps://github.com/HIT-wanglingtao/Thinking-Twice.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:07:46 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15115","submitter":"Yubao Tang","authors":"Yubao Tang, Ruqing Zhang, Jiafeng Guo, Jiangui Chen, Zuowei Zhu,\n  Shuaiqiang Wang, Dawei Yin, Xueqi Cheng","title":"Semantic-Enhanced Differentiable Search Index Inspired by Learning\n  Strategies","comments":"Accepted by KDD 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Recently, a new paradigm called Differentiable Search Index (DSI) has been\nproposed for document retrieval, wherein a sequence-to-sequence model is\nlearned to directly map queries to relevant document identifiers. The key idea\nbehind DSI is to fully parameterize traditional ``index-retrieve'' pipelines\nwithin a single neural model, by encoding all documents in the corpus into the\nmodel parameters. In essence, DSI needs to resolve two major questions: (1) how\nto assign an identifier to each document, and (2) how to learn the associations\nbetween a document and its identifier. In this work, we propose a\nSemantic-Enhanced DSI model (SE-DSI) motivated by Learning Strategies in the\narea of Cognitive Psychology. Our approach advances original DSI in two ways:\n(1) For the document identifier, we take inspiration from Elaboration\nStrategies in human learning. Specifically, we assign each document an\nElaborative Description based on the query generation technique, which is more\nmeaningful than a string of integers in the original DSI; and (2) For the\nassociations between a document and its identifier, we take inspiration from\nRehearsal Strategies in human learning. Specifically, we select fine-grained\nsemantic features from a document as Rehearsal Contents to improve document\nmemorization. Both the offline and online experiments show improved retrieval\nperformance over prevailing baselines.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:09:35 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15116","submitter":"Dominik Th\\\"onnes","authors":"Dominik Th\\\"onnes and Ulrich R\\\"ude","title":"Model-Based Performance Analysis of the HyTeG Finite Element Framework","comments":null,"journal-ref":null,"doi":"10.1145/3592979.3593422","report-no":null,"categories":"cs.PF","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this work, we present how code generation techniques significantly improve\nthe performance of the computational kernels in the HyTeG software framework.\nThis HPC framework combines the performance and memory advantages of\nmatrix-free multigrid solvers with the flexibility of unstructured meshes. The\npystencils code generation toolbox is used to replace the original abstract C++\nkernels with highly optimized loop nests. The performance of one of those\nkernels (the matrix-vector multiplication) is thoroughly analyzed using the\nExecution-Cache-Memory (ECM) performance model. We validate these predictions\nby measurements on the SuperMUC-NG supercomputer. The experiments show that the\nperformance mostly matches the predictions. In cases where the prediction does\nnot match, we discuss the discrepancies. Additionally, we conduct a node-level\nscaling study which shows the expected behavior for a memory-bound compute\nkernel.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:10:25 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15117","submitter":"Christian Herglotz","authors":"Christian Herglotz and Werner Robitza and Alexander Raake and Tobias\n  Hossfeld and Andr\\'e Kaup","title":"Power Reduction Opportunities on End-User Devices in Quality-Steady\n  Video Streaming","comments":"4 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper uses a crowdsourced dataset of online video streaming sessions to\ninvestigate opportunities to reduce the power consumption while considering\nQoE. For this, we base our work on prior studies which model both the\nend-user's QoE and the end-user device's power consumption with the help of\nhigh-level video features such as the bitrate, the frame rate, and the\nresolution. On top of existing research, which focused on reducing the power\nconsumption at the same QoE optimizing video parameters, we investigate\npotential power savings by other means such as using a different playback\ndevice, a different codec, or a predefined maximum quality level. We find that\nbased on the power consumption of the streaming sessions from the crowdsourcing\ndataset, devices could save more than 55% of power if all participants adhere\nto low-power settings.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:10:40 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15118","submitter":"Federico Fusco","authors":"Marwa El Halabi, Federico Fusco, Ashkan Norouzi-Fard, Jakab Tardos,\n  Jakub Tarnawski","title":"Fairness in Streaming Submodular Maximization over a Matroid Constraint","comments":"Accepted to ICML 23","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CY cs.DS","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Streaming submodular maximization is a natural model for the task of\nselecting a representative subset from a large-scale dataset. If datapoints\nhave sensitive attributes such as gender or race, it becomes important to\nenforce fairness to avoid bias and discrimination. This has spurred significant\ninterest in developing fair machine learning algorithms. Recently, such\nalgorithms have been developed for monotone submodular maximization under a\ncardinality constraint.\n  In this paper, we study the natural generalization of this problem to a\nmatroid constraint. We give streaming algorithms as well as impossibility\nresults that provide trade-offs between efficiency, quality and fairness. We\nvalidate our findings empirically on a range of well-known real-world\napplications: exemplar-based clustering, movie recommendation, and maximum\ncoverage in social networks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:10:46 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15119","submitter":"Alberto Mu\\~noz-Ortiz","authors":"Alberto Mu\\~noz-Ortiz and David Vilares","title":"Another Dead End for Morphological Tags? Perturbed Inputs and Parsing","comments":"Accepted at Findings of ACL 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The usefulness of part-of-speech tags for parsing has been heavily questioned\ndue to the success of word-contextualized parsers. Yet, most studies are\nlimited to coarse-grained tags and high quality written content; while we know\nlittle about their influence when it comes to models in production that face\nlexical errors. We expand these setups and design an adversarial attack to\nverify if the use of morphological information by parsers: (i) contributes to\nerror propagation or (ii) if on the other hand it can play a role to correct\nmistakes that word-only neural parsers make. The results on 14 diverse UD\ntreebanks show that under such attacks, for transition- and graph-based models\ntheir use contributes to degrade the performance even faster, while for the\n(lower-performing) sequence labeling parsers they are helpful. We also show\nthat if morphological tags were utopically robust against lexical\nperturbations, they would be able to correct parsing mistakes.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:11:04 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15120","submitter":"Patrick Meisner","authors":"Chantal David, Patrick Meisner","title":"Expected Values of $L$-functions Away from the Central Point","comments":"31 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.NT","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We compute the expected value of Dirichlet $L$-functions defined over\n$\\mathbb{F}_q[T]$ attached to cubic characters evaluated at an arbitrary $s \\in\n(0,1)$. We find a transition term at the point $s=\\frac{1}{3}$, reminiscent of\nthe transition at the point $s=\\frac{1}{2}$ of the bound for the size of an\n$L$-function implied by the Lindel\\\"of hypothesis. We show that at\n$s=\\frac{1}{3}$, the expected value matches corresponding statistics of the\ngroup of unitary matrices multiplied by a weight function.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:11:26 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15121","submitter":"Hugo Thimonier","authors":"Hugo Thimonier, Fabrice Popineau, Arpad Rimmel and Bich-Li\\^en Doan","title":"Beyond Individual Input for Deep Anomaly Detection on Tabular Data","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Anomaly detection is crucial in various domains, such as finance, healthcare,\nand cybersecurity. In this paper, we propose a novel deep anomaly detection\nmethod for tabular data that leverages Non-Parametric Transformers (NPTs), a\nmodel initially proposed for supervised tasks, to capture both feature-feature\nand sample-sample dependencies. In a reconstruction-based framework, we train\nthe NPT model to reconstruct masked features of normal samples. We use the\nmodel's ability to reconstruct the masked features during inference to generate\nan anomaly score. To the best of our knowledge, our proposed method is the\nfirst to combine both feature-feature and sample-sample dependencies for\nanomaly detection on tabular datasets. We evaluate our method on an extensive\nbenchmark of tabular datasets and demonstrate that our approach outperforms\nexisting state-of-the-art methods based on both the F1-Score and AUROC.\nMoreover, our work opens up new research directions for exploring the potential\nof NPTs for other tasks on tabular data.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:13:26 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15122","submitter":"Gon\\c{c}alo Paulo","authors":"Gon\\c{c}alo Paulo and Alberto Gubbiotti and Alberto Giacomello","title":"An atomistically informed multiscale approach to the intrusion and\n  extrusion of water in hydrophobic nanopores","comments":"This article may be downloaded for personal use only. Any other use\n  requires prior permission of the author and AIP Publishing. This article\n  appeared in \"Gon\\c{c}alo Paulo, Alberto Gubbiotti, Alberto Giacomello; J.\n  Chem. Phys. 28 May 2023; 158 (20)\" and may be found at\n  https://doi.org/10.1063/5.0147647","journal-ref":"J. Chem. Phys. 28 May 2023; 158 (20): 204707","doi":"10.1063/5.0147647","report-no":null,"categories":"cond-mat.soft","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Understanding intrusion and extrusion in nanoporous materials is a\nchallenging multiscale problem of utmost importance for applications ranging\nfrom energy storage and dissipation to water desalination and hydrophobic\ngating in ion channels. Including atomistic details in simulations is required\nto predict the overall behavior of such systems, because the statics and\ndynamics of these processes depend sensitively on microscopic features of the\npore such as the surface hydrophobicity, geometry, and charge distribution and\non the composition of the liquid. On the other hand, the transitions between\nthe filled (intruded) and empty (extruded) states are rare events which often\nrequire long simulation times difficult to achieve with standard atomistic\nsimulations. In this work, we explored the intrusion and extrusion processes by\na multiscale approach in which the atomistic details of the system, extracted\nfrom molecular dynamics simulations, inform a simple Langevin model of water\nintrusion/extrusion in the pore. We then used the Langevin simulations to\ncompute the transition times at different pressures, validating our\ncoarse-grained model by comparing it with nonequilibrium molecular dynamics\nsimulations. The proposed approach reproduces experimentally relevant features\nsuch as the time and temperature dependence of the intrusion/extrusion cycles,\nas well as specific details about the shape of the cycle. This approach also\ndrastically increases the timescales that can be simulated allowing to reduce\nthe gap between simulations and experiments and showing promise for more\ncomplex systems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:14:22 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15123","submitter":"Manas Kulkarni","authors":"Manas Kulkarni, Satya N. Majumdar","title":"First detection probability in quantum resetting via random projective\n  measurements","comments":"40 pages, 6 figures, 1 table","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.stat-mech","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We provide a general framework to compute the probability distribution\n$F_r(t)$ of the first detection time of a 'state of interest' in a generic\nquantum system subjected to random projective measurements. In our 'quantum\nresetting' protocol, resetting of a state is not implemented by an additional\nclassical stochastic move, but rather by the random projective measurement. We\nthen apply this general framework to Poissoinian measurement protocol with a\nconstant rate $r$ and demonstrate that exact results for $F_r(t)$ can be\nobtained for a generic two level system. Interestingly, the result depends\ncrucially on the detection schemes involved and we have studied two\ncomplementary schemes, where the state of interest either coincides or differs\nfrom the initial state. We show that $F_r(t)$ at short times vanishes\nuniversally as $F_r(t)\\sim t^2$ as $t\\to 0$ in the first scheme, while it\napproaches a constant as $t\\to 0$ in the second scheme. The mean first\ndetection time, as a function of the measurement rate $r$, also shows rather\ndifferent behaviors in the two schemes. In the former, the mean detection time\nis a nonmonotonic function of $r$ with a single minimum at an optimal value\n$r^*$, while in the later, it is a monotonically decreasing function of $r$,\nsignalling the absence of a finite optimal value. These general predictions for\narbitrary two level systems are then verified via explicit computation in the\nJaynes-Cummings model of light-matter interaction. We also generalise our\nresults to non-Poissonian measurement protocols with a renewal structure where\nthe intervals between successive independent measurements are distributed via a\ngeneral distribution $p(\\tau)$ and show that the short time behavior of\n$F_r(t)\\sim p(0)\\, t^2$ is universal as long as $p(0)\\ne 0$. This universal\n$t^2$ law emerges from purely quantum dynamics that dominates at early times.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:15:01 GMT"},{"version":"v2","created":"Sat, 3 Jun 2023 17:12:43 GMT"}],"update_date":"2023-06-06"}
{"id":"2305.15124","submitter":"Anurag Dey","authors":"Anurag Dey and Probal Chaudhuri","title":"On estimators of the mean of infinite dimensional data in finite\n  populations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.ST stat.TH","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  The Horvitz-Thompson (HT), the Rao-Hartley-Cochran (RHC) and the generalized\nregression (GREG) estimators of the finite population mean are considered, when\nthe observations are from an infinite dimensional space. We compare these\nestimators based on their asymptotic distributions under some commonly used\nsampling designs and some superpopulations satisfying linear regression models.\nWe show that the GREG estimator is asymptotically at least as efficient as any\nof the other two estimators under different sampling designs considered in this\npaper. Further, we show that the use of some well known sampling designs\nutilizing auxiliary information may have an adverse effect on the performance\nof the GREG estimator, when the degree of heteroscedasticity present in linear\nregression models is not very large. On the other hand, the use of those\nsampling designs improves the performance of this estimator, when the degree of\nheteroscedasticity present in linear regression models is large. We develop\nmethods for determining the degree of heteroscedasticity, which in turn\ndetermines the choice of appropriate sampling design to be used with the GREG\nestimator. We also investigate the consistency of the covariance operators of\nthe above estimators. We carry out some numerical studies using real and\nsynthetic data, and our theoretical results are supported by the results\nobtained from those numerical studies.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:15:41 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15125","submitter":"Kazuo Murota","authors":"Kazuo Murota and Akihisa Tamura","title":"Shapley-Folkman-type Theorem for Integrally Convex Sets","comments":"13 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.CO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The Shapley-Folkman theorem is a statement about the Minkowski sum of\n(non-convex) sets, expressing the closeness of the Minkowski sum to convexity\nin a quantitative manner. This paper establishes similar theorems for\nintegrally convex sets and M-natural-convex sets, which are major classes of\ndiscrete convex sets in discrete convex analysis.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:18:16 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15126","submitter":"Kotaro Kohno","authors":"K. Kohno, S. Fujimoto, A. Tsujita, V. Kokorev, G. Brammer, G. E.\n  Magdis, F. Valentino, N. Laporte, Fengwu Sun, E. Egami, F. E. Bauer, A.\n  Guerrero, N. Nagar, K. I. Caputi, G. B. Caminha, J.-B. Jolly, K. K. Knudsen,\n  R. Uematsu, Y. Ueda, M. Oguri, A. Zitrin, M. Ouchi, Y. Ono, J.\n  Gonzalez-Lopez, J. Richard, I. Smail, D. Coe, M. Postman, L. Bradley, A. M.\n  Koekemoer, A. M. Munoz Arancibia, M. Dessauges-Zavadsky, D. Espada, H.\n  Umehata, B. Hatsukade, F. Egusa, K. Shimasaku, K. Matsui-Morokuma, W.-H.\n  Wang, T. Wang, Y. Ao, A. J. Baker, Minju M. Lee, C. del P. Lagos, D. H.\n  Hughes and ALCS collaboration","title":"Unbiased surveys of dust-enshrouded galaxies using ALMA","comments":"6 pages, 4 figures, Proceedings of the 7th\n  Chile-Cologne-Bonn-Symposium: Physics and Chemistry of Star Formation, V.\n  Ossenkopf-Okada, R. Schaaf, I. Breloy (eds.)","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.GA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The ALMA lensing cluster survey (ALCS) is a 96-hr large program dedicated to\nuncovering and characterizing intrinsically faint continuum sources and line\nemitters with the assistance of gravitational lensing. All 33 cluster fields\nwere selected from HST/Spitzer treasury programs including CLASH, Hubble\nFrontier Fields, and RELICS, which also have Herschel and Chandra coverages.\nThe total sky area surveyed reaches $\\sim$133 arcmin$^2$ down to a depth of\n$\\sim$60 $\\mu$Jy beam$^{-1}$ (1$\\sigma$) at 1.2 mm, yielding 141 secure blind\ndetections of continuum sources and additional 39 sources aided by priors. We\npresent scientific motivation, survey design, the status of spectroscopy\nfollow-up observations, and number counts down to $\\sim$7 $\\mu$Jy. Synergies\nwith JWST are also discussed.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:19:41 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15127","submitter":"Lorenz Diener","authors":"Lorenz Diener, Marju Purin, Sten Sootla, Ando Saabas, Robert Aichner,\n  Ross Cutler","title":"PLCMOS -- a data-driven non-intrusive metric for the evaluation of\n  packet loss concealment algorithms","comments":"to appear: INTERSPEECH 2023, associated model release:\n  https://aka.ms/PLCMOS","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD eess.AS","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Speech quality assessment is a problem for every researcher working on models\nthat produce or process speech. Human subjective ratings, the gold standard in\nspeech quality assessment, are expensive and time-consuming to acquire in a\nquantity that is sufficient to get reliable data, while automated objective\nmetrics show a low correlation with gold standard ratings. This paper presents\nPLCMOS, a non-intrusive data-driven tool for generating a robust, accurate\nestimate of the mean opinion score a human rater would assign an audio file\nthat has been processed by being transmitted over a degraded packet-switched\nnetwork with missing packets being healed by a packet loss concealment\nalgorithm. Our new model shows a model-wise Pearson's correlation of ~0.97 and\nrank correlation of ~0.95 with human ratings, substantially above all other\navailable intrusive and non-intrusive metrics. The model is released as an ONNX\nmodel for other researchers to use when building PLC systems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:21:22 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15128","submitter":"He (Henry) Chen","authors":"Qian Wang and He (Henry) Chen","title":"Age of Information in Reservation Multi-Access Networks with Stochastic\n  Arrivals: Analysis and Optimization","comments":"This work has been submitted for possible publication. arXiv admin\n  note: substantial text overlap with arXiv:2206.00874","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT cs.NI math.IT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper analyzes and optimizes the average Age of Information (AAoI) of\nFrame Slotted ALOHA with Reservation and Data slots (FSA-RD) in a multi-access\nnetwork, where multiple users transmit their randomly generated status updates\nto a common access point in a framed manner. Each frame consists of one\nreservation slot and several data slots. The reservation slot is further split\ninto several mini-slots. In each reservation slot, users that want to transmit\na status update will randomly send short reservation packets in one of the\nmini-slots to contend for data slots of the current frame. The reservation is\nsuccessful only if one reservation packet is sent in a mini-slot. The data\nslots are then allocated to those users that succeed in the reservation slot.\nIn the considered FSA-RD scheme, one user with a status update for\ntransmission, termed active user, may need to perform multiple reservation\nattempts before successfully delivering it. As such, the number of active\nuser(s) in different frames are dependent and thus the probability of making a\nsuccessful reservation varies from frame to frame, making the AAoI analysis\nnon-trivial. We manage to derive an analytical expression of AAoI for FSA-RD by\ncharacterizing the evolution of the number of active user(s) in each frame as a\ndiscrete-time Markov chain. We then consider the FSA-RD scheme with one\nreservation attempt per status update, termed FSA-RD-One. Thanks to the\nindependent frame behaviors of FSA-RD-One, we attain a closed-form expression\nfor its AAoI, which is further used to find the near-optimal reservation\nprobability. Our analysis reveals the impact of key protocol parameters, such\nas frame size and reservation probability, on the AAoI. Simulation results\nvalidate our analysis and show that the optimized FSA-RD outperforms the\noptimized slotted ALOHA.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:22:45 GMT"},{"version":"v2","created":"Thu, 25 May 2023 07:04:03 GMT"},{"version":"v3","created":"Sun, 28 May 2023 02:45:33 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.15129","submitter":"Yong Pang Dr","authors":"Yong Pang, Tao Liu","title":"Quasi-static responses of marine mussel plaques attached to deformable\n  wet substrates under directional tensions","comments":"19 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.bio-ph","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Quantifying the response of marine mussel plaque attachment on wet surfaces\nremains a significant challenge to a mechanistic understanding of plaque\nadhesion. Here, we developed a customised microscopy system combined with\ntwo-dimensional (2D) in-situ digital image correlation (DIC) to quantify the\nin-plane deformation of a deformable substrate that interacts with a mussel\nplaque while under directional tension. By analysing the strain field in the\nsubstrate, we gained insight into how in-plane traction forces are transmitted\nfrom the mussel plaque to the underlying substrate. Finite element (FE) models\nwere developed to assist the interpretation of the experimental measurement.\nOur study revealed a synergistic effect of pulling angle and substrate\nstiffness on plaque detachment, with mussel plaques anchoring to a 'stiff'\nsubstrate at a smaller pulling angle having mechanical advantages with higher\nload-bearing capacity and less plaque deformation. We identified two distinct\nfailure modes, i.e., shear traction-governed failure (STGF) mode and normal\ntraction-governed failure (NTGF). It was found that increasing the substrate\nstiffness or reducing the pulling angle resulted in a failure mode change from\nNTGF to STGF. Our findings offer new insights into the mechanistic\nunderstanding of plaque and substrate interaction, which provides a general\nplaque-inspired strategy for wet adhesion.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:22:59 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15130","submitter":"Saku Sugawara","authors":"Saku Sugawara, Shun Tsugita","title":"On Degrees of Freedom in Defining and Testing Natural Language\n  Understanding","comments":"Accepted to Findings of ACL 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Natural language understanding (NLU) studies often exaggerate or\nunderestimate the capabilities of systems, thereby limiting the reproducibility\nof their findings. These erroneous evaluations can be attributed to the\ndifficulty of defining and testing NLU adequately. In this position paper, we\nreconsider this challenge by identifying two types of researcher degrees of\nfreedom. We revisit Turing's original interpretation of the Turing test and\nindicate that an NLU test does not provide an operational definition; it merely\nprovides inductive evidence that the test subject understands the language\nsufficiently well to meet stakeholder objectives. In other words, stakeholders\nare free to arbitrarily define NLU through their objectives. To use the test\nresults as inductive evidence, stakeholders must carefully assess if the\ninterpretation of test scores is valid or not. However, designing and using NLU\ntests involve other degrees of freedom, such as specifying target skills and\ndefining evaluation metrics. As a result, achieving consensus among\nstakeholders becomes difficult. To resolve this issue, we propose a validity\nargument, which is a framework comprising a series of validation criteria\nacross test components. By demonstrating that current practices in NLU studies\ncan be associated with those criteria and organizing them into a comprehensive\nchecklist, we prove that the validity argument can serve as a coherent\nguideline for designing credible test sets and facilitating scientific\ncommunication.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:25:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15131","submitter":"Praveen Chandra Srivastava Dr.","authors":"Sakshi Shukla, Praveen C. Srivastava, Larry Zamick","title":"Systematic shell-model study for structure and isomeric states in\n  $^{200-210}$Po isotopes","comments":"20 pages, 11 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"nucl-th nucl-ex","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We report systematic large-scale shell-model calculation for Po isotopes with\n$A=$ 200 to 210. We have performed calculations using KHH7B interaction in the\nmodel space $Z$ = 58-114 and $N$ = 100-164 around doubly-magic $^{208}$Pb. We\nallow valence neutrons to occupy in the $1f_{5/2}$, $2p_{3/2}$, $2p_{1/2}$, and\n$0i_{13/2}$ orbitals, while two valence protons beyond $Z=82$ are occupied in\n$0h_{9/2}$, $1f_{7/2}$ and $0i_{13/2}$ orbitals. The calculated energies and\nelectromagnetic properties are compared with the available experimental data\nand predicted where experimental data are not available. We have also reported\nshell-model results for different isomeric states of these nuclei.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:26:19 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15132","submitter":"Takatora Suzuki","authors":"Takatora Suzuki, Han Guo, Momoko Hayamizu","title":"Rooted Almost-binary Phylogenetic Networks for which the Maximum\n  Covering Subtree Problem is Solvable in Linear Time","comments":"16 pages, 12 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.CO cs.DM q-bio.PE","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Phylogenetic networks are a flexible model of evolution that can represent\nreticulate evolution and handle complex data. Tree-based networks, which are\nphylogenetic networks that have a spanning tree with the same root and leaf-set\nas the network itself, have been well studied. However, not all networks are\ntree-based. Francis-Semple-Steel (2018) thus introduced several indices to\nmeasure the deviation of rooted binary phylogenetic networks $N$ from being\ntree-based, such as the minimum number $\\delta^\\ast(N)$ of additional leaves\nneeded to make $N$ tree-based, and the minimum difference $\\eta^\\ast(N)$\nbetween the number of vertices of $N$ and the number of vertices of a subtree\nof $N$ that shares the root and leaf set with $N$. Hayamizu (2021) has\nestablished a canonical decomposition of almost-binary phylogenetic networks of\n$N$, called the maximal zig-zag trail decomposition, which has many\nimplications including a linear time algorithm for computing $\\delta^\\ast(N)$.\nThe Maximum Covering Subtree Problem (MCSP) is the problem of computing\n$\\eta^\\ast(N)$, and Davidov et al. (2022) showed that this can be solved in\npolynomial time (in cubic time when $N$ is binary) by an algorithm for the\nminimum cost flow problem. In this paper, under the assumption that $N$ is\nalmost-binary (i.e. each internal vertex has in-degree and out-degree at most\ntwo), we show that $\\delta^\\ast(N)\\leq \\eta^\\ast (N)$ holds, which is tight,\nand give a characterisation of such phylogenetic networks $N$ that satisfy\n$\\delta^\\ast(N)=\\eta^\\ast(N)$. Our approach uses the canonical decomposition of\n$N$ and focuses on how the maximal W-fences (i.e. the forbidden subgraphs of\ntree-based networks) are connected to maximal M-fences in the network $N$. Our\nresults introduce a new class of phylogenetic networks for which MCSP can be\nsolved in linear time, which can be seen as a generalisation of tree-based\nnetworks.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:26:41 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15133","submitter":"Katie Ansaldi","authors":"Katie Ansaldi, Gabriel Cowley, Eric Green, Kihyun Kim, JT Rapp","title":"Rainbow Free Colorings and Rainbow Numbers for $x-y=z^2$","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.CO math.NT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  An exact r-coloring of a set $S$ is a surjective function $c:S \\rightarrow\n\\{1, 2, \\ldots,r\\}$. A rainbow solution to an equation over $S$ is a solution\nsuch that all components are a different color. We prove that every 3-coloring\nof $\\mathbb{N}$ with an upper density greater than $(4^s-1)/(3 \\cdot 4^s)$\ncontains a rainbow solution to $x-y=z^k$. The rainbow number for an equation in\nthe set $S$ is the smallest integer $r$ such that every exact $r$-coloring has\na rainbow solution. We compute the rainbow numbers of $\\mathbb{Z}_p$ for the\nequation $x-y=z^k$, where $p$ is prime and $k\\geq 2$.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:26:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15134","submitter":"Jinjin Gu","authors":"Jinjin Gu, Xianzheng Ma, Xiangtao Kong, Yu Qiao, Chao Dong","title":"Networks are Slacking Off: Understanding Generalization Problem in Image\n  Deraining","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Deep deraining networks, while successful in laboratory benchmarks,\nconsistently encounter substantial generalization issues when deployed in\nreal-world applications. A prevailing perspective in deep learning encourages\nthe use of highly complex training data, with the expectation that a richer\nimage content knowledge will facilitate overcoming the generalization problem.\nHowever, through comprehensive and systematic experimentation, we discovered\nthat this strategy does not enhance the generalization capability of these\nnetworks. On the contrary, it exacerbates the tendency of networks to overfit\nto specific degradations. Our experiments reveal that better generalization in\na deraining network can be achieved by simplifying the complexity of the\ntraining data. This is due to the networks are slacking off during training,\nthat is, learning the least complex elements in the image content and\ndegradation to minimize training loss. When the complexity of the background\nimage is less than that of the rain streaks, the network will prioritize the\nreconstruction of the background, thereby avoiding overfitting to the rain\npatterns and resulting in improved generalization performance. Our research not\nonly offers a valuable perspective and methodology for better understanding the\ngeneralization problem in low-level vision tasks, but also displays promising\npractical potential.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:27:11 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15135","submitter":"David Salomoni","authors":"D. Salomoni, Y. Peng, L. Farcis, S. Auffret, M. Hehn, G. Malinowski,\n  S. Mangin, B. Dieny, L. D. Buda-Prejbeanu, R.C. Sousa, I. L. Prejbeanu","title":"Field-free all-optical switching and electrical read-out of Tb/Co-based\n  magnetic tunnel junctions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.app-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Switching of magnetic tunnel junction using femto-second laser enables a\npossible path for THz frequency memory operation, which means writing speeds 2\norders of magnitude faster than alternative electrical approaches based on spin\ntransfer or spin orbit torque. In this work we demonstrate successful\nfield-free 50fs single laser pulse driven magnetization reversal of [Tb/Co]\nbased storage layer in a perpendicular magnetic tunnel junction. The\nnanofabricated magnetic tunnel junction devices have an optimized bottom\nreference electrode and show Tunnel Magnetoresistance Ratio values (TMR) up to\n74\\% after patterning down to sub-100nm lateral dimensions. Experiments on\ncontinuous films reveal peculiar reversal patterns of concentric rings with\nopposite magnetic directions, above certain threshold fluence. These rings have\nbeen correlated to patterned device switching probability as a function of the\napplied laser fluence. Moreover, the magnetization reversal is independent on\nthe duration of the laser pulse. According to our macrospin model, the\nunderlying magnetization reversal mechanism can be attributed to an in-plane\nreorientation of the magnetization due to a fast reduction of the out-of-plane\nuniaxial anisotropy. These aspects are of great interest both for the physical\nunderstanding of the switching phenomenon and their consequences for\nall-optical-switching memory devices, since they allow for a large fluence\noperation window with high resilience to pulse length variability.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:29:18 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15136","submitter":"Xiao Li","authors":"Huikang Liu, Xiao Li, Anthony Man-Cho So","title":"ReSync: Riemannian Subgradient-based Robust Rotation Synchronization","comments":"24 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This work presents ReSync, a Riemannian subgradient-based algorithm for\nsolving the robust rotation synchronization problem, which arises in various\nengineering applications. ReSync solves a least-unsquared minimization\nformulation over the rotation group, which is nonsmooth and nonconvex, and aims\nat recovering the underlying rotations directly. We provide strong theoretical\nguarantees for ReSync under the random corruption setting. Specifically, we\nfirst show that the initialization procedure of ReSync yields a proper initial\npoint that lies in a local region around the ground-truth rotations. We next\nestablish the weak sharpness property of the aforementioned formulation and\nthen utilize this property to derive the local linear convergence of ReSync to\nthe ground-truth rotations. By combining these guarantees, we conclude that\nReSync converges linearly to the ground-truth rotations under appropriate\nconditions. Experiment results demonstrate the effectiveness of ReSync.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:31:11 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15137","submitter":"Matteo Rinaldi Dr.","authors":"Matteo Rinaldi, Matous Mrovec, Anton Bochkarev, Yury Lysogorskiy, and\n  Ralf Drautz","title":"Non-collinear Magnetic Atomic Cluster Expansion for Iron","comments":"19 pages, 20 figures, 2 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  The Atomic Cluster Expansion (ACE) provides a formally complete basis for the\nlocal atomic environment. ACE is not limited to representing energies as a\nfunction of atomic positions and chemical species, but can be generalized to\nvectorial or tensorial properties and to incorporate further degrees of freedom\n(DOF). This is crucial for magnetic materials with potential energy surfaces\nthat depend on atomic positions and atomic magnetic moments simultaneously. In\nthis work, we employ the ACE formalism to develop a non-collinear magnetic ACE\nparametrization for the prototypical magnetic element Fe. The model is trained\non a broad range of collinear and non-collinear magnetic structures calculated\nusing spin density functional theory. We demonstrate that the non-collinear\nmagnetic ACE is able to reproduce not only ground state properties of various\nmagnetic phases of Fe but also the magnetic and lattice excitations that are\nessential for a correct description of the finite temperature behavior and\nproperties of crystal defects.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:31:23 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15138","submitter":"Chunpu Xu","authors":"Chunpu Xu, Jing Li, Piji Li, Min Yang","title":"Topic-Guided Self-Introduction Generation for Social Media Users","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Millions of users are active on social media. To allow users to better\nshowcase themselves and network with others, we explore the auto-generation of\nsocial media self-introduction, a short sentence outlining a user's personal\ninterests. While most prior work profiles users with tags (e.g., ages), we\ninvestigate sentence-level self-introductions to provide a more natural and\nengaging way for users to know each other. Here we exploit a user's tweeting\nhistory to generate their self-introduction. The task is non-trivial because\nthe history content may be lengthy, noisy, and exhibit various personal\ninterests. To address this challenge, we propose a novel unified topic-guided\nencoder-decoder (UTGED) framework; it models latent topics to reflect salient\nuser interest, whose topic mixture then guides encoding a user's history and\ntopic words control decoding their self-introduction. For experiments, we\ncollect a large-scale Twitter dataset, and extensive results show the\nsuperiority of our UTGED to the advanced encoder-decoder models without topic\nmodeling.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:35:08 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15139","submitter":"Nicolas Blanco","authors":"Nicolas Blanco","title":"Bifibrations of polycategories and classical multiplicative linear logic","comments":"250 pages, 15 figures, PhD thesis in the Theory Group at the Computer\n  Science School of the University of Birmingham under the supervision of Noam\n  Zeilberger and Paul Levy","journal-ref":null,"doi":null,"report-no":null,"categories":"math.CT cs.LO","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this thesis, we develop the theory of bifibrations of polycategories.\n  We start by studying how to express certain categorical structures as\nuniversal properties by generalising the shape of morphism. We call this\nphenomenon representability and look at different variations, namely the\ncorrespondence between representable multicategories and monoidal categories,\nbirepresentable polycategories and $\\ast$-autonomous categories, and\nrepresentable virtual double categories and double categories.\n  We then move to introduce (bi)fibrations for these structures. We show that\nit generalises representability in the sense that these structures are\n(bi)representable when they are (bi)fibred over the terminal one. We show how\nto use this theory to lift models of logic to more refined ones. In particular,\nwe illustrate it by lifting the compact closed structure of the category of\nfinite dimensional vector spaces and linear maps to the (non-compact)\n$\\ast$-autonomous structure of the category of finite dimensional Banach spaces\nand contractive maps by passing to their respective polycategories. We also\ngive an operational reading of this example, where polylinear maps correspond\nto operations between systems that can act on their inputs and whose outputs\ncan be measured/probed and where norms correspond to properties of the systems\nthat are preserved by the operations.\n  Finally, we recall the B\\'enabou-Grothendieck correspondence linking\nfibrations to indexed categories. We show how the B-G construction can be\ndefined as a pullback of virtual double categories and we make use of\nfibrational properties of vdcs to get properties of this pullback. Then we\nprovide a polycategorical version of the B-G correspondence.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:35:51 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15140","submitter":"Hanlin Ren","authors":"Lijie Chen, Zhenjian Lu, Igor C. Oliveira, Hanlin Ren, and Rahul\n  Santhanam","title":"Polynomial-Time Pseudodeterministic Construction of Primes","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CC cs.DM cs.DS","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  A randomized algorithm for a search problem is *pseudodeterministic* if it\nproduces a fixed canonical solution to the search problem with high\nprobability. In their seminal work on the topic, Gat and Goldwasser posed as\ntheir main open problem whether prime numbers can be pseudodeterministically\nconstructed in polynomial time.\n  We provide a positive solution to this question in the infinitely-often\nregime. In more detail, we give an *unconditional* polynomial-time randomized\nalgorithm $B$ such that, for infinitely many values of $n$, $B(1^n)$ outputs a\ncanonical $n$-bit prime $p_n$ with high probability. More generally, we prove\nthat for every dense property $Q$ of strings that can be decided in polynomial\ntime, there is an infinitely-often pseudodeterministic polynomial-time\nconstruction of strings satisfying $Q$. This improves upon a\nsubexponential-time construction of Oliveira and Santhanam.\n  Our construction uses several new ideas, including a novel bootstrapping\ntechnique for pseudodeterministic constructions, and a quantitative\noptimization of the uniform hardness-randomness framework of Chen and Tell,\nusing a variant of the Shaltiel--Umans generator.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:35:57 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15141","submitter":"Guy Kornowski","authors":"Guy Kornowski, Gilad Yehudai, Ohad Shamir","title":"From Tempered to Benign Overfitting in ReLU Neural Networks","comments":"43 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.NE stat.ML","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Overparameterized neural networks (NNs) are observed to generalize well even\nwhen trained to perfectly fit noisy data. This phenomenon motivated a large\nbody of work on \"benign overfitting\", where interpolating predictors achieve\nnear-optimal performance. Recently, it was conjectured and empirically observed\nthat the behavior of NNs is often better described as \"tempered overfitting\",\nwhere the performance is non-optimal yet also non-trivial, and degrades as a\nfunction of the noise level. However, a theoretical justification of this claim\nfor non-linear NNs has been lacking so far. In this work, we provide several\nresults that aim at bridging these complementing views. We study a simple\nclassification setting with 2-layer ReLU NNs, and prove that under various\nassumptions, the type of overfitting transitions from tempered in the extreme\ncase of one-dimensional data, to benign in high dimensions. Thus, we show that\nthe input dimension has a crucial role on the type of overfitting in this\nsetting, which we also validate empirically for intermediate dimensions.\nOverall, our results shed light on the intricate connections between the\ndimension, sample size, architecture and training algorithm on the one hand,\nand the type of resulting overfitting on the other hand.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:36:06 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15142","submitter":"Clemens Thielen","authors":"Cristina Bazgan, Arne Herzel, Stefan Ruzika, Clemens Thielen, Daniel\n  Vanderpooten","title":"Approximating Multiobjective Optimization Problems: How exact can you\n  be?","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC cs.DS","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  It is well known that, under very weak assumptions, multiobjective\noptimization problems admit $(1+\\varepsilon,\\dots,1+\\varepsilon)$-approximation\nsets (also called $\\varepsilon$-Pareto sets) of polynomial cardinality (in the\nsize of the instance and in $\\frac{1}{\\varepsilon}$). While an approximation\nguarantee of $1+\\varepsilon$ for any $\\varepsilon>0$ is the best one can expect\nfor singleobjective problems (apart from solving the problem to optimality),\neven better approximation guarantees than $(1+\\varepsilon,\\dots,1+\\varepsilon)$\ncan be considered in the multiobjective case since the approximation might be\nexact in some of the objectives.\n  Hence, in this paper, we consider partially exact approximation sets that\nrequire to approximate each feasible solution exactly, i.e., with an\napproximation guarantee of $1$, in some of the objectives while still obtaining\na guarantee of $1+\\varepsilon$ in all others. We characterize the types of\npolynomial-cardinality, partially exact approximation sets that are guaranteed\nto exist for general multiobjective optimization problems. Moreover, we study\nminimum-cardinality partially exact approximation sets concerning (weak)\nefficiency of the contained solutions and relate their cardinalities to the\nminimum cardinality of a $(1+\\varepsilon,\\dots,1+\\varepsilon)$-approximation\nset.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:37:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15143","submitter":"Almudena Alonso-Herrero","authors":"A. Alonso-Herrero, S. Garcia-Burillo, M. Pereira-Santaella, T.\n  Shimizu, F. Combes, E. K. S. Hicks, R. Davies, C. Ramos Almeida, I.\n  Garcia-Bernete, S. F. Hoenig, N. A. Levenson, C. Packham, E. Bellocchi, L. K.\n  Hunt, M. Imanishi, C. Ricci, P. Roche","title":"AGN feedback in action in the molecular gas ring of the Seyfert galaxy\n  NGC7172","comments":"Accepted for publication to A&A","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.GA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We present new ALMA observations of the CO(3-2) transition and 854micron\ncontinuum at 0.06-0.3\" resolution, together with new VLT/SINFONI observations\nof NGC7172. This is a luminous (bolometric luminosity of ~10^44 erg/s) Seyfert\ngalaxy that belongs to the Galaxy Activity, Torus, and Outflow Survey (GATOS).\nThe CO(3-2) observations reveal the presence of a highly inclined cold\nmolecular gas ring with an approximate radius of 3-4\"~540-720 pc, which is\nlikely associated with an inner Lindblad resonance of a putative stellar bar.\nThere are noncircular motions in the VLT/SINFONI [SiVI]1.96micron and H2 at\n2.12micron, and ALMA CO(3-2) velocity fields. After subtracting the stellar\nvelocity field, we detected [SiVI] blueshifted velocities of a few hundred km/s\nto the south of the AGN. They trace outflowing ionized gas outside the plane of\nthe galaxy and out to projected distances of ~200 pc. The CO(3-2)\nposition-velocity diagram along the kinematic minor axis displays noncircular\nmotions with observed velocities of up to ~150 km/s. Assuming that these are\ntaking place in the disk of the galaxy, the observed velocity signs imply that\nthe molecular gas ring is not only rotating but also outflowing. We derived an\nintegrated cold molecular gas mass outflow rate of ~40 Msun/yr for the ring.\nUsing the 854micron map, we resolved a 32 pc radius torus with a gas mass of\n8x10^5 Msun. These torus properties are similar to other Seyfert galaxies in\nthe GATOS sample. We measured a decreased cold molecular gas concentration in\nthe nuclear-torus region relative to the circumnuclear region when compared to\nother less luminous Seyfert galaxies. We conclude that the effects of AGN\nfeedback in NGC7172, which are likely caused by the AGN wind and/or the\nmoderate luminosity radio jet, are seen as a large-scale outflowing molecular\ngas ring and accompanying redistribution of molecular gas in the nuclear\nregions.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:37:03 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15144","submitter":"Akihiro Ozawa","authors":"Akihiro Ozawa, Koji Kobayashi, and Kentaro Nomura","title":"Effective model analysis of intrinsic spin Hall effect with magnetism in\n  stacked-kagome Weyl semimetal Co3Sn2S2","comments":"8 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  We theoretically study the spin Hall effect in a simple tight-binding model\nof stacked-kagome Weyl semimetal Co3Sn2S2 with ferromagnetic ordering. We focus\non the two types of the spin Hall current: one flowing in the in-plane\ndirection with respect to the kagome lattice (in-plane spin Hall current), and\none flowing in the stacking direction (out-of-plane spin Hall current). We show\nthe spin Hall conductivities for those spin currents drastically change\ndepending on the direction of the magnetic moment. Especially, the out-of-plane\nspin Hall current may induce surface spin accumulation, which are useful for\nthe perpendicular magnetization switching via spin-orbit torque.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:37:04 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15145","submitter":"Zheng Hu","authors":"Zheng Hu, Fuji Ren","title":"Bert4CMR: Cross-Market Recommendation with Bidirectional Encoder\n  Representations from Transformer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Real-world multinational e-commerce companies, such as Amazon and eBay, serve\nin multiple countries and regions. Obviously, these markets have similar goods\nbut different users. Some markets are data-scarce, while others are data-rich.\nIn recent years, cross-market recommendation (CMR) has been proposed to enhance\ndata-scarce markets by leveraging auxiliary information from data-rich markets.\nPrevious works fine-tune the pre-trained model on the local market after\nfreezing part of the parameters or introducing inter-market similarity into the\nlocal market to improve the performance of CMR. However, they generally do not\nconsider eliminating the mutual interference between markets. Therefore, the\nexisting methods are neither unable to learn unbiased general knowledge nor\nefficient transfer reusable information across markets. In this paper, we\npropose a novel attention-based model called Bert4CMR to simultaneously improve\nall markets' recommendation performance. Specifically, we employ the attention\nmechanism to capture user interests by modelling user behavioural sequences. We\npre-train the proposed model on global data to learn the general knowledge of\nitems. Then we fine-tune specific target markets to perform local\nrecommendations. We propose market embedding to model the bias of each market\nand reduce the mutual inference between the parallel markets. Extensive\nexperiments conducted on seven markets show that our model is state-of-the-art.\nOur model outperforms the suboptimal model by 4.82%, 4.73%, 7.66% and 6.49% on\naverage of seven datasets in terms of four metrics, respectively. We conduct\nablation experiments to analyse the effectiveness of the proposed components.\nExperimental results indicate that our model is able to learn general knowledge\nthrough global data and shield the mutual interference between markets.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:39:14 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15146","submitter":"Clement Calvino","authors":"Cl\\'ement Calvino, Lucille Furgerot, Emmanuel Poizot, Pascal Bailly du\n  Bois, Anne-Claire Bennis","title":"Model and method to predict the turbulent kinetic energy induced by\n  tidal currents, application to the wave-induced turbulence","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.flu-dyn physics.ao-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  A prediction model for the turbulent kinetic energy (TKE) induced by\ntidal-currents is proposed as a function of the barotropic velocity only, along\nwith a robust method evaluating the different parameters involved using\nAcoustic Doppler Current Profiler (ADCP) measurements from Alderney Race. We\nfind that the model is able to reproduce correctly the TKE profiles with\ncoefficients of correlation on average higher than 0.90 and normalised\nroot-mean-square errors (NRMSE) less than 14%. Different profiles are also\ntested for the mean velocity, no satisfactory prediction model is found but we\nare able to have decent estimates of the velocity shear and friction velocity.\nTwo applications are then carried out. First the turbulent budget terms are\nestimated and discussed. We identify the turbulent production and dissipation\nof TKE as the most important mechanisms, then we discuss the validity of\nseveral theoretical results derived for isotropic turbulence for this\napplication. A strong departure for the estimation of the turbulent dissipation\nis notably found and explained by the turbulent anisotropy. At last the\nprediction model for the TKE is used to infer the wave-induced TKE. We show the\nimportance of removing the tidal component, waves can have a strong influence\ndown to mid-depth.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:41:05 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15147","submitter":"Veit Krause","authors":"Elena Bachini, Veit Krause, Ingo Nitschke and Axel Voigt","title":"Derivation and simulation of a two-phase fluid deformable surface model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA math-ph math.MP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We consider two-phase fluid deformable surfaces as model systems for\nbiomembranes. Such surfaces are modeled by incompressible surface\nNavier-Stokes-Cahn-Hilliard-like equations with bending forces. We derive this\nmodel using the Lagrange-D'Alembert principle considering various dissipation\nmechanisms. The highly nonlinear model is solved numerically to explore the\ntight interplay between surface evolution, surface phase composition, surface\ncurvature and surface hydrodynamics. It is demonstrated that hydrodynamics can\nenhance bulging and furrow formation, which both can further develop to\npinch-offs. The numerical approach builds on a Taylor-Hood element for the\nsurface Navier-Stokes part, a semi-implicit approach for the Cahn-Hilliard\npart, higher order surface parametrizations, appropriate approximations of the\ngeometric quantities, and mesh redistribution. We demonstrate convergence\nproperties that are known to be optimal for simplified sub-problems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:42:54 GMT"},{"version":"v2","created":"Thu, 25 May 2023 14:04:51 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.15148","submitter":"Xiaojin Zhang","authors":"Xiaojin Zhang, Wenjie Li, Kai Chen, Shutao Xia, Qiang Yang","title":"Theoretically Principled Federated Learning for Balancing Privacy and\n  Utility","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We propose a general learning framework for the protection mechanisms that\nprotects privacy via distorting model parameters, which facilitates the\ntrade-off between privacy and utility. The algorithm is applicable to arbitrary\nprivacy measurements that maps from the distortion to a real value. It can\nachieve personalized utility-privacy trade-off for each model parameter, on\neach client, at each communication round in federated learning. Such adaptive\nand fine-grained protection can improve the effectiveness of privacy-preserved\nfederated learning.\n  Theoretically, we show that gap between the utility loss of the protection\nhyperparameter output by our algorithm and that of the optimal protection\nhyperparameter is sub-linear in the total number of iterations. The\nsublinearity of our algorithm indicates that the average gap between the\nperformance of our algorithm and that of the optimal performance goes to zero\nwhen the number of iterations goes to infinity. Further, we provide the\nconvergence rate of our proposed algorithm. We conduct empirical results on\nbenchmark datasets to verify that our method achieves better utility than the\nbaseline methods under the same privacy budget.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:44:02 GMT"},{"version":"v2","created":"Sat, 3 Jun 2023 12:35:57 GMT"}],"update_date":"2023-06-06"}
{"id":"2305.15149","submitter":"Jana Kierdorf","authors":"Jana Kierdorf and Ribana Roscher","title":"Reliability Scores from Saliency Map Clusters for Improved Image-based\n  Harvest-Readiness Prediction in Cauliflower","comments":"Preprint, 8 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Cauliflower is a hand-harvested crop that must fulfill high-quality standards\nin sales making the timing of harvest important. However, accurately\ndetermining harvest-readiness can be challenging due to the cauliflower head\nbeing covered by its canopy. While deep learning enables automated\nharvest-readiness estimation, errors can occur due to field-variability and\nlimited training data. In this paper, we analyze the reliability of a\nharvest-readiness classifier with interpretable machine learning. By\nidentifying clusters of saliency maps, we derive reliability scores for each\nclassification result using knowledge about the domain and the image\nproperties. For unseen data, the reliability can be used to (i) inform farmers\nto improve their decision-making and (ii) increase the model prediction\naccuracy. Using RGB images of single cauliflower plants at different\ndevelopmental stages from the GrowliFlower dataset, we investigate various\nsaliency mapping approaches and find that they result in different quality of\nreliability scores. With the most suitable interpretation tool, we adjust the\nclassification result and achieve a 15.72% improvement of the overall accuracy\nto 88.14% and a 15.44% improvement of the average class accuracy to 88.52% for\nthe GrowliFlower dataset.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:48:36 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15150","submitter":"Alejandro Naser Pastoriza","authors":"Alejandro Naser Pastoriza, Gregory Chockler, Alexey Gotsman","title":"Fault-tolerant computing with unreliable channels","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We study implementations of basic fault-tolerant primitives, such as\nconsensus and registers, in message-passing systems subject to process crashes\nand a broad range of communication failures. Our results characterize the\nnecessary and sufficient conditions for implementing these primitives as a\nfunction of the connectivity constraints and synchrony assumptions. Our main\ncontribution is a new algorithm for partially synchronous consensus that is\nresilient to process crashes and channel failures and is optimal in its\nconnectivity requirements. In contrast to prior work, our algorithm assumes the\nmost general model of message loss where faulty channels are flaky, i.e., can\nlose messages without any guarantee of fairness. This failure model is\nparticularly challenging for consensus algorithms, as it rules out standard\nsolutions based on leader oracles and failure detectors. To circumvent this\nlimitation, we construct our solution using a new variant of the recently\nproposed view synchronizer abstraction, which we adapt to the crash-prone\nsetting with flaky channels.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:49:05 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15152","submitter":"Yi-Zhi Huang","authors":"Yi-Zhi Huang","title":"Modular invariance of (logarithmic) intertwining operators","comments":"81 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.QA hep-th math.RA math.RT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Let $V$ be a $C_2$-cofinite vertex operator algebra without nonzero elements\nof negative weights. We prove the conjecture that the spaces spanned by\nanalytic extensions of pseudo-$q$-traces ($q=e^{2\\pi i\\tau}$) shifted by\n$-\\frac{c}{24}$ of products of geometrically-modified (logarithmic)\nintertwining operators among grading-restricted generalized $V$-modules are\ninvariant under modular transformations. The convergence and analytic extension\nresult needed to formulate this conjecture and some consequences on such\nshifted pseudo-$q$-traces were proved by Fiordalisi [F1} and [F2] using the\nmethod developed by the author in [H2]. The method that we use to prove this\nconjecture is based on the theory of the associative algebras $A^{N}(V)$ for\n$N\\in \\mathbb{N}$, their graded modules and their bimodules introduced and\nstudied by the author in [H8] and [H9]. This modular invariance result gives a\nconstruction of $C_2$-cofinite genus-one logarithmic conformal field theories\nfrom the corresponding genus-zero logarithmic conformal field theories.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:51:29 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15154","submitter":"Kiran Kokilepersaud","authors":"Kiran Kokilepersaud, Stephanie Trejo Corona, Mohit Prabhushankar,\n  Ghassan AlRegib, Charles Wykoff","title":"Clinically Labeled Contrastive Learning for OCT Biomarker Classification","comments":"Accepted in IEEE Journal of Biomedical and Health Informatics. arXiv\n  admin note: text overlap with arXiv:2211.05092","journal-ref":null,"doi":"10.1109/JBHI.2023.3277789","report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  This paper presents a novel positive and negative set selection strategy for\ncontrastive learning of medical images based on labels that can be extracted\nfrom clinical data. In the medical field, there exists a variety of labels for\ndata that serve different purposes at different stages of a diagnostic and\ntreatment process. Clinical labels and biomarker labels are two examples. In\ngeneral, clinical labels are easier to obtain in larger quantities because they\nare regularly collected during routine clinical care, while biomarker labels\nrequire expert analysis and interpretation to obtain. Within the field of\nophthalmology, previous work has shown that clinical values exhibit\ncorrelations with biomarker structures that manifest within optical coherence\ntomography (OCT) scans. We exploit this relationship by using the clinical data\nas pseudo-labels for our data without biomarker labels in order to choose\npositive and negative instances for training a backbone network with a\nsupervised contrastive loss. In this way, a backbone network learns a\nrepresentation space that aligns with the clinical data distribution available.\nAfterwards, we fine-tune the network trained in this manner with the smaller\namount of biomarker labeled data with a cross-entropy loss in order to classify\nthese key indicators of disease directly from OCT scans. We also expand on this\nconcept by proposing a method that uses a linear combination of clinical\ncontrastive losses. We benchmark our methods against state of the art\nself-supervised methods in a novel setting with biomarkers of varying\ngranularity. We show performance improvements by as much as 5\\% in total\nbiomarker detection AUROC.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:51:48 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15155","submitter":"Ilyas Fatkhullin","authors":"Ilyas Fatkhullin, Alexander Tyurin, Peter Richt\\'arik","title":"Momentum Provably Improves Error Feedback!","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.DC math.OC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Due to the high communication overhead when training machine learning models\nin a distributed environment, modern algorithms invariably rely on lossy\ncommunication compression. However, when untreated, the errors caused by\ncompression propagate, and can lead to severely unstable behavior, including\nexponential divergence. Almost a decade ago, Seide et al [2014] proposed an\nerror feedback (EF) mechanism, which we refer to as EF14, as an immensely\neffective heuristic for mitigating this issue. However, despite steady\nalgorithmic and theoretical advances in the EF field in the last decade, our\nunderstanding is far from complete. In this work we address one of the most\npressing issues. In particular, in the canonical nonconvex setting, all known\nvariants of EF rely on very large batch sizes to converge, which can be\nprohibitive in practice. We propose a surprisingly simple fix which removes\nthis issue both theoretically, and in practice: the application of Polyak's\nmomentum to the latest incarnation of EF due to Richt\\'{a}rik et al. [2021]\nknown as EF21. Our algorithm, for which we coin the name EF21-SGDM, improves\nthe communication and sample complexities of previous error feedback algorithms\nunder standard smoothness and bounded variance assumptions, and does not\nrequire any further strong assumptions such as bounded gradient dissimilarity.\nMoreover, we propose a double momentum version of our method that improves the\ncomplexities even further. Our proof seems to be novel even when compression is\nremoved from the method, and as such, our proof technique is of independent\ninterest in the study of nonconvex stochastic optimization enriched with\nPolyak's momentum.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:52:02 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15157","submitter":"Li Shen","authors":"Yifan Shi, Yingqi Liu, Yan Sun, Zihao Lin, Li Shen, Xueqian Wang,\n  Dacheng Tao","title":"Towards More Suitable Personalization in Federated Learning via\n  Decentralized Partial Model Training","comments":"26 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.DC math.OC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Personalized federated learning (PFL) aims to produce the greatest\npersonalized model for each client to face an insurmountable problem--data\nheterogeneity in real FL systems. However, almost all existing works have to\nface large communication burdens and the risk of disruption if the central\nserver fails. Only limited efforts have been used in a decentralized way but\nstill suffers from inferior representation ability due to sharing the full\nmodel with its neighbors. Therefore, in this paper, we propose a personalized\nFL framework with a decentralized partial model training called DFedAlt. It\npersonalizes the \"right\" components in the modern deep models by alternately\nupdating the shared and personal parameters to train partially personalized\nmodels in a peer-to-peer manner. To further promote the shared parameters\naggregation process, we propose DFedSalt integrating the local Sharpness Aware\nMinimization (SAM) optimizer to update the shared parameters. It adds proper\nperturbation in the direction of the gradient to overcome the shared model\ninconsistency across clients. Theoretically, we provide convergence analysis of\nboth algorithms in the general non-convex setting for decentralized partial\nmodel training in PFL. Our experiments on several real-world data with various\ndata partition settings demonstrate that (i) decentralized training is more\nsuitable for partial personalization, which results in state-of-the-art (SOTA)\naccuracy compared with the SOTA PFL baselines; (ii) the shared parameters with\nproper perturbation make partial personalized FL more suitable for\ndecentralized training, where DFedSalt achieves most competitive performance.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:52:18 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15158","submitter":"David Fernandes E.","authors":"David E. Fernandes, M\\'ario G. Silveirinha","title":"Enhancing the Directional Violation of Kirchhoff's Law of Thermal\n  Radiation with a Nonreciprocal Wire Medium","comments":"34 pages, 11 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.app-ph physics.optics","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this work, we develop a homogenization model to determine the effective\nresponse of a metallic nanowire array embedded in an electric gyrotropic\nmaterial. We study the interaction of electromagnetic waves with the\nmetamaterial and demonstrate that the nanowire array can greatly enhance the\nnonreciprocal response of the gyrotropic substrate. In particular, the\nmetamaterial can either absorb the incoming energy almost entirely or reflect\nit with little loss, depending on the sign of the incidence angle. We explore\nthe implications of our findings in the context of Kirchhoff's law of thermal\nradiation. Our results demonstrate that the wire array can boost the difference\nbetween the emissivity and absorptivity in a broad spectrum of frequencies and\nincidence angles as compared to an unstructured gyrotropic substrate. These\nfindings suggest potential applications for the nonreciprocal wire medium in\nthermal management, radiative cooling, and others.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:52:47 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15159","submitter":"Zheng Hu","authors":"Zheng Hu, Shi-Min Cai, Jun Wang, Tao Zhou","title":"Collaborative Recommendation Model Based on Multi-modal Multi-view\n  Attention Network: Movie and literature cases","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The existing collaborative recommendation models that use multi-modal\ninformation emphasize the representation of users' preferences but easily\nignore the representation of users' dislikes. Nevertheless, modelling users'\ndislikes facilitates comprehensively characterizing user profiles. Thus, the\nrepresentation of users' dislikes should be integrated into the user modelling\nwhen we construct a collaborative recommendation model. In this paper, we\npropose a novel Collaborative Recommendation Model based on Multi-modal\nmulti-view Attention Network (CRMMAN), in which the users are represented from\nboth preference and dislike views. Specifically, the users' historical\ninteractions are divided into positive and negative interactions, used to model\nthe user's preference and dislike views, respectively. Furthermore, the\nsemantic and structural information extracted from the scene is employed to\nenrich the item representation. We validate CRMMAN by designing contrast\nexperiments based on two benchmark MovieLens-1M and Book-Crossing datasets.\nMovielens-1m has about a million ratings, and Book-Crossing has about 300,000\nratings. Compared with the state-of-the-art knowledge-graph-based and\nmulti-modal recommendation methods, the AUC, NDCG@5 and NDCG@10 are improved by\n2.08%, 2.20% and 2.26% on average of two datasets. We also conduct controlled\nexperiments to explore the effects of multi-modal information and multi-view\nmechanism. The experimental results show that both of them enhance the model's\nperformance.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:52:56 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15160","submitter":"Tetyana Shalomayeva","authors":"Jianpei Geng, Tetyana Shalomayeva, Mariia Gryzlova, Amlan Mukherjee,\n  Santo Santonocito, Dzhavid Dzhavadzade, Durga Dasari, Hiromitsu Kato, Rainer\n  St\\\"ohr, Andrej Denisenko, Norikazu Mizuochi, and J\\\"org Wrachtrup","title":"Dopant-assisted stabilization of negatively charged single\n  nitrogen-vacancy centers in phosphorus-doped diamond at low temperatures","comments":"8 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cond-mat.mes-hall","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Charge state instabilities have been a bottleneck for the implementation of\nsolid-state spin systems and pose a major challenge to the development of\nspin-based quantum technologies. Here we investigate the stabilization of\nnegatively charged nitrogen-vacancy (NV$^-$) centers in phosphorus-doped\ndiamond at liquid helium temperatures. Photoionization of phosphorous donors in\nconjunction with charge diffusion at the nanoscale enhances NV$^0$ to NV$^-$\nconversion and stabilizes the NV$^-$ charge state without the need for an\nadditional repump laser. The phosphorus-assisted stabilization is explored and\nconfirmed both with experiments and our theoretical model. Stable\nphotoluminescence-excitation spectra are obtained for NV$^-$ centers created\nduring the growth. The fluorescence is continuously recorded under resonant\nexcitation to real-time monitor the charge state and the ionization and\nrecombination rates are extracted from time traces. We find a linear laser\npower dependence of the recombination rate as opposed to the conventional\nquadratic dependence, which is attributed to the photo-ionization of phosphorus\natoms.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:53:10 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15161","submitter":"Takeshi Morita","authors":"Takeshi Morita","title":"A Derivation of the Critical Dimensions in Superstring Theories from\n  Scale Invariance and Electric-Magnetic Duality","comments":"6+3 pages, 2 figures, v2: minor changes","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-th gr-qc","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We consider a low energy effective theory of $p$-branes in a $D$-dimensional\nspacetime, and impose two conditions: 1) the system is scale invariant, and 2)\nthe electric-magnetic dual $(D-p-4)$-branes exist and they obey the same type\nof interactions to the $p$-branes. (We do not assume supersymmetry or general\nrelativity.) We then ask what $p$ and $D$ are consistent with these conditions.\nUsing a simple dimensional analysis, we find that only two solutions are\npossible: $(p,D)=(2,11)$ and $(p,D)=(2n-1,4n+2)$, ($n=1,2,3,\\cdots$). The first\nsolution corresponds to M-theory, and the second solutions at $n=1$ and $n=2$\ncorrespond to self-dual strings in little string theory and D3-branes in type\nIIB superstring theory, respectively, although the second solutions for $n \\ge\n3$ are unknown. Thus, our two conditions may be strong enough to characterize\nsuperstring theories.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:53:59 GMT"},{"version":"v2","created":"Wed, 31 May 2023 16:04:46 GMT"}],"update_date":"2023-06-01"}
{"id":"2305.15162","submitter":"Christopher Lutsko","authors":"Dubi Kelmer, Alex Kontorovich, Christopher Lutsko","title":"Mean square of Eisenstein series","comments":"13 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.NT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We study the sup-norm and mean-square-norm problems for Eisenstein series on\ncertain arithmetic hyperbolic orbifolds, producing sharp exponents for the\nmodular surface and Picard 3-fold. The methods involve bounds for Epstein zeta\nfunctions, and counting restricted values of indefinite quadratic forms at\ninteger points.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:55:46 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15163","submitter":"Alejandro Diaz","authors":"Alejandro N. Diaz, Youngsoo Choi, Matthias Heinkenschloss","title":"A fast and accurate domain-decomposition nonlinear manifold reduced\n  order model","comments":null,"journal-ref":null,"doi":null,"report-no":"LLNL-JRNL-849457","categories":"math.NA cs.NA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper integrates nonlinear-manifold reduced order models (NM-ROMs) with\ndomain decomposition (DD). NM-ROMs approximate the FOM state in a\nnonlinear-manifold by training a shallow, sparse autoencoder using FOM snapshot\ndata. These NM-ROMs can be advantageous over linear-subspace ROMs (LS-ROMs) for\nproblems with slowly decaying Kolmogorov $n$-width. However, the number of\nNM-ROM parameters that need to trained scales with the size of the FOM.\nMoreover, for \"extreme-scale\" problems, the storage of high-dimensional FOM\nsnapshots alone can make ROM training expensive. To alleviate the training\ncost, this paper applies DD to the FOM, computes NM-ROMs on each subdomain, and\ncouples them to obtain a global NM-ROM. This approach has several advantages:\nSubdomain NM-ROMs can be trained in parallel, each involve fewer parameters to\nbe trained than global NM-ROMs, require smaller subdomain FOM dimensional\ntraining data, and training of subdomain NM-ROMs can tailor them to\nsubdomain-specific features of the FOM. The shallow, sparse architecture of the\nautoencoder used in each subdomain NM-ROM allows application of hyper-reduction\n(HR), reducing the complexity caused by nonlinearity and yielding computational\nspeedup of the NM-ROM. This paper provides the first application of NM-ROM\n(with HR) to a DD problem. In particular, it details an algebraic DD\nformulation of the FOM, trains a NM-ROM with HR for each subdomain, and\ndevelops a sequential quadratic programming (SQP) solver to evaluate the\ncoupled global NM-ROM. Theoretical convergence results for the SQP method and a\npriori and a posteriori error estimates for the DD NM-ROM with HR are provided.\nThe proposed DD NM-ROM with HR approach is numerically compared to a DD LS-ROM\nwith HR on 2D steady-state Burgers' equation, showing an order of magnitude\nimprovement in accuracy of the proposed DD NM-ROM over the DD LS-ROM.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:56:37 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15164","submitter":"Daichi Takeuchi","authors":"Daichi Takeuchi","title":"Quadratic $\\ell$-adic sheaf and its Heisenberg group","comments":"39 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.NT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, we introduce a new class of $\\ell$-adic sheaves, which we call\nquadratic $\\ell$-adic sheaves, on connected unipotent commutative algebraic\ngroups over finite fields. They are sheaf-theoretic enhancements of quadratic\nforms on finite abelian groups in the spirit of the function-sheaf dictionary.\nWe show that a certain finite Heisenberg group acts on a quadratic sheaf and\nthat the cohomology of the quadratic sheaf gives an irreducible representation\nof the group. We also compute the Frobenius eigenvalues of the cohomology\ngroups. As a byproduct, we find a large number of examples of affine\nsupersingular varieties.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:56:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15165","submitter":"Geon Heo","authors":"Geon Heo, Junseok Seo, and Steven Euijong Whang","title":"Personalized DP-SGD using Sampling Mechanisms","comments":"10 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Personalized privacy becomes critical in deep learning for Trustworthy AI.\nWhile Differentially Private Stochastic Gradient Descent (DP-SGD) is widely\nused in deep learning methods supporting privacy, it provides the same level of\nprivacy to all individuals, which may lead to overprotection and low utility.\nIn practice, different users may require different privacy levels, and the\nmodel can be improved by using more information about the users with lower\nprivacy requirements. There are also recent works on differential privacy of\nindividuals when using DP-SGD, but they are mostly about individual privacy\naccounting and do not focus on satisfying different privacy levels. We thus\nextend DP-SGD to support a recent privacy notion called\n($\\Phi$,$\\Delta$)-Personalized Differential Privacy (($\\Phi$,$\\Delta$)-PDP),\nwhich extends an existing PDP concept called $\\Phi$-PDP. Our algorithm uses a\nmulti-round personalized sampling mechanism and embeds it within the DP-SGD\niterations. Experiments on real datasets show that our algorithm outperforms\nDP-SGD and simple combinations of DP-SGD with existing PDP mechanisms in terms\nof model performance and efficiency due to its embedded sampling mechanism.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:56:57 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15166","submitter":"Stephan Helfrich","authors":"Stephan Helfrich, Stefan Ruzika, Clemens Thielen","title":"Efficiently Constructing Convex Approximation Sets in Multiobjective\n  Optimization Problems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Convex approximation sets for multiobjective optimization problems are a\nwell-studied relaxation of the common notion of approximation sets. Instead of\napproximating each image of a feasible solution by the image of some solution\nin the approximation set up to a multiplicative factor in each component, a\nconvex approximation set only requires this multiplicative approximation to be\nachieved by some convex combination of finitely many images of solutions in the\nset. This makes convex approximation sets efficiently computable for a wide\nrange of multiobjective problems - even for many problems for which (classic)\napproximations sets are hard to compute.\n  In this article, we propose a polynomial-time algorithm to compute convex\napproximation sets that builds upon an exact or approximate algorithm for the\nweighted sum scalarization and is, therefore, applicable to a large variety of\nmultiobjective optimization problems. The provided convex approximation quality\nis arbitrarily close to the approximation quality of the underlying algorithm\nfor the weighted sum scalarization. In essence, our algorithm can be\ninterpreted as an approximate variant of the dual variant of Benson's Outer\nApproximation Algorithm. Thus, in contrast to existing convex approximation\nalgorithms from the literature, information on solutions obtained during the\napproximation process is utilized to significantly reduce both the practical\nrunning time and the cardinality of the returned solution sets while still\nguaranteeing the same worst-case approximation quality. We underpin these\nadvantages by the first comparison of all existing convex approximation\nalgorithms on several instances of the triobjective knapsack problem and the\ntriobjective symmetric metric traveling salesman problem.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:58:36 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15167","submitter":"Siu Lun Chau","authors":"Siu Lun Chau and Krikamol Muandet and Dino Sejdinovic","title":"Explaining the Uncertain: Stochastic Shapley Values for Gaussian Process\n  Models","comments":"26 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We present a novel approach for explaining Gaussian processes (GPs) that can\nutilize the full analytical covariance structure present in GPs. Our method is\nbased on the popular solution concept of Shapley values extended to stochastic\ncooperative games, resulting in explanations that are random variables. The GP\nexplanations generated using our approach satisfy similar favorable axioms to\nstandard Shapley values and possess a tractable covariance function across\nfeatures and data observations. This covariance allows for quantifying\nexplanation uncertainties and studying the statistical dependencies between\nexplanations. We further extend our framework to the problem of predictive\nexplanation, and propose a Shapley prior over the explanation function to\npredict Shapley values for new data based on previously computed ones. Our\nextensive illustrations demonstrate the effectiveness of the proposed approach.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:59:03 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15168","submitter":"Domenico Trotta","authors":"Domenico Trotta, Oreste Pezzi, David Burgess, Luis Preisser, Xochitl\n  Blanco-Cano, Primoz Kajdic, Heli Hietala, Timothy S. Horbury, Rami Vainio,\n  Nina Dresing, Alessandro Retino', Maria Federica Marcucci, Luca\n  Sorriso-Valvo, Sergio Servidio and Francesco Valentini","title":"Three-dimensional modelling of the shock-turbulence interaction","comments":"Submitted to MNRAS","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.space-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The complex interaction between shocks and plasma turbulence is extremely\nimportant to address crucial features of energy conversion in a broad range of\nastrophysical systems. We study the interaction between a supercritical,\nperpendicular shock and pre-existing, fully-developed plasma turbulence,\nemploying a novel combination of magnetohydrodynamic (MHD) and small-scale,\nhybrid-kinetic simulations where a shock is propagating through a turbulent\nmedium. The variability of the shock front in the unperturbed case and for two\nlevels of upstream fluctuations is addressed.We find that the behaviour of\nshock ripples, i.e., shock surface fluctuations with short (a few ion skin\ndepths, $d_i$) wavelengths, is modified by the presence of pre-existing\nturbulence, which also induces strong corrugations of the shock front at larger\nscales. We link this complex behaviour of the shock front and the shock\ndownstream structuring with the proton temperature anisotropies produced in the\nshock-turbulence system. Finally, we put our modelling effort in the context of\nspacecraft observations, elucidating the role of novel cross-scale,\nmulti-spacecraft measurements in resolving shock front irregularities at\ndifferent scales. These results are relevant for a broad range of astrophysical\nsystems characterised by the presence of shock waves interacting with plasma\nturbulence.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:59:18 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15169","submitter":"Ricardo G\\'azquez","authors":"Concepci\\'on Dom\\'inguez, Ricardo G\\'azquez, Juan Miguel Morales,\n  Salvador Pineda","title":"The Cooperative Maximum Capture Facility Location Problem","comments":"32 pages, 8 tables, 2 algorithms, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In the Maximum Capture Facility Location (MCFL) problem with a binary choice\nrule, a company intends to locate a series of facilities to maximize the\ncaptured demand, and customers patronize the facility that maximizes their\nutility. In this work, we generalize the MCFL problem assuming that the\nfacilities of the decision maker act cooperatively to increase the customers'\nutility over the company. We propose a utility maximization rule between the\ncaptured utility of the decision maker and the opt-out utility of a competitor\nalready installed in the market. Furthermore, we model the captured utility by\nmeans of an Ordered Median function (OMf) of the partial utilities of newly\nopen facilities. We name this problem \"the Cooperative Maximum Capture Facility\nLocation problem\" (CMCFL). The OMf serves as a means to compute the utility of\neach customer towards the company as an aggregation of ordered partial\nutilities, and constitutes a unifying framework for CMCFL models. We introduce\na multiperiod non-linear bilevel formulation for the CMCFL with an embedded\nassignment problem characterizing the captured utilities. For this model, two\nexact resolution approaches are presented: a MILP reformulation with valid\ninequalities and an effective approach based on Benders' decomposition.\nExtensive computational experiments are provided to test our results with\nrandomly generated data and an application to the location of charging stations\nfor electric vehicles in the city of Trois-Rivi\\`eres, Qu\\`ebec, is addressed.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:59:28 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15170","submitter":"Sebastian Buschow","authors":"Sebastian Buschow, Jan Keller and Sabrina Wahl","title":"Explaining heatwaves with machine learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.ao-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Heatwaves are known to arise from the interplay between large-scale climate\nvariability, synoptic weather patterns and regional to local scale surface\nprocesses. While recent research has made important progress for each\nindividual contributing factor, ways to properly incorporate multiple or all of\nthem in a unified analysis are still lacking. In this study, we consider a wide\nrange of possible predictor variables from the ERA5 reanalysis, and ask, how\nmuch information on heatwave occurrence in Europe can be learned from each of\nthem. To simplify the problem, we first adapt the recently developed logistic\nprincipal component analysis to the task of compressing large binary heatwave\nfields to a small number of interpretable principal components. The\nrelationships between heatwaves and various climate variables can then be\nlearned by a neural network. Starting from the simple notion that the\nimportance of a variable is given by its impact on the performance of our\nstatistical model, we arrive naturally at the definition of Shapley values.\nClassic results of game theory show that this is the only fair way of\ndistributing the overall success of a model among its inputs. With this\napproach, we find a non-linear model that explains 70% of reduced heatwave\nvariability, 27% of which are due to upper level geopotential while top level\nsoil moisture contributes 15% of the overall score. In addition, Shapley\ninteraction values enable us to quantify overlapping information and positive\nsynergies between all pairs of predictors.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:59:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15171","submitter":"Shiu-Hong Kao","authors":"Xinhang Liu, Shiu-hong Kao, Jiaben Chen, Yu-Wing Tai, Chi-Keung Tang","title":"Deceptive-NeRF: Enhancing NeRF Reconstruction using Pseudo-Observations\n  from Diffusion Models","comments":"Project page: https://deceptive-nerf.github.io/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper introduces Deceptive-NeRF, a new method for enhancing the quality\nof reconstructed NeRF models using synthetically generated pseudo-observations,\ncapable of handling sparse input and removing floater artifacts. Our proposed\nmethod involves three key steps: 1) reconstruct a coarse NeRF model from sparse\ninputs; 2) generate pseudo-observations based on the coarse model; 3) refine\nthe NeRF model using pseudo-observations to produce a high-quality\nreconstruction. To generate photo-realistic pseudo-observations that faithfully\npreserve the identity of the reconstructed scene while remaining consistent\nwith the sparse inputs, we develop a rectification latent diffusion model that\ngenerates images conditional on a coarse RGB image and depth map, which are\nderived from the coarse NeRF and latent text embedding from input images.\nExtensive experiments show that our method is effective and can generate\nperceptually high-quality NeRF even with very sparse inputs.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:00:32 GMT"},{"version":"v2","created":"Wed, 31 May 2023 20:41:29 GMT"}],"update_date":"2023-06-02"}
{"id":"2305.15172","submitter":"Rodrigo Aldana-L\\'opez","authors":"Rodrigo Aldana-Lopez, Eduardo Sebastian, Rosario Aragues, Eduardo\n  Montijano and Carlos Sagues","title":"Distributed outer approximation of the intersection of ellipsoids","comments":"This is the accepted version of the manuscript: \"Distributed outer\n  approximation of the intersection of ellipsoids,\" Rodrigo Aldana-Lopez,\n  Eduardo Sebastian, Rosario Aragues, Eduardo Montijano and Carlos Sagues, in\n  IEEE Control Systems Letters, 2023, DOI: 10.1109/LCSYS.2023.3280259","journal-ref":null,"doi":"10.1109/LCSYS.2023.3280259","report-no":null,"categories":"eess.SY cs.MA cs.SY math.OC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The outer Lowner-John method is widely used in sensor fusion applications to\nfind the smallest ellipsoid that can approximate the intersection of a set of\nellipsoids, described by positive definite covariance matrices modeling the\nquality of each sensor. We propose a distributed algorithm to solve this\nproblem when these matrices are defined over the network's nodes. This is of\nparticular significance as it is the first decentralized algorithm capable of\ncomputing the covariance intersection ellipsoid by combining information from\nthe entire network using only local interactions. The solution is based on a\nreformulation of the centralized problem, leading to a local protocol based on\nexact dynamic consensus tools. After reaching consensus, the protocol converges\nto an outer Lowner-John ellipsoid in finite time, and to the global optimum\nasymptotically. Formal convergence analysis and numerical experiments are\nprovided to validate the proposal's advantages.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:03:29 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15173","submitter":"Stephan Helfrich","authors":"Stephan Helfrich, Arne Herzel, Stefan Ruzika, Clemens Thielen","title":"Using Scalarizations for the Approximation of Multiobjective\n  Optimization Problems: Towards a General Theory","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC cs.DS","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We study the approximation of general multiobjective optimization problems\nwith the help of scalarizations. Existing results state that multiobjective\nminimization problems can be approximated well by norm-based scalarizations.\nHowever, for multiobjective maximization problems, only impossibility results\nare known so far. Countering this, we show that all multiobjective optimization\nproblems can, in principle, be approximated equally well by scalarizations. In\nthis context, we introduce a transformation theory for scalarizations that\nestablishes the following: Suppose there exists a scalarization that yields an\napproximation of a certain quality for arbitrary instances of multiobjective\noptimization problems with a given decomposition specifying which objective\nfunctions are to be minimized / maximized. Then, for each other decomposition,\nour transformation yields another scalarization that yields the same\napproximation quality for arbitrary instances of problems with this other\ndecomposition. In this sense, the existing results about the approximation via\nscalarizations for minimization problems carry over to any other objective\ndecomposition -- in particular, to maximization problems -- when suitably\nadapting the employed scalarization.\n  We further provide necessary and sufficient conditions on a scalarization\nsuch that its optimal solutions achieve a constant approximation quality. We\ngive an upper bound on the best achievable approximation quality that applies\nto general scalarizations and is tight for the majority of norm-based\nscalarizations applied in the context of multiobjective optimization. As a\nconsequence, none of these norm-based scalarizations can induce approximation\nsets for optimization problems with maximization objectives, which unifies and\ngeneralizes the existing impossibility results concerning the approximation of\nmaximization problems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:04:50 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15174","submitter":"Cornelius Schr\\\"oder","authors":"Cornelius Schr\\\"oder, Jakob H. Macke","title":"Simultaneous identification of models and parameters of scientific\n  simulators","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  Many scientific models are composed of multiple discrete components, and\nscien tists often make heuristic decisions about which components to include.\nBayesian inference provides a mathematical framework for systematically\nselecting model components, but defining prior distributions over model\ncomponents and developing associated inference schemes has been challenging. We\napproach this problem in an amortized simulation-based inference framework: We\ndefine implicit model priors over a fixed set of candidate components and train\nneural networks to infer joint probability distributions over both, model\ncomponents and associated parameters from simulations. To represent\ndistributions over model components, we introduce a conditional mixture of\nmultivariate binary distributions in the Grassmann formalism. Our approach can\nbe applied to any compositional stochastic simulator without requiring access\nto likelihood evaluations. We first illustrate our method on a simple time\nseries model with redundant components and show that it can retrieve joint\nposterior distribution over a set of symbolic expressions and their parameters\nwhile accurately capturing redundancy with strongly correlated posteriors. We\nthen apply our approach to drift-diffusion models, a commonly used model class\nin cognitive neuroscience. After validating the method on synthetic data, we\nshow that our approach explains experimental data as well as previous methods,\nbut that our fully probabilistic approach can help to discover multiple\ndata-consistent model configurations, as well as reveal non-identifiable model\ncomponents and parameters. Our method provides a powerful tool for data-driven\nscientific inquiry which will allow scientists to systematically identify\nessential model components and make uncertainty-informed modelling decisions.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:06:02 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15175","submitter":"Yiyang Li","authors":"Yiyang Li, Xinting Huang, Wei Bi, Hai Zhao","title":"Pre-training Multi-party Dialogue Models with Latent Discourse Inference","comments":"Accepted by ACL 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Multi-party dialogues are more difficult for models to understand than\none-to-one two-party dialogues, since they involve multiple interlocutors,\nresulting in interweaving reply-to relations and information flows. To step\nover these obstacles, an effective way is to pre-train a model that understands\nthe discourse structure of multi-party dialogues, namely, to whom each\nutterance is replying. However, due to the lack of explicitly annotated\ndiscourse labels in multi-party dialogue corpora, previous works fail to scale\nup the pre-training process by putting aside the unlabeled multi-party\nconversational data for nothing. To fully utilize the unlabeled data, we\npropose to treat the discourse structures as latent variables, then jointly\ninfer them and pre-train the discourse-aware model by unsupervised latent\nvariable inference methods. Experiments on multiple downstream tasks show that\nour pre-trained model outperforms strong baselines by large margins and\nachieves state-of-the-art (SOTA) results, justifying the effectiveness of our\nmethod. The official implementation of this paper is available at\nhttps://github.com/EricLee8/MPD_EMVI.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:06:27 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15176","submitter":"Matthew Zaremsky","authors":"Matthew C. B. Zaremsky","title":"Finitely presented simple groups with at least exponential Dehn function","comments":"10 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.GR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We construct examples of finitely presented simple groups whose Dehn\nfunctions are at least exponential. To the best of our knowledge, these are the\nfirst such examples known. Our examples arise from R\\\"over-Nekrashevych groups,\nusing carefully calibrated self-similar representations of Baumslag-Solitar\ngroups.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:09:01 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15177","submitter":"Hang Yu","authors":"Hang Yu, Zhenxing Dou, Zhiwei Chen and Xiaomeng Yan","title":"Optimal subsampling for large scale Elastic-net regression","comments":"28 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.ST stat.TH","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Datasets with sheer volume have been generated from fields including computer\nvision, medical imageology, and astronomy whose large-scale and\nhigh-dimensional properties hamper the implementation of classical statistical\nmodels. To tackle the computational challenges, one of the efficient approaches\nis subsampling which draws subsamples from the original large datasets\naccording to a carefully-design task-specific probability distribution to form\nan informative sketch. The computation cost is reduced by applying the original\nalgorithm to the substantially smaller sketch. Previous studies associated with\nsubsampling focused on non-regularized regression from the computational\nefficiency and theoretical guarantee perspectives, such as ordinary least\nsquare regression and logistic regression. In this article, we introduce a\nrandomized algorithm under the subsampling scheme for the Elastic-net\nregression which gives novel insights into L1-norm regularized regression\nproblem. To effectively conduct consistency analysis, a smooth approximation\ntechnique based on alpha absolute function is firstly employed and\ntheoretically verified. The concentration bounds and asymptotic normality for\nthe proposed randomized algorithm are then established under mild conditions.\nMoreover, an optimal subsampling probability is constructed according to\nA-optimality. The effectiveness of the proposed algorithm is demonstrated upon\nsynthetic and real data datasets.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:10:55 GMT"},{"version":"v2","created":"Mon, 29 May 2023 07:33:38 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.15178","submitter":"Yuchang Jiang","authors":"Yuchang Jiang, Vivien Sainte Fare Garnot, Konrad Schindler, Jan Dirk\n  Wegner","title":"Mixture of Experts with Uncertainty Voting for Imbalanced Deep\n  Regression Problems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Data imbalance is ubiquitous when applying machine learning to real-world\nproblems, particularly regression problems. If training data are imbalanced,\nthe learning is dominated by the densely covered regions of the target\ndistribution, consequently, the learned regressor tends to exhibit poor\nperformance in sparsely covered regions. Beyond standard measures like\nover-sampling or re-weighting, there are two main directions to handle learning\nfrom imbalanced data. For regression, recent work relies on the continuity of\nthe distribution; whereas for classification there has been a trend to employ\nmixture-of-expert models and let some ensemble members specialize in\npredictions for the sparser regions. Here, we adapt the mixture-of-experts\napproach to the regression setting. A main question when using this approach is\nhow to fuse the predictions from multiple experts into one output. Drawing\ninspiration from recent work on probabilistic deep learning, we propose to base\nthe fusion on the aleatoric uncertainties of individual experts, thus obviating\nthe need for a separate aggregation module. In our method, dubbed MOUV, each\nexpert predicts not only an output value but also its uncertainty, which in\nturn serves as a statistically motivated criterion to rely on the right\nexperts. We compare our method with existing alternatives on multiple public\nbenchmarks and show that MOUV consistently outperforms the prior art, while at\nthe same time producing better calibrated uncertainty estimates. Our code is\navailable at link-upon-publication.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:12:21 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15179","submitter":"Julien Donini","authors":"Louis Vaslin, Vincent Barra, Julien Donini","title":"GAN-AE : An anomaly detection algorithm for New Physics search in LHC\n  data","comments":"10 pages, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ex","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In recent years, interest has grown in alternative strategies for the search\nfor New Physics beyond the Standard Model. One envisaged solution lies in the\ndevelopment of anomaly detection algorithms based on unsupervised machine\nlearning techniques. In this paper, we propose a new Generative Adversarial\nNetwork-based auto-encoder model that allows both anomaly detection and\nmodel-independent background modeling. This algorithm can be integrated with\nother model-independent tools in a complete heavy resonance search strategy.\nThe proposed strategy has been tested on the LHC Olympics 2020 dataset with\npromising results.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:13:37 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15180","submitter":"Fei Yang","authors":"Yuming Fu and Fei Yang","title":"Mating Siegel and parabolic quadratic polynomials","comments":"40 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.DS math.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Let $f_\\theta(z)=e^{2\\pi i\\theta}z+z^2$ be the quadratic polynomial having an\nindifferent fixed point at the origin. For any bounded type irrational number\n$\\theta\\in\\mathbb{R}\\setminus\\mathbb{Q}$ and any rational number\n$\\nu\\in\\mathbb{Q}$, we prove that $f_\\theta$ and $f_\\nu$ are conformally\nmateable, and that the mating is unique up to conjugacy by a M\\\"{o}bius map.\nThis gives an affirmative (partial) answer to a question raised by Milnor in\n2004.\n  A crucial ingredient in the proof relies on an expansive property when\niterating certain rational maps near Siegel disk boundaries. Combining this\nwith the expanding property in repelling petals of parabolic points, we also\nprove that the Julia sets of a class of Siegel rational maps with parabolic\npoints are locally connected.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:13:43 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15181","submitter":"Anson Ka Long Yip","authors":"Anson Ka Long Yip, Patrick Chi-Kit Cheong, Tjonnie Guang Feng Li","title":"Gravitational wave signatures from the phase-transition-induced collapse\n  of a magnetized neutron star","comments":"12 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.HE","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Strong magnetic fields make neutron stars potential sources of detectable\nelectromagnetic and gravitational-wave signals. Hence, inferring these magnetic\nfields is critical to understand the emissions of neutron stars. However, due\nto the lack of direct observational evidence, the interior magnetic field\nconfiguration remains ambiguous. Here, for the first time, we show that the\ninternal magnetic field strength along with the composition of a neutron star\ncan be directly constrained by detecting the gravitational waves from the\nphase-transition-induced collapse of a magnetized neutron star. By dynamically\nsimulating this collapsing event, we first find that the dominant peaks in the\ngravitational waveform are the fundamental $l=0$ quasi-radial $F$ mode and the\nfundamental $l=2$ quadrupolar $^2f$ mode. We next show that the maximum\ngravitational wave amplitude $|h|_\\mathrm{max}$ increases with the maximum\nmagnetic field strength of the interior toroidal field\n$\\mathcal{B}_\\mathrm{max}$ until the maximum rest-mass density at bounce\n$\\rho_\\mathrm{max,b}$ decreases due to the increasing\n$\\mathcal{B}_\\mathrm{max}$. We then demonstrated that the magnetic suppression\nof fundamental modes found in our previous work remains valid for the hybrid\nstars formed after the phase-transition-induced collapses. We finally show that\nmeasuring the frequency ratio between the two fundamental modes $f_{^2f}/f_{F}$\nallows one to infer $\\mathcal{B}_\\mathrm{max}$ and the baryonic mass fraction\nof matter in the mixed phase $M_\\mathrm{mp} / M_{0}$ of the resulting hybrid\nstar. Consequently, taking $\\mathcal{B}_\\mathrm{max}$ and $M_\\mathrm{mp} /\nM_{0}$ as examples, this work has demonstrated that much information inside\nneutron stars could be extracted similarly through measuring the oscillation\nmodes of the stars.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:14:04 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15182","submitter":"Junran Wu","authors":"He Zhu, Chong Zhang, Junjie Huang, Junran Wu, Ke Xu","title":"HiTIN: Hierarchy-aware Tree Isomorphism Network for Hierarchical Text\n  Classification","comments":"Accepted by ACL'23","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Hierarchical text classification (HTC) is a challenging subtask of\nmulti-label classification as the labels form a complex hierarchical structure.\nExisting dual-encoder methods in HTC achieve weak performance gains with huge\nmemory overheads and their structure encoders heavily rely on domain knowledge.\nUnder such observation, we tend to investigate the feasibility of a\nmemory-friendly model with strong generalization capability that could boost\nthe performance of HTC without prior statistics or label semantics. In this\npaper, we propose Hierarchy-aware Tree Isomorphism Network (HiTIN) to enhance\nthe text representations with only syntactic information of the label\nhierarchy. Specifically, we convert the label hierarchy into an unweighted tree\nstructure, termed coding tree, with the guidance of structural entropy. Then we\ndesign a structure encoder to incorporate hierarchy-aware information in the\ncoding tree into text representations. Besides the text encoder, HiTIN only\ncontains a few multi-layer perceptions and linear transformations, which\ngreatly saves memory. We conduct experiments on three commonly used datasets\nand the results demonstrate that HiTIN could achieve better test performance\nand less memory consumption than state-of-the-art (SOTA) methods.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:14:08 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15183","submitter":"Chenming Tang","authors":"Chenming Tang, Xiuyu Wu and Yunfang Wu","title":"Are Pre-trained Language Models Useful for Model Ensemble in Chinese\n  Grammatical Error Correction?","comments":"7 pages, 1 figure. Accepted by ACL 2023 (main conference, short\n  paper)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Model ensemble has been in widespread use for Grammatical Error Correction\n(GEC), boosting model performance. We hypothesize that model ensemble based on\nthe perplexity (PPL) computed by pre-trained language models (PLMs) should\nbenefit the GEC system. To this end, we explore several ensemble strategies\nbased on strong PLMs with four sophisticated single models. However, the\nperformance does not improve but even gets worse after the PLM-based ensemble.\nThis surprising result sets us doing a detailed analysis on the data and coming\nup with some insights on GEC. The human references of correct sentences is far\nfrom sufficient in the test data, and the gap between a correct sentence and an\nidiomatic one is worth our attention. Moreover, the PLM-based ensemble\nstrategies provide an effective way to extend and improve GEC benchmark data.\nOur source code is available at\nhttps://github.com/JamyDon/PLM-based-CGEC-Model-Ensemble.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:18:52 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15184","submitter":"Ruiqi Liu","authors":"Ruiqi Liu, Meng Hua, Ke Guan, Xiping Wang, Leyi Zhang, Tianqi Mao, Di\n  Zhang, Qingqing Wu, Abbas Jamalipour","title":"6G Enabled Advanced Transportation Systems","comments":"Submitted to an open access journal","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT cs.NI cs.SY eess.SP eess.SY math.IT","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  The 6th generation (6G) wireless communication network is envisaged to be\nable to change our lives drastically, including transportation. In this paper,\ntwo ways of interactions between 6G communication networks and transportation\nare introduced. With the new usage scenarios and capabilities 6G is going to\nsupport, passengers on all sorts of transportation systems will be able to get\ndata more easily, even in the most remote areas on the planet. The quality of\ncommunication will also be improved significantly, thanks to the advanced\ncapabilities of 6G. On top of providing seamless and ubiquitous connectivity to\nall forms of transportation, 6G will also transform the transportation systems\nto make them more intelligent, more efficient, and safer. Based on the latest\nresearch and standardization progresses, technical analysis on how 6G can\nempower advanced transportation systems are provided, as well as challenges and\ninsights for a possible road ahead.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:23:32 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15185","submitter":"Andrew L. Miller","authors":"Andrew L. Miller","title":"Recent results from continuous gravitational wave searches using data\n  from LIGO/Virgo/KAGRA's third observing run","comments":"Contribution to the 2023 Gravitation session of the 57th Rencontres\n  de Moriond","journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc astro-ph.HE","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The third observing run of advanced LIGO, Virgo and KAGRA brought\nunprecedented sensitivity towards a variety of quasi-monochromatic, persistent\ngravitational-wave signals. Continuous waves allow us to probe not just the\nexistence of canonical asymmetrically rotating neutron stars, but also\ndifferent forms of dark matter, thus showing the wide-ranging astrophysical\nimplications of using a relatively simple signal model. I will describe the\nmajor results from the numerous continuous-wave searches that were performed in\nO3, both inside and outside the LIGO/Virgo/KAGRA collaborations, and show how\nimpactful to multi-messenger physics that they have been.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:25:43 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15186","submitter":"Tetsu Kasanishi","authors":"Tetsu Kasanishi, Masaru Isonuma, Junichiro Mori, Ichiro Sakata","title":"SciReviewGen: A Large-scale Dataset for Automatic Literature Review\n  Generation","comments":"ACL findings 2023 (to be appeared). arXiv admin note: text overlap\n  with arXiv:1810.04020 by other authors","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Automatic literature review generation is one of the most challenging tasks\nin natural language processing. Although large language models have tackled\nliterature review generation, the absence of large-scale datasets has been a\nstumbling block to the progress. We release SciReviewGen, consisting of over\n10,000 literature reviews and 690,000 papers cited in the reviews. Based on the\ndataset, we evaluate recent transformer-based summarization models on the\nliterature review generation task, including Fusion-in-Decoder extended for\nliterature review generation. Human evaluation results show that some\nmachine-generated summaries are comparable to human-written reviews, while\nrevealing the challenges of automatic literature review generation such as\nhallucinations and a lack of detailed information. Our dataset and code are\navailable at https://github.com/tetsu9923/SciReviewGen.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:26:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15187","submitter":"Julian Frederik Schumann","authors":"Julian F. Schumann, Aravinda Ramakrishnan Srinivasan, Jens Kober,\n  Gustav Markkula, Arkady Zgonnikov","title":"Using Models Based on Cognitive Theory to Predict Human Behavior in\n  Traffic: A Case Study","comments":"6 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The development of automated vehicles has the potential to revolutionize\ntransportation, but they are currently unable to ensure a safe and\ntime-efficient driving style. Reliable models predicting human behavior are\nessential for overcoming this issue. While data-driven models are commonly used\nto this end, they can be vulnerable in safety-critical edge cases. This has led\nto an interest in models incorporating cognitive theory, but as such models are\ncommonly developed for explanatory purposes, this approach's effectiveness in\nbehavior prediction has remained largely untested so far. In this article, we\ninvestigate the usefulness of the \\emph{Commotions} model -- a novel\ncognitively plausible model incorporating the latest theories of human\nperception, decision-making, and motor control -- for predicting human behavior\nin gap acceptance scenarios, which entail many important traffic interactions\nsuch as lane changes and intersections. We show that this model can compete\nwith or even outperform well-established data-driven prediction models across\nseveral naturalistic datasets. These results demonstrate the promise of\nincorporating cognitive theory in behavior prediction models for automated\nvehicles.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:27:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15188","submitter":"Wenjian Hao","authors":"Wenjian Hao, Paulo C. Heredia, Bowen Huang, Zehui Lu, Zihao Liang,\n  Shaoshuai Mou","title":"Policy Learning based on Deep Koopman Representation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.SY eess.SY","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  This paper proposes a policy learning algorithm based on the Koopman operator\ntheory and policy gradient approach, which seeks to approximate an unknown\ndynamical system and search for optimal policy simultaneously, using the\nobservations gathered through interaction with the environment. The proposed\nalgorithm has two innovations: first, it introduces the so-called deep Koopman\nrepresentation into the policy gradient to achieve a linear approximation of\nthe unknown dynamical system, all with the purpose of improving data\nefficiency; second, the accumulated errors for long-term tasks induced by\napproximating system dynamics are avoided by applying Bellman's principle of\noptimality. Furthermore, a theoretical analysis is provided to prove the\nasymptotic convergence of the proposed algorithm and characterize the\ncorresponding sampling complexity. These conclusions are also supported by\nsimulations on several challenging benchmark environments.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:27:22 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15189","submitter":"Jan Achterhold","authors":"Jan Achterhold, Philip Tobuschat, Hao Ma, Dieter Buechler, Michael\n  Muehlebach, Joerg Stueckler","title":"Black-Box vs. Gray-Box: A Case Study on Learning Table Tennis Ball\n  Trajectory Prediction with Spin and Impacts","comments":"Accepted for publication at the 5th Annual Conference on Learning for\n  Dynamics and Control (L4DC) 2023. With supplementary material","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.LG cs.SY eess.SY","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, we present a method for table tennis ball trajectory filtering\nand prediction. Our gray-box approach builds on a physical model. At the same\ntime, we use data to learn parameters of the dynamics model, of an extended\nKalman filter, and of a neural model that infers the ball's initial condition.\nWe demonstrate superior prediction performance of our approach over two\nblack-box approaches, which are not supplied with physical prior knowledge. We\ndemonstrate that initializing the spin from parameters of the ball launcher\nusing a neural network drastically improves long-time prediction performance\nover estimating the spin purely from measured ball positions. An accurate\nprediction of the ball trajectory is crucial for successful returns. We\ntherefore evaluate the return performance with a pneumatic artificial muscular\nrobot and achieve a return rate of 29/30 (97.7%).\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:28:22 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15190","submitter":"Changsub Kim","authors":"Changsub Kim, Christina Bell, Jake Evans, Jonathan Greenfield, Nathan\n  Lewis, Daniel Cunnane","title":"Wafer-scale magnesium diboride thin films and devices with tunable high\n  kinetic inductance","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.supr-con cond-mat.mtrl-sci physics.app-ph quant-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Progress in superconducting device and detector technologies over the past\ndecade have realized practical applications in quantum computers, detectors for\nfar-IR telescopes, and optical communications. Superconducting thin film\nmaterials, however, have remained largely unchanged, with aluminum still being\nthe material of choice for superconducting qubits, and Nb compounds for higher\nfrequency devices. $\\mathrm{MgB}_2$, known for its highest $\\mathrm{T}_c$ (39\nK) among metallic superconductors, is a viable material for higher frequency\nsuperconducting devices moving towards THz frequencies. However, difficulty in\nsynthesizing thin films have prevented implementation of $\\mathrm{MgB}_2$\ndevices into the application base of superconducting electronics, despite\npromising preliminary results for a number of applications. We have developed\nsmooth and uniform $\\mathrm{MgB}_2$ films on 4-inch Si wafers by depositing\nuniform Mg-B co-sputtered film, capping the film in situ to create a closed\nenvironment, followed by an optimized post-annealing step. We further report\nmature device fabrication processes and demonstrate test structures to measure\nproperties of the films. This includes resonators with internal Q factor over\n$\\mathrm{10}^4$ at 4.5 K and tunable high kinetic inductance (5-50 pH/$\\square$\nreadily achieved in a 40 nm film), opening up the path for development of high\nfrequency and high temperature $\\mathrm{MgB}_2$ microdevices.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:28:58 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15191","submitter":"Aldin Vehabovic","authors":"Farooq Shaikh, Elias Bou-Harb, Aldin Vehabovic, Jorge Crichigno,\n  Aysegul Yayimli and Nasir Ghani","title":"IoT Threat Detection Testbed Using Generative Adversarial Networks","comments":"8 pages, 5 figures","journal-ref":null,"doi":"10.1109/BlackSeaCom54372.2022.9858239","report-no":null,"categories":"cs.CR","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  The Internet of Things(IoT) paradigm provides persistent sensing and data\ncollection capabilities and is becoming increasingly prevalent across many\nmarket sectors. However, most IoT devices emphasize usability and function over\nsecurity, making them very vulnerable to malicious exploits. This concern is\nevidenced by the increased use of compromised IoT devices in large scale bot\nnetworks (botnets) to launch distributed denial of service(DDoS) attacks\nagainst high value targets. Unsecured IoT systems can also provide entry points\nto private networks, allowing adversaries relatively easy access to valuable\nresources and services. Indeed, these evolving IoT threat vectors (ranging from\nbrute force attacks to remote code execution exploits) are posing key\nchallenges. Moreover, many traditional security mechanisms are not amenable for\ndeployment on smaller resource-constrained IoT platforms. As a result,\nresearchers have been developing a range of methods for IoT security, with many\nstrategies using advanced machine learning(ML) techniques. Along these lines,\nthis paper presents a novel generative adversarial network(GAN) solution to\ndetect threats from malicious IoT devices both inside and outside a network.\nThis model is trained using both benign IoT traffic and global darknet data and\nfurther evaluated in a testbed with real IoT devices and malware threats.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:29:46 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15192","submitter":"Kiarash Banihashem","authors":"Kiarash Banihashem, Leyla Biabani, Samira Goudarzi, MohammadTaghi\n  Hajiaghayi, Peyman Jabbarzade, Morteza Monemizadeh","title":"Dynamic Constrained Submodular Optimization with Polylogarithmic Update\n  Time","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DS","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Maximizing a monotone submodular function under cardinality constraint $k$ is\na core problem in machine learning and database with many basic applications,\nincluding video and data summarization, recommendation systems, feature\nextraction, exemplar clustering, and coverage problems. We study this classic\nproblem in the fully dynamic model where a stream of insertions and deletions\nof elements of an underlying ground set is given and the goal is to maintain an\napproximate solution using a fast update time.\n  A recent paper at NeurIPS'20 by Lattanzi, Mitrovic, Norouzi{-}Fard,\nTarnawski, Zadimoghaddam claims to obtain a dynamic algorithm for this problem\nwith a $\\frac{1}{2} -\\epsilon$ approximation ratio and a query complexity\nbounded by $\\mathrm{poly}(\\log(n),\\log(k),\\epsilon^{-1})$. However, as we\nexplain in this paper, the analysis has some important gaps. Having a dynamic\nalgorithm for the problem with polylogarithmic update time is even more\nimportant in light of a recent result by Chen and Peng at STOC'22 who show a\nmatching lower bound for the problem -- any randomized algorithm with a\n$\\frac{1}{2}+\\epsilon$ approximation ratio must have an amortized query\ncomplexity that is polynomial in $n$.\n  In this paper, we develop a simpler algorithm for the problem that maintains\na $(\\frac{1}{2}-\\epsilon)$-approximate solution for submodular maximization\nunder cardinality constraint $k$ using a polylogarithmic amortized update time.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:30:07 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15193","submitter":"Wenjian Hao","authors":"Wenjian Hao, Zehui Lu, Zihao Liang, Tianyu Zhou, Shaoshuai Mou","title":"Adaptive Policy Learning to Additional Tasks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.SY eess.SY","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  This paper develops a policy learning method for tuning a pre-trained policy\nto adapt to additional tasks without altering the original task. A method named\nAdaptive Policy Gradient (APG) is proposed in this paper, which combines\nBellman's principle of optimality with the policy gradient approach to improve\nthe convergence rate. This paper provides theoretical analysis which guarantees\nthe convergence rate and sample complexity of $\\mathcal{O}(1/T)$ and\n$\\mathcal{O}(1/\\epsilon)$, respectively, where $T$ denotes the number of\niterations and $\\epsilon$ denotes the accuracy of the resulting stationary\npolicy. Furthermore, several challenging numerical simulations, including\ncartpole, lunar lander, and robot arm, are provided to show that APG obtains\nsimilar performance compared to existing deterministic policy gradient methods\nwhile utilizing much less data and converging at a faster rate.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:31:11 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15194","submitter":"Sungnyun Kim","authors":"Sungnyun Kim, Junsoo Lee, Kibeom Hong, Daesik Kim, Namhyuk Ahn","title":"DiffBlender: Scalable and Composable Multimodal Text-to-Image Diffusion\n  Models","comments":"18 pages, 16 figures, and 3 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The recent progress in diffusion-based text-to-image generation models has\nsignificantly expanded generative capabilities via conditioning the text\ndescriptions. However, since relying solely on text prompts is still\nrestrictive for fine-grained customization, we aim to extend the boundaries of\nconditional generation to incorporate diverse types of modalities, e.g.,\nsketch, box, and style embedding, simultaneously. We thus design a multimodal\ntext-to-image diffusion model, coined as DiffBlender, that achieves the\naforementioned goal in a single model by training only a few small\nhypernetworks. DiffBlender facilitates a convenient scaling of input\nmodalities, without altering the parameters of an existing large-scale\ngenerative model to retain its well-established knowledge. Furthermore, our\nstudy sets new standards for multimodal generation by conducting quantitative\nand qualitative comparisons with existing approaches. By diversifying the\nchannels of conditioning modalities, DiffBlender faithfully reflects the\nprovided information or, in its absence, creates imaginative generation.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:31:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15195","submitter":"Peihu Duan","authors":"Peihu Duan and Tao Liu and Yuezu Lv and Guanghui Wen","title":"Cooperative Control of Multi-Channel Linear Systems with Self-Organizing\n  Private Agents","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Cooperative behavior design for multi-agent systems with collective tasks is\na critical issue to promote swarm intelligence. This paper investigates\ncooperative control for a multi-channel system, where each channel is managed\nby an agent that can communicate with neighbors in a network. Each agent is\nexpected to self-organize a controller based only on local information and\nlocal interaction to stabilize the multi-channel system collaboratively. A\nnovel cooperative control strategy is designed for each agent by leveraging a\ndecomposing technique and a fusion approach. Then, a privacy-preserving\nmechanism is incorporated into this strategy to shield all private information\nfrom eavesdropping. Moreover, a fully distributed designing method for the\nstrategy parameters is developed. As a result, agents can self-design and\nself-perform their controllers with private information preserved. It is proved\nthat the multi-channel system stability can be ensured by the proposed strategy\nwith finite fusion steps during each control interval. In addition, the cost of\nintroducing the privacy-preserving mechanism and the effect of adding more\nchannels on the system performance are quantitatively analyzed, which benefits\nmechanism design and channel placement. Finally, several comparative simulation\nexamples are provided to demonstrate the effectiveness of the theoretical\nresults.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:31:28 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15196","submitter":"Kyunghyun Park","authors":"Myeongho Jeon, Myungjoo Kang, Joonhun Lee, Kyunghyun Park","title":"Feature-aligned N-BEATS with Sinkhorn divergence","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI math.OC math.PR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this study, we propose Feature-aligned N-BEATS as a domain generalization\nmodel for univariate time series forecasting problems. The proposed model is an\nextension of the doubly residual stacking architecture of N-BEATS (Oreshkin et\nal. [34]) into a representation learning framework. The model is a new\nstructure that involves marginal feature probability measures (i.e.,\npushforward measures of multiple source domains) induced by the intricate\ncomposition of residual operators of N-BEATS in each stack and aligns them\nstack-wise via an entropic regularized Wasserstein distance referred to as the\nSinkhorn divergence (Genevay et al. [14]). The loss function consists of a\ntypical forecasting loss for multiple source domains and an alignment loss\ncalculated with the Sinkhorn divergence, which allows the model to learn\ninvariant features stack-wise across multiple source data sequences while\nretaining N-BEATS's interpretable design. We conduct a comprehensive\nexperimental evaluation of the proposed approach and the results demonstrate\nthe model's forecasting and generalization capabilities in comparison with\nmethods based on the original N-BEATS.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:32:23 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15197","submitter":"Navdeep Rana","authors":"Navdeep Rana, Rayan Chatterjee, Sunghan Ro, Dov Levine, Sriram\n  Ramaswamy, Prasad Perlekar","title":"Defect turbulence in a dense suspension of polar, active swimmers","comments":"8 pages, 7 figures, 1 appendix","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.soft physics.flu-dyn","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We study the effects of inertia in dense suspensions of polar swimmers. The\nhydrodynamic velocity field and the polar order parameter field describe the\ndynamics of the suspension. We show that a dimensionless parameter $R$ (ratio\nof the swimmer self-advection speed to the active stress invasion speed)\ncontrols the stability of an ordered swimmer suspension. For $R$ smaller than a\nthreshold $R_1$, perturbations grow at a rate proportional to their wave number\n$q$. Beyond $R_1$, we show that the growth rate is $\\mathcal{O}(q^2)$ until a\nsecond threshold $R=R_2$ is reached. The suspension is stable for $R>R_2$. We\nperform direct numerical simulations to investigate the steady state properties\nand observe defect turbulence for $R<R_2$. An investigation of the spatial\norganisation of defects unravels a hidden transition: for small $R\\approx 0$\ndefects are uniformly distributed and cluster as $R\\to R_1$. Beyond $R_1$,\nclustering saturates and defects are arranged in nearly string-like structures.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:32:49 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15198","submitter":"Yuya Hattori","authors":"Yuya Hattori, Keisuke Sagisaka, Shunsuke Yoshizawa, Yuki Tokumoto, and\n  Keiichi Edagawa","title":"Topological surface states hybridized with bulk states of Bi-doped\n  PbSb2Te4 revealed in quasiparticle interference","comments":"7+8 pages, 4+5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Topological surface states of Bi-doped PbSb2Te4 [Pb(Bi0.20Sb0.80)2Te4] are\ninvestigated through analyses of quasiparticle interference (QPI) patterns\nobserved by scanning tunneling microscopy. Interpretation of the experimental\nQPI patterns in the reciprocal space is achieved by numerical QPI simulations\nusing two types of surface density of states produced by density functional\ntheory calculations or a kp surface state model. We found that the Dirac point\n(DP) of the surface state appears in the bulk band gap of this material and,\nwith the energy being away from the DP, the isoenergy contour of the surface\nstate is substantially deformed or separated into segments due to hybridization\nwith bulk electronic states. These findings provide a more accurate picture of\ntopological surface states, especially at energies away from the DP, providing\nvaluable insight into the electronic properties of topological insulators.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:34:38 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15199","submitter":"Nathan Vance","authors":"Nathan Vance, Jeremy Speth, Benjamin Sporrer, Patrick Flynn","title":"Promoting Generalization in Cross-Dataset Remote Photoplethysmography","comments":"8 pages, accepted for publication at CVPM 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Remote Photoplethysmography (rPPG), or the remote monitoring of a subject's\nheart rate using a camera, has seen a shift from handcrafted techniques to deep\nlearning models. While current solutions offer substantial performance gains,\nwe show that these models tend to learn a bias to pulse wave features inherent\nto the training dataset. We develop augmentations to mitigate this learned bias\nby expanding both the range and variability of heart rates that the model sees\nwhile training, resulting in improved model convergence when training and\ncross-dataset generalization at test time. Through a 3-way cross dataset\nanalysis we demonstrate a reduction in mean absolute error from over 13 beats\nper minute to below 3 beats per minute. We compare our method with other recent\nrPPG systems, finding similar performance under a variety of evaluation\nparameters.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:35:54 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15200","submitter":"Mark Saffman","authors":"J.C. Bohorquez, R. Chinnarasu, J. Isaacs, D. Booth, M. Beck, R.\n  McDermott, and M. Saffman","title":"Reducing Rydberg state dc polarizability by microwave dressing","comments":"factor of 4 in Table 3 corrected","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.atom-ph quant-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We demonstrate reduction of the dc polarizability of Cesium atom Rydberg\nstates in a 77 K environment utilizing microwave field dressing. In particular\nwe reduce the polarizability of $52P_{3/2}$ states which have resonances at\n5.35 GHz to $51D_{5/2}$, suitable for interfacing Rydberg atoms to\nsuperconducting resonators in a cryogenic environment. We measure the\npolarizability of the Rydberg states using Magneto-Optical-Trap (MOT) loss\nspectroscopy. Using an off-resonant radio-frequency (RF) dressing field\ncoupling $52P_{3/2}$ and $51D_{5/2}$ we demonstrate a reduction in dc\npolarizability of the $ 52P_{3/2}$ states over 80$\\%$. Experimental findings\nare in good agreement with a numerical model of the atom-dressing field system\ndeveloped using the Shirley-Floquet formalism. We also demonstrate that the dc\npolarizability reduction is highly anisotropic, with near total nulling\npossible when the dc and dressing fields are aligned, but only a factor of two\nreduction in polarizability when the fields are orthogonal. These results may\naid in stabilizing Rydberg resonances against varying dc fields present near\nsurfaces, enabling advancement in the development of hybrid Rydberg atom -\nsuperconducting resonator quantum gates.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:35:57 GMT"},{"version":"v2","created":"Thu, 25 May 2023 02:08:29 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.15201","submitter":"Dylan Herman","authors":"Shree Hari Sureshbabu, Dylan Herman, Ruslan Shaydulin, Joao Basso,\n  Shouvanik Chakrabarti, Yue Sun, and Marco Pistoia","title":"Parameter Setting in Quantum Approximate Optimization of Weighted\n  Problems","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Quantum Approximate Optimization Algorithm (QAOA) is a leading candidate\nalgorithm for solving combinatorial optimization problems on quantum computers.\nHowever, in many cases QAOA requires computationally intensive parameter\noptimization. The challenge of parameter optimization is particularly acute in\nthe case of weighted problems, for which the eigenvalues of the phase operator\nare non-integer and the QAOA energy landscape is not periodic. In this work, we\ndevelop parameter setting heuristics for QAOA applied to a general class of\nweighted problems. First, we derive optimal parameters for QAOA with depth\n$p=1$ applied to the weighted MaxCut problem under different assumptions on the\nweights. In particular, we rigorously prove the conventional wisdom that in the\naverage case the first local optimum near zero gives globally-optimal QAOA\nparameters. Second, for $p\\geq 1$ we prove that the QAOA energy landscape for\nweighted MaxCut approaches that for the unweighted case under a simple\nrescaling of parameters. Therefore, we can use parameters previously obtained\nfor unweighted MaxCut for weighted problems. Finally, we prove that for $p=1$\nthe QAOA objective sharply concentrates around its expectation, which means\nthat our parameter setting rules hold with high probability for a random\nweighted instance. We numerically validate this approach on general weighted\ngraphs and show that on average the QAOA energy with the proposed fixed\nparameters is only $1.1$ percentage points away from that with optimized\nparameters. Third, we propose a general heuristic rescaling scheme inspired by\nthe analytical results for weighted MaxCut and demonstrate its effectiveness\nusing QAOA with the XY Hamming-weight-preserving mixer applied to the portfolio\noptimization problem. Our heuristic improves the convergence of local\noptimizers, reducing the number of iterations by 7.2x on average.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:37:33 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15202","submitter":"Xiaomeng Chen","authors":"Xiaomeng Chen, Wei Jiang, Themistoklis Charalambous and Ling Shi","title":"A Privacy-Preserving Finite-Time Push-Sum based Gradient Method for\n  Distributed Optimization over Digraphs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC cs.SY eess.SY","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  This paper addresses the problem of distributed optimization, where a network\nof agents represented as a directed graph (digraph) aims to collaboratively\nminimize the sum of their individual cost functions. Existing approaches for\ndistributed optimization over digraphs, such as Push-Pull, require agents to\nexchange explicit state values with their neighbors in order to reach an\noptimal solution. However, this can result in the disclosure of sensitive and\nprivate information. To overcome this issue, we propose a\nstate-decomposition-based privacy-preserving finite-time push-sum (PrFTPS)\nalgorithm without any global information such as network size or graph\ndiameter. Then, based on PrFTPS, we design a gradient descent algorithm\n(PrFTPS-GD) to solve the distributed optimization problem. It is proved that\nunder PrFTPS-GD, the privacy of each agent is preserved and the linear\nconvergence rate related to the optimization iteration number is achieved.\nFinally, numerical simulations are provided to illustrate the effectiveness of\nthe proposed approach.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:38:18 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15203","submitter":"Lorenzo Basile","authors":"Lorenzo Basile, Nikos Karantzas, Alberto D'Onofrio, Luca Bortolussi,\n  Alex Rodriguez, Fabio Anselmi","title":"Relating Implicit Bias and Adversarial Attacks through Intrinsic\n  Dimension","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CR stat.ML","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Despite their impressive performance in classification, neural networks are\nknown to be vulnerable to adversarial attacks. These attacks are small\nperturbations of the input data designed to fool the model. Naturally, a\nquestion arises regarding the potential connection between the architecture,\nsettings, or properties of the model and the nature of the attack. In this\nwork, we aim to shed light on this problem by focusing on the implicit bias of\nthe neural network, which refers to its inherent inclination to favor specific\npatterns or outcomes. Specifically, we investigate one aspect of the implicit\nbias, which involves the essential Fourier frequencies required for accurate\nimage classification. We conduct tests to assess the statistical relationship\nbetween these frequencies and those necessary for a successful attack. To delve\ninto this relationship, we propose a new method that can uncover non-linear\ncorrelations between sets of coordinates, which, in our case, are the\naforementioned frequencies. By exploiting the entanglement between intrinsic\ndimension and correlation, we provide empirical evidence that the network bias\nin Fourier space and the target frequencies of adversarial attacks are closely\ntied.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:40:23 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15204","submitter":"Tyler Lawson","authors":"Tyler Lawson","title":"Lax monoidality for products of enriched higher categories","comments":"22 pages. Comments welcome","journal-ref":null,"doi":null,"report-no":null,"categories":"math.CT math.AT","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We prove that a lax $\\mathbb{E}_{n+1}$-monoidal functor from $\\mathcal V$ to\n$\\mathcal W$ induces a lax $\\mathbb{E}_n$-monoidal functor from $\\mathcal\nV$-enriched $\\infty$-categories to $\\mathcal W$-enriched $\\infty$-categories in\nthe sense of Gepner--Haugseng.\n  We prove this as part of a general-purpose interaction with the\nBoardman--Vogt tensor product $\\otimes$: given a construction that takes an\n$\\mathcal E$-monoidal $\\infty$-category to a category expressible in\ndiagrammatic terms, we give a criterion for it to take $(\\mathcal{O} \\otimes\n\\mathcal{E})$-monoidal $\\infty$-categories to $\\mathcal{O}$-monoidal\n$\\infty$-categories using a \"pointwise\" monoidal structure.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:40:27 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15205","submitter":"Anton Yurchenko-Tytarenko","authors":"Yuliya Mishura, Anton Yurchenko-Tytarenko","title":"Parameter estimation in rough Bessel model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper, we construct consistent statistical estimators of the Hurst\nindex, volatility coefficient, and drift parameter for Bessel processes driven\nby fractional Brownian motion with $H<1/2$. As an auxiliary result, we also\nprove the continuity of the fractional Bessel process. The results are\nillustrated with simulations.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:40:34 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15206","submitter":"Vasiliki Velona","authors":"Anna Ben-Hamou, Vasiliki Velona","title":"Inference in balanced community modulated recursive trees","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.ST math.PR stat.TH","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We introduce a random recursive tree model with two communities, called\nbalanced community modulated random recursive tree, or BCMRT in short. In this\nsetting, pairs of nodes of different type appear sequentially. Each one of them\ndecides independently to attach to their own type with probability 1-q, or to\nthe other type with probability q, and then chooses its parent uniformly within\nthe set of existing nodes with the selected type. We find that the limiting\ndegree distributions coincide for different q. Therefore, as far as inference\nis concerned, other statistics have to be studied. We first consider the\nsetting where the time-labels of the nodes, i.e. their time of arrival, are\nobserved but their type is not. In this setting, we design a consistent\nestimator for q and provide bounds for the feasibility of testing between two\ndifferent values of q. Moreover, we show that if q is small enough, then it is\npossible to cluster in a way correlated with the true partition, even though\nthe algorithm is exponential in time. In the unlabelled setting, i.e. when only\nthe tree structure is observed, we show that it is possible to test between\ndifferent values of q in a strictly better way than by random guessing. This\nfollows from a delicate analysis of the sum-of-distances statistic.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:40:37 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15207","submitter":"Edwin van Dam","authors":"Pepijn Wissing and Edwin R. van Dam","title":"Symmetry in complex unit gain graphs and their spectra","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.CO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Complex unit gain graphs may exhibit various kinds of symmetry. In this work,\nwe explore structural symmetry, spectral symmetry and sign-symmetry in gain\ngraphs, and their respective relations to one-another. Our main result is a\nconstruction that transforms an arbitrary gain graph into infinitely many\nswitching-distinct gain graphs whose spectral symmetry does not imply\nsign-symmetry. This provides a more general answer to the gain graph analogue\nof an existence question that was recently treated in the context of signed\ngraphs.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:42:51 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15208","submitter":"Michael Deistler","authors":"Richard Gao, Michael Deistler, Jakob H. Macke","title":"Generalized Bayesian Inference for Scientific Simulators via Amortized\n  Cost Estimation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ML cs.LG","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Simulation-based inference (SBI) enables amortized Bayesian inference for\nsimulators with implicit likelihoods. But when we are primarily interested in\nthe quality of predictive simulations, or when the model cannot exactly\nreproduce the observed data (i.e., is misspecified), targeting the Bayesian\nposterior may be overly restrictive. Generalized Bayesian Inference (GBI) aims\nto robustify inference for (misspecified) simulator models, replacing the\nlikelihood-function with a cost function that evaluates the goodness of\nparameters relative to data. However, GBI methods generally require running\nmultiple simulations to estimate the cost function at each parameter value\nduring inference, making the approach computationally infeasible for even\nmoderately complex simulators. Here, we propose amortized cost estimation (ACE)\nfor GBI to address this challenge: We train a neural network to approximate the\ncost function, which we define as the expected distance between simulations\nproduced by a parameter and observed data. The trained network can then be used\nwith MCMC to infer GBI posteriors for any observation without running\nadditional simulations. We show that, on several benchmark tasks, ACE\naccurately predicts cost and provides predictive simulations that are closer to\nsynthetic observations than other SBI methods, especially for misspecified\nsimulators. Finally, we apply ACE to infer parameters of the Hodgkin-Huxley\nmodel given real intracellular recordings from the Allen Cell Types Database.\nACE identifies better data-matching parameters while being an order of\nmagnitude more simulation-efficient than a standard SBI method. In summary, ACE\ncombines the strengths of SBI methods and GBI to perform robust and\nsimulation-amortized inference for scientific simulators.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:45:03 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15209","submitter":"Graham Manuell","authors":"Graham Manuell and Joshua L. Wrigley","title":"The representing localic groupoid for a geometric theory","comments":"36 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.CT math.AG math.LO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We give an expository, and hopefully approachable, account of the\nJoyal-Tierney result that every topos can be represented as a topos of sheaves\non a localic groupoid. We give an explicit presentation of a representing\nlocalic groupoid for the classifying topos of a given geometric theory and\ndiscuss links with the topological groupoids of Forssell.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:45:37 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15210","submitter":"Matt Franchi","authors":"Matt Franchi, J.D. Zamfirescu-Pereira, Wendy Ju, Emma Pierson","title":"Detecting disparities in police deployments using dashcam data","comments":"To appear in ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT) '23","journal-ref":null,"doi":"10.1145/3593013.3594020","report-no":null,"categories":"cs.CY","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Large-scale policing data is vital for detecting inequity in police behavior\nand policing algorithms. However, one important type of policing data remains\nlargely unavailable within the United States: aggregated police deployment data\ncapturing which neighborhoods have the heaviest police presences. Here we show\nthat disparities in police deployment levels can be quantified by detecting\npolice vehicles in dashcam images of public street scenes. Using a dataset of\n24,803,854 dashcam images from rideshare drivers in New York City, we find that\npolice vehicles can be detected with high accuracy (average precision 0.82, AUC\n0.99) and identify 233,596 images which contain police vehicles. There is\nsubstantial inequality across neighborhoods in police vehicle deployment\nlevels. The neighborhood with the highest deployment levels has almost 20 times\nhigher levels than the neighborhood with the lowest. Two strikingly different\ntypes of areas experience high police vehicle deployments - 1) dense,\nhigher-income, commercial areas and 2) lower-income neighborhoods with higher\nproportions of Black and Hispanic residents. We discuss the implications of\nthese disparities for policing equity and for algorithms trained on policing\ndata.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:48:59 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15211","submitter":"Arpita Mondal","authors":"Arghya Choudhury, Sourav Mitra, Arpita Mondal and Subhadeep Mondal","title":"Bilinear R-parity violating supersymmetry under the light of neutrino\n  oscillation, higgs and flavor data","comments":"35 pages, 9 figures, references added","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ph hep-ex","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this work, we explore a well motivated beyond the Standard Model scenario,\nnamely, R-parity violating Supersymmetry, in the context of light neutrino\nmasses and mixing. We assume that the R-parity is only broken by the lepton\nnumber violating bilinear term. We try to fit two non-zero neutrino mass square\ndifferences and three mixing angle values obtained from the global $\\chi^2$\nanalysis of neutrino oscillation data. We have also taken into account the\nupdated data of the standard model (SM) Higgs mass and its coupling strengths\nwith other SM particles from LHC Run-II along with low energy flavor violating\nconstraints like rare b-hadron decays. We have used a Markov Chain Monte Carlo\n(MCMC) analysis to constrain the new physics parameter space. While doing so,\nwe ensure that all the existing collider constraints are duly taken into\naccount. Through our analysis we have derived the most stringent constraints\npossible till date with existing data on the 9 bilinear R-parity violating\nparameters along with $\\mu$ and $\\tan\\beta$. We further explore the possibility\nof explaining the anomalous muon (g - 2) measurement staying within the\nparameter space allowed by neutrino, Higgs and flavor data while satisfying the\ncollider constraints as well. We find that there still remains a small sub-TeV\nparameter space where the required excess can be obtained.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:48:59 GMT"},{"version":"v2","created":"Wed, 7 Jun 2023 09:53:32 GMT"}],"update_date":"2023-06-08"}
{"id":"2305.15212","submitter":"Zhen-Ru Zhang","authors":"Zhen-Ru Zhang, Chuanqi Tan, Haiyang Xu, Chengyu Wang, Jun Huang,\n  Songfang Huang","title":"Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model\n  Fine-tuning","comments":"Accepted to ACL 2023 (Main conference)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Fine-tuning large pre-trained language models on various downstream tasks\nwith whole parameters is prohibitively expensive. Hence, Parameter-efficient\nfine-tuning has attracted attention that only optimizes a few task-specific\nparameters with the frozen pre-trained model. In this work, we focus on prefix\ntuning, which only optimizes continuous prefix vectors (i.e. pseudo tokens)\ninserted into Transformer layers. Based on the observation that the learned\nsyntax and semantics representation varies a lot at different layers, we argue\nthat the adaptive prefix will be further tailored to each layer than the fixed\none, enabling the fine-tuning more effective and efficient. Thus, we propose\nAdaptive Prefix Tuning (APT) to adjust the prefix in terms of both fine-grained\ntoken level and coarse-grained layer level with a gate mechanism. Experiments\non the SuperGLUE and NER datasets show the effectiveness of APT. In addition,\ntaking the gate as a probing, we validate the efficiency and effectiveness of\nthe variable prefix.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:51:01 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15213","submitter":"Qian Wang","authors":"Wei Zhou, Qian Wang, Weiwei Jin, Xinzhe Shi, Dekui Wang, Xingxing Hao,\n  Yongxiang Yu","title":"GTNet: Graph Transformer Network for 3D Point Cloud Classification and\n  Semantic Segmentation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Recently, graph-based and Transformer-based deep learning networks have\ndemonstrated excellent performances on various point cloud tasks. Most of the\nexisting graph methods are based on static graph, which take a fixed input to\nestablish graph relations. Moreover, many graph methods apply maximization and\naveraging to aggregate neighboring features, so that only a single neighboring\npoint affects the feature of centroid or different neighboring points have the\nsame influence on the centroid's feature, which ignoring the correlation and\ndifference between points. Most Transformer-based methods extract point cloud\nfeatures based on global attention and lack the feature learning on local\nneighbors. To solve the problems of these two types of models, we propose a new\nfeature extraction block named Graph Transformer and construct a 3D point point\ncloud learning network called GTNet to learn features of point clouds on local\nand global patterns. Graph Transformer integrates the advantages of graph-based\nand Transformer-based methods, and consists of Local Transformer and Global\nTransformer modules. Local Transformer uses a dynamic graph to calculate all\nneighboring point weights by intra-domain cross-attention with dynamically\nupdated graph relations, so that every neighboring point could affect the\nfeatures of centroid with different weights; Global Transformer enlarges the\nreceptive field of Local Transformer by a global self-attention. In addition,\nto avoid the disappearance of the gradient caused by the increasing depth of\nnetwork, we conduct residual connection for centroid features in GTNet; we also\nadopt the features of centroid and neighbors to generate the local geometric\ndescriptors in Local Transformer to strengthen the local information learning\ncapability of the model. Finally, we use GTNet for shape classification, part\nsegmentation and semantic segmentation tasks in this paper.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:51:18 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15214","submitter":"Michele Pizzochero","authors":"Nikita V. Tepliakov, Ruize Ma, Johannes Lischner, Efthimios Kaxiras,\n  Arash A. Mostofi, Michele Pizzochero","title":"Dirac half-semimetallicity and antiferromagnetism in graphene\n  nanoribbon/hexagonal boron nitride heterojunctions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci cond-mat.mes-hall","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Half-metals have been envisioned as active components in spintronic devices\nby virtue of their completely spin-polarized electrical currents. Actual\nmaterials hosting half-metallic phases, however, remain scarce. Here, we\npredict that recently fabricated heterojunctions of zigzag nanoribbons embedded\nin two-dimensional hexagonal boron nitride are half-semimetallic, featuring\nfully spin-polarized Dirac points at the Fermi level. The half-semimetallicity\noriginates from the transfer of charges from hexagonal boron nitride to the\nembedded graphene nanoribbon. These charges give rise to opposite energy shifts\nof the states residing at the two edges while preserving their intrinsic\nantiferromagnetic exchange coupling. Upon doping, an\nantiferromagnetic-to-ferrimagnetic phase transition occurs in these\nheterojunctions, with the sign of the excess charge controlling the spatial\nlocalization of the net magnetic moments. Our findings demonstrate that such\nheterojunctions realize tunable one-dimensional conducting channels of\nspin-polarized Dirac fermions that are seamlessly integrated into a\ntwo-dimensional insulator, thus holding promise for the development of\ncarbon-based spintronics.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:52:07 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15215","submitter":"Tao Yu","authors":"Tao Yu, Toni J.B. Liu, Albert Tseng, Christopher De Sa","title":"Shadow Cones: Unveiling Partial Orders in Hyperbolic Space","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Hyperbolic space has been shown to produce superior low-dimensional\nembeddings of hierarchical structures that are unattainable in Euclidean space.\nBuilding upon this, the entailment cone formulation of Ganea et al. uses\ngeodesically convex cones to embed partial orderings in hyperbolic space.\nHowever, these entailment cones lack intuitive interpretations due to their\ndefinitions via complex concepts such as tangent vectors and the exponential\nmap in Riemannian space. In this paper, we present shadow cones, an innovative\nframework that provides a physically intuitive interpretation for defining\npartial orders on general manifolds. This is achieved through the use of\nmetaphoric light sources and object shadows, inspired by the sun-earth-moon\nrelationship. Shadow cones consist of two primary classes: umbral and penumbral\ncones. Our results indicate that shadow cones offer robust representation and\ngeneralization capabilities across a variety of datasets, such as WordNet and\nConceptNet, thereby outperforming the top-performing entailment cones. Our\nfindings indicate that shadow cones offer an innovative, general approach to\ngeometrically encode partial orders, enabling better representation and\nanalysis of datasets with hierarchical structures.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:52:56 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15216","submitter":"Tanveer Hussain","authors":"Tanveer Hussain, Juan Gallego-Calderon, S M Shafiul Alam","title":"Open Source High Fidelity Modeling of a Type 5 Wind Turbine Drivetrain\n  for Grid Integration","comments":"This manuscript is originally submitted to Journal of Physics","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The increasing integration of renewable energy resources in evolving bulk\npower system (BPS) is impacting the system inertia. Type-5 wind turbine\ngeneration has the potential to behave like a traditional synchronous generator\nand can help improve system inertia. Hydraulic torque converter (TC) and\ngearbox with torque limiting feature are integral parts of a Type-5 wind\nturbine unit. High fidelity model of Type-5 wind turbine including these core\ncomponents is not openly and widely available for grid integration and\ntransient stability studies. This hinders appropriate assessment of Type-5 wind\npower plant's contribution to bulk grid resilience. This work develops a TC\nmodel based on those generally used in automobile's transmission system.\nMoreover, the concept of torsional coupling is leveraged to integrate the TC\nand gearbox system dynamics. The entire integrated model will be open sourced\nand publicly available for grid integration studies.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:53:06 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15217","submitter":"Shuchen Weng","authors":"Zheng Chang, Shuchen Weng, Peixuan Zhang, Yu Li, Si Li, Boxin Shi","title":"L-CAD: Language-based Colorization with Any-level Descriptions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Language-based colorization produces plausible and visually pleasing colors\nunder the guidance of user-friendly natural language descriptions. Previous\nmethods implicitly assume that users provide comprehensive color descriptions\nfor most of the objects in the image, which leads to suboptimal performance. In\nthis paper, we propose a unified model to perform language-based colorization\nwith any-level descriptions. We leverage the pretrained cross-modality\ngenerative model for its robust language understanding and rich color priors to\nhandle the inherent ambiguity of any-level descriptions. We further design\nmodules to align with input conditions to preserve local spatial structures and\nprevent the ghosting effect. With the proposed novel sampling strategy, our\nmodel achieves instance-aware colorization in diverse and complex scenarios.\nExtensive experimental results demonstrate our advantages of effectively\nhandling any-level descriptions and outperforming both language-based and\nautomatic colorization methods. The code and pretrained models are available\nat: https://github.com/changzheng123/L-CAD.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:57:42 GMT"},{"version":"v2","created":"Fri, 26 May 2023 11:37:53 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.15218","submitter":"Hanqi Su","authors":"Hanqi Su, Binyang Song and Faez Ahmed","title":"Multi-modal Machine Learning for Vehicle Rating Predictions Using Image,\n  Text, and Parametric Data","comments":"The paper submitted to IDETC/CIE2023, the International Design\n  Engineering Technical Conferences & Computers and Information in Engineering\n  Conference, has been accepted","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Accurate vehicle rating prediction can facilitate designing and configuring\ngood vehicles. This prediction allows vehicle designers and manufacturers to\noptimize and improve their designs in a timely manner, enhance their product\nperformance, and effectively attract consumers. However, most of the existing\ndata-driven methods rely on data from a single mode, e.g., text, image, or\nparametric data, which results in a limited and incomplete exploration of the\navailable information. These methods lack comprehensive analyses and\nexploration of data from multiple modes, which probably leads to inaccurate\nconclusions and hinders progress in this field. To overcome this limitation, we\npropose a multi-modal learning model for more comprehensive and accurate\nvehicle rating predictions. Specifically, the model simultaneously learns\nfeatures from the parametric specifications, text descriptions, and images of\nvehicles to predict five vehicle rating scores, including the total score,\ncritics score, performance score, safety score, and interior score. We compare\nthe multi-modal learning model to the corresponding unimodal models and find\nthat the multi-modal model's explanatory power is 4% - 12% higher than that of\nthe unimodal models. On this basis, we conduct sensitivity analyses using SHAP\nto interpret our model and provide design and optimization directions to\ndesigners and manufacturers. Our study underscores the importance of the\ndata-driven multi-modal learning approach for vehicle design, evaluation, and\noptimization. We have made the code publicly available at\nhttp://decode.mit.edu/projects/vehicleratings/.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:58:49 GMT"},{"version":"v2","created":"Sat, 27 May 2023 10:20:16 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.15219","submitter":"Yao Rong","authors":"Yao Rong, Xiangyu Wei, Tianwei Lin, Yueyu Wang, Enkelejda Kasneci","title":"DynStatF: An Efficient Feature Fusion Strategy for LiDAR 3D Object\n  Detection","comments":"Accepted to CVPR2023 Workshop on End-to-End Autonomous Driving","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Augmenting LiDAR input with multiple previous frames provides richer semantic\ninformation and thus boosts performance in 3D object detection, However,\ncrowded point clouds in multi-frames can hurt the precise position information\ndue to the motion blur and inaccurate point projection. In this work, we\npropose a novel feature fusion strategy, DynStaF (Dynamic-Static Fusion), which\nenhances the rich semantic information provided by the multi-frame (dynamic\nbranch) with the accurate location information from the current single-frame\n(static branch). To effectively extract and aggregate complimentary features,\nDynStaF contains two modules, Neighborhood Cross Attention (NCA) and\nDynamic-Static Interaction (DSI), operating through a dual pathway\narchitecture. NCA takes the features in the static branch as queries and the\nfeatures in the dynamic branch as keys (values). When computing the attention,\nwe address the sparsity of point clouds and take only neighborhood positions\ninto consideration. NCA fuses two features at different feature map scales,\nfollowed by DSI providing the comprehensive interaction. To analyze our\nproposed strategy DynStaF, we conduct extensive experiments on the nuScenes\ndataset. On the test set, DynStaF increases the performance of PointPillars in\nNDS by a large margin from 57.7% to 61.6%. When combined with CenterPoint, our\nframework achieves 61.0% mAP and 67.7% NDS, leading to state-of-the-art\nperformance without bells and whistles.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:00:01 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15220","submitter":"Caitlin Grasso","authors":"Caitlin Grasso and Josh Bongard","title":"Selection for short-term empowerment accelerates the evolution of\n  homeostatic neural cellular automata","comments":"To be published in the Proceedings of the Genetic and Evolutionary\n  Computation Conference 2023 (GECCO'23), 8 pages, 9 figures","journal-ref":null,"doi":"10.1145/3583131.3590469","report-no":null,"categories":"cs.NE cs.AI cs.IT math.IT","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Empowerment -- a domain independent, information-theoretic metric -- has\npreviously been shown to assist in the evolutionary search for neural cellular\nautomata (NCA) capable of homeostasis when employed as a fitness function. In\nour previous study, we successfully extended empowerment, defined as maximum\ntime-lagged mutual information between agents' actions and future sensations,\nto a distributed sensorimotor system embodied as an NCA. However, the\ntime-delay between actions and their corresponding sensations was arbitrarily\nchosen. Here, we expand upon previous work by exploring how the time scale at\nwhich empowerment operates impacts its efficacy as an auxiliary objective to\naccelerate the discovery of homeostatic NCAs. We show that shorter time delays\nresult in marked improvements over empowerment with longer delays, when\ncompared to evolutionary selection only for homeostasis. Moreover, we evaluate\nstability and adaptability of evolved NCAs, both hallmarks of living systems\nthat are of interest to replicate in artificial ones. We find that short-term\nempowered NCA are more stable and are capable of generalizing better to unseen\nhomeostatic challenges. Taken together, these findings motivate the use of\nempowerment during the evolution of other artifacts, and suggest how it should\nbe incorporated to accelerate evolution of desired behaviors for them. Source\ncode for the experiments in this paper can be found at:\nhttps://github.com/caitlingrasso/empowered-nca-II.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:01:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15221","submitter":"Kenneth Dere","authors":"Kenneth Dere, Peter Young, Giulio Del Zanna, Enrico Landi","title":"CHIANTI -- an atomic database for emission lines -- Paper XVII: Version\n  10.1, revised ionization and recombination rates and other updates","comments":"24 pages, 18 figures, submitted to The Astrophysical Journal\n  Supplement Series","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.atom-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The CHIANTI atomic database provides sets of assessed data used for\nsimulating spectral observations of astrophysical plasmas. This article\ndescribes updates that will be released as version~10.1 of the database. A key\ncomponent of CHIANTI is the provision of ionization and recombination rates\nthat are used to compute the ionization balance of a plasma over a range of\ntemperatures. Parameters for calculating the ionization rates of all stages of\nions from H through Zn were compiled and inserted into the CHIANTI database in\n2009. These were based on all measurements that were available at the time and\nsupplemented with distorted wave calculations. Since then, there have been a\nnumber of new laboratory measurements for ions that produce spectral lines that\nare commonly observed. Parameters have been fit to these new measurements to\nprovide improved ability to reproduce the ionization cross sections and rate\ncoefficients, and these are added to the database. CHIANTI 10.1 also includes\nnew recombination rates for the phosphorus isoelectronic sequence, and the\nupdated ionization and recombination rates have been used to calculate a new\nionization equilibrium file. In addition, CHIANTI 10.1 has new electron\ncollision and radiative datasets for eight ions in the nitrogen and oxygen\nisoelectronic sequences, and updated energy level and wavelength data for seven\nother ions.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:03:37 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15222","submitter":"Koyena Pal","authors":"Koyena Pal, Seyed Ali Bahrainian, Laura Mercurio, Carsten Eickhoff","title":"Neural Summarization of Electronic Health Records","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.IR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Hospital discharge documentation is among the most essential, yet\ntime-consuming documents written by medical practitioners. The objective of\nthis study was to automatically generate hospital discharge summaries using\nneural network summarization models. We studied various data preparation and\nneural network training techniques that generate discharge summaries. Using\nnursing notes and discharge summaries from the MIMIC-III dataset, we studied\nthe viability of the automatic generation of various sections of a discharge\nsummary using four state-of-the-art neural network summarization models (BART,\nT5, Longformer and FLAN-T5). Our experiments indicated that training\nenvironments including nursing notes as the source, and discrete sections of\nthe discharge summary as the target output (e.g. \"History of Present Illness\")\nimprove language model efficiency and text quality. According to our findings,\nthe fine-tuned BART model improved its ROUGE F1 score by 43.6% against its\nstandard off-the-shelf version. We also found that fine-tuning the baseline\nBART model with other setups caused different degrees of improvement (up to 80%\nrelative improvement). We also observed that a fine-tuned T5 generally achieves\nhigher ROUGE F1 scores than other fine-tuned models and a fine-tuned FLAN-T5\nachieves the highest ROUGE score overall, i.e., 45.6. For majority of the\nfine-tuned language models, summarizing discharge summary report sections\nseparately outperformed the summarization the entire report quantitatively. On\nthe other hand, fine-tuning language models that were previously instruction\nfine-tuned showed better performance in summarizing entire reports. This study\nconcludes that a focused dataset designed for the automatic generation of\ndischarge summaries by a language model can produce coherent Discharge Summary\nsections.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:05:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15223","submitter":"Yuzhu Han","authors":"Qian Zhang and Yuzhu Han","title":"Existence of nontrivial solutions to a fourth-order Kirchhoff type\n  elliptic equation with critical exponent","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, a critical fourth-order Kirchhoff type elliptic equation with\na subcritical perturbation is studied. The main feature of this problem is that\nit involves both a nonlocal coefficient and a critical term, which bring\nessential difficulty for the proof of the existence of weak solutions. When the\ndimension of the space is smaller than or equals to $7$, the existence of weak\nsolution is obtained by combining the Mountain Pass Lemma with some delicate\nestimate on the Talenti's functions. When the dimension of the space is larger\nthan or equals to $8$, the above argument no longer works. By introducing an\nappropriate truncation on the nonlocal coefficient, it is shown that the\nproblem admits a nontrivial solution under appropriate conditions on the\nparameter.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:06:20 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15224","submitter":"Alexander Cliffe","authors":"Gui-Qiang G. Chen, Alexander Cliffe, Feimin Huang, Song Liu, Qin Wang","title":"Global Solutions of the Two-Dimensional Riemann Problem with Four-Shock\n  Interactions for the Euler Equations for Potential Flow","comments":"75 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP math-ph math.MP physics.flu-dyn","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We present a rigorous approach and related techniques to construct global\nsolutions of the 2-D Riemann problem with four-shock interactions for the Euler\nequations for potential flow. With the introduction of three critical angles:\nthe vacuum critical angle from the compatibility conditions, the detachment\nangle, and the sonic angle, we clarify all configurations of the Riemann\nsolutions for the interactions of two-forward and two-backward shocks,\nincluding the subsonic-subsonic reflection configuration that has not emerged\nin previous results. To achieve this, we first identify the three critical\nangles that determine the configurations, whose existence and uniqueness follow\nfrom our rigorous proof of the strict monotonicity of the steady detachment and\nsonic angles for 2-D steady potential flow with respect to the Mach number of\nthe upstream state. Then we reformulate the 2-D Riemann problem into the shock\nreflection-diffraction problem with respect to a symmetric line, along with two\nindependent incident angles and two sonic boundaries varying with the choice of\nincident angles. With these, the problem can be further reformulated as a free\nboundary problem for a second-order quasilinear equation of mixed\nelliptic-hyperbolic type. The difficulties arise from the degenerate\nellipticity of the nonlinear equation near the sonic boundaries, the\nnonlinearity of the free boundary condition, the singularity of the solution\nnear the corners of the domain, and the geometric properties of the free\nboundary. To the best of our knowledge, this is the first rigorous result for\nthe 2-D Riemann problem with four-shock interactions for the Euler equations.\nThe approach and techniques developed for the Riemann problem for four-wave\ninteractions should be useful for solving other 2-D Riemann problems for more\ngeneral Euler equations and related nonlinear hyperbolic systems of\nconservation laws.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:06:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15225","submitter":"Hongyin Luo","authors":"Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim,\n  Xixin Wu, Danny Fox, Helen Meng, James Glass","title":"SAIL: Search-Augmented Instruction Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large language models (LLMs) have been significantly improved by instruction\nfine-tuning, but still lack transparency and the ability to utilize up-to-date\nknowledge and information. In this work, we propose search-augmented\ninstruction learning (SAIL), which grounds the language generation and\ninstruction following abilities on complex search results generated by in-house\nand external search engines. With an instruction tuning corpus, we collect\nsearch results for each training case from different search APIs and domains,\nand construct a new search-grounded training set containing\n\\textit{(instruction, grounding information, response)} triplets. We then\nfine-tune the LLaMA-7B model on the constructed training set. Since the\ncollected results contain unrelated and disputing languages, the model needs to\nlearn to ground on trustworthy search results, filter out distracting passages,\nand generate the target response. The search result-denoising process entails\nexplicit trustworthy information selection and multi-hop reasoning, since the\nretrieved passages might be informative but not contain the\ninstruction-following answer. Experiments show that the fine-tuned SAIL-7B\nmodel has a strong instruction-following ability, and it performs significantly\nbetter on transparency-sensitive tasks, including open-ended question answering\nand fact checking.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:07:30 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15226","submitter":"Matteo Baggioli","authors":"Matteo Baggioli","title":"Topological defects reveal the plasticity of glasses","comments":"News and Views commentary for the paper by Wu et Al. [Nat Commun 14,\n  2955 (2023)]","journal-ref":"Nat Commun 14, 2956 (2023)","doi":"10.1038/s41467-023-38549-8","report-no":null,"categories":"cond-mat.soft cond-mat.dis-nn cond-mat.mtrl-sci","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Mixing theoretical topological structures with cutting-edge simulation\nmethods, a recent study in Nature Communications has finally confirmed the\nexistence of topological defects in glasses and their crucial role for\nplasticity.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:08:36 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15227","submitter":"Anja Deli\\'c","authors":"Anja Deli\\'c and Matej Grci\\'c and Sini\\v{s}a \\v{S}egvi\\'c","title":"Real time dense anomaly detection by learning on synthetic negative data","comments":"3 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Most approaches to dense anomaly detection rely on generative modeling or on\ndiscriminative methods that train with negative data. We consider a recent\nhybrid method that optimizes the same shared representation according to\ncross-entropy of the discriminative predictions, and negative log likelihood of\nthe predicted energy-based density. We extend that work with a jointly trained\ngenerative flow that samples synthetic negatives at the border of the inlier\ndistribution. The proposed extension provides potential to learn the hybrid\nmethod without real negative data. Our experiments analyze the impact of\ntraining with synthetic negative data and validate contribution of the\nenergy-based density during training and evaluation.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:09:41 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15228","submitter":"Daniel Kelshaw","authors":"Daniel Kelshaw, Luca Magri","title":"Short and Straight: Geodesics on Differentiable Manifolds","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Manifolds discovered by machine learning models provide a compact\nrepresentation of the underlying data. Geodesics on these manifolds define\nlocally length-minimising curves and provide a notion of distance, which are\nkey for reduced-order modelling, statistical inference, and interpolation. In\nthis work, we first analyse existing methods for computing length-minimising\ngeodesics. We find that these are not suitable for obtaining valid paths, and\nthus, geodesic distances. We remedy these shortcomings by leveraging numerical\ntools from differential geometry, which provide the means to obtain\nHamiltonian-conserving geodesics. Second, we propose a model-based\nparameterisation for distance fields and geodesic flows on continuous\nmanifolds. Our approach exploits a manifold-aware extension to the Eikonal\nequation, eliminating the need for approximations or discretisation. Finally,\nwe develop a curvature-based training mechanism, sampling and scaling points in\nregions of the manifold exhibiting larger values of the Ricci scalar. This\nsampling and scaling approach ensures that we capture regions of the manifold\nsubject to higher degrees of geodesic deviation. Our proposed methods provide\nprincipled means to compute valid geodesics and geodesic distances on\nmanifolds. This work opens opportunities for latent-space interpolation,\noptimal control, and distance computation on differentiable manifolds.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:09:41 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15229","submitter":"Rhiannon Griffiths","authors":"Rhiannon Griffiths","title":"Higher Categories and Slices of Globular Operads","comments":"arXiv admin note: text overlap with arXiv:2101.00077","journal-ref":null,"doi":null,"report-no":null,"categories":"math.CT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In an unpublished preprint \\cite{batanin}, Batanin conjectures that it is\npossible to take `slices' of a globular operad, thereby isolating the algebraic\nstructure in each dimension. It was further hypothesised that the slices of a\nglobular operad for some theory of higher category contain essential\ninformation about those higher categories, namely whether or not they are\nequivalent to the fully weak variety. In this paper, we use the theory of\npresentations for globular operads developed in \\cite{Me} to provide a concrete\ndefinition of slices, and calculate the slices for several key theories of\n$n$-category.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:11:12 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15230","submitter":"Ron Folman","authors":"Carsten Henkel and Ron Folman","title":"Universal limit on spatial quantum superpositions with massive objects\n  due to phonons","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph gr-qc physics.atom-ph","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  The superposition principle is one of the founding principles of quantum\ntheory. Spatial quantum superpositions have so far been tested only with small\nsystems, from photons and elementary particles to atoms and molecules. Such\nsuperpositions for massive objects have been a long-standing sought-after goal.\nThis is important not only in order to confirm quantum theory in new regimes,\nbut also in order to probe the quantum-gravity interface. In addition, such an\nexperiment will enable to test exotic theories, and may even enable new\ntechnology. Creating such superpositions is notoriously hard because of\nenvironmental decoherence, whereby the large object couples strongly to the\nenvironment which turns the delicate quantum state into a statistical mixture\n(classical state). However, advances in the technology of isolation could in\nfuture suppress such decoherence. Here we present a decoherence channel which\nis not external but internal to the object, and consequently improved isolation\nwould not help. This channel originates from the phonons (sound waves) within\nthe object. We show that such phonons are excited as part of any splitting\nprocess, and thus we establish a fundamental and universal limit on the\npossibility of future spatial quantum superpositions with massive objects.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:13:05 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15231","submitter":"Madalina Erascu","authors":"Bogdan David and Madalina Erascu","title":"Benchmarking Optimization Solvers and Symmetry Breakers for the\n  Automated Deployment of Component-based Applications in the Cloud (EXTENDED\n  ABSTRACT)","comments":"Presented at 7th International Workshop on Satisfiability Checking\n  and Symbolic Computation (SC-square), Part of IJCAR 22, at FLOC 2022, August\n  12, 2022, Haifa, Israel. arXiv admin note: substantial text overlap with\n  arXiv:2006.05401","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LO","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Optimization solvers based on methods from constraint programming (OR-Tools,\nChuffed, Gecode), optimization modulo theory (Z3), and mathematical programming\n(CPLEX) are successfully applied nowadays to solve many non-trivial examples.\nHowever, for solving the problem of automated deployment in the Cloud of\ncomponent-based applications, their computational requirements are huge making\nautomatic optimization practically impossible with the current general\noptimization techniques. To overcome the difficulty, we exploited the sweet\nspots of the underlying problem in order to identify search space reduction\nmethods. We came up with 15 symmetry breaking strategies which we tested in a\nstatic symmetry breaking setting on the solvers enumerated above and on 4\nclasses of problems. As a result, all symmetry breaking strategies led to\nsignificant improvement of the computational time of all solvers, most notably,\nZ3 performed the best compared to the others. As an observation, the symmetry\nbreaking strategies confirmed that, when applied in a static setting, they may\ninteract badly with the underlying techniques implemented by the solvers.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:13:41 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15232","submitter":"Ernesto Contreras","authors":"A. Di Teodoro, E. Contreras","title":"A vacuum solution of modified Einstein equations based on fractional\n  calculus","comments":null,"journal-ref":"Eur. Phys. J. C (2023) 83:434","doi":"10.1140/epjc/s10052-023-11626-4","report-no":null,"categories":"gr-qc","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this work, we construct a modified version of the Einstein field equations\nfor a vacuum and spherically symmetric spacetime in terms of the\nRiemann-Louville fractional derivative. The main difference between our\napproach and other works is that we ensure that both the classical differential\nequations and the classical solutions are exactly recovered in the limit when\nthe fractional parameter is turned off. We assume that the fractional equations\nare valid inside and near the horizon radius and match the classical solution\nat the horizon. Our approach resembles the Herrera--Witten strategy shown in\nAdv.High Energy Phys. 2018 (2018) 3839103, where the authors constructed an\nalternative black hole solution by assuming that inside the horizon the\nspacetime is hyperbolically symmetric and matches the classical spherically\nsymmetric exterior solution at one point at the horizon. We obtain that,\ndepending on the value of the fractional parameter, the solutions can be\ninterpreted as a regular black hole or a gravatar. As a final step, we compute\nthe fractional curvature scalars and show that the solution is regular\neverywhere inside the horizon.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:13:59 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15233","submitter":"Sunkyoung Kim","authors":"Sunkyoung Kim, Dayeon Ki, Yireun Kim, Jinsik Lee","title":"Boosting Cross-lingual Transferability in Multilingual Models via\n  In-Context Learning","comments":"Work In Progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Existing cross-lingual transfer (CLT) prompting methods are only concerned\nwith monolingual demonstration examples in the source language. In this paper,\nwe propose In-CLT, a novel cross-lingual transfer prompting method that\nleverages both source and target languages to construct the demonstration\nexamples. We conduct comprehensive evaluations on multilingual benchmarks,\nfocusing on question answering tasks. Experiment results show that In-CLT\nprompt not only improves multilingual models' cross-lingual transferability,\nbut also demonstrates remarkable unseen language generalization ability. In-CLT\nprompting, in particular, improves model performance by 10 to 20\\% points on\naverage when compared to prior cross-lingual transfer approaches. We also\nobserve the surprising performance gain on the other multilingual benchmarks,\nespecially in reasoning tasks. Furthermore, we investigate the relationship\nbetween lexical similarity and pre-training corpora in terms of the\ncross-lingual transfer gap.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:14:49 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15234","submitter":"Natalia Vesselinova","authors":"Natalia Vassileva Vesselinova","title":"On the road to more accurate mobile cellular traffic predictions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.NI","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  The main contribution reported in the paper is a novel paradigm through which\nmobile cellular traffic forecasting is made substantially more accurate.\nSpecifically, by incorporating freely available road metrics we characterise\nthe data generation process and spatial dependencies. Therefore, this provides\na means for improving the forecasting estimates. We employ highway flow and\naverage speed variables together with a cellular network traffic metric in a\nlight learning structure to predict the short-term future load on a cell\ncovering a segment of a highway. This is in sharp contrast to prior art that\nmainly studies urban scenarios (with pedestrian and limited vehicular speeds)\nand develops machine learning approaches that use exclusively network metrics\nand meta information to make mid-term and long-term predictions. The learning\nstructure can be used at a cell or edge level, and can find application in both\nfederated and centralised learning.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:18:46 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15235","submitter":"Jiatu Li","authors":"Jiatu Li and Igor Carboni Oliveira","title":"Unprovability of Strong Complexity Lower Bounds in Bounded Arithmetic","comments":"full version of a conference paper to appear in STOC 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CC cs.LO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  While there has been progress in establishing the unprovability of complexity\nstatements in lower fragments of bounded arithmetic, understanding the limits\nof Je\\v{r}\\'abek's theory $APC_1$ (2007) and of higher levels of Buss's\nhierarchy $S^i_2$ (1986) has been a more elusive task. Even in the more\nrestricted setting of Cook's theory PV (1975), known results often rely on a\nless natural formalization that encodes a complexity statement using a\ncollection of sentences instead of a single sentence. This is done to reduce\nthe quantifier complexity of the resulting sentences so that standard\nwitnessing results can be invoked.\n  In this work, we establish unprovability results for stronger theories and\nfor sentences of higher quantifier complexity. In particular, we\nunconditionally show that $APC_1$ cannot prove strong complexity lower bounds\nseparating the third level of the polynomial hierarchy. In more detail, we\nconsider non-uniform average-case separations, and establish that $APC_1$\ncannot prove a sentence stating that $\\forall n \\ge n_0\\;\\exists\\,f_n \\in\n\\Pi_{3}$-$SIZE[n^d]$ that is $(1/n)$-far from every\n$\\Sigma_{3}$-$SIZE[2^{n^{\\delta}}]$ circuit. This is a consequence of a much\nmore general result showing that, for every $i \\geq 1$, strong separations for\n$\\Pi_{i}$-$SIZE[poly(n)]$ versus $\\Sigma_{i}$-$SIZE[2^{n^{\\Omega(1)}}]$ cannot\nbe proved in the theory $T_{PV}^i$ consisting of all true $\\forall\n\\Sigma^b_{i-1}$-sentences in the language of Cook's theory PV.\n  Our argument employs a convenient game-theoretic witnessing result that can\nbe applied to sentences of arbitrary quantifier complexity. We combine it with\nextensions of a technique introduced by Kraj\\'i\\v{c}ek (2011) that was recently\nemployed by Pich and Santhanam (2021) to establish the unprovability of lower\nbounds in PV (i.e., the case $i=1$ above, but under a weaker formalization) and\nin a fragment of $APC_1$.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:19:44 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15236","submitter":"Ettore Vicari","authors":"Claudio Bonati, Andrea Pelissetto, Ettore Vicari","title":"The Coulomb-Higgs phase transition of three-dimensional lattice\n  Abelian-Higgs gauge models with noncompact gauge variables and gauge fixing","comments":"14 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.stat-mech hep-lat","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We study the critical behavior of three-dimensional (3D) lattice\nAbelian-Higgs (AH) gauge models with noncompact gauge variables and\nmulticomponent complex scalar fields, along the transition line between the\nCoulomb and Higgs phases. Previous works that focused on gauge-invariant\ncorrelations provided evidence that, for a sufficiently large number of scalar\ncomponents, these transitions are continuous and associated with the stable\ncharged fixed point of the renormalization-group flow of the 3D AH field theory\n(scalar electrodynamics), in which charged scalar matter is minimally coupled\nwith an electromagnetic field. Here we extend these studies by considering\ngauge-dependent correlations of the gauge and matter fields, in the presence of\ntwo different gauge fixings, the Lorenz and the axial gauge fixing. Our results\nfor N=25 are definitely consistent with the predictions of the AH field theory\nand therefore provide additional evidence for the characterization of the 3D AH\ntransitions along the Coulomb-Higgs line as charged transitions in the AH\nfield-theory universality class. Moreover, our results give additional insights\non the role of the gauge fixing at charged transitions. In particular, we show\nthat scalar correlations are critical only if a hard Lorenz gauge fixing is\nimposed.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:19:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15237","submitter":"Ramy Taki Eldin F.","authors":"Ramy Taki Eldin and Patrick Sole","title":"Generator polynomial matrices of the Galois hulls of multi-twisted codes","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT math.IT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this study, we consider the Euclidean and Galois hulls of multi-twisted\n(MT) codes over a finite field $\\mathbb{F}_{p^e}$ of characteristic $p$. Let\n$\\mathbf{G}$ be a generator polynomial matrix (GPM) of a MT code $\\mathcal{C}$.\nFor any $0\\le \\kappa<e$, the $\\kappa$-Galois hull of $\\mathcal{C}$, denoted by\n$h_\\kappa\\left(\\mathcal{C}\\right)$, is the intersection of $\\mathcal{C}$ with\nits $\\kappa$-Galois dual. The main result in this paper is that a GPM for\n$h_\\kappa\\left(\\mathcal{C}\\right)$ has been obtained from $\\mathbf{G}$. We\nstart by associating a linear code $\\mathcal{Q}_\\mathbf{G}$ with $\\mathbf{G}$.\nWe show that $\\mathcal{Q}_\\mathbf{G}$ is quasi-cyclic. In addition, we prove\nthat the dimension of $h_\\kappa\\left(\\mathcal{C}\\right)$ is the difference\nbetween the dimension of $\\mathcal{C}$ and that of $\\mathcal{Q}_\\mathbf{G}$.\nThus the determinantal divisors are used to derive a formula for the dimension\nof $h_\\kappa\\left(\\mathcal{C}\\right)$. Finally, we deduce a GPM formula for\n$h_\\kappa\\left(\\mathcal{C}\\right)$. In particular, we handle the cases of\n$\\kappa$-Galois self-orthogonal and linear complementary dual MT codes; we\nestablish equivalent conditions that characterize these cases. Equivalent\nresults can be deduced immediately for the classes of cyclic, constacyclic,\nquasi-cyclic, generalized quasi-cyclic, and quasi-twisted codes, because they\nare all special cases of MT codes. Some numerical examples, containing optimal\nand maximum distance separable codes, are used to illustrate the theoretical\nresults.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:20:45 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15238","submitter":"Alessandra Fumagalli","authors":"Alessandra Fumagalli, Yodovina Pi\\v{s}kur, An\\v{z}e Slosar","title":"Superhorizon isocurvature fluctuations relax tensions","comments":"14 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"astro-ph.CO","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  We present a new class of models that have potential to alleviate tensions\npresent in the cosmological data today. We postulate variation in the sound\nhorizon scale on super-horizon scales, i.e. on scales that are larger than that\nof the present observable low-redshift universe ($\\gtrsim 1\\,$Gpc) while at the\nsame time smaller than the largest scales probed by the cosmic microwave\nbackground (CMB) ($\\lesssim10\\,$Gpc). In this scenario, CMB peaks are naturally\nsmoothed as preferred by the Planck data, while at the same time the\nlow-redshift baryon acoustic oscillation calibration is partially decoupled\nfrom the CMB. Taking super-horizon variations in baryon fraction as an example\nand using approximate modeling, we find improvement in the best fit Planck\npower spectrum model $\\Delta \\chi^2 \\sim 6$ for one extra degree of freedom\nwith the relevant extension parameter $10^3 \\sigma_b = 2.12 \\pm 0.50 $,\nimplying about $10\\%$ variations in baryon fraction across the universe. At the\nsame time, $S_8$ drops by about 1 sigma, easing tension with weak lensing\nsurveys. While $H_0$ increases in this model by about 1 sigma, this is\ninsufficient to explain the Hubble tension in $\\Lambda$CDM. Since the power of\nlow redshift BAO is relaxed, we find that the combination of Planck 2018 data,\neBOSS BAO data and Riess et al distance ladder Hubble parameter determination\nproduces a satisfactory fit in the model with free dark energy equation of\nstate. Such a fit, however, favors a phantom dark energy equation of state\n$w<-1$ at 2-3 sigma.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:22:09 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15239","submitter":"Travis LaCroix","authors":"Travis LaCroix and Simon J. D. Prince","title":"Ethics and Deep Learning","comments":"Copyright in this Work has been licensed exclusively to The MIT\n  Press, https://mitpress.mit.edu, which will be releasing the final version to\n  the public in 2023. All inquiries regarding rights should be addressed to The\n  MIT Press, Rights and Permissions Department","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CY cs.LG","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  This article appears as chapter 21 of Prince (2023, Understanding Deep\nLearning); a complete draft of the textbook is available here:\nhttp://udlbook.com. This chapter considers potential harms arising from the\ndesign and use of AI systems. These include algorithmic bias, lack of\nexplainability, data privacy violations, militarization, fraud, and\nenvironmental concerns. The aim is not to provide advice on being more ethical.\nInstead, the goal is to express ideas and start conversations in key areas that\nhave received attention in philosophy, political science, and the broader\nsocial sciences.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:24:19 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15240","submitter":"Tomas Lazna","authors":"Tomas Lazna, Ludek Zalud","title":"Localizing Multiple Radiation Sources Actively with a Particle Filter","comments":"9 pages, 2 tables, 3 figures; submitted to IEEE RA-L","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The article discusses the localization of radiation sources whose number and\nother relevant parameters are not known in advance. The data collection is\nensured by an autonomous mobile robot that performs a survey in a defined\nregion of interest populated with static obstacles. The measurement trajectory\nis information-driven rather than pre-planned. The localization exploits a\nregularized particle filter estimating the sources' parameters continuously.\nThe dynamic robot control switches between two modes, one attempting to\nminimize the Shannon entropy and the other aiming to reduce the variance of\nexpected measurements in unexplored parts of the target area; both of the modes\nmaintain safe clearance from the obstacles. The performance of the algorithms\nwas tested in a simulation study based on real-world data acquired previously\nfrom three radiation sources exhibiting various activities. Our approach\nreduces the time necessary to explore the region and to find the sources by\napproximately 40 %; at present, however, the method is unable to reliably\nlocalize sources that have a relatively low intensity. In this context,\nadditional research has been planned to increase the credibility and robustness\nof the procedure and to improve the robotic platform autonomy.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:24:42 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15241","submitter":"Huanran Chen","authors":"Huanran Chen, Yinpeng Dong, Zhengyi Wang, Xiao Yang, Chengqi Duan,\n  Hang Su, Jun Zhu","title":"Robust Classification via a Single Diffusion Model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.CR cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Recently, diffusion models have been successfully applied to improving\nadversarial robustness of image classifiers by purifying the adversarial noises\nor generating realistic data for adversarial training. However, the\ndiffusion-based purification can be evaded by stronger adaptive attacks while\nadversarial training does not perform well under unseen threats, exhibiting\ninevitable limitations of these methods. To better harness the expressive power\nof diffusion models, in this paper we propose Robust Diffusion Classifier\n(RDC), a generative classifier that is constructed from a pre-trained diffusion\nmodel to be adversarially robust. Our method first maximizes the data\nlikelihood of a given input and then predicts the class probabilities of the\noptimized input using the conditional likelihood of the diffusion model through\nBayes' theorem. Since our method does not require training on particular\nadversarial attacks, we demonstrate that it is more generalizable to defend\nagainst multiple unseen threats. In particular, RDC achieves $73.24\\%$ robust\naccuracy against $\\ell_\\infty$ norm-bounded perturbations with\n$\\epsilon_\\infty=8/255$ on CIFAR-10, surpassing the previous state-of-the-art\nadversarial training models by $+2.34\\%$. The findings highlight the potential\nof generative classifiers by employing diffusion models for adversarial\nrobustness compared with the commonly studied discriminative classifiers.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:25:19 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15242","submitter":"Luciano Floridi","authors":"Luciano Floridi","title":"Machine Unlearning: its nature, scope, and importance for a \"delete\n  culture\"","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CY cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The article explores the cultural shift from recording to deleting\ninformation in the digital age and its implications on privacy, intellectual\nproperty (IP), and Large Language Models like ChatGPT. It begins by defining a\ndelete culture where information, in principle legal, is made unavailable or\ninaccessible because unacceptable or undesirable, especially but not only due\nto its potential to infringe on privacy or IP. Then it focuses on two\nstrategies in this context: deleting, to make information unavailable; and\nblocking, to make it inaccessible. The article argues that both strategies have\nsignificant implications, particularly for machine learning (ML) models where\ninformation is not easily made unavailable. However, the emerging research area\nof Machine Unlearning (MU) is highlighted as a potential solution. MU, still in\nits infancy, seeks to remove specific data points from ML models, effectively\nmaking them 'forget' completely specific information. If successful, MU could\nprovide a feasible means to manage the overabundance of information and ensure\na better protection of privacy and IP. However, potential ethical risks, such\nas misuse, overuse, and underuse of MU, should be systematically studied to\ndevise appropriate policies.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:27:04 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15243","submitter":"Jacob B Khurgin","authors":"Jacob B Khurgin","title":"Photonic Time Crystals and Parametric Amplification: similarity and\n  distinction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.optics","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Photonic Time crystals (PTC) arise in time-modulated media when the frequency\nof modulation of permittivity is on the order of twice the frequency of light\nand are manifested by the generation and amplification of so-called time\nreversed waves propagating in the direction opposite to the incoming light.\nSuperficially, the observed phenomenon bears resemblance to the widely known\nphenomena of optical parametric generation (OPG) and amplification (OPA) using\nsecond or third order optical nonlinearities. I show that while indeed the same\nphysical mechanism underpins both PTC and OPA , the difference arises from the\nboundary conditions. Thus , while dispersion for both PTC and OPA exhibit the\nsame bandgap in momentum space, only in the case of PTC can one have\npropagation in that bandgap with exponential amplification. I also show that\nPTC can be engineered with both second and third order nonlinearities, and that\nrather unexpectedly, modulating permittivity on the ultrafast (few fs) rate is\nnot a necessity, and that one can emulate all the PTC features using materials\nwith a few picoseconds response time commensurate with the propagation time\nthrough the medium.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:29:29 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15244","submitter":"Daniel Layeghi","authors":"Daniel Layeghi, Steve Tonneau, Michael Mistry","title":"Neural Lyapunov and Optimal Control","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Optimal control (OC) is an effective approach to controlling complex\ndynamical systems. However, traditional approaches to parameterising and\nlearning controllers in optimal control have been ad-hoc, collecting data and\nfitting it to neural networks. However, this can lead to learnt controllers\nignoring constraints like optimality and time variability. We introduce a\nunified framework that simultaneously solves control problems while learning\ncorresponding Lyapunov or value functions. Our method formulates OC-like\nmathematical programs based on the Hamilton-Jacobi-Bellman (HJB) equation. We\nleverage the HJB optimality constraint and its relaxation to learn time-varying\nvalue and Lyapunov functions, implicitly ensuring the inclusion of constraints.\nWe show the effectiveness of our approach on linear and nonlinear\ncontrol-affine problems. Additionally, we demonstrate significant reductions in\nplanning horizons (up to a factor of 25) when incorporating the learnt\nfunctions into Model Predictive Controllers.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:29:59 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15245","submitter":"Fu Xing Long","authors":"Fu Xing Long, Diederick Vermetten, Anna V. Kononova, Roman Kalkreuth,\n  Kaifeng Yang, Thomas B\\\"ack, Niki van Stein","title":"Challenges of ELA-guided Function Evolution using Genetic Programming","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.NE","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Within the optimization community, the question of how to generate new\noptimization problems has been gaining traction in recent years. Within topics\nsuch as instance space analysis (ISA), the generation of new problems can\nprovide new benchmarks which are not yet explored in existing research. Beyond\nthat, this function generation can also be exploited for solving complex\nreal-world optimization problems. By generating functions with similar\nproperties to the target problem, we can create a robust test set for algorithm\nselection and configuration.\n  However, the generation of functions with specific target properties remains\nchallenging. While features exist to capture low-level landscape properties,\nthey might not always capture the intended high-level features. We show that a\ngenetic programming (GP) approach guided by these exploratory landscape\nanalysis (ELA) properties is not always able to find satisfying functions. Our\nresults suggest that careful considerations of the weighting of landscape\nproperties, as well as the distance measure used, might be required to evolve\nfunctions that are sufficiently representative to the target landscape.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:31:01 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15246","submitter":"B\\'alint Vet\\H{o}","authors":"B\\'alint Vet\\H{o} and B\\'alint Vir\\'ag","title":"The geometry of coalescing random walks, the Brownian web distance and\n  KPZ universality","comments":"33 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Coalescing simple random walks in the plane form an infinite tree. A natural\ndirected distance on this tree is given by the number of jumps between branches\nwhen one is only allowed to move in one direction. The Brownian web distance is\nthe scale-invariant limit of this directed metric. It is integer-valued and has\nscaling exponents 0:1:2 as compared to 1:2:3 in the KPZ world. However, we show\nthat the shear limit of the Brownian web distance is still given by the Airy\nprocess. We conjecture that our limit theorem can be extended to the full\ndirected landscape.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:31:18 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15247","submitter":"Anna Nickolaevna Morozovska","authors":"Anna N. Morozovska, Eugene A. Eliseev, Yongtao Liu, Kyle P. Kelley,\n  Ayana Ghosh, Ying Liu, Jinyuan Yao, Nicholas V. Morozovsky, Andrei L Kholkin,\n  Yulian M. Vysochanskii, and Sergei V. Kalinin","title":"Bending-induced isostructural transitions in ultrathin layers of van der\n  Waals ferrielectrics","comments":"26 pages, 7 figures and Appendices A-C","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci cond-mat.mes-hall","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Using Landau-Ginzburg-Devonshire (LGD) phenomenological approach we analyze\nthe bending-induced re-distribution of electric polarization and field, elastic\nstresses and strains inside ultrathin layers of van der Waals ferrielectrics.\nWe consider a CuInP2S6 (CIPS) thin layer with fixed edges and suspended central\npart, the bending of which is induced by external forces. The unique aspect of\nCIPS is the existence of two ferrielectric states, FI1 and FI2, corresponding\nto big and small polarization values, which arise due to the specific four-well\npotential of the eighth-order LGD functional. When the CIPS layer is flat, the\nsingle-domain FI1 state is stable in the central part of the layer, and the FI2\nstates are stable near the fixed edges. With an increase of the layer bending\nbelow the critical value, the sizes of the FI2 states near the fixed edges\ndecreases, and the size of the FI1 region increases. When the bending exceeds\nthe critical value, the edge FI2 states disappear being substituted by the FI1\nstate, but they appear abruptly near the inflection regions and expand as the\nbending increases. The bending-induced isostructural FI1-FI2 transition is\nspecific for the bended van der Waals ferrielectrics described by the eighth\n(or higher) order LGD functional with consideration of linear and nonlinear\nelectrostriction couplings. The isostructural transition, which is revealed in\nthe vicinity of room temperature, can significantly reduce the coercive voltage\nof ferroelectric polarization reversal in CIPS nanoflakes, allowing for the\ncurvature-engineering control of various flexible nanodevices.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:31:36 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15248","submitter":"Cheng-Ze Lu","authors":"Cheng-Ze Lu, Xiaojie Jin, Qibin Hou, Jun Hao Liew, Ming-Ming Cheng,\n  Jiashi Feng","title":"Delving Deeper into Data Scaling in Masked Image Modeling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Understanding whether self-supervised learning methods can scale with\nunlimited data is crucial for training large-scale models. In this work, we\nconduct an empirical study on the scaling capability of masked image modeling\n(MIM) methods (e.g., MAE) for visual recognition. Unlike most previous works\nthat depend on the widely-used ImageNet dataset, which is manually curated and\nobject-centric, we take a step further and propose to investigate this problem\nin a more practical setting. Specifically, we utilize the web-collected\nCoyo-700M dataset. We randomly sample varying numbers of training images from\nthe Coyo dataset and construct a series of sub-datasets, containing 0.5M, 1M,\n5M, 10M, and 100M images, for pre-training. Our goal is to investigate how the\nperformance changes on downstream tasks when scaling with different sizes of\ndata and models. The study reveals that: 1) MIM can be viewed as an effective\nmethod to improve the model capacity when the scale of the training data is\nrelatively small; 2) Strong reconstruction targets can endow the models with\nincreased capacities on downstream tasks; 3) MIM pre-training is data-agnostic\nunder most scenarios, which means that the strategy of sampling pre-training\ndata is non-critical. We hope these observations could provide valuable\ninsights for future research on MIM.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:33:46 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15249","submitter":"Sharan Vaswani","authors":"Sharan Vaswani, Amirreza Kazemi, Reza Babanezhad, Nicolas Le Roux","title":"Decision-Aware Actor-Critic with Function Approximation and Theoretical\n  Guarantees","comments":"44 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI math.OC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Actor-critic (AC) methods are widely used in reinforcement learning (RL) and\nbenefit from the flexibility of using any policy gradient method as the actor\nand value-based method as the critic. The critic is usually trained by\nminimizing the TD error, an objective that is potentially decorrelated with the\ntrue goal of achieving a high reward with the actor. We address this mismatch\nby designing a joint objective for training the actor and critic in a\ndecision-aware fashion. We use the proposed objective to design a generic, AC\nalgorithm that can easily handle any function approximation. We explicitly\ncharacterize the conditions under which the resulting algorithm guarantees\nmonotonic policy improvement, regardless of the choice of the policy and critic\nparameterization. Instantiating the generic algorithm results in an actor that\ninvolves maximizing a sequence of surrogate functions (similar to TRPO, PPO)\nand a critic that involves minimizing a closely connected objective. Using\nsimple bandit examples, we provably establish the benefit of the proposed\ncritic objective over the standard squared error. Finally, we empirically\ndemonstrate the benefit of our decision-aware actor-critic framework on simple\nRL problems.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:34:21 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15250","submitter":"Alberto Giacomello Ph. D.","authors":"Sonia Cambiaso, Fabio Rasera, Antonio Tinti, Davide Bochicchio,\n  Yaroslav Grosu, Giulia Rossi, Alberto Giacomello","title":"Grafting heterogeneities rule water intrusion and extrusion in nanopores","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.soft","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Hydrophobic nanoporous materials can be intruded by water only by exerting an\nexternal action, typically increasing pressure. For some materials, water\nextrudes when the pressure is lowered again. Controlling intrusion/extrusion\nhysteresis is central in a number of technological applications, including\nmaterials for energy applications and for high performance liquid\nchromatography, and experimental techniques, as liquid porosimetry, but is\nstill far from being understood. In this work, we consider water intrusion and\nextrusion in common mesoporous materials grafted with hydrophobic chains,\nshowing that the macroscopic properties of the system are significantly\naffected by subnanometric heterogeneities in the grafting. For example,\nintrusion and extrusion pressures can vary more than 20 MPa depending on the\nchain length and density of the grafting. Coarse-grained molecular dynamics\nsimulations reveal that local changes of radius and contact angle produced by\ngrafting heterogeneities can pin the interface during intrusion or facilitate\nbubble nucleation in extrusion. These unprecedented microscopic insights can\ndirectly impact the design of energy materials and chromatography columns, as\nwell as the interpretation of porosimetry results.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:34:35 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15251","submitter":"Taiping Zhang","authors":"Taiping Zhang","title":"Plasmonic-Photonic Hybrid Nanodevice","comments":"Doctoral thesis","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.optics physics.app-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this thesis, we propose to tackle this important issue by designing and\nrealizing a novel nano-optical device based on the use of a photonic crystal\n(PC) structure to generate an efficient coupling between the external source\nand a NA. In this dissertation, the content is arranged into three charpters.\nChapter 1 introduces the theoritical background of this research including\nsurface plasmon and photonic crystal concepts. This chapter also shows the\ndesign of the hybrid devices and demonstrates the numerical simulation of their\noptical properties. Chapter 2 mainly describes the process and the fabricated\nsamples. The nanodevices are fabricated on an InP membrane substrate. The\ncritical technology for the fabrication is complex electron beam lithography.\nWith this technology the alignment of the positions of PC structure and NA is\nwell controlled. Chapter 3 demonstrates the optical characterizations of the\nhybrid nanodevices including far-field characterizations and near-field\ncharacterizations. The far-field measurement is performed by\nmicro-photoluminescence spectroscopy at room temperature. The results show that\nfor the defect PC cavities, the presence of the NA influences the optical\nproperties of the laser, such as lasing threshold and laser wavelength. The\nnear-field measurement is performed by near-field scanning microscopy, at room\ntemperature also. The investigation shows that the NA modifies the optical\nfield distribution of the laser mode. The modification depends on the position\nand direction of the NA and it is sensitive to the polarization of the optical\nfield.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:36:00 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15252","submitter":"Sadaf Ul Zuhra Dr","authors":"Sadaf Ul Zuhra, Prasanna Chaporkar, Abhay Karandikar, H. Vincent Poor","title":"Multi-Connectivity for Multicast Video Streaming in Cellular Networks\n  (Extended Abstract)","comments":"arXiv admin note: substantial text overlap with arXiv:2202.05053","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.NI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In video streaming applications especially during live streaming events (such\nas the Super Bowl), video traffic can account for a significant portion of\nnetwork traffic and can lead to severe network congestion. During such events,\nmulticast transmission can be used to avoid network congestion since the same\nvideo content is being streamed to multiple users simultaneously. However,\nproviding seamless connectivity to cellular users in multicast streaming\nremains an open problem. To address this issue, this paper explores the\npotential of using multi-connectivity (MC) in wireless multicast streaming. Our\nresults reveal that MC significantly improves the performance of multicast\nservices, especially for cell edge users who often suffer from poor channel\nconditions. We prove that optimal resource allocation in MC multicast streaming\nis an NP-hard problem. Therefore, we propose a greedy approximation algorithm\nfor this problem with an approximation factor of $(1-1/e)$. We also prove that\nno other polynomial-time algorithm can provide a better approximation.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:36:21 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15253","submitter":"Han Yu","authors":"Han Yu, Xingxuan Zhang, Renzhe Xu, Jiashuo Liu, Yue He, Peng Cui","title":"Rethinking the Evaluation Protocol of Domain Generalization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Domain generalization aims to solve the challenge of Out-of-Distribution\n(OOD) generalization by leveraging common knowledge learned from multiple\ntraining domains to generalize to unseen test domains. To accurately evaluate\nthe OOD generalization ability, it is necessary to ensure that test data\ninformation is unavailable. However, the current domain generalization protocol\nmay still have potential test data information leakage. This paper examines the\npotential risks of test data information leakage in two aspects of the current\nprotocol: pretraining on ImageNet and oracle model selection. We propose that\ntraining from scratch and using multiple test domains would result in a more\nprecise evaluation of OOD generalization ability. We also rerun the algorithms\nwith the modified protocol and introduce a new leaderboard to encourage future\nresearch in domain generalization with a fairer comparison.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:36:46 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15254","submitter":"Benno K\\\"ach","authors":"Benno K\\\"ach, Isabell Melzer-Pellmann","title":"Attention to Mean-Fields for Particle Cloud Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"hep-ex cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The generation of collider data using machine learning has emerged as a\nprominent research topic in particle physics due to the increasing\ncomputational challenges associated with traditional Monte Carlo simulation\nmethods, particularly for future colliders with higher luminosity. Although\ngenerating particle clouds is analogous to generating point clouds, accurately\nmodelling the complex correlations between the particles presents a\nconsiderable challenge. Additionally, variable particle cloud sizes further\nexacerbate these difficulties, necessitating more sophisticated models. In this\nwork, we propose a novel model that utilizes an attention-based aggregation\nmechanism to address these challenges. The model is trained in an adversarial\ntraining paradigm, ensuring that both the generator and critic exhibit\npermutation equivariance/invariance with respect to their input. A novel\nfeature matching loss in the critic is introduced to stabilize the training.\nThe proposed model performs competitively to the state-of-art whilst having\nsignificantly fewer parameters.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:38:43 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15255","submitter":"Eliya Nachmani","authors":"Eliya Nachmani, Alon Levkovitch, Julian Salazar, Chulayuth\n  Asawaroengchai, Soroosh Mariooryad, RJ Skerry-Ryan, Michelle Tadmor\n  Ramanovich","title":"LMs with a Voice: Spoken Language Modeling beyond Speech Tokens","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG cs.SD eess.AS","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We present SPECTRON, a novel approach to adapting pre-trained language models\n(LMs) to perform speech continuation. By leveraging pre-trained speech\nencoders, our model generates both text and speech outputs with the entire\nsystem being trained end-to-end operating directly on spectrograms. Training\nthe entire model in the spectrogram domain simplifies our speech continuation\nsystem versus existing cascade methods which use discrete speech\nrepresentations. We further show our method surpasses existing spoken language\nmodels both in semantic content and speaker preservation while also benefiting\nfrom the knowledge transferred from pre-existing models. Audio samples can be\nfound in our website https://michelleramanovich.github.io/spectron/spectron\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:39:43 GMT"},{"version":"v2","created":"Thu, 1 Jun 2023 08:04:19 GMT"}],"update_date":"2023-06-02"}
{"id":"2305.15256","submitter":"Munyque Mittelmann","authors":"Munyque Mittelmann, Aniello Murano, Laurent Perrussel","title":"Discounting in Strategy Logic","comments":"Extended version of the paper accepted at IJCAI 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Discounting is an important dimension in multi-agent systems as long as we\nwant to reason about strategies and time. It is a key aspect in economics as it\ncaptures the intuition that the far-away future is not as important as the near\nfuture. Traditional verification techniques allow to check whether there is a\nwinning strategy for a group of agents but they do not take into account the\nfact that satisfying a goal sooner is different from satisfying it after a long\nwait. In this paper, we augment Strategy Logic with future discounting over a\nset of discounted functions D, denoted SLdisc[D]. We consider \"until\" operators\nwith discounting functions: the satisfaction value of a specification in\nSLdisc[D] is a value in [0, 1], where the longer it takes to fulfill\nrequirements, the smaller the satisfaction value is. We motivate our approach\nwith classical examples from Game Theory and study the complexity of\nmodel-checking SLdisc[D]-formulas.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:40:53 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15257","submitter":"Min Liu","authors":"Min Liu, Zhiqiang Cai and Karthik Ramani","title":"Deep Ritz Method with Adaptive Quadrature for Linear Elasticity","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA math-ph math.MP","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  In this paper, we study the deep Ritz method for solving the linear\nelasticity equation from a numerical analysis perspective. A modified Ritz\nformulation using the $H^{1/2}(\\Gamma_D)$ norm is introduced and analyzed for\nlinear elasticity equation in order to deal with the (essential) Dirichlet\nboundary condition. We show that the resulting deep Ritz method provides the\nbest approximation among the set of deep neural network (DNN) functions with\nrespect to the ``energy'' norm. Furthermore, we demonstrate that the total\nerror of the deep Ritz simulation is bounded by the sum of the network\napproximation error and the numerical integration error, disregarding the\nalgebraic error. To effectively control the numerical integration error, we\npropose an adaptive quadrature-based numerical integration technique with a\nresidual-based local error indicator. This approach enables efficient\napproximation of the modified energy functional. Through numerical experiments\ninvolving smooth and singular problems, as well as problems with stress\nconcentration, we validate the effectiveness and efficiency of the proposed\ndeep Ritz method with adaptive quadrature.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:41:47 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15258","submitter":"Ningchen Bai","authors":"Ning-Chen Bai, Lei Li and Jun Tao","title":"Superfluid $\\lambda$ Transition in Charged AdS Black Holes","comments":"8 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-th gr-qc","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this work, we reconsider $P-V$ criticality of charged AdS black holes in a\nholographic extended thermodynamics that considers the variation of Newton's\nconstant $G$. Interestingly, a superfluid-like $\\lambda$ phase transition is\nobserved. We calculate the critical exponents and find that they coincide with\nthose of a superfluid transition in liquid $^4\\text{He}$ and the Bose-Einstein\ncondensation of hard-sphere Bose gas. Moreover, the independence of entropy and\nthermodynamic volume in the holographic framework allows us to construct a\nwell-defined Ruppeiner metric. The associated scalar curvature suggests that\nthe black holes show similar microscopic interactions with the hard-sphere Bose\ngas, where the superfluid (condensed) phase is dominated by repulsive\ninteractions, while the normal (gas) phase is dominated by attractive\ninteractions. These findings might provide us with new insights into the\nquantum aspect of charged AdS black holes.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:42:13 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15259","submitter":"Marcel Moosbrugger","authors":"Marcel Moosbrugger, Julian M\\\"ullner, Laura Kov\\'acs","title":"Automated Sensitivity Analysis for Probabilistic Loops","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.PL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We present an exact approach to analyze and quantify the sensitivity of\nhigher moments of probabilistic loops with symbolic parameters, polynomial\narithmetic and potentially uncountable state spaces. Our approach integrates\nmethods from symbolic computation, probability theory, and static analysis in\norder to automatically capture sensitivity information about probabilistic\nloops. Sensitivity information allows us to formally establish how value\ndistributions of probabilistic loop variables influence the functional behavior\nof loops, which can in particular be helpful when choosing values of loop\nvariables in order to ensure efficient/expected computations. Our work uses\nalgebraic techniques to model higher moments of loop variables via linear\nrecurrence equations and introduce the notion of sensitivity recurrences. We\nshow that sensitivity recurrences precisely model loop sensitivities, even in\ncases where the moments of loop variables do not satisfy a system of linear\nrecurrences. As such, we enlarge the class of probabilistic loops for which\nsensitivity analysis was so far feasible. We demonstrate the success of our\napproach while analyzing the sensitivities of probabilistic loops.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:43:29 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15260","submitter":"Qi Wang","authors":"Qi Wang, Junming Yang, Yunbo Wang, Xin Jin, Wenjun Zeng, Xiaokang Yang","title":"Collaborative World Models: An Online-Offline Transfer RL Approach","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Training visual reinforcement learning (RL) models in offline datasets is\nchallenging due to overfitting issues in representation learning and\noverestimation problems in value function. In this paper, we propose a transfer\nlearning method called Collaborative World Models (CoWorld) to improve the\nperformance of visual RL under offline conditions. The core idea is to use an\neasy-to-interact, off-the-shelf simulator to train an auxiliary RL model as the\nonline \"test bed\" for the offline policy learned in the target domain, which\nprovides a flexible constraint for the value function -- Intuitively, we want\nto mitigate the overestimation problem of value functions outside the offline\ndata distribution without impeding the exploration of actions with potential\nadvantages. Specifically, CoWorld performs domain-collaborative representation\nlearning to bridge the gap between online and offline hidden state\ndistributions. Furthermore, it performs domain-collaborative behavior learning\nthat enables the source RL agent to provide target-aware value estimation,\nallowing for effective offline policy regularization. Experiments show that\nCoWorld significantly outperforms existing methods in offline visual control\ntasks in DeepMind Control and Meta-World.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:45:35 GMT"},{"version":"v2","created":"Thu, 25 May 2023 05:31:38 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.15261","submitter":"Diana Carbajal","authors":"Jorge Antezana, Diana Carbajal and Jos\\'e Luis Romero","title":"Random periodic sampling patterns for shift-invariant spaces","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.FA cs.IT math.CA math.IT","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We consider multi-variate signals spanned by the integer shifts of a set of\ngenerating functions with distinct frequency profiles and the problem of\nreconstructing them from samples taken on a random periodic set. We show that\nsuch a sampling strategy succeeds with high probability provided that the\ndensity of the sampling pattern exceeds the number of frequency profiles by a\nlogarithmic factor.\n  The signal model includes bandlimited functions with multi-band spectra.\nWhile in this well-studied setting delicate constructions provide sampling\nstrategies that meet the information theoretic benchmark of Shannon and Landau,\nthe sampling pattern that we consider provides, at the price of a logarithmic\noversampling factor, a simple alternative that is accompanied by favorable a\npriori stability margins (snug frames). More generally, we also treat\nbandlimited functions with arbitrary compact spectra, and different measures of\nits complexity and approximation rates by integer tiles.\n  At the technical level, we elaborate on recent work on relevant sampling,\nwith the key difference that the reconstruction guarantees that we provide hold\nuniformly for all signals, rather than for a subset of well-concentrated ones.\nThis is achieved by methods of concentration of measure formulated on the Zak\ndomain.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:47:34 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15262","submitter":"Kejuan Yang","authors":"Kejuan Yang, Xiao Liu, Kaiwen Men, Aohan Zeng, Yuxiao Dong, Jie Tang","title":"Revisiting Parallel Context Windows: A Frustratingly Simple Alternative\n  and Chain-of-Thought Deterioration","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We identify two crucial limitations in the evaluation of recent\nparallel-integrated method Parallel Context Windows (PCW), which extends the\nmaximum context lengths of language models, e.g., 2048 for LLaMA, by harnessing\nwindow-wise attention and positional embedding techniques. We first show that a\nsimple yet strong baseline, weighted sum ensemble, is missing for the\nin-context few-shot classification. Moreover, on more challenging\nChain-of-Thought (CoT) reasoning (e.g., HotpotQA), PCW would present unexpected\ndeterioration regarding question miscomprehension and false inference. Based on\nour findings, we suggest that the existing PCW design may not guarantee\nsufficient improvement and practicality in handling lengthy documents in\nreal-world applications. More community efforts on enabling language models'\nlong context understanding ability should be paid.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:48:29 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15263","submitter":"Michael Hahsler","authors":"Michael Hahsler","title":"ARULESPY: Exploring Association Rules and Frequent Itemsets in Python","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DB","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  The R arules package implements a comprehensive infrastructure for\nrepresenting, manipulating, and analyzing transaction data and patterns using\nfrequent itemsets and association rules. The package also provides a wide range\nof interest measures and mining algorithms, including the code of Christian\nBorgelt's popular and efficient C implementations of the association mining\nalgorithms Apriori and Eclat, and optimized C/C++ code for mining and\nmanipulating association rules using sparse matrix representation. This\ndocument describes the new Python package arulespy, which makes this\ninfrastructure available for Python users.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:52:01 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15264","submitter":"Elnur Gasanov","authors":"Peter Richt\\'arik, Elnur Gasanov, Konstantin Burlachenko","title":"Error Feedback Shines when Features are Rare","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC cs.DC cs.LG stat.ML","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We provide the first proof that gradient descent $\\left({\\color{green}\\sf\nGD}\\right)$ with greedy sparsification $\\left({\\color{green}\\sf TopK}\\right)$\nand error feedback $\\left({\\color{green}\\sf EF}\\right)$ can obtain better\ncommunication complexity than vanilla ${\\color{green}\\sf GD}$ when solving the\ndistributed optimization problem $\\min_{x\\in \\mathbb{R}^d}\n{f(x)=\\frac{1}{n}\\sum_{i=1}^n f_i(x)}$, where $n$ = # of clients, $d$ = # of\nfeatures, and $f_1,\\dots,f_n$ are smooth nonconvex functions. Despite intensive\nresearch since 2014 when ${\\color{green}\\sf EF}$ was first proposed by Seide et\nal., this problem remained open until now. We show that ${\\color{green}\\sf EF}$\nshines in the regime when features are rare, i.e., when each feature is present\nin the data owned by a small number of clients only. To illustrate our main\nresult, we show that in order to find a random vector $\\hat{x}$ such that\n$\\lVert {\\nabla f(\\hat{x})} \\rVert^2 \\leq \\varepsilon$ in expectation,\n${\\color{green}\\sf GD}$ with the ${\\color{green}\\sf Top1}$ sparsifier and\n${\\color{green}\\sf EF}$ requires ${\\cal O} \\left(\\left( L+{\\color{blue}r}\n\\sqrt{ \\frac{{\\color{red}c}}{n} \\min \\left( \\frac{{\\color{red}c}}{n} \\max_i\nL_i^2, \\frac{1}{n}\\sum_{i=1}^n L_i^2 \\right) }\\right) \\frac{1}{\\varepsilon}\n\\right)$ bits to be communicated by each worker to the server only, where $L$\nis the smoothness constant of $f$, $L_i$ is the smoothness constant of $f_i$,\n${\\color{red}c}$ is the maximal number of clients owning any feature ($1\\leq\n{\\color{red}c} \\leq n$), and ${\\color{blue}r}$ is the maximal number of\nfeatures owned by any client ($1\\leq {\\color{blue}r} \\leq d$). Clearly, the\ncommunication complexity improves as ${\\color{red}c}$ decreases (i.e., as\nfeatures become more rare), and can be much better than the ${\\cal\nO}({\\color{blue}r} L \\frac{1}{\\varepsilon})$ communication complexity of\n${\\color{green}\\sf GD}$ in the same regime.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:52:07 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15265","submitter":"Zirui Liu","authors":"Zirui Liu, Guanchu Wang, Shaochen Zhong, Zhaozhuo Xu, Daochen Zha,\n  Ruixiang Tang, Zhimeng Jiang, Kaixiong Zhou, Vipin Chaudhary, Shuai Xu, Xia\n  Hu","title":"Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of\n  Language Model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  With the rapid growth in model size, fine-tuning the large pre-trained\nlanguage model has become increasingly difficult due to its extensive memory\nusage. Previous works usually focus on reducing the number of trainable\nparameters in the network. While the model parameters do contribute to memory\nusage, the primary memory bottleneck during training arises from storing\nfeature maps, also known as activations, as they are crucial for gradient\ncalculation. Notably, neural networks are usually trained using stochastic\ngradient descent. We argue that in stochastic optimization, models can handle\nnoisy gradients as long as the gradient estimator is unbiased with reasonable\nvariance. Following this motivation, we propose a new family of unbiased\nestimators called WTA-CRS, for matrix production with reduced variance, which\nonly requires storing the sub-sampled activations for calculating the gradient.\nOur work provides both theoretical and experimental evidence that, in the\ncontext of tuning transformers, our proposed estimators exhibit lower variance\ncompared to existing ones. By replacing the linear operation with our\napproximated one in transformers, we can achieve up to 2.7$\\times$ peak memory\nreduction with almost no accuracy drop and enables up to $6.4\\times$ larger\nbatch size. Under the same hardware, WTA-CRS enables better down-streaming task\nperformance by applying larger models and/or faster training speed with larger\nbatch sizes.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:52:08 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15266","submitter":"Eloi Moliner","authors":"Eloi Moliner, Vesa V\\\"alim\\\"aki","title":"Diffusion-Based Audio Inpainting","comments":"Submitted for publication to the Journal of Audio Engineering Society\n  on January 30th, 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.AS cs.SD","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Audio inpainting aims to reconstruct missing segments in corrupted\nrecordings. Previous methods produce plausible reconstructions when the gap\nlength is shorter than about 100\\;ms, but the quality decreases for longer\ngaps. This paper explores recent advancements in deep learning and,\nparticularly, diffusion models, for the task of audio inpainting. The proposed\nmethod uses an unconditionally trained generative model, which can be\nconditioned in a zero-shot fashion for audio inpainting, offering high\nflexibility to regenerate gaps of arbitrary length. An improved deep neural\nnetwork architecture based on the constant-Q transform, which allows the model\nto exploit pitch-equivariant symmetries in audio, is also presented. The\nperformance of the proposed algorithm is evaluated through objective and\nsubjective metrics for the task of reconstructing short to mid-sized gaps. The\nresults of a formal listening test show that the proposed method delivers a\ncomparable performance against state-of-the-art for short gaps, while retaining\na good audio quality and outperforming the baselines for the longest gap\nlengths tested, 150\\;ms and 200\\;ms. This work helps improve the restoration of\nsound recordings having fairly long local disturbances or dropouts, which must\nbe reconstructed.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:52:11 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15267","submitter":"Chen-Hao Chao","authors":"Chen-Hao Chao, Wei-Fang Sun, Yen-Chang Hsu, Zsolt Kira, Chun-Yi Lee","title":"Training Energy-Based Normalizing Flow with Score-Matching Objectives","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, we establish a connection between the parameterization of\nflow-based and energy-based generative models, and present a new flow-based\nmodeling approach called energy-based normalizing flow (EBFlow). We demonstrate\nthat by optimizing EBFlow with score-matching objectives, the computation of\nJacobian determinants for linear transformations can be entirely bypassed. This\nfeature enables the use of arbitrary linear layers in the construction of\nflow-based models without increasing the computational time complexity of each\ntraining iteration from $\\mathcal{O}(D^2L)$ to $\\mathcal{O}(D^3L)$ for an\n$L$-layered model that accepts $D$-dimensional inputs. This makes the training\nof EBFlow more efficient than the commonly-adopted maximum likelihood training\nmethod. In addition to the reduction in runtime, we enhance the training\nstability and empirical performance of EBFlow through a number of techniques\ndeveloped based on our analysis on the score-matching methods. The experimental\nresults demonstrate that our approach achieves a significant speedup compared\nto maximum likelihood estimation, while outperforming prior efficient training\ntechniques with a noticeable margin in terms of negative log-likelihood (NLL).\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:54:29 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15268","submitter":"Zhengwei Tao","authors":"Zhengwei Tao, Zhi Jin, Xiaoying Bai, Haiyan Zhao, Yanlin Feng, Jia Li,\n  Wenpeng Hu","title":"EvEval: A Comprehensive Evaluation of Event Semantics for Large Language\n  Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Events serve as fundamental units of occurrence within various contexts. The\nprocessing of event semantics in textual information forms the basis of\nnumerous natural language processing (NLP) applications. Recent studies have\nbegun leveraging large language models (LLMs) to address event semantic\nprocessing. However, the extent that LLMs can effectively tackle these\nchallenges remains uncertain. Furthermore, the lack of a comprehensive\nevaluation framework for event semantic processing poses a significant\nchallenge in evaluating these capabilities. In this paper, we propose an\noverarching framework for event semantic processing, encompassing\nunderstanding, reasoning, and prediction, along with their fine-grained\naspects. To comprehensively evaluate the event semantic processing abilities of\nmodels, we introduce a novel benchmark called EVEVAL. We collect 8 datasets\nthat cover all aspects of event semantic processing. Extensive experiments are\nconducted on EVEVAL, leading to several noteworthy findings based on the\nobtained results.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:55:40 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15269","submitter":"Abulhair Saparov","authors":"Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish\n  Joshi, Seyed Mehran Kazemi, Najoung Kim, He He","title":"Testing the General Deductive Reasoning Capacity of Large Language\n  Models Using OOD Examples","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  Given the intractably large size of the space of proofs, any model that is\ncapable of general deductive reasoning must generalize to proofs of greater\ncomplexity. Recent studies have shown that large language models (LLMs) possess\nsome abstract deductive reasoning ability given chain-of-thought prompts.\nHowever, they have primarily been tested on proofs using modus ponens or of a\nspecific size, and from the same distribution as the in-context examples. To\nmeasure the general deductive reasoning ability of LLMs, we test on a broad set\nof deduction rules and measure their ability to generalize to more complex\nproofs from simpler demonstrations from multiple angles: depth-, width-, and\ncompositional generalization. To facilitate systematic exploration, we\nconstruct a new synthetic and programmable reasoning dataset that enables\ncontrol over deduction rules and proof complexity. Our experiments on four LLMs\nof various sizes and training objectives show that they are able to generalize\nto longer and compositional proofs. However, they require explicit\ndemonstrations to produce hypothetical subproofs, specifically in proof by\ncases and proof by contradiction.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:55:51 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15270","submitter":"Siyang Song","authors":"Tong Xu, Micol Spitale, Hao Tang, Lu Liu, Hatice Gunes, Siyang Song","title":"Reversible Graph Neural Network-based Reaction Distribution Learning for\n  Multiple Appropriate Facial Reactions Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  Generating facial reactions in a human-human dyadic interaction is complex\nand highly dependent on the context since more than one facial reactions can be\nappropriate for the speaker's behaviour. This has challenged existing machine\nlearning (ML) methods, whose training strategies enforce models to reproduce a\nspecific (not multiple) facial reaction from each input speaker behaviour. This\npaper proposes the first multiple appropriate facial reaction generation\nframework that re-formulates the one-to-many mapping facial reaction generation\nproblem as a one-to-one mapping problem. This means that we approach this\nproblem by considering the generation of a distribution of the listener's\nappropriate facial reactions instead of multiple different appropriate facial\nreactions, i.e., 'many' appropriate facial reaction labels are summarised as\n'one' distribution label during training. Our model consists of a perceptual\nprocessor, a cognitive processor, and a motor processor. The motor processor is\nimplemented with a novel Reversible Multi-dimensional Edge Graph Neural Network\n(REGNN). This allows us to obtain a distribution of appropriate real facial\nreactions during the training process, enabling the cognitive processor to be\ntrained to predict the appropriate facial reaction distribution. At the\ninference stage, the REGNN decodes an appropriate facial reaction by using this\ndistribution as input. Experimental results demonstrate that our approach\noutperforms existing models in generating more appropriate, realistic, and\nsynchronized facial reactions. The improved performance is largely attributed\nto the proposed appropriate facial reaction distribution learning strategy and\nthe use of a REGNN. The code is available at\nhttps://github.com/TongXu-05/REGNN-Multiple-Appropriate-Facial-Reaction-Generation.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:56:26 GMT"},{"version":"v2","created":"Thu, 25 May 2023 17:41:49 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.15271","submitter":"Enrico Valdinoci","authors":"Serena Dipierro, Ovidiu Savin, Enrico Valdinoci","title":"Boundary continuity of nonlocal minimal surfaces in domains with\n  singularities and a problem posed by Borthagaray, Li, and Nochetto","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Differently from their classical counterpart, nonlocal minimal surfaces are\nknown to present boundary discontinuities, by sticking at the boundary of\nsmooth domains.\n  It has been observed numerically by J. P. Borthagaray, W. Li, and R. H.\nNochetto ``that stickiness is larger near the concave portions of the boundary\nthan near the convex ones, and that it is absent in the corners of the\nsquare'', leading to the conjecture ``that there is a relation between the\namount of stickiness on $\\partial\\Omega$ and the nonlocal mean curvature of\n$\\partial\\Omega$''.\n  In this paper, we give a positive answer to this conjecture, by showing that\nthe nonlocal minimal surfaces are continuous at convex corners of the domain\nboundary and discontinuous at concave corners.\n  More generally, we show that boundary continuity for nonlocal minimal\nsurfaces holds true at all points in which the domain is not better than\n$C^{1,s}$, with the singularity pointing outward, while, as pointed out by a\nconcrete example, discontinuities may occur at all point in which the domain\npossesses an interior touching set of class $C^{1,\\alpha}$ with $\\alpha>s$.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:57:03 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15272","submitter":"Jingfeng Yao","authors":"Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang","title":"ViTMatte: Boosting Image Matting with Pretrained Plain Vision\n  Transformers","comments":"codes: https://github.com/hustvl/ViTMatte","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Recently, plain vision Transformers (ViTs) have shown impressive performance\non various computer vision tasks, thanks to their strong modeling capacity and\nlarge-scale pretraining. However, they have not yet conquered the problem of\nimage matting. We hypothesize that image matting could also be boosted by ViTs\nand present a new efficient and robust ViT-based matting system, named\nViTMatte. Our method utilizes (i) a hybrid attention mechanism combined with a\nconvolution neck to help ViTs achieve an excellent performance-computation\ntrade-off in matting tasks. (ii) Additionally, we introduce the detail capture\nmodule, which just consists of simple lightweight convolutions to complement\nthe detailed information required by matting. To the best of our knowledge,\nViTMatte is the first work to unleash the potential of ViT on image matting\nwith concise adaptation. It inherits many superior properties from ViT to\nmatting, including various pretraining strategies, concise architecture design,\nand flexible inference strategies. We evaluate ViTMatte on Composition-1k and\nDistinctions-646, the most commonly used benchmark for image matting, our\nmethod achieves state-of-the-art performance and outperforms prior matting\nworks by a large margin.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:59:35 GMT"},{"version":"v2","created":"Wed, 31 May 2023 09:12:06 GMT"}],"update_date":"2023-06-01"}
{"id":"2305.15273","submitter":"Qihuang Zhong","authors":"Qihuang Zhong, Liang Ding, Juhua Liu, Xuebo Liu, Min Zhang, Bo Du and\n  Dacheng Tao","title":"Revisiting Token Dropping Strategy in Efficient BERT Pretraining","comments":"Accepted to ACL2023 Main Conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Token dropping is a recently-proposed strategy to speed up the pretraining of\nmasked language models, such as BERT, by skipping the computation of a subset\nof the input tokens at several middle layers. It can effectively reduce the\ntraining time without degrading much performance on downstream tasks. However,\nwe empirically find that token dropping is prone to a semantic loss problem and\nfalls short in handling semantic-intense tasks. Motivated by this, we propose a\nsimple yet effective semantic-consistent learning method (ScTD) to improve the\ntoken dropping. ScTD aims to encourage the model to learn how to preserve the\nsemantic information in the representation space. Extensive experiments on 12\ntasks show that, with the help of our ScTD, token dropping can achieve\nconsistent and significant performance gains across all task types and model\nsizes. More encouragingly, ScTD saves up to 57% of pretraining time and brings\nup to +1.56% average improvement over the vanilla token dropping.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:59:44 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15274","submitter":"Giulio Romani","authors":"Daniele Cassani, Zhisu Liu, Giulio Romani","title":"Nonlocal planar Schr\\\"odinger-Poisson systems in the fractional Sobolev\n  limiting case","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We study the nonlinear Schr\\\"odinger equation for the $s-$fractional\n$p-$Laplacian strongly coupled with the Poisson equation in dimension two and\nwith $p=\\frac2s$, which is the limiting case for the embedding of the\nfractional Sobolev space $W^{s,p}(\\mathbb{R}^2)$. We prove existence of\nsolutions by means of a variational approximating procedure for an auxiliary\nChoquard equation in which the uniformly approximated sign-changing logarithmic\nkernel competes with the exponential nonlinearity. Qualitative properties of\nsolutions such as symmetry and decay are also established by exploiting a\nsuitable moving planes technique.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:59:55 GMT"}],"update_date":"2023-05-25"}
{"id":"2305.15439","submitter":"Elisa Maggio","authors":"Elisa Maggio","title":"Tests of general relativity in the nonlinear regime: a parametrized\n  plunge-merger-ringdown waveform model","comments":"2 pages, 1 figure, contribution to the 2023 Gravitation session of\n  the 57th Rencontres de Moriond","journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc astro-ph.HE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Gravitational waves provide a unique opportunity to test gravity in the\ndynamical and nonlinear regime. We present a parametrized test of general\nrelativity (GR) that introduces generic deviations to the plunge, merger and\nringdown stages of binary-black-hole coalescences. The novel feature of the\nmodel is that it can capture signatures of beyond-GR physics in the\nplunge-merger phase. We use the model to provide constraints on the\nplunge-merger parameters from the analysis of GW150914. Alarmingly, we find\nthat GW200129 shows a strong violation of GR. We interpret this result as a\nfalse violation of GR either due to waveform systematics (mismodeling of spin\nprecession) or data-quality issues.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:53:40 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.15440","submitter":"Oliver Janssen","authors":"Thomas Hertog, Oliver Janssen and Joel Karlsson","title":"The KS-criterion constrains inflation in the no-boundary state","comments":"Dedicated to the memory of Jim Hartle","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-th gr-qc","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We show that the Kontsevich-Segal (KS) criterion, applied to the complex\nsaddles that specify the semiclassical no-boundary wave function, acts as a\nselection mechanism on inflationary scalar field potentials. In this context\nthe KS-criterion effectively bounds the tensor-to-scalar ratio of cosmic\nmicrowave background fluctuations to be less than 0.08, in line with current\nobservations. We trace the failure of complex saddles to meet the KS-criterion\nto the development of a tachyon in their spectrum of perturbations.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 20:03:34 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.15441","submitter":"Timothy Daley","authors":"M. Zaki Jawaid and Robin W. Yeo and Aayushma Gautam and T. Blair\n  Gainous and Daniel O. Hart and Timothy P. Daley","title":"Improving few-shot learning-based protein engineering with evolutionary\n  sampling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"q-bio.QM cs.LG","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Designing novel functional proteins remains a slow and expensive process due\nto a variety of protein engineering challenges; in particular, the number of\nprotein variants that can be experimentally tested in a given assay pales in\ncomparison to the vastness of the overall sequence space, resulting in low hit\nrates and expensive wet lab testing cycles. In this paper, we propose a\nfew-shot learning approach to novel protein design that aims to accelerate the\nexpensive wet lab testing cycle and is capable of leveraging a training dataset\nthat is both small and skewed ($\\approx 10^5$ datapoints, $< 1\\%$ positive\nhits). Our approach is composed of two parts: a semi-supervised transfer\nlearning approach to generate a discrete fitness landscape for a desired\nprotein function and a novel evolutionary Monte Carlo Markov Chain sampling\nalgorithm to more efficiently explore the fitness landscape. We demonstrate the\nperformance of our approach by experimentally screening predicted high fitness\ngene activators, resulting in a dramatically improved hit rate compared to\nexisting methods. Our method can be easily adapted to other protein engineering\nand design problems, particularly where the cost associated with obtaining\nlabeled data is significantly high. We have provided open source code for our\nmethod at https://\ngithub.com/SuperSecretBioTech/evolutionary_monte_carlo_search.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 23:07:53 GMT"},{"version":"v2","created":"Fri, 26 May 2023 00:53:37 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.15442","submitter":"Per Enflo","authors":"Per H. Enflo","title":"On the invariant subspace problem in Hilbert spaces","comments":"13 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.FA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper we show that every bounded linear operator T on a Hilbert space\nH has a closed non-trivial invariant subspace.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 05:21:46 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.15443","submitter":"Farhod Halimjonovich Haydarov","authors":"F.H.Haydarov","title":"Kolmogorov extension theorem for non-probability measures on Cayley\n  trees","comments":"Cayley tree, cylinder sets, Kolmogorov's extension theorem, spin\n  values, non-probability measures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR math.FA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper, we shall discuss the extendability of probability and\nnon-probability measures on Cayley trees to a $\\sigma$-additive measure on\nBorel fields which has a fundamental role in the theory of Gibbs measures.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 06:37:58 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.15444","submitter":"Dhananjay Ashok","authors":"Dhananjay Ashok, Zachary C. Lipton","title":"PromptNER: Prompting For Named Entity Recognition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In a surprising turn, Large Language Models (LLMs) together with a growing\narsenal of prompt-based heuristics now offer powerful off-the-shelf approaches\nproviding few-shot solutions to myriad classic NLP problems. However, despite\npromising early results, these LLM-based few-shot methods remain far from the\nstate of the art in Named Entity Recognition (NER), where prevailing methods\ninclude learning representations via end-to-end structural understanding and\nfine-tuning on standard labeled corpora. In this paper, we introduce PromptNER,\na new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to\nany new NER task PromptNER requires a set of entity definitions in addition to\nthe standard few-shot examples. Given a sentence, PromptNER prompts an LLM to\nproduce a list of potential entities along with corresponding explanations\njustifying their compatibility with the provided entity type definitions.\nRemarkably, PromptNER achieves state-of-the-art performance on few-shot NER,\nachieving an 11% (absolute) improvement in F1 score on the ConLL dataset, and a\n10% (absolute) improvement on the FewNERD dataset. PromptNER also moves the\nstate of the art on Cross Domain NER, outperforming all prior methods\n(including those not limited to the few-shot setting), setting a new mark on\nall 5 CrossNER target domains, with an average F1 gain of 9%, despite using\nless than 2% of the available data.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:38:24 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.15445","submitter":"Andreas Bott","authors":"Andreas Bott, Tim Janke, Florian Steinke","title":"Deep Learning-enabled MCMC for Probabilistic State Estimation in\n  District Heating Grids","comments":"The code for this paper is available under\n  https://github.com/EINS-TUDa/DNN_MCMC4DH","journal-ref":"Applied Energy 336 (2023): 120837","doi":"10.1016/j.apenergy.2023.120837","report-no":null,"categories":"cs.LG cs.NA cs.SY eess.SY math.NA stat.ME","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Flexible district heating grids form an important part of future, low-carbon\nenergy systems. We examine probabilistic state estimation in such grids, i.e.,\nwe aim to estimate the posterior probability distribution over all grid state\nvariables such as pressures, temperatures, and mass flows conditional on\nmeasurements of a subset of these states. Since the posterior state\ndistribution does not belong to a standard class of probability distributions,\nwe use Markov Chain Monte Carlo (MCMC) sampling in the space of network heat\nexchanges and evaluate the samples in the grid state space to estimate the\nposterior. Converting the heat exchange samples into grid states by solving the\nnon-linear grid equations makes this approach computationally burdensome.\nHowever, we propose to speed it up by employing a deep neural network that is\ntrained to approximate the solution of the exact but slow non-linear solver.\nThis novel approach is shown to deliver highly accurate posterior distributions\nboth for classic tree-shaped as well as meshed heating grids, at significantly\nreduced computational costs that are acceptable for online control. Our state\nestimation approach thus enables tightening the safety margins for temperature\nand pressure control and thereby a more efficient grid operation.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:47:01 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.15446","submitter":"Alexander Zakharov","authors":"Alexander F. Zakharov","title":"Constraints on black hole charges in M87* and Sgr A* with the EHT\n  observations","comments":"presented as a talk at ICRANet Meeting in May 2022","journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc physics.hist-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In May 2022 ICRANet organized the Workshop dedicated to the 80th anniversary\nof Professor Ruffini. This paper is based on the talk delivered at the meeting.\n  Professor Ruffini was well known for Soviet scientific community not only due\nto his publications in leading journals but also due Russian translations of\nhis books where he was an author or a contributor in collection of articles.\n  But only in 1988 I had an opportunity to watch and listen professor R.\nRuffini at the Conference dedicated to the century since the birthday of\nAlexander Alexandrovich Friedmann. This conference was organized in Leningrad\n(Soviet Union) in June during a short magic period when there are white nights\nthere. In June 2023 we celebrate the 135th anniversary of Friedmann's birth.\nFriedmann and his closed friend V. K. Frederics were the founders of Soviet\nschool of general relativity and George Gamow was one of the brilliant\nrepresentative of the school and he was the author of the hot Universe model\nwhich is the most popular now. In the USSR a development of general relativity\nand relativistic cosmology was not smooth and only in sixties of the last\ncentury these branches of science freed from the total control of\nrepresentatives of the ideology of Marxism -- Leninism. I also discussed a\nSoviet contribution in a discovery of cosmic microwave background radiation\ndone by T. Shmaonov in 1957 and reasons why his supervisors did not connect\nthese results with the hot Universe models discussed by G. Gamow. Author's\nresults about observational features of supemassive black holes (including the\nblack hole in our Galactic Center) are also briefly discussed, it was\nconsidered\n  an opportunity to evaluate a (tidal) charge of Reissner -- Nordstr\\\"om black\nhole from observational estimates of shadow size in the Galactic Center and\nM87* done by the EHT Collaboration based its observations in April 2017.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 09:42:38 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.15447","submitter":"Andrew Hyman","authors":"Andrew T. Hyman","title":"Alternative Formulation of Classical Electrodynamics that Eliminates the\n  Infinite Self-Energies of Point-Charges but Leaves their Motion and\n  Interaction Unaffected","comments":"13 pages, no figures, miscellaneous minor improvements in response to\n  comments","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.class-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The theory of point-particles in classical electrodynamics has a well-known\nproblem of infinite self-energy, and the same is true of quantum\nelectrodynamics. Instead of concluding that there is no such thing as a true\npoint-particle, it is shown here how to remove the infinities by supposing that\nthe electromagnetic field tensor has a symmetric part. This does not change the\nphysics, as the equation of motion and the antisymmetric part of the retarded\nfields appearing in the equation of motion are unaffected. The symmetric part\nof the field tensor is not observable and therefore it need not be\ngauge-invariant, whereas the antisymmetric part is observable, gauge-invariant,\nand satisfies both the Maxwell Equations and the field equations governing the\nwhole field tensor. This approach goes well beyond prior efforts at classical\nrenormalization, and also entails a new derivation of the Lorentz-Abraham-Dirac\n(LAD) equation of motion.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:29:12 GMT"},{"version":"v2","created":"Mon, 29 May 2023 12:05:49 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.15448","submitter":"Javier Molina Dr","authors":"Javier Molina-Vilaplana","title":"A post-Gaussian approach to dipole symmetries and interacting fractons","comments":"34 pages, 4 figures, 4 appendices","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-th cond-mat.str-el nucl-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We use a post-Gaussian variational approach to non-perturbatively study a\ngeneral class of interacting bosonic quantum field theories with generalized\ndipole symmetries and fractonic behaviour. We find that while a Gaussian\napproach allows to carry out a consistent renormalization group (RG) flow\nanalysis of these theories, this only grasps the interaction terms associated\nto the longitudinal motion of dipoles, which is consistent with previous\nanalysis using large $N$ techniques. Remarkably, our post-Gaussian proposal, by\nproviding a variational improved effective potential, is able to capture the\ntransverse part of the interaction between dipoles in such a way that a non\ntrivial RG flow for this term is obtained and analyzed. Our results suggest\nthat dipole symmetries that manifest due to the strong coupling of dipoles, may\nrobustly emerge at low energies from short distance models without that\nsymmetry.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:15:26 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.15449","submitter":"Qian Zhang","authors":"Jianqing Chen and Qian Zhang","title":"Existence of ground state solution of Nehari-Poho\\v{z}aev type for a\n  quasilinear Schr\\\"{o}dinger system","comments":"arXiv admin note: substantial text overlap with arXiv:2305.14911","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper is concerned with the following quasilinear Schr\\\"{o}dinger system\nin the entire space $\\mathbb R^{N}$($N\\geq3$): $$\\left\\{\\begin{align} &-\\Delta\nu+A(x)u-\\frac{1}{2}\\triangle(u^{2})u =\n\\frac{2\\alpha}{\\alpha+\\beta}|u|^{\\alpha-2}u|v|^{\\beta},\\\\ &-\\Delta\nv+Bv-\\frac{1}{2}\\triangle(v^{2})v=\\frac{2\\beta}{\\alpha+\\beta}|u|^{\\alpha}|v|^{\\beta-2}v.\\end{align}\\right.\n$$ By establishing a suitable constraint set and studying related minimization\nproblem, we prove the existence of ground state solution for $\\alpha,\\beta>1$,\n$2<\\alpha+\\beta<\\frac{4N}{N-2}$. Our results can be looked on as a\ngeneralization to results by Guo and Tang (Ground state solutions for\nquasilinear Schr\\\"{o}dinger systems, J. Math. Anal. Appl. 389 (2012) 322).\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:45:10 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.15450","submitter":"Hoang Nguyen","authors":"Hoang Ky Nguyen, Mustapha Azreg-A\\\"inou","title":"New insights into Weak Energy Condition and wormholes in Brans-Dicke\n  gravity","comments":"11 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc hep-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The formation of a wormhole typically involves a violation of the Weak Energy\nCondition (WEC), but the reverse is not necessarily true. In the context of\nBrans-Dicke gravity, the $\\textit{generalized}$ Campanelli-Lousto solution,\nwhich we shall unveil in this paper, demonstrates a WEC violation that\ncoincides with the appearance of $\\textit{unbounded}$ sheets of spacetime\nwithin the ``interior'' section. The emergence of a wormhole in the\n``exterior'' section is thus only an indirect consequence of the WEC violation.\nAdditionally, we use the generalized Campanelli-Lousto solution to construct a\nKruskal-Szekeres diagram, which exhibits a ``gulf'' sandwiched between the four\nquadrants in the diagram, a novel feature in Brans-Dicke gravity. Overall, our\nfindings shed new light onto a complex interplay between the WEC and wormholes\nin the Brans-Dicke theory.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 12:59:54 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.15451","submitter":"Atakan Coban","authors":"Atakan Coban","title":"Algodoo for Online Education: Impulse and Momentum Activities","comments":"7 pages, published article","journal-ref":"Physics Education, 2021","doi":"10.1088/1361-6552/abd1e9","report-no":null,"categories":"physics.ed-ph cs.CY","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  During the periods of sudden transition to online education, the opportunity\nto make applications that might attract students' attention to the course has\ndecreased even more. Although this deficiency was tried to be eliminated with\nvideos and simulations, it was not possible to ensure active participation of\nstudents in some cases. In this study, the Algodoo program, which can increase\nthe efficiency of the teaching environment by ensuring active participation of\nstudents in online lessons and the applications that can be done about Impulse\nand momentum are explained in detail. A total of 6 different applications were\ncarried out, 1 related to the subject of impulse, 1 related to the momentum, 2\nrelated to the relationship between impulse and momentum change, and 2 related\nto momentum conservation. At the same time, while developing these\napplications, the adjustments made on the simulation and the reasons are\nexplained in detail. In this way, both the introduction of the program and the\nsample application suggestion were presented. The values obtained as a result\nof the applications were calculated and compared both theoretically and on\nsimulation in different ways. As a result, it has been observed that the values\nhave internal consistency with each other and are also compatible with\ntheoretical calculations. Algodoo program, which allows many interactive\napplications and can be downloaded for free, is a program that can be used both\nin lecturing and evaluation processes in physics lessons while online education\nprocess.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 13:10:59 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.15452","submitter":"Eliad Tsfadia","authors":"Kobbi Nissim, Uri Stemmer, Eliad Tsfadia","title":"Adaptive Data Analysis in a Balanced Adversarial Model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CR cs.DS","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In adaptive data analysis, a mechanism gets $n$ i.i.d. samples from an\nunknown distribution $D$, and is required to provide accurate estimations to a\nsequence of adaptively chosen statistical queries with respect to $D$. Hardt\nand Ullman (FOCS 2014) and Steinke and Ullman (COLT 2015) showed that in\ngeneral, it is computationally hard to answer more than $\\Theta(n^2)$ adaptive\nqueries, assuming the existence of one-way functions.\n  However, these negative results strongly rely on an adversarial model that\nsignificantly advantages the adversarial analyst over the mechanism, as the\nanalyst, who chooses the adaptive queries, also chooses the underlying\ndistribution $D$. This imbalance raises questions with respect to the\napplicability of the obtained hardness results -- an analyst who has complete\nknowledge of the underlying distribution $D$ would have little need, if at all,\nto issue statistical queries to a mechanism which only holds a finite number of\nsamples from $D$.\n  We consider more restricted adversaries, called \\emph{balanced}, where each\nsuch adversary consists of two separated algorithms: The \\emph{sampler} who is\nthe entity that chooses the distribution and provides the samples to the\nmechanism, and the \\emph{analyst} who chooses the adaptive queries, but does\nnot have a prior knowledge of the underlying distribution. We improve the\nquality of previous lower bounds by revisiting them using an efficient\n\\emph{balanced} adversary, under standard public-key cryptography assumptions.\nWe show that these stronger hardness assumptions are unavoidable in the sense\nthat any computationally bounded \\emph{balanced} adversary that has the\nstructure of all known attacks, implies the existence of public-key\ncryptography.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:08:05 GMT"}],"update_date":"2023-05-26"}
{"id":"2305.16336","submitter":"Sergey Ershkov","authors":"Sergey Ershkov, Dmytro Leshchenko, Evgeniy Prosviryakov","title":"On the motion of satellite around the natural moons of planets using the\n  concept of ER3BP with variable eccentricity","comments":"24 pages, 10 Figures; Key words: elliptic restricted three-body\n  problem (ER3BP), three body problem, trapped motion, tidal phenomena,\n  satellite","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.gen-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In the current study, we explore stability of motion of satellite around the\nnatural moons of planets in Solar system using the novel concept of ER3BP with\nvariable eccentricity. This concept was introduced earlier when novel type of\nER3BP (Sun-planet-satellite) was investigated with variable spin state of\nsecondary planet correlated implicitly to the motion of satellite (in the\nsynodic co-rotating Cartesian coordinate system) for its trapped orbit near the\nsecondary planet (which is involved in kepler duet <Sun-planet>). But it is of\nreal interest to explore another kind of aforedescribed problem, ER3BP\n(planet-moon-satellite) with respect to investigation of motion of satellite m\naround the natural moon m_moon of planet in Solar system with variable\neccentricity of the moon in its motion around the planet. So, we consider here\ntwo primaries, M_planet and m_moon, the last is orbiting around their common\nbarycenter on quasi-elliptic orbit with slow-changing, not constant\neccentricity (on a large-time scale) due to tidal phenomena. Our aim is to\ninvestigate motion of small dot satellite around the natural moon of planet on\nquasi-stable elliptic orbit. Both novel theoretical and numerical findings (for\nvarious cases of trio <planet-moon-satellite>) are presented in the current\nresearch.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:38:06 GMT"},{"version":"v2","created":"Mon, 29 May 2023 06:13:00 GMT"},{"version":"v3","created":"Wed, 7 Jun 2023 10:55:03 GMT"}],"update_date":"2023-06-08"}
{"id":"2305.16337","submitter":"Maha Agro","authors":"Maha Tufail Agro, Hanan Aldarmaki","title":"Handling Realistic Label Noise in BERT Text Classification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Labels noise refers to errors in training labels caused by cheap data\nannotation methods, such as web scraping or crowd-sourcing, which can be\ndetrimental to the performance of supervised classifiers. Several methods have\nbeen proposed to counteract the effect of random label noise in supervised\nclassification, and some studies have shown that BERT is already robust against\nhigh rates of randomly injected label noise. However, real label noise is not\nrandom; rather, it is often correlated with input features or other\nannotator-specific factors. In this paper, we evaluate BERT in the presence of\ntwo types of realistic label noise: feature-dependent label noise, and\nsynthetic label noise from annotator disagreements. We show that the presence\nof these types of noise significantly degrades BERT classification performance.\nTo improve robustness, we evaluate different types of ensembles and\nnoise-cleaning methods and compare their effectiveness against label noise\nacross different datasets.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 18:30:31 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.16338","submitter":"Jikun Kang","authors":"Jikun Kang, Romain Laroche, Xindi Yuan, Adam Trischler, Xue Liu, Jie\n  Fu","title":"Think Before You Act: Decision Transformers with Internal Working Memory","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Large language model (LLM)-based decision-making agents have shown the\nability to generalize across multiple tasks. However, their performance relies\non massive data and compute. We argue that this inefficiency stems from the\nforgetting phenomenon, in which a model memorizes its behaviors in parameters\nthroughout training. As a result, training on a new task may deteriorate the\nmodel's performance on previous tasks. In contrast to LLMs' implicit memory\nmechanism, the human brain utilizes distributed memory storage, which helps\nmanage and organize multiple skills efficiently, mitigating the forgetting\nphenomenon. Thus inspired, we propose an internal working memory module to\nstore, blend, and retrieve information for different downstream tasks.\nEvaluation results show that the proposed method improves training efficiency\nand generalization in both Atari games and meta-world object manipulation\ntasks. Moreover, we demonstrate that memory fine-tuning further enhances the\nadaptability of the proposed architecture.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 01:20:22 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.16339","submitter":"Xiang Zhang","authors":"Xiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, Grzegorz Kondrak","title":"Don't Trust GPT When Your Question Is Not In English","comments":"10 pages, 6 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large Language Models (LLMs) have demonstrated exceptional natural language\nunderstanding abilities and have excelled in a variety of natural language\nprocessing (NLP)tasks in recent years. Despite the fact that most LLMs are\ntrained predominantly in English, multiple studies have demonstrated their\ncomparative performance in many other languages. However, fundamental questions\npersist regarding how LLMs acquire their multi-lingual abilities and how\nperformance varies across different languages. These inquiries are crucial for\nthe study of LLMs since users and researchers often come from diverse language\nbackgrounds, potentially influencing their utilization and interpretation of\nLLMs' results. In this work, we propose a systematic way of qualifying the\nperformance disparities of LLMs under multilingual settings. We investigate the\nphenomenon of across-language generalizations in LLMs, wherein insufficient\nmulti-lingual training data leads to advanced multi-lingual capabilities. To\naccomplish this, we employ a novel back-translation-based prompting method. The\nresults show that GPT exhibits highly translating-like behaviour in\nmultilingual settings.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 02:05:03 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.16340","submitter":"Yinghan Long","authors":"Yinghan Long, Sayeed Shafayet Chowdhury, Kaushik Roy","title":"Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Transformers have shown dominant performance across a range of domains\nincluding language and vision. However, their computational cost grows\nquadratically with the sequence length, making their usage prohibitive for\nresource-constrained applications. To counter this, our approach is to divide\nthe whole sequence into segments. The information across segments can then be\naggregated using neurons with recurrence leveraging their inherent memory. Such\nan approach leads to models with sequential processing capability at a lower\ncomputation/memory cost. To investigate this idea, first, we examine the\neffects of using local attention mechanism on the individual segments. Then we\npropose a segmented recurrent transformer (SRformer) that combines segmented\nattention with recurrent attention. It uses recurrent accumulate and fire (RAF)\nlayers to process information between consecutive segments. The loss caused by\nreducing the attention window length is compensated by updating the product of\nkeys and values with RAF neurons' inherent recurrence. The segmented attention\nand lightweight RAF gates ensure the efficiency of the proposed transformer. We\napply the proposed method to T5 and BART transformers. The modified models are\ntested on summarization datasets including CNN-dailymail and XSUM. Notably,\nusing segmented inputs of different sizes, the proposed model achieves 4-19%\nhigher ROUGE1 scores than the segmented transformer baseline. Compared to full\nattention, the proposed model largely reduces the complexity of cross attention\nand results in around 40% reduction in computation cost.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 03:47:22 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.16341","submitter":"Mohsen Pourvali","authors":"Mohsen Pourvali, Yao Meng, Chen Sheng, Yangzhou Du","title":"TaxoKnow: Taxonomy as Prior Knowledge in the Loss Function of\n  Multi-class Classification","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper, we investigate the effectiveness of integrating a hierarchical\ntaxonomy of labels as prior knowledge into the learning algorithm of a flat\nclassifier. We introduce two methods to integrate the hierarchical taxonomy as\nan explicit regularizer into the loss function of learning algorithms. By\nreasoning on a hierarchical taxonomy, a neural network alleviates its output\ndistributions over the classes, allowing conditioning on upper concepts for a\nminority class. We limit ourselves to the flat classification task and provide\nour experimental results on two industrial in-house datasets and two public\nbenchmarks, RCV1 and Amazon product reviews. Our obtained results show the\nsignificant effect of a taxonomy in increasing the performance of a learner in\nsemisupervised multi-class classification and the considerable results obtained\nin a fully supervised fashion.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:08:56 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.16342","submitter":"Zhihao Lai","authors":"Zhi-Hao Lai, Tian-Hao Zhang, Qi Liu, Xinyuan Qian, Li-Fang Wei,\n  Song-Lu Chen, Feng Chen, Xu-Cheng Yin","title":"InterFormer: Interactive Local and Global Features Fusion for Automatic\n  Speech Recognition","comments":"Accepted by Interspeech 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.SD eess.AS","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The local and global features are both essential for automatic speech\nrecognition (ASR). Many recent methods have verified that simply combining\nlocal and global features can further promote ASR performance. However, these\nmethods pay less attention to the interaction of local and global features, and\ntheir series architectures are rigid to reflect local and global relationships.\nTo address these issues, this paper proposes InterFormer for interactive local\nand global features fusion to learn a better representation for ASR.\nSpecifically, we combine the convolution block with the transformer block in a\nparallel design. Besides, we propose a bidirectional feature interaction module\n(BFIM) and a selective fusion module (SFM) to implement the interaction and\nfusion of local and global features, respectively. Extensive experiments on\npublic ASR datasets demonstrate the effectiveness of our proposed InterFormer\nand its superior performance over the other Transformer and Conformer models.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:43:44 GMT"},{"version":"v2","created":"Mon, 29 May 2023 11:28:27 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.16343","submitter":"Ciprian-Octavian Truic\\u{a}","authors":"Ciprian-Octavian Truic\\u{a} and Neculai-Ovidiu Istrate and\n  Elena-Simona Apostol","title":"A Distributed Automatic Domain-Specific Multi-Word Term Recognition\n  Architecture using Spark Ecosystem","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Automatic Term Recognition is used to extract domain-specific terms that\nbelong to a given domain. In order to be accurate, these corpus and\nlanguage-dependent methods require large volumes of textual data that need to\nbe processed to extract candidate terms that are afterward scored according to\na given metric. To improve text preprocessing and candidate terms extraction\nand scoring, we propose a distributed Spark-based architecture to automatically\nextract domain-specific terms. The main contributions are as follows: (1)\npropose a novel distributed automatic domain-specific multi-word term\nrecognition architecture built on top of the Spark ecosystem; (2) perform an\nin-depth analysis of our architecture in terms of accuracy and scalability; (3)\ndesign an easy-to-integrate Python implementation that enables the use of Big\nData processing in fields such as Computational Linguistics and Natural\nLanguage Processing. We prove empirically the feasibility of our architecture\nby performing experiments on two real-world datasets.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:05:59 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.16344","submitter":"Chongjian Yue","authors":"Chongjian Yue, Xinrun Xu, Xiaojun Ma, Lun Du, Hengyu Liu, Zhiming\n  Ding, Yanbing Jiang, Shi Han, Dongmei Zhang","title":"Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A\n  Comprehensive Framework and Dataset","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Large Language Models (LLMs) demonstrate exceptional performance in textual\nunderstanding and tabular reasoning tasks. However, their ability to comprehend\nand analyze hybrid text, containing textual and tabular data, remains\nunderexplored. In this research, we specialize in harnessing the potential of\nLLMs to comprehend critical information from financial reports, which are\nhybrid long-documents. We propose an Automated Financial Information Extraction\n(AFIE) framework that enhances LLMs' ability to comprehend and extract\ninformation from financial reports. To evaluate AFIE, we develop a Financial\nReports Numerical Extraction (FINE) dataset and conduct an extensive\nexperimental analysis. Our framework is effectively validated on GPT-3.5 and\nGPT-4, yielding average accuracy increases of 53.94% and 33.77%, respectively,\ncompared to a naive method. These results suggest that the AFIE framework\noffers accuracy for automated numerical extraction from complex, hybrid\ndocuments.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:35:58 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.16345","submitter":"A. D. Alhaidari","authors":"A. D. Alhaidari and A. Laradji","title":"A Novel Algebraic System in Quantum Field Theory","comments":null,"journal-ref":"AppliedMath 3 (2023) 461-467","doi":"10.3390/appliedmath3020024","report-no":null,"categories":"physics.gen-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  An algebraic system is introduced, which is very useful for doing scattering\ncalculations in quantum field theory. It is the set of all real numbers greater\nthan or equal to -m^2 with parity designation and a special rule for addition\nand subtraction, where m is the rest mass of the scattered particle.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:57:02 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.16346","submitter":"Farida Mohsen","authors":"Farida Mohsen, Hamada R. H. Al-Absi, Noha A.Yousri, Nady El Hajj, and\n  Zubair Shah","title":"Artificial Intelligence-Based Methods for Precision Medicine: Diabetes\n  Risk Prediction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The rising prevalence of type 2 diabetes mellitus (T2DM) necessitates the\ndevelopment of predictive models for T2DM risk assessment. Artificial\nintelligence (AI) models are being extensively used for this purpose, but a\ncomprehensive review of their advancements and challenges is lacking. This\nscoping review analyzes existing literature on AI-based models for T2DM risk\nprediction. Forty studies were included, mainly published in the past four\nyears. Traditional machine learning models were more prevalent than deep\nlearning models. Electronic health records were the most commonly used data\nsource. Unimodal AI models relying on EHR data were prominent, while only a few\nutilized multimodal models. Both unimodal and multimodal models showed\npromising performance, with the latter outperforming the former. Internal\nvalidation was common, while external validation was limited. Interpretability\nmethods were reported in half of the studies. Few studies reported novel\nbiomarkers, and open-source code availability was limited. This review provides\ninsights into the current state and limitations of AI-based T2DM risk\nprediction models and highlights challenges for their development and clinical\nimplementation.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:45:54 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.16347","submitter":"Melvin Wong","authors":"Melvin Wong, Yew-Soon Ong, Abhishek Gupta, Kavitesh K. Bali, Caishun\n  Chen","title":"Prompt Evolution for Generative AI: A Classifier-Guided Approach","comments":"To appear in Proceedings of the 2023 IEEE Conference on Artificial\n  Intelligence (CAI'23)","journal-ref":null,"doi":"10.1109/CAI54212.2023.00105","report-no":null,"categories":"cs.LG cs.AI cs.CV cs.NE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Synthesis of digital artifacts conditioned on user prompts has become an\nimportant paradigm facilitating an explosion of use cases with generative AI.\nHowever, such models often fail to connect the generated outputs and desired\ntarget concepts/preferences implied by the prompts. Current research addressing\nthis limitation has largely focused on enhancing the prompts before output\ngeneration or improving the model's performance up front. In contrast, this\npaper conceptualizes prompt evolution, imparting evolutionary selection\npressure and variation during the generative process to produce multiple\noutputs that satisfy the target concepts/preferences better. We propose a\nmulti-objective instantiation of this broader idea that uses a multi-label\nimage classifier-guided approach. The predicted labels from the classifiers\nserve as multiple objectives to optimize, with the aim of producing diversified\nimages that meet user preferences. A novelty of our evolutionary algorithm is\nthat the pre-trained generative model gives us implicit mutation operations,\nleveraging the model's stochastic generative capability to automate the\ncreation of Pareto-optimized images more faithful to user preferences.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 14:48:18 GMT"}],"update_date":"2023-06-06"}
{"id":"2305.16402","submitter":"Yanran Wang","authors":"Yanran Wang, Jonghyuk Baek, Yichun Tang, Jing Du, Mike Hillman, J. S.\n  Chen","title":"Support Vector Machine Guided Reproducing Kernel Particle Method for\n  Image-Based Modeling of Microstructures","comments":"58 pages, 51 figures, keywords: image-based modeling, support vector\n  machine, reproducing kernel particle method, weak discontinuity,\n  microstructures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CE cs.NA math.NA physics.app-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This work presents an approach for automating the discretization and\napproximation procedures in constructing digital representations of composites\nfrom Micro-CT images featuring intricate microstructures. The proposed method\nis guided by the Support Vector Machine (SVM) classification, offering an\neffective approach for discretizing microstructural images. An SVM soft margin\ntraining process is introduced as a classification of heterogeneous material\npoints, and image segmentation is accomplished by identifying support vectors\nthrough a local regularized optimization problem. In addition, an\nInterface-Modified Reproducing Kernel Particle Method (IM-RKPM) is proposed for\nappropriate approximations of weak discontinuities across material interfaces.\nThe proposed method modifies the smooth kernel functions with a regularized\nheavy-side function concerning the material interfaces to alleviate Gibb's\noscillations. This IM-RKPM is formulated without introducing duplicated degrees\nof freedom associated with the interface nodes commonly needed in the\nconventional treatments of weak discontinuities in the meshfree methods.\nMoreover, IM-RKPM can be implemented with various domain integration\ntechniques, such as Stabilized Conforming Nodal Integration (SCNI). The\nextension of the proposed method to 3-dimension is straightforward, and the\neffectiveness of the proposed method is validated through the image-based\nmodeling of polymer-ceramic composite microstructures.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 19:00:09 GMT"}],"update_date":"2023-05-29"}
{"id":"2305.17453","submitter":"N\\'ickolas De Aguiar Alves","authors":"N\\'ickolas de Aguiar Alves","title":"Nonperturbative Aspects of Quantum Field Theory in Curved Spacetime","comments":"MSc thesis defended at the Federal University of ABC (Brazil) on 28\n  April 2023. xxiv + 152 pages, 22 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc hep-th","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Quantum field theory in curved spacetime is perhaps the most reliable\nframework in which one can investigate quantum effects in the presence of\nstrong gravitational fields. Nevertheless, it is often studied by means of\nperturbative treatments. In this thesis, we aim at using the functional\nrenormalization group -- a nonperturbative realization of the renormalization\ngroup -- as a technique to describe nonperturbative quantum phenomena in curved\nspacetimes. The chosen system is an Unruh--DeWitt particle detector coupled to\na scalar quantum field. We discuss how to formulate such a system in terms of\nan action and how one can compute its renormalization group flow in the case of\nan inertial detector in flat spacetime, for simplicity. We learn, however, that\nthe results are divergent in the limit in which the detector's energy gap\nvanishes. Possible workarounds are discussed at the end.\n  This thesis also presents a review of quantum field theory in curved\nspacetimes by means of the algebraic approach, although it assumes no previous\nexperience with functional analysis. Hence, it fills a pedagogical gap in the\nliterature. Furthermore, we also review the functional renormalization group\nand derive the Wetterich equation assuming a general field content that might\ninclude both bosonic and fermionic fields. Such a derivation is also hardly\nfound in pedagogical introductions available in the high energy physics\nliterature.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 22:34:24 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.18219","submitter":"Michele Albano","authors":"Alexander Droob, Daniel Morratz, Frederik Langkilde Jakobsen, Jacob\n  Carstensen, Magnus Mathiesen, Rune Bohnstedt, Michele Albano, Sergio\n  Moreschini, Davide Taibi","title":"Workrs: Fault Tolerant Horizontal Computation Offloading","comments":"extended version of a paper accepted at IEEE Edge 2023","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DC cs.NI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The broad development and usage of edge devices has highlighted the\nimportance of creating resilient and computationally advanced environments.\nWhen working with edge devices these desiderata are usually achieved through\nreplication and offloading. This paper reports on the design and implementation\nof Workrs, a fault tolerant service that enables the offloading of jobs from\ndevices with limited computational power. We propose a solution that allows\nusers to upload jobs through a web service, which will be executed on edge\nnodes within the system. The solution is designed to be fault tolerant and\nscalable, with no single point of failure as well as the ability to accommodate\ngrowth, if the service is expanded. The use of Docker checkpointing on the\nworker machines ensures that jobs can be resumed in the event of a fault. We\nprovide a mathematical approach to optimize the number of checkpoints that are\ncreated along a computation, given that we can forecast the time needed to\nexecute a job. We present experiments that indicate in which scenarios\ncheckpointing benefits job execution. The results achieved are based on a\nworking prototype which shows clear benefits of using checkpointing and restore\nwhen the completion jobs' time rises compared with the forecast fault rate. The\ncode of Workrs is released as open source, and it is available at\n\\url{https://github.com/orgs/P7-workrs/repositories}. This paper is an extended\nversion of \\cite{edge2023paper}.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 08:08:51 GMT"}],"update_date":"2023-05-30"}
{"id":"2305.18242","submitter":"Haoran Liu","authors":"Runxi Liu, Peng Li, Haoran Liu","title":"Dataset for neutron and gamma-ray pulse shape discrimination","comments":"11 pages,10 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SP physics.ins-det","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The publicly accessible dataset includes neutron and gamma-ray pulse signals\nfor conducting pulse shape discrimination experiments. Several traditional and\nrecently proposed pulse shape discrimination algorithms are utilized to\nevaluate the performance of pulse shape discrimination under raw pulse signals\nand noise-enhanced datasets. These algorithms comprise zero-crossing (ZC),\ncharge comparison (CC), falling edge percentage slope (FEPS), frequency\ngradient analysis (FGA), pulse-coupled neural network (PCNN), ladder gradient\n(LG), and het-erogeneous quasi-continuous spiking cortical model (HQC-SCM). In\naddition to the pulse signals, this dataset includes the source code for all\nthe aforementioned pulse shape discrimination methods. Moreover, the dataset\nprovides the source code for schematic pulse shape discrimination performance\nevaluation and anti-noise performance evaluation. This feature enables\nresearchers to evaluate the performance of these methods using standard\nprocedures and assess their anti-noise ability under various noise conditions.\nIn conclusion, this dataset offers a comprehensive set of resources for\nconducting pulse shape discrimination experiments and evaluating the\nperformance of various pulse shape discrimination methods under different noise\nscenarios.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:12:45 GMT"},{"version":"v2","created":"Tue, 30 May 2023 11:16:27 GMT"}],"update_date":"2023-05-31"}
{"id":"2305.18329","submitter":"Pengfei Zhang","authors":"Pengfei Zhang","title":"Perturbative Page Curve Induced by External Impulse","comments":"19 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.stat-mech cond-mat.str-el hep-th quant-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this work, we extend the recent study of entropy dynamics induced by an\nexternal impulse in open quantum systems, where the entropy response follows\nthe Page curve. For small system-bath coupling $\\kappa$, we expect that the\nentropy first increases exponentially $\\kappa^2 e^{\\varkappa t}$ in the\nearly-time regime $t\\lesssim |\\log \\kappa|$ due to quantum many-body chaos, and\nthen decreases as $~e^{-\\lambda_0 t}$ with $\\lambda_0 \\propto \\kappa^2$ due to\nthe energy relaxation. These results are confirmed through explicit\ncalculations using two methods: 1) generalized Boltzmann equation for systems\nwith quasi-particles; 2) scramblon effective theory in the early-time regime\nand perturbation theory in the late-time regime for 0+1-d systems. We also\nprove that in the second stage, the entropy of the system is equal to the\ncoarse-grained entropy.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:08:12 GMT"}],"update_date":"2023-05-31"}
{"id":"2305.18330","submitter":"Areej Alsini","authors":"Areej Alsini, Du Q. Huynh and Amitava Datta","title":"#REVAL: a semantic evaluation framework for hashtag recommendation","comments":"18 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR cs.AI cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Automatic evaluation of hashtag recommendation models is a fundamental task\nin many online social network systems. In the traditional evaluation method,\nthe recommended hashtags from an algorithm are firstly compared with the ground\ntruth hashtags for exact correspondences. The number of exact matches is then\nused to calculate the hit rate, hit ratio, precision, recall, or F1-score. This\nway of evaluating hashtag similarities is inadequate as it ignores the semantic\ncorrelation between the recommended and ground truth hashtags. To tackle this\nproblem, we propose a novel semantic evaluation framework for hashtag\nrecommendation, called #REval. This framework includes an internal module\nreferred to as BERTag, which automatically learns the hashtag embeddings. We\ninvestigate on how the #REval framework performs under different word embedding\nmethods and different numbers of synonyms and hashtags in the recommendation\nusing our proposed #REval-hit-ratio measure. Our experiments of the proposed\nframework on three large datasets show that #REval gave more meaningful hashtag\nsynonyms for hashtag recommendation evaluation. Our analysis also highlights\nthe sensitivity of the framework to the word embedding technique, with #REval\nbased on BERTag more superior over #REval based on FastText and Word2Vec.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 07:10:56 GMT"}],"update_date":"2023-05-31"}
{"id":"2305.18331","submitter":"Valentina Belova","authors":"Valentina Belova, Hao Gao, Wissal Sghaier, Anastasios Manikas, Mehdi\n  Saedi, Hendrik H. Heenen, Costas Galiotis, Gilles Renaud, Oleg V. Konovalov,\n  Irene M. N. Groot, Karsten Reuter, Maciej Jankowski","title":"Kinetics of Graphene Growth on Liquid Copper by Chemical Vapor\n  Deposition","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci physics.chem-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We report a combined experimental and computational study of the kinetics of\ngraphene growth during chemical vapor deposition on a liquid copper catalyst.\nThe use of liquid metal catalysts offers bright perspectives for controllable\nlarge-scale, high-quality synthesis technologies of two-dimensional materials.\nWe carried out a series of growth experiments varying CH4-to-H2 pressure ratios\nand deposition temperature. By monitoring the graphene flake morphology in real\ntime during growth using in situ optical microscopy in radiation mode, we\nexplored the morphology and kinetics of the growth within a wide range of\nexperimental conditions. Following an analysis of the flakes' growth rates, we\nconclude that the growth mode was attachment-limited. The attachment and\ndetachment activation energies of carbon species are derived as 1.9 +- 0.3 eV\nand 2.0 +- 0.1 eV, respectively. We also conducted free-energy calculations by\na moment tensor potential trained to density functional theory data. Our\nsimulations propose that carbon dimers are most likely the active carbon\nspecies during growth, with attachment and detachment barriers of 1.71 +- 0.15\neV and 2.09 +- 0.02 eV, respectively, being in good agreement with the\nexperimental results.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:47:41 GMT"}],"update_date":"2023-05-31"}
{"id":"2305.19329","submitter":"Fanjie Kong","authors":"Fanjie Kong, Shuai Yuan, Weituo Hao, Ricardo Henao","title":"Mitigating Test-Time Bias for Fair Image Retrieval","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.IR cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We address the challenge of generating fair and unbiased image retrieval\nresults given neutral textual queries (with no explicit gender or race\nconnotations), while maintaining the utility (performance) of the underlying\nvision-language (VL) model. Previous methods aim to disentangle learned\nrepresentations of images and text queries from gender and racial\ncharacteristics. However, we show these are inadequate at alleviating bias for\nthe desired equal representation result, as there usually exists test-time bias\nin the target retrieval set. So motivated, we introduce a straightforward\ntechnique, Post-hoc Bias Mitigation (PBM), that post-processes the outputs from\nthe pre-trained vision-language model. We evaluate our algorithm on real-world\nimage search datasets, Occupation 1 and 2, as well as two large-scale\nimage-text datasets, MS-COCO and Flickr30k. Our approach achieves the lowest\nbias, compared with various existing bias-mitigation methods, in text-based\nimage retrieval result while maintaining satisfactory retrieval performance.\nThe source code is publicly available at\n\\url{https://anonymous.4open.science/r/Fair_Text_based_Image_Retrieval-D8B2}.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 21:31:16 GMT"}],"update_date":"2023-06-01"}
{"id":"2306.01756","submitter":"Jingtao Guo","authors":"Jingtao Guo, Ivan Wang-Hei Ho","title":"CSI-Based Efficient Self-Quarantine Monitoring System Using Branchy\n  Convolution Neural Network","comments":"6 pages, 7 figures, to be published in Proceedings of the 8th IEEE\n  World Forum on the Internet of Things","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Nowadays, Coronavirus disease (COVID-19) has become a global pandemic because\nof its fast spread in various countries. To build an anti-epidemic barrier,\nself-isolation is required for people who have been to any at-risk places or\nhave been in close contact with infected people. However, existing camera or\nwearable device-based monitoring systems may present privacy leakage risks or\ncause user inconvenience in some cases. In this paper, we propose a Wi-Fi-based\ndevice-free self-quarantine monitoring system. Specifically, we exploit channel\nstate information (CSI) derived from Wi-Fi signals as human activity features.\nWe collect CSI data in a simulated self-quarantine scenario and present\nBranchyGhostNet, a lightweight convolution neural network (CNN) with an early\nexit prediction branch, for the efficient joint task of room occupancy\ndetection (ROD) and human activity recognition (HAR). The early exiting branch\nis used for ROD, and the final one is used for HAR. Our experimental results\nindicate that the proposed model can achieve an average accuracy of 98.19% for\nclassifying five different human activities. They also confirm that after\nleveraging the early exit prediction mechanism, the inference latency for ROD\ncan be significantly reduced by 54.04% when compared with the final exiting\nbranch while guaranteeing the accuracy of ROD.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 04:02:49 GMT"}],"update_date":"2023-06-06"}
{"id":"2306.04629","submitter":"Szabolcs Cs\\'efalvay","authors":"Arturo Salmi, Szabolcs Cs\\'efalvay, James Imber","title":"Generative Adversarial Shaders for Real-Time Realism Enhancement","comments":"12 pages, 9 figures, 2 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GR cs.CV cs.LG","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Application of realism enhancement methods, particularly in real-time and\nresource-constrained settings, has been frustrated by the expense of existing\nmethods. These achieve high quality results only at the cost of long runtimes\nand high bandwidth, memory, and power requirements. We present an efficient\nalternative: a high-performance, generative shader-based approach that adapts\nmachine learning techniques to real-time applications, even in\nresource-constrained settings such as embedded and mobile GPUs. The proposed\nlearnable shader pipeline comprises differentiable functions that can be\ntrained in an end-to-end manner using an adversarial objective, allowing for\nfaithful reproduction of the appearance of a target image set without manual\ntuning. The shader pipeline is optimized for highly efficient execution on the\ntarget device, providing temporally stable, faster-than-real time results with\nquality competitive with many neural network-based methods.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 15:42:38 GMT"}],"update_date":"2023-06-08"}
{"id":"2306.04657","submitter":"Hua Cai","authors":"Hua Cai, Xuli Shen, Qing Xu, Weilin Shen, Xiaomei Wang, Weifeng Ge,\n  Xiaoqing Zheng and Xiangyang Xue","title":"Improving Empathetic Dialogue Generation by Dynamically Infusing\n  Commonsense Knowledge","comments":"Accepted by ACL 2023. arXiv admin note: substantial text overlap with\n  arXiv:2109.05739 by other authors","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In empathetic conversations, individuals express their empathy towards\nothers. Previous work has mainly focused on generating empathetic responses by\nutilizing the speaker's emotion. Besides, external commonsense knowledge has\nbeen applied to enhance the system's understandings of the speaker's situation.\nHowever, given an event, commonsense knowledge base contains various relations,\npotentially leading to confusion for the dialogue system. Consequently,\ninconsistencies arise among the emotion, generated response and speaker's\ncontextual information. To this end, we propose a novel approach for empathetic\nresponse generation, which incorporates an adaptive module for commonsense\nknowledge selection to ensure consistency between the generated empathetic\nresponses and the speaker's situation. This selected knowledge is used to\nrefine the commonsense cognition and empathy expression for generated\nresponses. Experimental results show that our approach significantly\noutperforms baseline models in both automatic and human evaluations, exhibiting\nthe generation of more coherent and empathetic responses. Moreover, case\nstudies highlight the interpretability of knowledge selection in the responses\nand the effectiveness of adaptive module in our model. Code:\nhttps://github.com/Hanscal/DCKS.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 10:25:12 GMT"}],"update_date":"2023-06-09"}
{"id":"2306.05375","submitter":"Anwar Said","authors":"Ammar Ahmed, Anwar Said, Mudassir Shabbir, Xenofon Koutsoukos","title":"Sequential Graph Neural Networks for Source Code Vulnerability\n  Identification","comments":"7 pages paper presented at HotSoS2023 conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Vulnerability identification constitutes a task of high importance for cyber\nsecurity. It is quite helpful for locating and fixing vulnerable functions in\nlarge applications. However, this task is rather challenging owing to the\nabsence of reliable and adequately managed datasets and learning models.\nExisting solutions typically rely on human expertise to annotate datasets or\nspecify features, which is prone to error. In addition, the learning models\nhave a high rate of false positives. To bridge this gap, in this paper, we\npresent a properly curated C/C++ source code vulnerability dataset, denoted as\nCVEFunctionGraphEmbeddings (CVEFGE), to aid in developing models. CVEFGE is\nautomatically crawled from the CVE database, which contains authentic and\npublicly disclosed source code vulnerabilities. We also propose a learning\nframework based on graph neural networks, denoted SEquential Graph Neural\nNetwork (SEGNN) for learning a large number of code semantic representations.\nSEGNN consists of a sequential learning module, graph convolution, pooling, and\nfully connected layers. Our evaluations on two datasets and four baseline\nmethods in a graph classification setting demonstrate state-of-the-art results.\n","versions":[{"version":"v1","created":"Tue, 23 May 2023 17:25:51 GMT"}],"update_date":"2023-06-09"}
{"id":"2306.05380","submitter":"Jiacheng Yao","authors":"Jiacheng Yao, Zhaohui Yang, Wei Xu, Mingzhe Chen, and Dusit Niyato","title":"GoMORE: Global Model Reuse for Resource-Constrained Wireless Federated\n  Learning","comments":"accepted by TEEE WCL","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT math.IT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Due to the dynamics of wireless environment and limited bandwidth, wireless\nfederated learning (FL) is challenged by frequent transmission errors and\nincomplete aggregation from devices. In order to overcome these challenges, we\npropose a global model reuse strategy (GoMORE) that reuses the outdated global\nmodel to replace the local model parameters once a transmission error occurs.\nWe analytically prove that the proposed GoMORE is strictly superior over the\nexisting strategy, especially at low signal-to-noise ratios (SNRs). In\naddition, based on the derived expression of weight divergence, we further\noptimize the number of participating devices in the model aggregation to\nmaximize the FL performance with limited communication resources. Numerical\nresults verify that the proposed GoMORE successfully approaches the performance\nupper bound by an ideal transmission. It also mitigates the negative impact of\nnon-independent and non-identically distributed (non-IID) data while achieving\nover 5 dB reduction in energy consumption.\n","versions":[{"version":"v1","created":"Wed, 24 May 2023 11:50:21 GMT"}],"update_date":"2023-06-09"}
